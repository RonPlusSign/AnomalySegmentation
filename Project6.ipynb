{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypo8OBRZ-1p3"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/RonPlusSign/AnomalySegmentation/blob/mahalanobis_2/Project6.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUjRrYEW8-uz"
      },
      "source": [
        "# **Anomaly Segmentation Project 6**\n",
        "##*Andrea Delli, Christian Dellisanti, Giorgia Modi*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4x3MajLNeXhX"
      },
      "source": [
        "##**Dataset Preparation**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/AnomalySegmentation"
      ],
      "metadata": {
        "id": "yorO9_xVX2bJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_tj8W3BeVPo",
        "outputId": "c068be58-6f94-4f33-c051-63b4e8618ff4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.5/1.4 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m473.6/473.6 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for visdom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for typing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=12YJq48XkCxQHjN3CmLc-zM5dThSak4Ta\n",
            "From (redirected): https://drive.google.com/uc?id=12YJq48XkCxQHjN3CmLc-zM5dThSak4Ta&confirm=t&uuid=1e754e94-936a-421c-a973-c34affcb27bd\n",
            "To: /content/Validation_Dataset.zip\n",
            "100% 329M/329M [00:03<00:00, 88.5MB/s]\n",
            "Cloning into 'AnomalySegmentation'...\n",
            "remote: Enumerating objects: 1287, done.\u001b[K\n",
            "remote: Counting objects: 100% (107/107), done.\u001b[K\n",
            "remote: Compressing objects: 100% (75/75), done.\u001b[K\n",
            "remote: Total 1287 (delta 67), reused 65 (delta 32), pack-reused 1180 (from 2)\u001b[K\n",
            "Receiving objects: 100% (1287/1287), 285.33 MiB | 34.79 MiB/s, done.\n",
            "Resolving deltas: 100% (874/874), done.\n",
            "Updating files: 100% (71/71), done.\n",
            "Already up to date.\n"
          ]
        }
      ],
      "source": [
        "!pip  install -q numpy matplotlib Pillow torchvision visdom ood_metrics icecream cityscapesscripts tqdm #triton\n",
        "\n",
        "import sys, os\n",
        "if not os.path.isfile('/content/Validation_Dataset.zip'):\n",
        "  !gdown 12YJq48XkCxQHjN3CmLc-zM5dThSak4Ta\n",
        "if not os.path.isdir('/content/Validation_Dataset'):\n",
        "  !unzip -q Validation_Dataset.zip\n",
        "if not os.path.isdir('/content/AnomalySegmentation'):\n",
        "  #!git clone https://github.com/shyam671/AnomalySegmentation_CourseProjectBaseCode.git\n",
        "  #token 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\n",
        "  !git clone -b mahalanobis_2 https://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@github.com/RonPlusSign/AnomalySegmentation.git\n",
        "!cd /content/AnomalySegmentation && git pull\n",
        "#!cd /content/AnomalySegmentation && git checkout main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAmA1igJhHhR"
      },
      "source": [
        "##**mIoU**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3w0jewGkphY",
        "outputId": "0cc3b8ac-9108-451b-adbb-190eca240bbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1J31rnVd33GBt-IYGYqC9mv73q7vc55pw\n",
            "From (redirected): https://drive.google.com/uc?id=1J31rnVd33GBt-IYGYqC9mv73q7vc55pw&confirm=t&uuid=9ee6947a-017f-4ac9-ab3d-67d059ba7e8d\n",
            "To: /content/cityscapes/gtFine_trainvaltest.zip\n",
            "100% 253M/253M [00:07<00:00, 35.4MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1m8Y3Zc6vG11Q9SxW7Be5EGXTDq4s4RlJ\n",
            "From (redirected): https://drive.google.com/uc?id=1m8Y3Zc6vG11Q9SxW7Be5EGXTDq4s4RlJ&confirm=t&uuid=3640803e-7b3a-42e7-be97-03db99f13d37\n",
            "To: /content/cityscapes/leftImg8bit_trainvaltest.zip\n",
            "100% 11.6G/11.6G [02:10<00:00, 88.7MB/s]\n",
            "Processing 5000 annotation files\n",
            "Progress: 100.0 % "
          ]
        }
      ],
      "source": [
        "import  os\n",
        "# s306027@studenti.polito.it\n",
        "# %mR+g$L\\~5U03O9)IZ-_\n",
        "# Per Eseguire tutto ci mette 23 min sia CPU che GPU\n",
        "createLabel = True\n",
        "fast_download = True\n",
        "super_fast_download = False\n",
        "if super_fast_download:\n",
        "  !gdown 1-fjLAk4_-GkixW1-GP_cYDBUmhnVbApL\n",
        "  !unzip -q cityscapes.zip\n",
        "  !mv  ./content/cityscapes /content/cityscapes\n",
        "  !rm -rf ./content\n",
        "else:\n",
        "  if not os.path.isdir('/content/cityscapes'):\n",
        "    !mkdir /content/cityscapes\n",
        "\n",
        "  if not os.path.isfile('/content/cityscapes/gtFine_trainvaltest.zip'):\n",
        "    if fast_download:\n",
        "      !gdown 1J31rnVd33GBt-IYGYqC9mv73q7vc55pw -O /content/cityscapes/\n",
        "    else:\n",
        "      !csDownload gtFine_trainvaltest.zip -d /content/cityscapes/\n",
        "\n",
        "  if not os.path.isdir('/content/cityscapes/gtFine'):\n",
        "    !unzip -q /content/cityscapes/gtFine_trainvaltest.zip -d /content/cityscapes/\n",
        "    createLabel = True\n",
        "    !rm /content/cityscapes/README\n",
        "    !rm /content/cityscapes/license.txt\n",
        "\n",
        "\n",
        "  if not os.path.isfile('/content/cityscapes/leftImg8bit_trainvaltest.zip'):\n",
        "    if fast_download:\n",
        "      #https://drive.google.com/file/d/1m8Y3Zc6vG11Q9SxW7Be5EGXTDq4s4RlJ/view?usp=sharing\n",
        "      !gdown 1m8Y3Zc6vG11Q9SxW7Be5EGXTDq4s4RlJ -O /content/cityscapes/\n",
        "    else:\n",
        "      !csDownload leftImg8bit_trainvaltest.zip -d /content/cityscapes/\n",
        "\n",
        "\n",
        "  if not os.path.isdir('/content/cityscapes/leftImg8bit'):\n",
        "    !unzip -q /content/cityscapes/leftImg8bit_trainvaltest.zip -d /content/cityscapes/\n",
        "    createLabel = True\n",
        "    !rm /content/cityscapes/README\n",
        "    !rm /content/cityscapes/license.txt\n",
        "\n",
        "  if createLabel:\n",
        "    os.environ['CITYSCAPES_DATASET'] = '/content/cityscapes/'\n",
        "    !csCreateTrainIdLabelImgs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAeuewkbhM3p",
        "outputId": "91464ffe-a919-48c1-cf3e-c3a79a673dcb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "498 val/munster/munster_000172_000019_leftImg8bit.png\n",
            "499 val/munster/munster_000173_000019_leftImg8bit.png\n",
            "-------------MSP-------------------\n",
            "---------------------------------------\n",
            "Took  80.77754092216492 seconds\n",
            "=======================================\n",
            "Per-Class IoU:\n",
            "\u001b[0m97.62\u001b[0m Road\n",
            "\u001b[0m81.37\u001b[0m sidewalk\n",
            "\u001b[0m90.77\u001b[0m building\n",
            "\u001b[0m49.43\u001b[0m wall\n",
            "\u001b[0m54.93\u001b[0m fence\n",
            "\u001b[0m60.81\u001b[0m pole\n",
            "\u001b[0m62.60\u001b[0m traffic light\n",
            "\u001b[0m72.32\u001b[0m traffic sign\n",
            "\u001b[0m91.35\u001b[0m vegetation\n",
            "\u001b[0m60.97\u001b[0m terrain\n",
            "\u001b[0m93.38\u001b[0m sky\n",
            "\u001b[0m76.11\u001b[0m person\n",
            "\u001b[0m53.45\u001b[0m rider\n",
            "\u001b[0m92.91\u001b[0m car\n",
            "\u001b[0m72.78\u001b[0m truck\n",
            "\u001b[0m78.87\u001b[0m bus\n",
            "\u001b[0m63.86\u001b[0m train\n",
            "\u001b[0m46.41\u001b[0m motorcycle\n",
            "\u001b[0m71.89\u001b[0m bicycle\n",
            "=======================================\n",
            "MEAN IoU:  \u001b[0m72.20\u001b[0m %\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "# ci mette 7 min con la GPU\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  !python -W ignore /content/AnomalySegmentation/eval/eval_iou.py --loadDir /content/AnomalySegmentation/trained_models/ --datadir /content/cityscapes/  | tail -n 28\n",
        "else:\n",
        "  !python -W ignore /content/AnomalySegmentation/eval/eval_iou.py  --loadDir  /content/AnomalySegmentation/trained_models/ --datadir /content/cityscapes/  --cpu | tail -n 28"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RZTrDS4Mysu"
      },
      "source": [
        "##**Anomaly Inference**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9afwM8zdM7_l",
        "outputId": "80d0aaaa-35f3-4bb8-a9fb-b46181f31866"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------\n",
            "\n",
            "Dataset: RoadAnomaly21 method: MSP\n",
            "AUPRC score: 29.100168300581203\n",
            "FPR@TPR95: 62.51075321069286\n",
            "\n",
            "Dataset: RoadAnomaly21 method: MaxLogit\n",
            "AUPRC score: 38.31957797222208\n",
            "FPR@TPR95: 59.3370558914899\n",
            "\n",
            "Dataset: RoadAnomaly21 method: MaxEntropy\n",
            "AUPRC score: 31.005102648344756\n",
            "FPR@TPR95: 62.593151130093226\n",
            "----------------------------\n",
            "\n",
            "Dataset: RoadObsticle21 method: MSP\n",
            "AUPRC score: 2.7116243119338366\n",
            "FPR@TPR95: 64.9739786894368\n",
            "\n",
            "Dataset: RoadObsticle21 method: MaxLogit\n",
            "AUPRC score: 4.626567617520253\n",
            "FPR@TPR95: 48.443439151949555\n",
            "\n",
            "Dataset: RoadObsticle21 method: MaxEntropy\n",
            "AUPRC score: 3.051560023478638\n",
            "FPR@TPR95: 65.59968252759046\n",
            "----------------------------\n",
            "\n",
            "Dataset: FS_LostFound_full method: MSP\n",
            "AUPRC score: 1.747872547607269\n",
            "FPR@TPR95: 50.76348570192957\n",
            "\n",
            "Dataset: FS_LostFound_full method: MaxLogit\n",
            "AUPRC score: 3.3014401015087245\n",
            "FPR@TPR95: 45.494876929038305\n",
            "\n",
            "Dataset: FS_LostFound_full method: MaxEntropy\n",
            "AUPRC score: 2.581709137723009\n",
            "FPR@TPR95: 50.368099783135676\n",
            "----------------------------\n",
            "\n",
            "Dataset: fs_static method: MSP\n",
            "AUPRC score: 7.4700433549050915\n",
            "FPR@TPR95: 41.82346831776172\n",
            "\n",
            "Dataset: fs_static method: MaxLogit\n",
            "AUPRC score: 9.498677970785756\n",
            "FPR@TPR95: 40.3000747567442\n",
            "\n",
            "Dataset: fs_static method: MaxEntropy\n",
            "AUPRC score: 8.82636607633996\n",
            "FPR@TPR95: 41.52332673090571\n",
            "----------------------------\n",
            "\n",
            "Dataset: RoadAnomaly method: MSP\n",
            "AUPRC score: 12.426265849563665\n",
            "FPR@TPR95: 82.49244029880458\n",
            "\n",
            "Dataset: RoadAnomaly method: MaxLogit\n",
            "AUPRC score: 15.581983301641019\n",
            "FPR@TPR95: 73.24766535735604\n",
            "\n",
            "Dataset: RoadAnomaly method: MaxEntropy\n",
            "AUPRC score: 12.678035094227063\n",
            "FPR@TPR95: 82.63192451735861\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "no_execute = False\n",
        "just_once = False\n",
        "\n",
        "for dataset_dir in ['RoadAnomaly21', 'RoadObsticle21', 'FS_LostFound_full', 'fs_static', 'RoadAnomaly']:\n",
        "  print(\"----------------------------\")\n",
        "  for method in [\"MSP\", \"MaxLogit\", \"MaxEntropy\"]:\n",
        "\n",
        "    if no_execute:\n",
        "      break\n",
        "\n",
        "    format_file = os.listdir(f'/content/Validation_Dataset/{dataset_dir}/images')[0].split(\".\")[1]\n",
        "    input =f'/content/Validation_Dataset/{dataset_dir}/images/\\*.{format_file}'\n",
        "    print(f\"\\nDataset: {dataset_dir} method: {method}\")\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "      !python -W ignore /content/AnomalySegmentation/eval/evalAnomaly.py --input {input} --method  {method}  | tail -n 2\n",
        "    else:\n",
        "      !python -W ignore /content/AnomalySegmentation/eval/evalAnomaly.py --input {input} --method {method}  --cpu | tail -n 2\n",
        "\n",
        "    if just_once:\n",
        "      no_execute = True\n",
        "      just_once = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhQWIx8rklfO"
      },
      "source": [
        "##**Temperature Scaling**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7DsE7oO1n9G"
      },
      "source": [
        "**Anomaly Inference with temperature**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Zu-dIEeqFLq3",
        "outputId": "47825080-3d12-479f-c2c2-51ca15ecab1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------\n",
            "\n",
            "Dataset: RoadAnomaly21 method : MSP Temperature: 0.5\n",
            "AUPRC score: 27.060833635879618\n",
            "FPR@TPR95: 62.730810427606734\n",
            "\n",
            "Dataset: RoadAnomaly21 method : MSP Temperature: 0.75\n",
            "AUPRC score: 28.156063054348103\n",
            "FPR@TPR95: 62.478737323984326\n",
            "\n",
            "Dataset: RoadAnomaly21 method : MSP Temperature: 1.1\n",
            "AUPRC score: 29.40955379121979\n",
            "FPR@TPR95: 62.58986549662704\n",
            "----------------------------\n",
            "\n",
            "Dataset: RoadObsticle21 method : MSP Temperature: 0.5\n",
            "AUPRC score: 2.4195519558429823\n",
            "FPR@TPR95: 63.22544524787239\n",
            "\n",
            "Dataset: RoadObsticle21 method : MSP Temperature: 0.75\n",
            "AUPRC score: 2.5668802249367677\n",
            "FPR@TPR95: 64.05285534718263\n",
            "\n",
            "Dataset: RoadObsticle21 method : MSP Temperature: 1.1\n",
            "AUPRC score: 2.7658075767433776\n",
            "FPR@TPR95: 65.52358106228223\n",
            "----------------------------\n",
            "\n",
            "Dataset: FS_LostFound_full method : MSP Temperature: 0.5\n",
            "AUPRC score: 1.2802500246431052\n",
            "FPR@TPR95: 66.73710676943257\n",
            "\n",
            "Dataset: FS_LostFound_full method : MSP Temperature: 0.75\n",
            "AUPRC score: 1.4927065686510383\n",
            "FPR@TPR95: 51.848262648332636\n",
            "\n",
            "Dataset: FS_LostFound_full method : MSP Temperature: 1.1\n",
            "AUPRC score: 1.8596703140506141\n",
            "FPR@TPR95: 50.38650128754133\n",
            "----------------------------\n",
            "\n",
            "Dataset: fs_static method : MSP Temperature: 0.5\n",
            "AUPRC score: 6.6011970066164665\n",
            "FPR@TPR95: 43.47565874225287\n",
            "\n",
            "Dataset: fs_static method : MSP Temperature: 0.75\n",
            "AUPRC score: 6.99079114995491\n",
            "FPR@TPR95: 42.49329123307483\n",
            "\n",
            "Dataset: fs_static method : MSP Temperature: 1.1\n",
            "AUPRC score: 7.686696846804934\n",
            "FPR@TPR95: 41.586844199987\n",
            "----------------------------\n",
            "\n",
            "Dataset: RoadAnomaly method : MSP Temperature: 0.5\n",
            "AUPRC score: 12.187681345765725\n",
            "FPR@TPR95: 82.02224728951396\n",
            "\n",
            "Dataset: RoadAnomaly method : MSP Temperature: 0.75\n",
            "AUPRC score: 12.319186617225913\n",
            "FPR@TPR95: 82.28451947325927\n",
            "\n",
            "Dataset: RoadAnomaly method : MSP Temperature: 1.1\n",
            "AUPRC score: 12.465779148190585\n",
            "FPR@TPR95: 82.62125003163526\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "no_execute = False\n",
        "just_once = False\n",
        "for dataset_dir in ['RoadAnomaly21', 'RoadObsticle21', 'FS_LostFound_full', 'fs_static', 'RoadAnomaly']:\n",
        "  print(\"----------------------------\")\n",
        "  for t in [0.5, 0.75, 1.1]:\n",
        "    if no_execute:\n",
        "        break\n",
        "\n",
        "    format_file = os.listdir(f'/content/Validation_Dataset/{dataset_dir}/images')[0].split(\".\")[1]\n",
        "    input =f'/content/Validation_Dataset/{dataset_dir}/images/\\*.{format_file}'\n",
        "    print(f\"\\nDataset: {dataset_dir}, method: MSP, Temperature: {t}\")\n",
        "    if torch.cuda.is_available():\n",
        "      !python -W ignore /content/AnomalySegmentation/eval/evalAnomaly.py --input {input} --method 'MSP' --temperature {t} | tail -n 2\n",
        "    else:\n",
        "      !python -W ignore /content/AnomalySegmentation/eval/evalAnomaly.py --input {input} --method 'MSP' --cpu --temperature {t} | tail -n 2\n",
        "\n",
        "    if just_once:\n",
        "      no_execute = True\n",
        "      just_once = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nY-OAYlIjGaG"
      },
      "source": [
        "## **Void Classifier**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine-Tuning"
      ],
      "metadata": {
        "id": "vNdTJZh4IP7G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####ERFNET"
      ],
      "metadata": {
        "id": "LYpg0U39MrDL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQhmfT3zjJcG",
        "outputId": "7a42935a-0014-4f51-acca-c8057510b8a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Import Model erfnet with weights erfnet_pretrained.pth to FineTune\n",
            "========== DECODER TRAINING ===========\n",
            "/content/cityscapes/leftImg8bit/train\n",
            "/content/cityscapes/leftImg8bit/val\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/AnomalySegmentation/train/main.py\", line 604, in <module>\n",
            "    main(parser.parse_args())\n",
            "  File \"/content/AnomalySegmentation/train/main.py\", line 566, in main\n",
            "    model = train(args, model, False)   #Train decoder\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py\", line 465, in _fn\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/content/AnomalySegmentation/train/main.py\", line 150, in train\n",
            "    assert os.path.exists(args.datadir), \"Error: datadir (dataset directory) could not be loaded\"\n",
            "  File \"/content/AnomalySegmentation/train/main.py\", line 157, in torch_dynamo_resume_in_train_at_150\n",
            "    dataset_train = cityscapes(args.datadir, co_transform, 'train')\n",
            "  File \"/content/AnomalySegmentation/train/main.py\", line 158, in torch_dynamo_resume_in_train_at_157\n",
            "    dataset_val = cityscapes(args.datadir, co_transform_val, 'val')\n",
            "  File \"/content/AnomalySegmentation/train/main.py\", line 164, in torch_dynamo_resume_in_train_at_158\n",
            "    loader = DataLoader(dataset_train, num_workers=args.num_workers, batch_size=args.batch_size, shuffle=True)\n",
            "  File \"/content/AnomalySegmentation/train/main.py\", line 165, in torch_dynamo_resume_in_train_at_164\n",
            "    loader_val = DataLoader(dataset_val, num_workers=args.num_workers, batch_size=args.batch_size, shuffle=False)\n",
            "  File \"/content/AnomalySegmentation/train/main.py\", line 169, in torch_dynamo_resume_in_train_at_165\n",
            "    criterion = CrossEntropyLoss2d(weight)\n",
            "  File \"/content/AnomalySegmentation/train/main.py\", line 78, in __init__\n",
            "    self.loss = torch.nn.NLLLoss2d(weight)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/typing_extensions.py\", line 2811, in __new__\n",
            "    warnings.warn(msg, category=category, stacklevel=stacklevel + 1)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/typing_extensions.py\", line 2813, in torch_dynamo_resume_in___new___at_2811\n",
            "    return original_new(cls, *args, **kwargs)\n",
            "TypeError: object.__new__() takes exactly one argument (the type to instantiate)\n"
          ]
        }
      ],
      "source": [
        "# Fine tune ERFNet (10 epochs)\n",
        "!cd /content/AnomalySegmentation/train; python -W ignore main.py --savedir erfnet_training1 --datadir /content/cityscapes --model erfnet --cuda --num-epochs=10 --epochs-save=1 --FineTune --decoder --state=/content/AnomalySegmentation/trained_models/erfnet_pretrained.pth --loadWeights=erfnet_pretrained.pth"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r save_erfnet_training1.zip /content/AnomalySegmentation/save/erfnet_training1"
      ],
      "metadata": {
        "id": "N1UvkfWcM1ob",
        "outputId": "ba6f76b8-ba99-4a75-bfde-63692cc68dcc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/AnomalySegmentation/save/erfnet_training1/ (stored 0%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training1/model_best.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training1/model-002.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training1/automated_log.txt (deflated 63%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training1/model-001.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training1/model-003.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training1/model-006.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training1/model-010.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training1/model.txt (deflated 92%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training1/model-005.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training1/model-007.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training1/best.txt (stored 0%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training1/model_best.pth.tar (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training1/opts.txt (deflated 40%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training1/model-008.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training1/model-009.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training1/checkpoint.pth.tar (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training1/erfnet.py (deflated 78%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training1/model-004.pth (deflated 10%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine tune ERFNet (20 epochs)\n",
        "!cd /content/AnomalySegmentation/train; python -W ignore main.py --savedir erfnet_training2 --datadir /content/cityscapes --model erfnet --cuda --num-epochs=20 --epochs-save=1 --FineTune --decoder --state=/content/AnomalySegmentation/trained_models/erfnet_pretrained.pth --loadWeights=erfnet_pretrained.pth\n",
        "!zip -r save_erfnet_training2.zip /content/AnomalySegmentation/save/erfnet_training2"
      ],
      "metadata": {
        "id": "wSJMvmnGLtsU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d51917f5-33dc-4e45-fd58-6f49c9fdbbe2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Import Model erfnet with weights erfnet_pretrained.pth to FineTune\n",
            "========== DECODER TRAINING ===========\n",
            "/content/cityscapes/leftImg8bit/train\n",
            "/content/cityscapes/leftImg8bit/val\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/AnomalySegmentation/train/main.py\", line 604, in <module>\n",
            "    main(parser.parse_args())\n",
            "  File \"/content/AnomalySegmentation/train/main.py\", line 566, in main\n",
            "    model = train(args, model, False)   #Train decoder\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py\", line 465, in _fn\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/content/AnomalySegmentation/train/main.py\", line 150, in train\n",
            "    assert os.path.exists(args.datadir), \"Error: datadir (dataset directory) could not be loaded\"\n",
            "  File \"/content/AnomalySegmentation/train/main.py\", line 157, in torch_dynamo_resume_in_train_at_150\n",
            "    dataset_train = cityscapes(args.datadir, co_transform, 'train')\n",
            "  File \"/content/AnomalySegmentation/train/main.py\", line 158, in torch_dynamo_resume_in_train_at_157\n",
            "    dataset_val = cityscapes(args.datadir, co_transform_val, 'val')\n",
            "  File \"/content/AnomalySegmentation/train/main.py\", line 164, in torch_dynamo_resume_in_train_at_158\n",
            "    loader = DataLoader(dataset_train, num_workers=args.num_workers, batch_size=args.batch_size, shuffle=True)\n",
            "  File \"/content/AnomalySegmentation/train/main.py\", line 165, in torch_dynamo_resume_in_train_at_164\n",
            "    loader_val = DataLoader(dataset_val, num_workers=args.num_workers, batch_size=args.batch_size, shuffle=False)\n",
            "  File \"/content/AnomalySegmentation/train/main.py\", line 169, in torch_dynamo_resume_in_train_at_165\n",
            "    criterion = CrossEntropyLoss2d(weight)\n",
            "  File \"/content/AnomalySegmentation/train/main.py\", line 78, in __init__\n",
            "    self.loss = torch.nn.NLLLoss2d(weight)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/typing_extensions.py\", line 2811, in __new__\n",
            "    warnings.warn(msg, category=category, stacklevel=stacklevel + 1)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/typing_extensions.py\", line 2813, in torch_dynamo_resume_in___new___at_2811\n",
            "    return original_new(cls, *args, **kwargs)\n",
            "TypeError: object.__new__() takes exactly one argument (the type to instantiate)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training2/ (stored 0%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training2/opts.txt (deflated 40%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training2/erfnet.py (deflated 78%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####ENET"
      ],
      "metadata": {
        "id": "G3aC1UuCMz9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FineTune ENet (20 epochs)\n",
        "!cd /content/AnomalySegmentation/train; python -W ignore main.py --savedir enet_training1 --datadir /content/cityscapes --model enet  --num-epochs=20 --epochs-save=1 --cuda --FineTune --loadWeights=enet_pretrained\n",
        "!zip -r save_enet_training1.zip /content/AnomalySegmentation/save/enet_training1"
      ],
      "metadata": {
        "id": "9_1Uo-kgXFGK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cda0a369-7b7c-42be-9211-0f1ee2ad9036"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "odict_keys(['initial_block.main_branch.weight', 'initial_block.batch_norm.weight', 'initial_block.batch_norm.bias', 'initial_block.batch_norm.running_mean', 'initial_block.batch_norm.running_var', 'initial_block.batch_norm.num_batches_tracked', 'initial_block.out_activation.weight', 'downsample1_0.ext_conv1.0.weight', 'downsample1_0.ext_conv1.1.weight', 'downsample1_0.ext_conv1.1.bias', 'downsample1_0.ext_conv1.1.running_mean', 'downsample1_0.ext_conv1.1.running_var', 'downsample1_0.ext_conv1.1.num_batches_tracked', 'downsample1_0.ext_conv1.2.weight', 'downsample1_0.ext_conv2.0.weight', 'downsample1_0.ext_conv2.1.weight', 'downsample1_0.ext_conv2.1.bias', 'downsample1_0.ext_conv2.1.running_mean', 'downsample1_0.ext_conv2.1.running_var', 'downsample1_0.ext_conv2.1.num_batches_tracked', 'downsample1_0.ext_conv2.2.weight', 'downsample1_0.ext_conv3.0.weight', 'downsample1_0.ext_conv3.1.weight', 'downsample1_0.ext_conv3.1.bias', 'downsample1_0.ext_conv3.1.running_mean', 'downsample1_0.ext_conv3.1.running_var', 'downsample1_0.ext_conv3.1.num_batches_tracked', 'downsample1_0.ext_conv3.2.weight', 'downsample1_0.out_activation.weight', 'regular1_1.ext_conv1.0.weight', 'regular1_1.ext_conv1.1.weight', 'regular1_1.ext_conv1.1.bias', 'regular1_1.ext_conv1.1.running_mean', 'regular1_1.ext_conv1.1.running_var', 'regular1_1.ext_conv1.1.num_batches_tracked', 'regular1_1.ext_conv1.2.weight', 'regular1_1.ext_conv2.0.weight', 'regular1_1.ext_conv2.1.weight', 'regular1_1.ext_conv2.1.bias', 'regular1_1.ext_conv2.1.running_mean', 'regular1_1.ext_conv2.1.running_var', 'regular1_1.ext_conv2.1.num_batches_tracked', 'regular1_1.ext_conv2.2.weight', 'regular1_1.ext_conv3.0.weight', 'regular1_1.ext_conv3.1.weight', 'regular1_1.ext_conv3.1.bias', 'regular1_1.ext_conv3.1.running_mean', 'regular1_1.ext_conv3.1.running_var', 'regular1_1.ext_conv3.1.num_batches_tracked', 'regular1_1.ext_conv3.2.weight', 'regular1_1.out_activation.weight', 'regular1_2.ext_conv1.0.weight', 'regular1_2.ext_conv1.1.weight', 'regular1_2.ext_conv1.1.bias', 'regular1_2.ext_conv1.1.running_mean', 'regular1_2.ext_conv1.1.running_var', 'regular1_2.ext_conv1.1.num_batches_tracked', 'regular1_2.ext_conv1.2.weight', 'regular1_2.ext_conv2.0.weight', 'regular1_2.ext_conv2.1.weight', 'regular1_2.ext_conv2.1.bias', 'regular1_2.ext_conv2.1.running_mean', 'regular1_2.ext_conv2.1.running_var', 'regular1_2.ext_conv2.1.num_batches_tracked', 'regular1_2.ext_conv2.2.weight', 'regular1_2.ext_conv3.0.weight', 'regular1_2.ext_conv3.1.weight', 'regular1_2.ext_conv3.1.bias', 'regular1_2.ext_conv3.1.running_mean', 'regular1_2.ext_conv3.1.running_var', 'regular1_2.ext_conv3.1.num_batches_tracked', 'regular1_2.ext_conv3.2.weight', 'regular1_2.out_activation.weight', 'regular1_3.ext_conv1.0.weight', 'regular1_3.ext_conv1.1.weight', 'regular1_3.ext_conv1.1.bias', 'regular1_3.ext_conv1.1.running_mean', 'regular1_3.ext_conv1.1.running_var', 'regular1_3.ext_conv1.1.num_batches_tracked', 'regular1_3.ext_conv1.2.weight', 'regular1_3.ext_conv2.0.weight', 'regular1_3.ext_conv2.1.weight', 'regular1_3.ext_conv2.1.bias', 'regular1_3.ext_conv2.1.running_mean', 'regular1_3.ext_conv2.1.running_var', 'regular1_3.ext_conv2.1.num_batches_tracked', 'regular1_3.ext_conv2.2.weight', 'regular1_3.ext_conv3.0.weight', 'regular1_3.ext_conv3.1.weight', 'regular1_3.ext_conv3.1.bias', 'regular1_3.ext_conv3.1.running_mean', 'regular1_3.ext_conv3.1.running_var', 'regular1_3.ext_conv3.1.num_batches_tracked', 'regular1_3.ext_conv3.2.weight', 'regular1_3.out_activation.weight', 'regular1_4.ext_conv1.0.weight', 'regular1_4.ext_conv1.1.weight', 'regular1_4.ext_conv1.1.bias', 'regular1_4.ext_conv1.1.running_mean', 'regular1_4.ext_conv1.1.running_var', 'regular1_4.ext_conv1.1.num_batches_tracked', 'regular1_4.ext_conv1.2.weight', 'regular1_4.ext_conv2.0.weight', 'regular1_4.ext_conv2.1.weight', 'regular1_4.ext_conv2.1.bias', 'regular1_4.ext_conv2.1.running_mean', 'regular1_4.ext_conv2.1.running_var', 'regular1_4.ext_conv2.1.num_batches_tracked', 'regular1_4.ext_conv2.2.weight', 'regular1_4.ext_conv3.0.weight', 'regular1_4.ext_conv3.1.weight', 'regular1_4.ext_conv3.1.bias', 'regular1_4.ext_conv3.1.running_mean', 'regular1_4.ext_conv3.1.running_var', 'regular1_4.ext_conv3.1.num_batches_tracked', 'regular1_4.ext_conv3.2.weight', 'regular1_4.out_activation.weight', 'downsample2_0.ext_conv1.0.weight', 'downsample2_0.ext_conv1.1.weight', 'downsample2_0.ext_conv1.1.bias', 'downsample2_0.ext_conv1.1.running_mean', 'downsample2_0.ext_conv1.1.running_var', 'downsample2_0.ext_conv1.1.num_batches_tracked', 'downsample2_0.ext_conv1.2.weight', 'downsample2_0.ext_conv2.0.weight', 'downsample2_0.ext_conv2.1.weight', 'downsample2_0.ext_conv2.1.bias', 'downsample2_0.ext_conv2.1.running_mean', 'downsample2_0.ext_conv2.1.running_var', 'downsample2_0.ext_conv2.1.num_batches_tracked', 'downsample2_0.ext_conv2.2.weight', 'downsample2_0.ext_conv3.0.weight', 'downsample2_0.ext_conv3.1.weight', 'downsample2_0.ext_conv3.1.bias', 'downsample2_0.ext_conv3.1.running_mean', 'downsample2_0.ext_conv3.1.running_var', 'downsample2_0.ext_conv3.1.num_batches_tracked', 'downsample2_0.ext_conv3.2.weight', 'downsample2_0.out_activation.weight', 'regular2_1.ext_conv1.0.weight', 'regular2_1.ext_conv1.1.weight', 'regular2_1.ext_conv1.1.bias', 'regular2_1.ext_conv1.1.running_mean', 'regular2_1.ext_conv1.1.running_var', 'regular2_1.ext_conv1.1.num_batches_tracked', 'regular2_1.ext_conv1.2.weight', 'regular2_1.ext_conv2.0.weight', 'regular2_1.ext_conv2.1.weight', 'regular2_1.ext_conv2.1.bias', 'regular2_1.ext_conv2.1.running_mean', 'regular2_1.ext_conv2.1.running_var', 'regular2_1.ext_conv2.1.num_batches_tracked', 'regular2_1.ext_conv2.2.weight', 'regular2_1.ext_conv3.0.weight', 'regular2_1.ext_conv3.1.weight', 'regular2_1.ext_conv3.1.bias', 'regular2_1.ext_conv3.1.running_mean', 'regular2_1.ext_conv3.1.running_var', 'regular2_1.ext_conv3.1.num_batches_tracked', 'regular2_1.ext_conv3.2.weight', 'regular2_1.out_activation.weight', 'dilated2_2.ext_conv1.0.weight', 'dilated2_2.ext_conv1.1.weight', 'dilated2_2.ext_conv1.1.bias', 'dilated2_2.ext_conv1.1.running_mean', 'dilated2_2.ext_conv1.1.running_var', 'dilated2_2.ext_conv1.1.num_batches_tracked', 'dilated2_2.ext_conv1.2.weight', 'dilated2_2.ext_conv2.0.weight', 'dilated2_2.ext_conv2.1.weight', 'dilated2_2.ext_conv2.1.bias', 'dilated2_2.ext_conv2.1.running_mean', 'dilated2_2.ext_conv2.1.running_var', 'dilated2_2.ext_conv2.1.num_batches_tracked', 'dilated2_2.ext_conv2.2.weight', 'dilated2_2.ext_conv3.0.weight', 'dilated2_2.ext_conv3.1.weight', 'dilated2_2.ext_conv3.1.bias', 'dilated2_2.ext_conv3.1.running_mean', 'dilated2_2.ext_conv3.1.running_var', 'dilated2_2.ext_conv3.1.num_batches_tracked', 'dilated2_2.ext_conv3.2.weight', 'dilated2_2.out_activation.weight', 'asymmetric2_3.ext_conv1.0.weight', 'asymmetric2_3.ext_conv1.1.weight', 'asymmetric2_3.ext_conv1.1.bias', 'asymmetric2_3.ext_conv1.1.running_mean', 'asymmetric2_3.ext_conv1.1.running_var', 'asymmetric2_3.ext_conv1.1.num_batches_tracked', 'asymmetric2_3.ext_conv1.2.weight', 'asymmetric2_3.ext_conv2.0.weight', 'asymmetric2_3.ext_conv2.1.weight', 'asymmetric2_3.ext_conv2.1.bias', 'asymmetric2_3.ext_conv2.1.running_mean', 'asymmetric2_3.ext_conv2.1.running_var', 'asymmetric2_3.ext_conv2.1.num_batches_tracked', 'asymmetric2_3.ext_conv2.2.weight', 'asymmetric2_3.ext_conv2.3.weight', 'asymmetric2_3.ext_conv2.4.weight', 'asymmetric2_3.ext_conv2.4.bias', 'asymmetric2_3.ext_conv2.4.running_mean', 'asymmetric2_3.ext_conv2.4.running_var', 'asymmetric2_3.ext_conv2.4.num_batches_tracked', 'asymmetric2_3.ext_conv2.5.weight', 'asymmetric2_3.ext_conv3.0.weight', 'asymmetric2_3.ext_conv3.1.weight', 'asymmetric2_3.ext_conv3.1.bias', 'asymmetric2_3.ext_conv3.1.running_mean', 'asymmetric2_3.ext_conv3.1.running_var', 'asymmetric2_3.ext_conv3.1.num_batches_tracked', 'asymmetric2_3.ext_conv3.2.weight', 'asymmetric2_3.out_activation.weight', 'dilated2_4.ext_conv1.0.weight', 'dilated2_4.ext_conv1.1.weight', 'dilated2_4.ext_conv1.1.bias', 'dilated2_4.ext_conv1.1.running_mean', 'dilated2_4.ext_conv1.1.running_var', 'dilated2_4.ext_conv1.1.num_batches_tracked', 'dilated2_4.ext_conv1.2.weight', 'dilated2_4.ext_conv2.0.weight', 'dilated2_4.ext_conv2.1.weight', 'dilated2_4.ext_conv2.1.bias', 'dilated2_4.ext_conv2.1.running_mean', 'dilated2_4.ext_conv2.1.running_var', 'dilated2_4.ext_conv2.1.num_batches_tracked', 'dilated2_4.ext_conv2.2.weight', 'dilated2_4.ext_conv3.0.weight', 'dilated2_4.ext_conv3.1.weight', 'dilated2_4.ext_conv3.1.bias', 'dilated2_4.ext_conv3.1.running_mean', 'dilated2_4.ext_conv3.1.running_var', 'dilated2_4.ext_conv3.1.num_batches_tracked', 'dilated2_4.ext_conv3.2.weight', 'dilated2_4.out_activation.weight', 'regular2_5.ext_conv1.0.weight', 'regular2_5.ext_conv1.1.weight', 'regular2_5.ext_conv1.1.bias', 'regular2_5.ext_conv1.1.running_mean', 'regular2_5.ext_conv1.1.running_var', 'regular2_5.ext_conv1.1.num_batches_tracked', 'regular2_5.ext_conv1.2.weight', 'regular2_5.ext_conv2.0.weight', 'regular2_5.ext_conv2.1.weight', 'regular2_5.ext_conv2.1.bias', 'regular2_5.ext_conv2.1.running_mean', 'regular2_5.ext_conv2.1.running_var', 'regular2_5.ext_conv2.1.num_batches_tracked', 'regular2_5.ext_conv2.2.weight', 'regular2_5.ext_conv3.0.weight', 'regular2_5.ext_conv3.1.weight', 'regular2_5.ext_conv3.1.bias', 'regular2_5.ext_conv3.1.running_mean', 'regular2_5.ext_conv3.1.running_var', 'regular2_5.ext_conv3.1.num_batches_tracked', 'regular2_5.ext_conv3.2.weight', 'regular2_5.out_activation.weight', 'dilated2_6.ext_conv1.0.weight', 'dilated2_6.ext_conv1.1.weight', 'dilated2_6.ext_conv1.1.bias', 'dilated2_6.ext_conv1.1.running_mean', 'dilated2_6.ext_conv1.1.running_var', 'dilated2_6.ext_conv1.1.num_batches_tracked', 'dilated2_6.ext_conv1.2.weight', 'dilated2_6.ext_conv2.0.weight', 'dilated2_6.ext_conv2.1.weight', 'dilated2_6.ext_conv2.1.bias', 'dilated2_6.ext_conv2.1.running_mean', 'dilated2_6.ext_conv2.1.running_var', 'dilated2_6.ext_conv2.1.num_batches_tracked', 'dilated2_6.ext_conv2.2.weight', 'dilated2_6.ext_conv3.0.weight', 'dilated2_6.ext_conv3.1.weight', 'dilated2_6.ext_conv3.1.bias', 'dilated2_6.ext_conv3.1.running_mean', 'dilated2_6.ext_conv3.1.running_var', 'dilated2_6.ext_conv3.1.num_batches_tracked', 'dilated2_6.ext_conv3.2.weight', 'dilated2_6.out_activation.weight', 'asymmetric2_7.ext_conv1.0.weight', 'asymmetric2_7.ext_conv1.1.weight', 'asymmetric2_7.ext_conv1.1.bias', 'asymmetric2_7.ext_conv1.1.running_mean', 'asymmetric2_7.ext_conv1.1.running_var', 'asymmetric2_7.ext_conv1.1.num_batches_tracked', 'asymmetric2_7.ext_conv1.2.weight', 'asymmetric2_7.ext_conv2.0.weight', 'asymmetric2_7.ext_conv2.1.weight', 'asymmetric2_7.ext_conv2.1.bias', 'asymmetric2_7.ext_conv2.1.running_mean', 'asymmetric2_7.ext_conv2.1.running_var', 'asymmetric2_7.ext_conv2.1.num_batches_tracked', 'asymmetric2_7.ext_conv2.2.weight', 'asymmetric2_7.ext_conv2.3.weight', 'asymmetric2_7.ext_conv2.4.weight', 'asymmetric2_7.ext_conv2.4.bias', 'asymmetric2_7.ext_conv2.4.running_mean', 'asymmetric2_7.ext_conv2.4.running_var', 'asymmetric2_7.ext_conv2.4.num_batches_tracked', 'asymmetric2_7.ext_conv2.5.weight', 'asymmetric2_7.ext_conv3.0.weight', 'asymmetric2_7.ext_conv3.1.weight', 'asymmetric2_7.ext_conv3.1.bias', 'asymmetric2_7.ext_conv3.1.running_mean', 'asymmetric2_7.ext_conv3.1.running_var', 'asymmetric2_7.ext_conv3.1.num_batches_tracked', 'asymmetric2_7.ext_conv3.2.weight', 'asymmetric2_7.out_activation.weight', 'dilated2_8.ext_conv1.0.weight', 'dilated2_8.ext_conv1.1.weight', 'dilated2_8.ext_conv1.1.bias', 'dilated2_8.ext_conv1.1.running_mean', 'dilated2_8.ext_conv1.1.running_var', 'dilated2_8.ext_conv1.1.num_batches_tracked', 'dilated2_8.ext_conv1.2.weight', 'dilated2_8.ext_conv2.0.weight', 'dilated2_8.ext_conv2.1.weight', 'dilated2_8.ext_conv2.1.bias', 'dilated2_8.ext_conv2.1.running_mean', 'dilated2_8.ext_conv2.1.running_var', 'dilated2_8.ext_conv2.1.num_batches_tracked', 'dilated2_8.ext_conv2.2.weight', 'dilated2_8.ext_conv3.0.weight', 'dilated2_8.ext_conv3.1.weight', 'dilated2_8.ext_conv3.1.bias', 'dilated2_8.ext_conv3.1.running_mean', 'dilated2_8.ext_conv3.1.running_var', 'dilated2_8.ext_conv3.1.num_batches_tracked', 'dilated2_8.ext_conv3.2.weight', 'dilated2_8.out_activation.weight', 'regular3_0.ext_conv1.0.weight', 'regular3_0.ext_conv1.1.weight', 'regular3_0.ext_conv1.1.bias', 'regular3_0.ext_conv1.1.running_mean', 'regular3_0.ext_conv1.1.running_var', 'regular3_0.ext_conv1.1.num_batches_tracked', 'regular3_0.ext_conv1.2.weight', 'regular3_0.ext_conv2.0.weight', 'regular3_0.ext_conv2.1.weight', 'regular3_0.ext_conv2.1.bias', 'regular3_0.ext_conv2.1.running_mean', 'regular3_0.ext_conv2.1.running_var', 'regular3_0.ext_conv2.1.num_batches_tracked', 'regular3_0.ext_conv2.2.weight', 'regular3_0.ext_conv3.0.weight', 'regular3_0.ext_conv3.1.weight', 'regular3_0.ext_conv3.1.bias', 'regular3_0.ext_conv3.1.running_mean', 'regular3_0.ext_conv3.1.running_var', 'regular3_0.ext_conv3.1.num_batches_tracked', 'regular3_0.ext_conv3.2.weight', 'regular3_0.out_activation.weight', 'dilated3_1.ext_conv1.0.weight', 'dilated3_1.ext_conv1.1.weight', 'dilated3_1.ext_conv1.1.bias', 'dilated3_1.ext_conv1.1.running_mean', 'dilated3_1.ext_conv1.1.running_var', 'dilated3_1.ext_conv1.1.num_batches_tracked', 'dilated3_1.ext_conv1.2.weight', 'dilated3_1.ext_conv2.0.weight', 'dilated3_1.ext_conv2.1.weight', 'dilated3_1.ext_conv2.1.bias', 'dilated3_1.ext_conv2.1.running_mean', 'dilated3_1.ext_conv2.1.running_var', 'dilated3_1.ext_conv2.1.num_batches_tracked', 'dilated3_1.ext_conv2.2.weight', 'dilated3_1.ext_conv3.0.weight', 'dilated3_1.ext_conv3.1.weight', 'dilated3_1.ext_conv3.1.bias', 'dilated3_1.ext_conv3.1.running_mean', 'dilated3_1.ext_conv3.1.running_var', 'dilated3_1.ext_conv3.1.num_batches_tracked', 'dilated3_1.ext_conv3.2.weight', 'dilated3_1.out_activation.weight', 'asymmetric3_2.ext_conv1.0.weight', 'asymmetric3_2.ext_conv1.1.weight', 'asymmetric3_2.ext_conv1.1.bias', 'asymmetric3_2.ext_conv1.1.running_mean', 'asymmetric3_2.ext_conv1.1.running_var', 'asymmetric3_2.ext_conv1.1.num_batches_tracked', 'asymmetric3_2.ext_conv1.2.weight', 'asymmetric3_2.ext_conv2.0.weight', 'asymmetric3_2.ext_conv2.1.weight', 'asymmetric3_2.ext_conv2.1.bias', 'asymmetric3_2.ext_conv2.1.running_mean', 'asymmetric3_2.ext_conv2.1.running_var', 'asymmetric3_2.ext_conv2.1.num_batches_tracked', 'asymmetric3_2.ext_conv2.2.weight', 'asymmetric3_2.ext_conv2.3.weight', 'asymmetric3_2.ext_conv2.4.weight', 'asymmetric3_2.ext_conv2.4.bias', 'asymmetric3_2.ext_conv2.4.running_mean', 'asymmetric3_2.ext_conv2.4.running_var', 'asymmetric3_2.ext_conv2.4.num_batches_tracked', 'asymmetric3_2.ext_conv2.5.weight', 'asymmetric3_2.ext_conv3.0.weight', 'asymmetric3_2.ext_conv3.1.weight', 'asymmetric3_2.ext_conv3.1.bias', 'asymmetric3_2.ext_conv3.1.running_mean', 'asymmetric3_2.ext_conv3.1.running_var', 'asymmetric3_2.ext_conv3.1.num_batches_tracked', 'asymmetric3_2.ext_conv3.2.weight', 'asymmetric3_2.out_activation.weight', 'dilated3_3.ext_conv1.0.weight', 'dilated3_3.ext_conv1.1.weight', 'dilated3_3.ext_conv1.1.bias', 'dilated3_3.ext_conv1.1.running_mean', 'dilated3_3.ext_conv1.1.running_var', 'dilated3_3.ext_conv1.1.num_batches_tracked', 'dilated3_3.ext_conv1.2.weight', 'dilated3_3.ext_conv2.0.weight', 'dilated3_3.ext_conv2.1.weight', 'dilated3_3.ext_conv2.1.bias', 'dilated3_3.ext_conv2.1.running_mean', 'dilated3_3.ext_conv2.1.running_var', 'dilated3_3.ext_conv2.1.num_batches_tracked', 'dilated3_3.ext_conv2.2.weight', 'dilated3_3.ext_conv3.0.weight', 'dilated3_3.ext_conv3.1.weight', 'dilated3_3.ext_conv3.1.bias', 'dilated3_3.ext_conv3.1.running_mean', 'dilated3_3.ext_conv3.1.running_var', 'dilated3_3.ext_conv3.1.num_batches_tracked', 'dilated3_3.ext_conv3.2.weight', 'dilated3_3.out_activation.weight', 'regular3_4.ext_conv1.0.weight', 'regular3_4.ext_conv1.1.weight', 'regular3_4.ext_conv1.1.bias', 'regular3_4.ext_conv1.1.running_mean', 'regular3_4.ext_conv1.1.running_var', 'regular3_4.ext_conv1.1.num_batches_tracked', 'regular3_4.ext_conv1.2.weight', 'regular3_4.ext_conv2.0.weight', 'regular3_4.ext_conv2.1.weight', 'regular3_4.ext_conv2.1.bias', 'regular3_4.ext_conv2.1.running_mean', 'regular3_4.ext_conv2.1.running_var', 'regular3_4.ext_conv2.1.num_batches_tracked', 'regular3_4.ext_conv2.2.weight', 'regular3_4.ext_conv3.0.weight', 'regular3_4.ext_conv3.1.weight', 'regular3_4.ext_conv3.1.bias', 'regular3_4.ext_conv3.1.running_mean', 'regular3_4.ext_conv3.1.running_var', 'regular3_4.ext_conv3.1.num_batches_tracked', 'regular3_4.ext_conv3.2.weight', 'regular3_4.out_activation.weight', 'dilated3_5.ext_conv1.0.weight', 'dilated3_5.ext_conv1.1.weight', 'dilated3_5.ext_conv1.1.bias', 'dilated3_5.ext_conv1.1.running_mean', 'dilated3_5.ext_conv1.1.running_var', 'dilated3_5.ext_conv1.1.num_batches_tracked', 'dilated3_5.ext_conv1.2.weight', 'dilated3_5.ext_conv2.0.weight', 'dilated3_5.ext_conv2.1.weight', 'dilated3_5.ext_conv2.1.bias', 'dilated3_5.ext_conv2.1.running_mean', 'dilated3_5.ext_conv2.1.running_var', 'dilated3_5.ext_conv2.1.num_batches_tracked', 'dilated3_5.ext_conv2.2.weight', 'dilated3_5.ext_conv3.0.weight', 'dilated3_5.ext_conv3.1.weight', 'dilated3_5.ext_conv3.1.bias', 'dilated3_5.ext_conv3.1.running_mean', 'dilated3_5.ext_conv3.1.running_var', 'dilated3_5.ext_conv3.1.num_batches_tracked', 'dilated3_5.ext_conv3.2.weight', 'dilated3_5.out_activation.weight', 'asymmetric3_6.ext_conv1.0.weight', 'asymmetric3_6.ext_conv1.1.weight', 'asymmetric3_6.ext_conv1.1.bias', 'asymmetric3_6.ext_conv1.1.running_mean', 'asymmetric3_6.ext_conv1.1.running_var', 'asymmetric3_6.ext_conv1.1.num_batches_tracked', 'asymmetric3_6.ext_conv1.2.weight', 'asymmetric3_6.ext_conv2.0.weight', 'asymmetric3_6.ext_conv2.1.weight', 'asymmetric3_6.ext_conv2.1.bias', 'asymmetric3_6.ext_conv2.1.running_mean', 'asymmetric3_6.ext_conv2.1.running_var', 'asymmetric3_6.ext_conv2.1.num_batches_tracked', 'asymmetric3_6.ext_conv2.2.weight', 'asymmetric3_6.ext_conv2.3.weight', 'asymmetric3_6.ext_conv2.4.weight', 'asymmetric3_6.ext_conv2.4.bias', 'asymmetric3_6.ext_conv2.4.running_mean', 'asymmetric3_6.ext_conv2.4.running_var', 'asymmetric3_6.ext_conv2.4.num_batches_tracked', 'asymmetric3_6.ext_conv2.5.weight', 'asymmetric3_6.ext_conv3.0.weight', 'asymmetric3_6.ext_conv3.1.weight', 'asymmetric3_6.ext_conv3.1.bias', 'asymmetric3_6.ext_conv3.1.running_mean', 'asymmetric3_6.ext_conv3.1.running_var', 'asymmetric3_6.ext_conv3.1.num_batches_tracked', 'asymmetric3_6.ext_conv3.2.weight', 'asymmetric3_6.out_activation.weight', 'dilated3_7.ext_conv1.0.weight', 'dilated3_7.ext_conv1.1.weight', 'dilated3_7.ext_conv1.1.bias', 'dilated3_7.ext_conv1.1.running_mean', 'dilated3_7.ext_conv1.1.running_var', 'dilated3_7.ext_conv1.1.num_batches_tracked', 'dilated3_7.ext_conv1.2.weight', 'dilated3_7.ext_conv2.0.weight', 'dilated3_7.ext_conv2.1.weight', 'dilated3_7.ext_conv2.1.bias', 'dilated3_7.ext_conv2.1.running_mean', 'dilated3_7.ext_conv2.1.running_var', 'dilated3_7.ext_conv2.1.num_batches_tracked', 'dilated3_7.ext_conv2.2.weight', 'dilated3_7.ext_conv3.0.weight', 'dilated3_7.ext_conv3.1.weight', 'dilated3_7.ext_conv3.1.bias', 'dilated3_7.ext_conv3.1.running_mean', 'dilated3_7.ext_conv3.1.running_var', 'dilated3_7.ext_conv3.1.num_batches_tracked', 'dilated3_7.ext_conv3.2.weight', 'dilated3_7.out_activation.weight', 'upsample4_0.main_conv1.0.weight', 'upsample4_0.main_conv1.1.weight', 'upsample4_0.main_conv1.1.bias', 'upsample4_0.main_conv1.1.running_mean', 'upsample4_0.main_conv1.1.running_var', 'upsample4_0.main_conv1.1.num_batches_tracked', 'upsample4_0.ext_conv1.0.weight', 'upsample4_0.ext_conv1.1.weight', 'upsample4_0.ext_conv1.1.bias', 'upsample4_0.ext_conv1.1.running_mean', 'upsample4_0.ext_conv1.1.running_var', 'upsample4_0.ext_conv1.1.num_batches_tracked', 'upsample4_0.ext_tconv1.weight', 'upsample4_0.ext_tconv1_bnorm.weight', 'upsample4_0.ext_tconv1_bnorm.bias', 'upsample4_0.ext_tconv1_bnorm.running_mean', 'upsample4_0.ext_tconv1_bnorm.running_var', 'upsample4_0.ext_tconv1_bnorm.num_batches_tracked', 'upsample4_0.ext_conv2.0.weight', 'upsample4_0.ext_conv2.1.weight', 'upsample4_0.ext_conv2.1.bias', 'upsample4_0.ext_conv2.1.running_mean', 'upsample4_0.ext_conv2.1.running_var', 'upsample4_0.ext_conv2.1.num_batches_tracked', 'regular4_1.ext_conv1.0.weight', 'regular4_1.ext_conv1.1.weight', 'regular4_1.ext_conv1.1.bias', 'regular4_1.ext_conv1.1.running_mean', 'regular4_1.ext_conv1.1.running_var', 'regular4_1.ext_conv1.1.num_batches_tracked', 'regular4_1.ext_conv2.0.weight', 'regular4_1.ext_conv2.1.weight', 'regular4_1.ext_conv2.1.bias', 'regular4_1.ext_conv2.1.running_mean', 'regular4_1.ext_conv2.1.running_var', 'regular4_1.ext_conv2.1.num_batches_tracked', 'regular4_1.ext_conv3.0.weight', 'regular4_1.ext_conv3.1.weight', 'regular4_1.ext_conv3.1.bias', 'regular4_1.ext_conv3.1.running_mean', 'regular4_1.ext_conv3.1.running_var', 'regular4_1.ext_conv3.1.num_batches_tracked', 'regular4_2.ext_conv1.0.weight', 'regular4_2.ext_conv1.1.weight', 'regular4_2.ext_conv1.1.bias', 'regular4_2.ext_conv1.1.running_mean', 'regular4_2.ext_conv1.1.running_var', 'regular4_2.ext_conv1.1.num_batches_tracked', 'regular4_2.ext_conv2.0.weight', 'regular4_2.ext_conv2.1.weight', 'regular4_2.ext_conv2.1.bias', 'regular4_2.ext_conv2.1.running_mean', 'regular4_2.ext_conv2.1.running_var', 'regular4_2.ext_conv2.1.num_batches_tracked', 'regular4_2.ext_conv3.0.weight', 'regular4_2.ext_conv3.1.weight', 'regular4_2.ext_conv3.1.bias', 'regular4_2.ext_conv3.1.running_mean', 'regular4_2.ext_conv3.1.running_var', 'regular4_2.ext_conv3.1.num_batches_tracked', 'upsample5_0.main_conv1.0.weight', 'upsample5_0.main_conv1.1.weight', 'upsample5_0.main_conv1.1.bias', 'upsample5_0.main_conv1.1.running_mean', 'upsample5_0.main_conv1.1.running_var', 'upsample5_0.main_conv1.1.num_batches_tracked', 'upsample5_0.ext_conv1.0.weight', 'upsample5_0.ext_conv1.1.weight', 'upsample5_0.ext_conv1.1.bias', 'upsample5_0.ext_conv1.1.running_mean', 'upsample5_0.ext_conv1.1.running_var', 'upsample5_0.ext_conv1.1.num_batches_tracked', 'upsample5_0.ext_tconv1.weight', 'upsample5_0.ext_tconv1_bnorm.weight', 'upsample5_0.ext_tconv1_bnorm.bias', 'upsample5_0.ext_tconv1_bnorm.running_mean', 'upsample5_0.ext_tconv1_bnorm.running_var', 'upsample5_0.ext_tconv1_bnorm.num_batches_tracked', 'upsample5_0.ext_conv2.0.weight', 'upsample5_0.ext_conv2.1.weight', 'upsample5_0.ext_conv2.1.bias', 'upsample5_0.ext_conv2.1.running_mean', 'upsample5_0.ext_conv2.1.running_var', 'upsample5_0.ext_conv2.1.num_batches_tracked', 'regular5_1.ext_conv1.0.weight', 'regular5_1.ext_conv1.1.weight', 'regular5_1.ext_conv1.1.bias', 'regular5_1.ext_conv1.1.running_mean', 'regular5_1.ext_conv1.1.running_var', 'regular5_1.ext_conv1.1.num_batches_tracked', 'regular5_1.ext_conv2.0.weight', 'regular5_1.ext_conv2.1.weight', 'regular5_1.ext_conv2.1.bias', 'regular5_1.ext_conv2.1.running_mean', 'regular5_1.ext_conv2.1.running_var', 'regular5_1.ext_conv2.1.num_batches_tracked', 'regular5_1.ext_conv3.0.weight', 'regular5_1.ext_conv3.1.weight', 'regular5_1.ext_conv3.1.bias', 'regular5_1.ext_conv3.1.running_mean', 'regular5_1.ext_conv3.1.running_var', 'regular5_1.ext_conv3.1.num_batches_tracked', 'transposed_conv.weight'])\n",
            "odict_keys(['initial_block.main_branch.weight', 'initial_block.batch_norm.weight', 'initial_block.batch_norm.bias', 'initial_block.batch_norm.running_mean', 'initial_block.batch_norm.running_var', 'initial_block.batch_norm.num_batches_tracked', 'initial_block.out_activation.weight', 'downsample1_0.ext_conv1.0.weight', 'downsample1_0.ext_conv1.1.weight', 'downsample1_0.ext_conv1.1.bias', 'downsample1_0.ext_conv1.1.running_mean', 'downsample1_0.ext_conv1.1.running_var', 'downsample1_0.ext_conv1.1.num_batches_tracked', 'downsample1_0.ext_conv1.2.weight', 'downsample1_0.ext_conv2.0.weight', 'downsample1_0.ext_conv2.1.weight', 'downsample1_0.ext_conv2.1.bias', 'downsample1_0.ext_conv2.1.running_mean', 'downsample1_0.ext_conv2.1.running_var', 'downsample1_0.ext_conv2.1.num_batches_tracked', 'downsample1_0.ext_conv2.2.weight', 'downsample1_0.ext_conv3.0.weight', 'downsample1_0.ext_conv3.1.weight', 'downsample1_0.ext_conv3.1.bias', 'downsample1_0.ext_conv3.1.running_mean', 'downsample1_0.ext_conv3.1.running_var', 'downsample1_0.ext_conv3.1.num_batches_tracked', 'downsample1_0.ext_conv3.2.weight', 'downsample1_0.out_activation.weight', 'regular1_1.ext_conv1.0.weight', 'regular1_1.ext_conv1.1.weight', 'regular1_1.ext_conv1.1.bias', 'regular1_1.ext_conv1.1.running_mean', 'regular1_1.ext_conv1.1.running_var', 'regular1_1.ext_conv1.1.num_batches_tracked', 'regular1_1.ext_conv1.2.weight', 'regular1_1.ext_conv2.0.weight', 'regular1_1.ext_conv2.1.weight', 'regular1_1.ext_conv2.1.bias', 'regular1_1.ext_conv2.1.running_mean', 'regular1_1.ext_conv2.1.running_var', 'regular1_1.ext_conv2.1.num_batches_tracked', 'regular1_1.ext_conv2.2.weight', 'regular1_1.ext_conv3.0.weight', 'regular1_1.ext_conv3.1.weight', 'regular1_1.ext_conv3.1.bias', 'regular1_1.ext_conv3.1.running_mean', 'regular1_1.ext_conv3.1.running_var', 'regular1_1.ext_conv3.1.num_batches_tracked', 'regular1_1.ext_conv3.2.weight', 'regular1_1.out_activation.weight', 'regular1_2.ext_conv1.0.weight', 'regular1_2.ext_conv1.1.weight', 'regular1_2.ext_conv1.1.bias', 'regular1_2.ext_conv1.1.running_mean', 'regular1_2.ext_conv1.1.running_var', 'regular1_2.ext_conv1.1.num_batches_tracked', 'regular1_2.ext_conv1.2.weight', 'regular1_2.ext_conv2.0.weight', 'regular1_2.ext_conv2.1.weight', 'regular1_2.ext_conv2.1.bias', 'regular1_2.ext_conv2.1.running_mean', 'regular1_2.ext_conv2.1.running_var', 'regular1_2.ext_conv2.1.num_batches_tracked', 'regular1_2.ext_conv2.2.weight', 'regular1_2.ext_conv3.0.weight', 'regular1_2.ext_conv3.1.weight', 'regular1_2.ext_conv3.1.bias', 'regular1_2.ext_conv3.1.running_mean', 'regular1_2.ext_conv3.1.running_var', 'regular1_2.ext_conv3.1.num_batches_tracked', 'regular1_2.ext_conv3.2.weight', 'regular1_2.out_activation.weight', 'regular1_3.ext_conv1.0.weight', 'regular1_3.ext_conv1.1.weight', 'regular1_3.ext_conv1.1.bias', 'regular1_3.ext_conv1.1.running_mean', 'regular1_3.ext_conv1.1.running_var', 'regular1_3.ext_conv1.1.num_batches_tracked', 'regular1_3.ext_conv1.2.weight', 'regular1_3.ext_conv2.0.weight', 'regular1_3.ext_conv2.1.weight', 'regular1_3.ext_conv2.1.bias', 'regular1_3.ext_conv2.1.running_mean', 'regular1_3.ext_conv2.1.running_var', 'regular1_3.ext_conv2.1.num_batches_tracked', 'regular1_3.ext_conv2.2.weight', 'regular1_3.ext_conv3.0.weight', 'regular1_3.ext_conv3.1.weight', 'regular1_3.ext_conv3.1.bias', 'regular1_3.ext_conv3.1.running_mean', 'regular1_3.ext_conv3.1.running_var', 'regular1_3.ext_conv3.1.num_batches_tracked', 'regular1_3.ext_conv3.2.weight', 'regular1_3.out_activation.weight', 'regular1_4.ext_conv1.0.weight', 'regular1_4.ext_conv1.1.weight', 'regular1_4.ext_conv1.1.bias', 'regular1_4.ext_conv1.1.running_mean', 'regular1_4.ext_conv1.1.running_var', 'regular1_4.ext_conv1.1.num_batches_tracked', 'regular1_4.ext_conv1.2.weight', 'regular1_4.ext_conv2.0.weight', 'regular1_4.ext_conv2.1.weight', 'regular1_4.ext_conv2.1.bias', 'regular1_4.ext_conv2.1.running_mean', 'regular1_4.ext_conv2.1.running_var', 'regular1_4.ext_conv2.1.num_batches_tracked', 'regular1_4.ext_conv2.2.weight', 'regular1_4.ext_conv3.0.weight', 'regular1_4.ext_conv3.1.weight', 'regular1_4.ext_conv3.1.bias', 'regular1_4.ext_conv3.1.running_mean', 'regular1_4.ext_conv3.1.running_var', 'regular1_4.ext_conv3.1.num_batches_tracked', 'regular1_4.ext_conv3.2.weight', 'regular1_4.out_activation.weight', 'downsample2_0.ext_conv1.0.weight', 'downsample2_0.ext_conv1.1.weight', 'downsample2_0.ext_conv1.1.bias', 'downsample2_0.ext_conv1.1.running_mean', 'downsample2_0.ext_conv1.1.running_var', 'downsample2_0.ext_conv1.1.num_batches_tracked', 'downsample2_0.ext_conv1.2.weight', 'downsample2_0.ext_conv2.0.weight', 'downsample2_0.ext_conv2.1.weight', 'downsample2_0.ext_conv2.1.bias', 'downsample2_0.ext_conv2.1.running_mean', 'downsample2_0.ext_conv2.1.running_var', 'downsample2_0.ext_conv2.1.num_batches_tracked', 'downsample2_0.ext_conv2.2.weight', 'downsample2_0.ext_conv3.0.weight', 'downsample2_0.ext_conv3.1.weight', 'downsample2_0.ext_conv3.1.bias', 'downsample2_0.ext_conv3.1.running_mean', 'downsample2_0.ext_conv3.1.running_var', 'downsample2_0.ext_conv3.1.num_batches_tracked', 'downsample2_0.ext_conv3.2.weight', 'downsample2_0.out_activation.weight', 'regular2_1.ext_conv1.0.weight', 'regular2_1.ext_conv1.1.weight', 'regular2_1.ext_conv1.1.bias', 'regular2_1.ext_conv1.1.running_mean', 'regular2_1.ext_conv1.1.running_var', 'regular2_1.ext_conv1.1.num_batches_tracked', 'regular2_1.ext_conv1.2.weight', 'regular2_1.ext_conv2.0.weight', 'regular2_1.ext_conv2.1.weight', 'regular2_1.ext_conv2.1.bias', 'regular2_1.ext_conv2.1.running_mean', 'regular2_1.ext_conv2.1.running_var', 'regular2_1.ext_conv2.1.num_batches_tracked', 'regular2_1.ext_conv2.2.weight', 'regular2_1.ext_conv3.0.weight', 'regular2_1.ext_conv3.1.weight', 'regular2_1.ext_conv3.1.bias', 'regular2_1.ext_conv3.1.running_mean', 'regular2_1.ext_conv3.1.running_var', 'regular2_1.ext_conv3.1.num_batches_tracked', 'regular2_1.ext_conv3.2.weight', 'regular2_1.out_activation.weight', 'dilated2_2.ext_conv1.0.weight', 'dilated2_2.ext_conv1.1.weight', 'dilated2_2.ext_conv1.1.bias', 'dilated2_2.ext_conv1.1.running_mean', 'dilated2_2.ext_conv1.1.running_var', 'dilated2_2.ext_conv1.1.num_batches_tracked', 'dilated2_2.ext_conv1.2.weight', 'dilated2_2.ext_conv2.0.weight', 'dilated2_2.ext_conv2.1.weight', 'dilated2_2.ext_conv2.1.bias', 'dilated2_2.ext_conv2.1.running_mean', 'dilated2_2.ext_conv2.1.running_var', 'dilated2_2.ext_conv2.1.num_batches_tracked', 'dilated2_2.ext_conv2.2.weight', 'dilated2_2.ext_conv3.0.weight', 'dilated2_2.ext_conv3.1.weight', 'dilated2_2.ext_conv3.1.bias', 'dilated2_2.ext_conv3.1.running_mean', 'dilated2_2.ext_conv3.1.running_var', 'dilated2_2.ext_conv3.1.num_batches_tracked', 'dilated2_2.ext_conv3.2.weight', 'dilated2_2.out_activation.weight', 'asymmetric2_3.ext_conv1.0.weight', 'asymmetric2_3.ext_conv1.1.weight', 'asymmetric2_3.ext_conv1.1.bias', 'asymmetric2_3.ext_conv1.1.running_mean', 'asymmetric2_3.ext_conv1.1.running_var', 'asymmetric2_3.ext_conv1.1.num_batches_tracked', 'asymmetric2_3.ext_conv1.2.weight', 'asymmetric2_3.ext_conv2.0.weight', 'asymmetric2_3.ext_conv2.1.weight', 'asymmetric2_3.ext_conv2.1.bias', 'asymmetric2_3.ext_conv2.1.running_mean', 'asymmetric2_3.ext_conv2.1.running_var', 'asymmetric2_3.ext_conv2.1.num_batches_tracked', 'asymmetric2_3.ext_conv2.2.weight', 'asymmetric2_3.ext_conv2.3.weight', 'asymmetric2_3.ext_conv2.4.weight', 'asymmetric2_3.ext_conv2.4.bias', 'asymmetric2_3.ext_conv2.4.running_mean', 'asymmetric2_3.ext_conv2.4.running_var', 'asymmetric2_3.ext_conv2.4.num_batches_tracked', 'asymmetric2_3.ext_conv2.5.weight', 'asymmetric2_3.ext_conv3.0.weight', 'asymmetric2_3.ext_conv3.1.weight', 'asymmetric2_3.ext_conv3.1.bias', 'asymmetric2_3.ext_conv3.1.running_mean', 'asymmetric2_3.ext_conv3.1.running_var', 'asymmetric2_3.ext_conv3.1.num_batches_tracked', 'asymmetric2_3.ext_conv3.2.weight', 'asymmetric2_3.out_activation.weight', 'dilated2_4.ext_conv1.0.weight', 'dilated2_4.ext_conv1.1.weight', 'dilated2_4.ext_conv1.1.bias', 'dilated2_4.ext_conv1.1.running_mean', 'dilated2_4.ext_conv1.1.running_var', 'dilated2_4.ext_conv1.1.num_batches_tracked', 'dilated2_4.ext_conv1.2.weight', 'dilated2_4.ext_conv2.0.weight', 'dilated2_4.ext_conv2.1.weight', 'dilated2_4.ext_conv2.1.bias', 'dilated2_4.ext_conv2.1.running_mean', 'dilated2_4.ext_conv2.1.running_var', 'dilated2_4.ext_conv2.1.num_batches_tracked', 'dilated2_4.ext_conv2.2.weight', 'dilated2_4.ext_conv3.0.weight', 'dilated2_4.ext_conv3.1.weight', 'dilated2_4.ext_conv3.1.bias', 'dilated2_4.ext_conv3.1.running_mean', 'dilated2_4.ext_conv3.1.running_var', 'dilated2_4.ext_conv3.1.num_batches_tracked', 'dilated2_4.ext_conv3.2.weight', 'dilated2_4.out_activation.weight', 'regular2_5.ext_conv1.0.weight', 'regular2_5.ext_conv1.1.weight', 'regular2_5.ext_conv1.1.bias', 'regular2_5.ext_conv1.1.running_mean', 'regular2_5.ext_conv1.1.running_var', 'regular2_5.ext_conv1.1.num_batches_tracked', 'regular2_5.ext_conv1.2.weight', 'regular2_5.ext_conv2.0.weight', 'regular2_5.ext_conv2.1.weight', 'regular2_5.ext_conv2.1.bias', 'regular2_5.ext_conv2.1.running_mean', 'regular2_5.ext_conv2.1.running_var', 'regular2_5.ext_conv2.1.num_batches_tracked', 'regular2_5.ext_conv2.2.weight', 'regular2_5.ext_conv3.0.weight', 'regular2_5.ext_conv3.1.weight', 'regular2_5.ext_conv3.1.bias', 'regular2_5.ext_conv3.1.running_mean', 'regular2_5.ext_conv3.1.running_var', 'regular2_5.ext_conv3.1.num_batches_tracked', 'regular2_5.ext_conv3.2.weight', 'regular2_5.out_activation.weight', 'dilated2_6.ext_conv1.0.weight', 'dilated2_6.ext_conv1.1.weight', 'dilated2_6.ext_conv1.1.bias', 'dilated2_6.ext_conv1.1.running_mean', 'dilated2_6.ext_conv1.1.running_var', 'dilated2_6.ext_conv1.1.num_batches_tracked', 'dilated2_6.ext_conv1.2.weight', 'dilated2_6.ext_conv2.0.weight', 'dilated2_6.ext_conv2.1.weight', 'dilated2_6.ext_conv2.1.bias', 'dilated2_6.ext_conv2.1.running_mean', 'dilated2_6.ext_conv2.1.running_var', 'dilated2_6.ext_conv2.1.num_batches_tracked', 'dilated2_6.ext_conv2.2.weight', 'dilated2_6.ext_conv3.0.weight', 'dilated2_6.ext_conv3.1.weight', 'dilated2_6.ext_conv3.1.bias', 'dilated2_6.ext_conv3.1.running_mean', 'dilated2_6.ext_conv3.1.running_var', 'dilated2_6.ext_conv3.1.num_batches_tracked', 'dilated2_6.ext_conv3.2.weight', 'dilated2_6.out_activation.weight', 'asymmetric2_7.ext_conv1.0.weight', 'asymmetric2_7.ext_conv1.1.weight', 'asymmetric2_7.ext_conv1.1.bias', 'asymmetric2_7.ext_conv1.1.running_mean', 'asymmetric2_7.ext_conv1.1.running_var', 'asymmetric2_7.ext_conv1.1.num_batches_tracked', 'asymmetric2_7.ext_conv1.2.weight', 'asymmetric2_7.ext_conv2.0.weight', 'asymmetric2_7.ext_conv2.1.weight', 'asymmetric2_7.ext_conv2.1.bias', 'asymmetric2_7.ext_conv2.1.running_mean', 'asymmetric2_7.ext_conv2.1.running_var', 'asymmetric2_7.ext_conv2.1.num_batches_tracked', 'asymmetric2_7.ext_conv2.2.weight', 'asymmetric2_7.ext_conv2.3.weight', 'asymmetric2_7.ext_conv2.4.weight', 'asymmetric2_7.ext_conv2.4.bias', 'asymmetric2_7.ext_conv2.4.running_mean', 'asymmetric2_7.ext_conv2.4.running_var', 'asymmetric2_7.ext_conv2.4.num_batches_tracked', 'asymmetric2_7.ext_conv2.5.weight', 'asymmetric2_7.ext_conv3.0.weight', 'asymmetric2_7.ext_conv3.1.weight', 'asymmetric2_7.ext_conv3.1.bias', 'asymmetric2_7.ext_conv3.1.running_mean', 'asymmetric2_7.ext_conv3.1.running_var', 'asymmetric2_7.ext_conv3.1.num_batches_tracked', 'asymmetric2_7.ext_conv3.2.weight', 'asymmetric2_7.out_activation.weight', 'dilated2_8.ext_conv1.0.weight', 'dilated2_8.ext_conv1.1.weight', 'dilated2_8.ext_conv1.1.bias', 'dilated2_8.ext_conv1.1.running_mean', 'dilated2_8.ext_conv1.1.running_var', 'dilated2_8.ext_conv1.1.num_batches_tracked', 'dilated2_8.ext_conv1.2.weight', 'dilated2_8.ext_conv2.0.weight', 'dilated2_8.ext_conv2.1.weight', 'dilated2_8.ext_conv2.1.bias', 'dilated2_8.ext_conv2.1.running_mean', 'dilated2_8.ext_conv2.1.running_var', 'dilated2_8.ext_conv2.1.num_batches_tracked', 'dilated2_8.ext_conv2.2.weight', 'dilated2_8.ext_conv3.0.weight', 'dilated2_8.ext_conv3.1.weight', 'dilated2_8.ext_conv3.1.bias', 'dilated2_8.ext_conv3.1.running_mean', 'dilated2_8.ext_conv3.1.running_var', 'dilated2_8.ext_conv3.1.num_batches_tracked', 'dilated2_8.ext_conv3.2.weight', 'dilated2_8.out_activation.weight', 'regular3_0.ext_conv1.0.weight', 'regular3_0.ext_conv1.1.weight', 'regular3_0.ext_conv1.1.bias', 'regular3_0.ext_conv1.1.running_mean', 'regular3_0.ext_conv1.1.running_var', 'regular3_0.ext_conv1.1.num_batches_tracked', 'regular3_0.ext_conv1.2.weight', 'regular3_0.ext_conv2.0.weight', 'regular3_0.ext_conv2.1.weight', 'regular3_0.ext_conv2.1.bias', 'regular3_0.ext_conv2.1.running_mean', 'regular3_0.ext_conv2.1.running_var', 'regular3_0.ext_conv2.1.num_batches_tracked', 'regular3_0.ext_conv2.2.weight', 'regular3_0.ext_conv3.0.weight', 'regular3_0.ext_conv3.1.weight', 'regular3_0.ext_conv3.1.bias', 'regular3_0.ext_conv3.1.running_mean', 'regular3_0.ext_conv3.1.running_var', 'regular3_0.ext_conv3.1.num_batches_tracked', 'regular3_0.ext_conv3.2.weight', 'regular3_0.out_activation.weight', 'dilated3_1.ext_conv1.0.weight', 'dilated3_1.ext_conv1.1.weight', 'dilated3_1.ext_conv1.1.bias', 'dilated3_1.ext_conv1.1.running_mean', 'dilated3_1.ext_conv1.1.running_var', 'dilated3_1.ext_conv1.1.num_batches_tracked', 'dilated3_1.ext_conv1.2.weight', 'dilated3_1.ext_conv2.0.weight', 'dilated3_1.ext_conv2.1.weight', 'dilated3_1.ext_conv2.1.bias', 'dilated3_1.ext_conv2.1.running_mean', 'dilated3_1.ext_conv2.1.running_var', 'dilated3_1.ext_conv2.1.num_batches_tracked', 'dilated3_1.ext_conv2.2.weight', 'dilated3_1.ext_conv3.0.weight', 'dilated3_1.ext_conv3.1.weight', 'dilated3_1.ext_conv3.1.bias', 'dilated3_1.ext_conv3.1.running_mean', 'dilated3_1.ext_conv3.1.running_var', 'dilated3_1.ext_conv3.1.num_batches_tracked', 'dilated3_1.ext_conv3.2.weight', 'dilated3_1.out_activation.weight', 'asymmetric3_2.ext_conv1.0.weight', 'asymmetric3_2.ext_conv1.1.weight', 'asymmetric3_2.ext_conv1.1.bias', 'asymmetric3_2.ext_conv1.1.running_mean', 'asymmetric3_2.ext_conv1.1.running_var', 'asymmetric3_2.ext_conv1.1.num_batches_tracked', 'asymmetric3_2.ext_conv1.2.weight', 'asymmetric3_2.ext_conv2.0.weight', 'asymmetric3_2.ext_conv2.1.weight', 'asymmetric3_2.ext_conv2.1.bias', 'asymmetric3_2.ext_conv2.1.running_mean', 'asymmetric3_2.ext_conv2.1.running_var', 'asymmetric3_2.ext_conv2.1.num_batches_tracked', 'asymmetric3_2.ext_conv2.2.weight', 'asymmetric3_2.ext_conv2.3.weight', 'asymmetric3_2.ext_conv2.4.weight', 'asymmetric3_2.ext_conv2.4.bias', 'asymmetric3_2.ext_conv2.4.running_mean', 'asymmetric3_2.ext_conv2.4.running_var', 'asymmetric3_2.ext_conv2.4.num_batches_tracked', 'asymmetric3_2.ext_conv2.5.weight', 'asymmetric3_2.ext_conv3.0.weight', 'asymmetric3_2.ext_conv3.1.weight', 'asymmetric3_2.ext_conv3.1.bias', 'asymmetric3_2.ext_conv3.1.running_mean', 'asymmetric3_2.ext_conv3.1.running_var', 'asymmetric3_2.ext_conv3.1.num_batches_tracked', 'asymmetric3_2.ext_conv3.2.weight', 'asymmetric3_2.out_activation.weight', 'dilated3_3.ext_conv1.0.weight', 'dilated3_3.ext_conv1.1.weight', 'dilated3_3.ext_conv1.1.bias', 'dilated3_3.ext_conv1.1.running_mean', 'dilated3_3.ext_conv1.1.running_var', 'dilated3_3.ext_conv1.1.num_batches_tracked', 'dilated3_3.ext_conv1.2.weight', 'dilated3_3.ext_conv2.0.weight', 'dilated3_3.ext_conv2.1.weight', 'dilated3_3.ext_conv2.1.bias', 'dilated3_3.ext_conv2.1.running_mean', 'dilated3_3.ext_conv2.1.running_var', 'dilated3_3.ext_conv2.1.num_batches_tracked', 'dilated3_3.ext_conv2.2.weight', 'dilated3_3.ext_conv3.0.weight', 'dilated3_3.ext_conv3.1.weight', 'dilated3_3.ext_conv3.1.bias', 'dilated3_3.ext_conv3.1.running_mean', 'dilated3_3.ext_conv3.1.running_var', 'dilated3_3.ext_conv3.1.num_batches_tracked', 'dilated3_3.ext_conv3.2.weight', 'dilated3_3.out_activation.weight', 'regular3_4.ext_conv1.0.weight', 'regular3_4.ext_conv1.1.weight', 'regular3_4.ext_conv1.1.bias', 'regular3_4.ext_conv1.1.running_mean', 'regular3_4.ext_conv1.1.running_var', 'regular3_4.ext_conv1.1.num_batches_tracked', 'regular3_4.ext_conv1.2.weight', 'regular3_4.ext_conv2.0.weight', 'regular3_4.ext_conv2.1.weight', 'regular3_4.ext_conv2.1.bias', 'regular3_4.ext_conv2.1.running_mean', 'regular3_4.ext_conv2.1.running_var', 'regular3_4.ext_conv2.1.num_batches_tracked', 'regular3_4.ext_conv2.2.weight', 'regular3_4.ext_conv3.0.weight', 'regular3_4.ext_conv3.1.weight', 'regular3_4.ext_conv3.1.bias', 'regular3_4.ext_conv3.1.running_mean', 'regular3_4.ext_conv3.1.running_var', 'regular3_4.ext_conv3.1.num_batches_tracked', 'regular3_4.ext_conv3.2.weight', 'regular3_4.out_activation.weight', 'dilated3_5.ext_conv1.0.weight', 'dilated3_5.ext_conv1.1.weight', 'dilated3_5.ext_conv1.1.bias', 'dilated3_5.ext_conv1.1.running_mean', 'dilated3_5.ext_conv1.1.running_var', 'dilated3_5.ext_conv1.1.num_batches_tracked', 'dilated3_5.ext_conv1.2.weight', 'dilated3_5.ext_conv2.0.weight', 'dilated3_5.ext_conv2.1.weight', 'dilated3_5.ext_conv2.1.bias', 'dilated3_5.ext_conv2.1.running_mean', 'dilated3_5.ext_conv2.1.running_var', 'dilated3_5.ext_conv2.1.num_batches_tracked', 'dilated3_5.ext_conv2.2.weight', 'dilated3_5.ext_conv3.0.weight', 'dilated3_5.ext_conv3.1.weight', 'dilated3_5.ext_conv3.1.bias', 'dilated3_5.ext_conv3.1.running_mean', 'dilated3_5.ext_conv3.1.running_var', 'dilated3_5.ext_conv3.1.num_batches_tracked', 'dilated3_5.ext_conv3.2.weight', 'dilated3_5.out_activation.weight', 'asymmetric3_6.ext_conv1.0.weight', 'asymmetric3_6.ext_conv1.1.weight', 'asymmetric3_6.ext_conv1.1.bias', 'asymmetric3_6.ext_conv1.1.running_mean', 'asymmetric3_6.ext_conv1.1.running_var', 'asymmetric3_6.ext_conv1.1.num_batches_tracked', 'asymmetric3_6.ext_conv1.2.weight', 'asymmetric3_6.ext_conv2.0.weight', 'asymmetric3_6.ext_conv2.1.weight', 'asymmetric3_6.ext_conv2.1.bias', 'asymmetric3_6.ext_conv2.1.running_mean', 'asymmetric3_6.ext_conv2.1.running_var', 'asymmetric3_6.ext_conv2.1.num_batches_tracked', 'asymmetric3_6.ext_conv2.2.weight', 'asymmetric3_6.ext_conv2.3.weight', 'asymmetric3_6.ext_conv2.4.weight', 'asymmetric3_6.ext_conv2.4.bias', 'asymmetric3_6.ext_conv2.4.running_mean', 'asymmetric3_6.ext_conv2.4.running_var', 'asymmetric3_6.ext_conv2.4.num_batches_tracked', 'asymmetric3_6.ext_conv2.5.weight', 'asymmetric3_6.ext_conv3.0.weight', 'asymmetric3_6.ext_conv3.1.weight', 'asymmetric3_6.ext_conv3.1.bias', 'asymmetric3_6.ext_conv3.1.running_mean', 'asymmetric3_6.ext_conv3.1.running_var', 'asymmetric3_6.ext_conv3.1.num_batches_tracked', 'asymmetric3_6.ext_conv3.2.weight', 'asymmetric3_6.out_activation.weight', 'dilated3_7.ext_conv1.0.weight', 'dilated3_7.ext_conv1.1.weight', 'dilated3_7.ext_conv1.1.bias', 'dilated3_7.ext_conv1.1.running_mean', 'dilated3_7.ext_conv1.1.running_var', 'dilated3_7.ext_conv1.1.num_batches_tracked', 'dilated3_7.ext_conv1.2.weight', 'dilated3_7.ext_conv2.0.weight', 'dilated3_7.ext_conv2.1.weight', 'dilated3_7.ext_conv2.1.bias', 'dilated3_7.ext_conv2.1.running_mean', 'dilated3_7.ext_conv2.1.running_var', 'dilated3_7.ext_conv2.1.num_batches_tracked', 'dilated3_7.ext_conv2.2.weight', 'dilated3_7.ext_conv3.0.weight', 'dilated3_7.ext_conv3.1.weight', 'dilated3_7.ext_conv3.1.bias', 'dilated3_7.ext_conv3.1.running_mean', 'dilated3_7.ext_conv3.1.running_var', 'dilated3_7.ext_conv3.1.num_batches_tracked', 'dilated3_7.ext_conv3.2.weight', 'dilated3_7.out_activation.weight', 'upsample4_0.main_conv1.0.weight', 'upsample4_0.main_conv1.1.weight', 'upsample4_0.main_conv1.1.bias', 'upsample4_0.main_conv1.1.running_mean', 'upsample4_0.main_conv1.1.running_var', 'upsample4_0.main_conv1.1.num_batches_tracked', 'upsample4_0.ext_conv1.0.weight', 'upsample4_0.ext_conv1.1.weight', 'upsample4_0.ext_conv1.1.bias', 'upsample4_0.ext_conv1.1.running_mean', 'upsample4_0.ext_conv1.1.running_var', 'upsample4_0.ext_conv1.1.num_batches_tracked', 'upsample4_0.ext_tconv1.weight', 'upsample4_0.ext_tconv1_bnorm.weight', 'upsample4_0.ext_tconv1_bnorm.bias', 'upsample4_0.ext_tconv1_bnorm.running_mean', 'upsample4_0.ext_tconv1_bnorm.running_var', 'upsample4_0.ext_tconv1_bnorm.num_batches_tracked', 'upsample4_0.ext_conv2.0.weight', 'upsample4_0.ext_conv2.1.weight', 'upsample4_0.ext_conv2.1.bias', 'upsample4_0.ext_conv2.1.running_mean', 'upsample4_0.ext_conv2.1.running_var', 'upsample4_0.ext_conv2.1.num_batches_tracked', 'regular4_1.ext_conv1.0.weight', 'regular4_1.ext_conv1.1.weight', 'regular4_1.ext_conv1.1.bias', 'regular4_1.ext_conv1.1.running_mean', 'regular4_1.ext_conv1.1.running_var', 'regular4_1.ext_conv1.1.num_batches_tracked', 'regular4_1.ext_conv2.0.weight', 'regular4_1.ext_conv2.1.weight', 'regular4_1.ext_conv2.1.bias', 'regular4_1.ext_conv2.1.running_mean', 'regular4_1.ext_conv2.1.running_var', 'regular4_1.ext_conv2.1.num_batches_tracked', 'regular4_1.ext_conv3.0.weight', 'regular4_1.ext_conv3.1.weight', 'regular4_1.ext_conv3.1.bias', 'regular4_1.ext_conv3.1.running_mean', 'regular4_1.ext_conv3.1.running_var', 'regular4_1.ext_conv3.1.num_batches_tracked', 'regular4_2.ext_conv1.0.weight', 'regular4_2.ext_conv1.1.weight', 'regular4_2.ext_conv1.1.bias', 'regular4_2.ext_conv1.1.running_mean', 'regular4_2.ext_conv1.1.running_var', 'regular4_2.ext_conv1.1.num_batches_tracked', 'regular4_2.ext_conv2.0.weight', 'regular4_2.ext_conv2.1.weight', 'regular4_2.ext_conv2.1.bias', 'regular4_2.ext_conv2.1.running_mean', 'regular4_2.ext_conv2.1.running_var', 'regular4_2.ext_conv2.1.num_batches_tracked', 'regular4_2.ext_conv3.0.weight', 'regular4_2.ext_conv3.1.weight', 'regular4_2.ext_conv3.1.bias', 'regular4_2.ext_conv3.1.running_mean', 'regular4_2.ext_conv3.1.running_var', 'regular4_2.ext_conv3.1.num_batches_tracked', 'upsample5_0.main_conv1.0.weight', 'upsample5_0.main_conv1.1.weight', 'upsample5_0.main_conv1.1.bias', 'upsample5_0.main_conv1.1.running_mean', 'upsample5_0.main_conv1.1.running_var', 'upsample5_0.main_conv1.1.num_batches_tracked', 'upsample5_0.ext_conv1.0.weight', 'upsample5_0.ext_conv1.1.weight', 'upsample5_0.ext_conv1.1.bias', 'upsample5_0.ext_conv1.1.running_mean', 'upsample5_0.ext_conv1.1.running_var', 'upsample5_0.ext_conv1.1.num_batches_tracked', 'upsample5_0.ext_tconv1.weight', 'upsample5_0.ext_tconv1_bnorm.weight', 'upsample5_0.ext_tconv1_bnorm.bias', 'upsample5_0.ext_tconv1_bnorm.running_mean', 'upsample5_0.ext_tconv1_bnorm.running_var', 'upsample5_0.ext_tconv1_bnorm.num_batches_tracked', 'upsample5_0.ext_conv2.0.weight', 'upsample5_0.ext_conv2.1.weight', 'upsample5_0.ext_conv2.1.bias', 'upsample5_0.ext_conv2.1.running_mean', 'upsample5_0.ext_conv2.1.running_var', 'upsample5_0.ext_conv2.1.num_batches_tracked', 'regular5_1.ext_conv1.0.weight', 'regular5_1.ext_conv1.1.weight', 'regular5_1.ext_conv1.1.bias', 'regular5_1.ext_conv1.1.running_mean', 'regular5_1.ext_conv1.1.running_var', 'regular5_1.ext_conv1.1.num_batches_tracked', 'regular5_1.ext_conv2.0.weight', 'regular5_1.ext_conv2.1.weight', 'regular5_1.ext_conv2.1.bias', 'regular5_1.ext_conv2.1.running_mean', 'regular5_1.ext_conv2.1.running_var', 'regular5_1.ext_conv2.1.num_batches_tracked', 'regular5_1.ext_conv3.0.weight', 'regular5_1.ext_conv3.1.weight', 'regular5_1.ext_conv3.1.bias', 'regular5_1.ext_conv3.1.running_mean', 'regular5_1.ext_conv3.1.running_var', 'regular5_1.ext_conv3.1.num_batches_tracked', 'transposed_conv.weight'])\n",
            "Import Model enet with weights enet_pretrained to FineTune\n",
            "========== TRAINING ===========\n",
            "/content/cityscapes/leftImg8bit/train\n",
            "/content/cityscapes/leftImg8bit/val\n",
            "<class '__main__.CrossEntropyLoss2d'>\n",
            "----- TRAINING - EPOCH 1 -----\n",
            "LEARNING RATE:  5e-05\n",
            "loss: 10.41 (epoch: 1, step: 0) // Avg time/img: 0.6229 s\n",
            "loss: 10.08 (epoch: 1, step: 50) // Avg time/img: 0.0636 s\n",
            "loss: 9.986 (epoch: 1, step: 100) // Avg time/img: 0.0579 s\n",
            "loss: 9.887 (epoch: 1, step: 150) // Avg time/img: 0.0568 s\n",
            "loss: 9.761 (epoch: 1, step: 200) // Avg time/img: 0.0562 s\n",
            "loss: 9.651 (epoch: 1, step: 250) // Avg time/img: 0.0558 s\n",
            "loss: 9.534 (epoch: 1, step: 300) // Avg time/img: 0.0552 s\n",
            "loss: 9.426 (epoch: 1, step: 350) // Avg time/img: 0.0550 s\n",
            "loss: 9.327 (epoch: 1, step: 400) // Avg time/img: 0.0548 s\n",
            "loss: 9.227 (epoch: 1, step: 450) // Avg time/img: 0.0548 s\n",
            "----- VALIDATING - EPOCH 1 -----\n",
            "VAL loss: 8.321 (epoch: 1, step: 0) // Avg time/img: 0.0417 s\n",
            "VAL loss: 7.994 (epoch: 1, step: 50) // Avg time/img: 0.0318 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m1.24\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training1/model-001.pth (epoch: 1)\n",
            "save: ../save/enet_training1/model_best.pth (epoch: 1)\n",
            "----- TRAINING - EPOCH 2 -----\n",
            "LEARNING RATE:  4.774426908107499e-05\n",
            "loss: 7.93 (epoch: 2, step: 0) // Avg time/img: 0.0601 s\n",
            "loss: 7.96 (epoch: 2, step: 50) // Avg time/img: 0.0531 s\n",
            "loss: 7.928 (epoch: 2, step: 100) // Avg time/img: 0.0541 s\n",
            "loss: 7.81 (epoch: 2, step: 150) // Avg time/img: 0.0538 s\n",
            "loss: 7.702 (epoch: 2, step: 200) // Avg time/img: 0.0540 s\n",
            "loss: 7.6 (epoch: 2, step: 250) // Avg time/img: 0.0535 s\n",
            "loss: 7.512 (epoch: 2, step: 300) // Avg time/img: 0.0537 s\n",
            "loss: 7.419 (epoch: 2, step: 350) // Avg time/img: 0.0535 s\n",
            "loss: 7.327 (epoch: 2, step: 400) // Avg time/img: 0.0537 s\n",
            "loss: 7.242 (epoch: 2, step: 450) // Avg time/img: 0.0537 s\n",
            "----- VALIDATING - EPOCH 2 -----\n",
            "VAL loss: 6.321 (epoch: 2, step: 0) // Avg time/img: 0.0525 s\n",
            "VAL loss: 6.296 (epoch: 2, step: 50) // Avg time/img: 0.0338 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m2.38\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training1/model-002.pth (epoch: 2)\n",
            "save: ../save/enet_training1/model_best.pth (epoch: 2)\n",
            "----- TRAINING - EPOCH 3 -----\n",
            "LEARNING RATE:  4.547662880414811e-05\n",
            "loss: 6.217 (epoch: 3, step: 0) // Avg time/img: 0.0586 s\n",
            "loss: 6.255 (epoch: 3, step: 50) // Avg time/img: 0.0535 s\n",
            "loss: 6.177 (epoch: 3, step: 100) // Avg time/img: 0.0529 s\n",
            "loss: 6.106 (epoch: 3, step: 150) // Avg time/img: 0.0526 s\n",
            "loss: 6.04 (epoch: 3, step: 200) // Avg time/img: 0.0529 s\n",
            "loss: 5.954 (epoch: 3, step: 250) // Avg time/img: 0.0528 s\n",
            "loss: 5.891 (epoch: 3, step: 300) // Avg time/img: 0.0533 s\n",
            "loss: 5.815 (epoch: 3, step: 350) // Avg time/img: 0.0533 s\n",
            "loss: 5.741 (epoch: 3, step: 400) // Avg time/img: 0.0532 s\n",
            "loss: 5.675 (epoch: 3, step: 450) // Avg time/img: 0.0534 s\n",
            "----- VALIDATING - EPOCH 3 -----\n",
            "VAL loss: 4.804 (epoch: 3, step: 0) // Avg time/img: 0.0549 s\n",
            "VAL loss: 4.963 (epoch: 3, step: 50) // Avg time/img: 0.0338 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m5.06\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training1/model-003.pth (epoch: 3)\n",
            "save: ../save/enet_training1/model_best.pth (epoch: 3)\n",
            "----- TRAINING - EPOCH 4 -----\n",
            "LEARNING RATE:  4.319634861514096e-05\n",
            "loss: 5.254 (epoch: 4, step: 0) // Avg time/img: 0.0636 s\n",
            "loss: 4.952 (epoch: 4, step: 50) // Avg time/img: 0.0542 s\n",
            "loss: 4.894 (epoch: 4, step: 100) // Avg time/img: 0.0541 s\n",
            "loss: 4.842 (epoch: 4, step: 150) // Avg time/img: 0.0537 s\n",
            "loss: 4.747 (epoch: 4, step: 200) // Avg time/img: 0.0537 s\n",
            "loss: 4.703 (epoch: 4, step: 250) // Avg time/img: 0.0537 s\n",
            "loss: 4.639 (epoch: 4, step: 300) // Avg time/img: 0.0536 s\n",
            "loss: 4.591 (epoch: 4, step: 350) // Avg time/img: 0.0539 s\n",
            "loss: 4.537 (epoch: 4, step: 400) // Avg time/img: 0.0540 s\n",
            "loss: 4.491 (epoch: 4, step: 450) // Avg time/img: 0.0541 s\n",
            "----- VALIDATING - EPOCH 4 -----\n",
            "VAL loss: 3.702 (epoch: 4, step: 0) // Avg time/img: 0.0559 s\n",
            "VAL loss: 3.96 (epoch: 4, step: 50) // Avg time/img: 0.0340 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m8.24\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training1/model-004.pth (epoch: 4)\n",
            "save: ../save/enet_training1/model_best.pth (epoch: 4)\n",
            "----- TRAINING - EPOCH 5 -----\n",
            "LEARNING RATE:  4.090260730254292e-05\n",
            "loss: 4.192 (epoch: 5, step: 0) // Avg time/img: 0.0414 s\n",
            "loss: 4.0 (epoch: 5, step: 50) // Avg time/img: 0.0526 s\n",
            "loss: 3.942 (epoch: 5, step: 100) // Avg time/img: 0.0534 s\n",
            "loss: 3.891 (epoch: 5, step: 150) // Avg time/img: 0.0529 s\n",
            "loss: 3.831 (epoch: 5, step: 200) // Avg time/img: 0.0532 s\n",
            "loss: 3.792 (epoch: 5, step: 250) // Avg time/img: 0.0530 s\n",
            "loss: 3.762 (epoch: 5, step: 300) // Avg time/img: 0.0531 s\n",
            "loss: 3.716 (epoch: 5, step: 350) // Avg time/img: 0.0533 s\n",
            "loss: 3.684 (epoch: 5, step: 400) // Avg time/img: 0.0533 s\n",
            "loss: 3.657 (epoch: 5, step: 450) // Avg time/img: 0.0534 s\n",
            "----- VALIDATING - EPOCH 5 -----\n",
            "VAL loss: 3.008 (epoch: 5, step: 0) // Avg time/img: 0.0495 s\n",
            "VAL loss: 3.35 (epoch: 5, step: 50) // Avg time/img: 0.0348 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m12.11\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training1/model-005.pth (epoch: 5)\n",
            "save: ../save/enet_training1/model_best.pth (epoch: 5)\n",
            "----- TRAINING - EPOCH 6 -----\n",
            "LEARNING RATE:  3.859447533617852e-05\n",
            "loss: 2.847 (epoch: 6, step: 0) // Avg time/img: 0.0525 s\n",
            "loss: 3.266 (epoch: 6, step: 50) // Avg time/img: 0.0549 s\n",
            "loss: 3.236 (epoch: 6, step: 100) // Avg time/img: 0.0541 s\n",
            "loss: 3.186 (epoch: 6, step: 150) // Avg time/img: 0.0544 s\n",
            "loss: 3.159 (epoch: 6, step: 200) // Avg time/img: 0.0542 s\n",
            "loss: 3.147 (epoch: 6, step: 250) // Avg time/img: 0.0537 s\n",
            "loss: 3.121 (epoch: 6, step: 300) // Avg time/img: 0.0541 s\n",
            "loss: 3.082 (epoch: 6, step: 350) // Avg time/img: 0.0539 s\n",
            "loss: 3.056 (epoch: 6, step: 400) // Avg time/img: 0.0541 s\n",
            "loss: 3.027 (epoch: 6, step: 450) // Avg time/img: 0.0540 s\n",
            "----- VALIDATING - EPOCH 6 -----\n",
            "VAL loss: 2.454 (epoch: 6, step: 0) // Avg time/img: 0.0584 s\n",
            "VAL loss: 2.802 (epoch: 6, step: 50) // Avg time/img: 0.0339 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m14.85\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training1/model-006.pth (epoch: 6)\n",
            "save: ../save/enet_training1/model_best.pth (epoch: 6)\n",
            "----- TRAINING - EPOCH 7 -----\n",
            "LEARNING RATE:  3.6270892346861e-05\n",
            "loss: 2.981 (epoch: 7, step: 0) // Avg time/img: 0.0772 s\n",
            "loss: 2.752 (epoch: 7, step: 50) // Avg time/img: 0.0566 s\n",
            "loss: 2.704 (epoch: 7, step: 100) // Avg time/img: 0.0551 s\n",
            "loss: 2.685 (epoch: 7, step: 150) // Avg time/img: 0.0554 s\n",
            "loss: 2.645 (epoch: 7, step: 200) // Avg time/img: 0.0550 s\n",
            "loss: 2.626 (epoch: 7, step: 250) // Avg time/img: 0.0552 s\n",
            "loss: 2.619 (epoch: 7, step: 300) // Avg time/img: 0.0549 s\n",
            "loss: 2.582 (epoch: 7, step: 350) // Avg time/img: 0.0548 s\n",
            "loss: 2.56 (epoch: 7, step: 400) // Avg time/img: 0.0547 s\n",
            "loss: 2.537 (epoch: 7, step: 450) // Avg time/img: 0.0545 s\n",
            "----- VALIDATING - EPOCH 7 -----\n",
            "VAL loss: 2.024 (epoch: 7, step: 0) // Avg time/img: 0.0571 s\n",
            "VAL loss: 2.394 (epoch: 7, step: 50) // Avg time/img: 0.0331 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m17.25\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training1/model-007.pth (epoch: 7)\n",
            "save: ../save/enet_training1/model_best.pth (epoch: 7)\n",
            "----- TRAINING - EPOCH 8 -----\n",
            "LEARNING RATE:  3.393063796290625e-05\n",
            "loss: 2.452 (epoch: 8, step: 0) // Avg time/img: 0.0613 s\n",
            "loss: 2.323 (epoch: 8, step: 50) // Avg time/img: 0.0529 s\n",
            "loss: 2.302 (epoch: 8, step: 100) // Avg time/img: 0.0542 s\n",
            "loss: 2.277 (epoch: 8, step: 150) // Avg time/img: 0.0543 s\n",
            "loss: 2.265 (epoch: 8, step: 200) // Avg time/img: 0.0544 s\n",
            "loss: 2.252 (epoch: 8, step: 250) // Avg time/img: 0.0544 s\n",
            "loss: 2.225 (epoch: 8, step: 300) // Avg time/img: 0.0539 s\n",
            "loss: 2.208 (epoch: 8, step: 350) // Avg time/img: 0.0541 s\n",
            "loss: 2.203 (epoch: 8, step: 400) // Avg time/img: 0.0540 s\n",
            "loss: 2.172 (epoch: 8, step: 450) // Avg time/img: 0.0541 s\n",
            "----- VALIDATING - EPOCH 8 -----\n",
            "VAL loss: 1.754 (epoch: 8, step: 0) // Avg time/img: 0.0584 s\n",
            "VAL loss: 2.102 (epoch: 8, step: 50) // Avg time/img: 0.0345 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m18.76\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training1/model-008.pth (epoch: 8)\n",
            "save: ../save/enet_training1/model_best.pth (epoch: 8)\n",
            "----- TRAINING - EPOCH 9 -----\n",
            "LEARNING RATE:  3.157229337446777e-05\n",
            "loss: 1.984 (epoch: 9, step: 0) // Avg time/img: 0.0516 s\n",
            "loss: 2.026 (epoch: 9, step: 50) // Avg time/img: 0.0556 s\n",
            "loss: 2.006 (epoch: 9, step: 100) // Avg time/img: 0.0545 s\n",
            "loss: 1.997 (epoch: 9, step: 150) // Avg time/img: 0.0543 s\n",
            "loss: 1.965 (epoch: 9, step: 200) // Avg time/img: 0.0542 s\n",
            "loss: 1.955 (epoch: 9, step: 250) // Avg time/img: 0.0540 s\n",
            "loss: 1.933 (epoch: 9, step: 300) // Avg time/img: 0.0545 s\n",
            "loss: 1.912 (epoch: 9, step: 350) // Avg time/img: 0.0544 s\n",
            "loss: 1.903 (epoch: 9, step: 400) // Avg time/img: 0.0545 s\n",
            "loss: 1.881 (epoch: 9, step: 450) // Avg time/img: 0.0544 s\n",
            "----- VALIDATING - EPOCH 9 -----\n",
            "VAL loss: 1.539 (epoch: 9, step: 0) // Avg time/img: 0.0760 s\n",
            "VAL loss: 1.862 (epoch: 9, step: 50) // Avg time/img: 0.0329 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m22.78\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training1/model-009.pth (epoch: 9)\n",
            "save: ../save/enet_training1/model_best.pth (epoch: 9)\n",
            "----- TRAINING - EPOCH 10 -----\n",
            "LEARNING RATE:  2.9194189645999014e-05\n",
            "loss: 1.598 (epoch: 10, step: 0) // Avg time/img: 0.0831 s\n",
            "loss: 1.736 (epoch: 10, step: 50) // Avg time/img: 0.0527 s\n",
            "loss: 1.701 (epoch: 10, step: 100) // Avg time/img: 0.0534 s\n",
            "loss: 1.726 (epoch: 10, step: 150) // Avg time/img: 0.0538 s\n",
            "loss: 1.726 (epoch: 10, step: 200) // Avg time/img: 0.0539 s\n",
            "loss: 1.721 (epoch: 10, step: 250) // Avg time/img: 0.0539 s\n",
            "loss: 1.712 (epoch: 10, step: 300) // Avg time/img: 0.0539 s\n",
            "loss: 1.7 (epoch: 10, step: 350) // Avg time/img: 0.0542 s\n",
            "loss: 1.694 (epoch: 10, step: 400) // Avg time/img: 0.0540 s\n",
            "loss: 1.685 (epoch: 10, step: 450) // Avg time/img: 0.0540 s\n",
            "----- VALIDATING - EPOCH 10 -----\n",
            "VAL loss: 1.355 (epoch: 10, step: 0) // Avg time/img: 0.0579 s\n",
            "VAL loss: 1.684 (epoch: 10, step: 50) // Avg time/img: 0.0350 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m25.10\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training1/model-010.pth (epoch: 10)\n",
            "save: ../save/enet_training1/model_best.pth (epoch: 10)\n",
            "----- TRAINING - EPOCH 11 -----\n",
            "LEARNING RATE:  2.679433656340733e-05\n",
            "loss: 1.355 (epoch: 11, step: 0) // Avg time/img: 0.0565 s\n",
            "loss: 1.636 (epoch: 11, step: 50) // Avg time/img: 0.0520 s\n",
            "loss: 1.587 (epoch: 11, step: 100) // Avg time/img: 0.0540 s\n",
            "loss: 1.562 (epoch: 11, step: 150) // Avg time/img: 0.0538 s\n",
            "loss: 1.558 (epoch: 11, step: 200) // Avg time/img: 0.0541 s\n",
            "loss: 1.541 (epoch: 11, step: 250) // Avg time/img: 0.0539 s\n",
            "loss: 1.538 (epoch: 11, step: 300) // Avg time/img: 0.0539 s\n",
            "loss: 1.527 (epoch: 11, step: 350) // Avg time/img: 0.0542 s\n",
            "loss: 1.52 (epoch: 11, step: 400) // Avg time/img: 0.0540 s\n",
            "loss: 1.512 (epoch: 11, step: 450) // Avg time/img: 0.0540 s\n",
            "----- VALIDATING - EPOCH 11 -----\n",
            "VAL loss: 1.227 (epoch: 11, step: 0) // Avg time/img: 0.0578 s\n",
            "VAL loss: 1.548 (epoch: 11, step: 50) // Avg time/img: 0.0354 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m26.59\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training1/model-011.pth (epoch: 11)\n",
            "save: ../save/enet_training1/model_best.pth (epoch: 11)\n",
            "----- TRAINING - EPOCH 12 -----\n",
            "LEARNING RATE:  2.437032195894977e-05\n",
            "loss: 1.219 (epoch: 12, step: 0) // Avg time/img: 0.0555 s\n",
            "loss: 1.434 (epoch: 12, step: 50) // Avg time/img: 0.0517 s\n",
            "loss: 1.442 (epoch: 12, step: 100) // Avg time/img: 0.0524 s\n",
            "loss: 1.45 (epoch: 12, step: 150) // Avg time/img: 0.0526 s\n",
            "loss: 1.437 (epoch: 12, step: 200) // Avg time/img: 0.0532 s\n",
            "loss: 1.415 (epoch: 12, step: 250) // Avg time/img: 0.0531 s\n",
            "loss: 1.416 (epoch: 12, step: 300) // Avg time/img: 0.0534 s\n",
            "loss: 1.4 (epoch: 12, step: 350) // Avg time/img: 0.0533 s\n",
            "loss: 1.389 (epoch: 12, step: 400) // Avg time/img: 0.0532 s\n",
            "loss: 1.387 (epoch: 12, step: 450) // Avg time/img: 0.0532 s\n",
            "----- VALIDATING - EPOCH 12 -----\n",
            "VAL loss: 1.138 (epoch: 12, step: 0) // Avg time/img: 0.0574 s\n",
            "VAL loss: 1.443 (epoch: 12, step: 50) // Avg time/img: 0.0327 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m28.11\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training1/model-012.pth (epoch: 12)\n",
            "save: ../save/enet_training1/model_best.pth (epoch: 12)\n",
            "----- TRAINING - EPOCH 13 -----\n",
            "LEARNING RATE:  2.191916452770435e-05\n",
            "loss: 1.304 (epoch: 13, step: 0) // Avg time/img: 0.0546 s\n",
            "loss: 1.334 (epoch: 13, step: 50) // Avg time/img: 0.0547 s\n",
            "loss: 1.306 (epoch: 13, step: 100) // Avg time/img: 0.0540 s\n",
            "loss: 1.319 (epoch: 13, step: 150) // Avg time/img: 0.0541 s\n",
            "loss: 1.319 (epoch: 13, step: 200) // Avg time/img: 0.0540 s\n",
            "loss: 1.299 (epoch: 13, step: 250) // Avg time/img: 0.0541 s\n",
            "loss: 1.295 (epoch: 13, step: 300) // Avg time/img: 0.0541 s\n",
            "loss: 1.287 (epoch: 13, step: 350) // Avg time/img: 0.0538 s\n",
            "loss: 1.279 (epoch: 13, step: 400) // Avg time/img: 0.0541 s\n",
            "loss: 1.278 (epoch: 13, step: 450) // Avg time/img: 0.0541 s\n",
            "----- VALIDATING - EPOCH 13 -----\n",
            "VAL loss: 1.03 (epoch: 13, step: 0) // Avg time/img: 0.0693 s\n",
            "VAL loss: 1.342 (epoch: 13, step: 50) // Avg time/img: 0.0359 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m29.28\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training1/model-013.pth (epoch: 13)\n",
            "save: ../save/enet_training1/model_best.pth (epoch: 13)\n",
            "----- TRAINING - EPOCH 14 -----\n",
            "LEARNING RATE:  1.9437089939938174e-05\n",
            "loss: 1.462 (epoch: 14, step: 0) // Avg time/img: 0.0675 s\n",
            "loss: 1.215 (epoch: 14, step: 50) // Avg time/img: 0.0552 s\n",
            "loss: 1.192 (epoch: 14, step: 100) // Avg time/img: 0.0544 s\n",
            "loss: 1.201 (epoch: 14, step: 150) // Avg time/img: 0.0545 s\n",
            "loss: 1.193 (epoch: 14, step: 200) // Avg time/img: 0.0546 s\n",
            "loss: 1.196 (epoch: 14, step: 250) // Avg time/img: 0.0551 s\n",
            "loss: 1.206 (epoch: 14, step: 300) // Avg time/img: 0.0545 s\n",
            "loss: 1.2 (epoch: 14, step: 350) // Avg time/img: 0.0548 s\n",
            "loss: 1.199 (epoch: 14, step: 400) // Avg time/img: 0.0546 s\n",
            "loss: 1.201 (epoch: 14, step: 450) // Avg time/img: 0.0546 s\n",
            "----- VALIDATING - EPOCH 14 -----\n",
            "VAL loss: 0.9944 (epoch: 14, step: 0) // Avg time/img: 0.0537 s\n",
            "VAL loss: 1.267 (epoch: 14, step: 50) // Avg time/img: 0.0343 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m30.69\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training1/model-014.pth (epoch: 14)\n",
            "save: ../save/enet_training1/model_best.pth (epoch: 14)\n",
            "----- TRAINING - EPOCH 15 -----\n",
            "LEARNING RATE:  1.6919173095082493e-05\n",
            "loss: 1.076 (epoch: 15, step: 0) // Avg time/img: 0.0529 s\n",
            "loss: 1.125 (epoch: 15, step: 50) // Avg time/img: 0.0529 s\n",
            "loss: 1.137 (epoch: 15, step: 100) // Avg time/img: 0.0548 s\n",
            "loss: 1.152 (epoch: 15, step: 150) // Avg time/img: 0.0543 s\n",
            "loss: 1.156 (epoch: 15, step: 200) // Avg time/img: 0.0548 s\n",
            "loss: 1.148 (epoch: 15, step: 250) // Avg time/img: 0.0547 s\n",
            "loss: 1.145 (epoch: 15, step: 300) // Avg time/img: 0.0549 s\n",
            "loss: 1.136 (epoch: 15, step: 350) // Avg time/img: 0.0547 s\n",
            "loss: 1.133 (epoch: 15, step: 400) // Avg time/img: 0.0547 s\n",
            "loss: 1.127 (epoch: 15, step: 450) // Avg time/img: 0.0548 s\n",
            "----- VALIDATING - EPOCH 15 -----\n",
            "VAL loss: 0.9062 (epoch: 15, step: 0) // Avg time/img: 0.0501 s\n",
            "VAL loss: 1.202 (epoch: 15, step: 50) // Avg time/img: 0.0352 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m31.87\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training1/model-015.pth (epoch: 15)\n",
            "save: ../save/enet_training1/model_best.pth (epoch: 15)\n",
            "----- TRAINING - EPOCH 16 -----\n",
            "LEARNING RATE:  1.4358729437462937e-05\n",
            "loss: 1.037 (epoch: 16, step: 0) // Avg time/img: 0.0593 s\n",
            "loss: 1.129 (epoch: 16, step: 50) // Avg time/img: 0.0555 s\n",
            "loss: 1.119 (epoch: 16, step: 100) // Avg time/img: 0.0546 s\n",
            "loss: 1.089 (epoch: 16, step: 150) // Avg time/img: 0.0549 s\n",
            "loss: 1.086 (epoch: 16, step: 200) // Avg time/img: 0.0547 s\n",
            "loss: 1.072 (epoch: 16, step: 250) // Avg time/img: 0.0548 s\n",
            "loss: 1.063 (epoch: 16, step: 300) // Avg time/img: 0.0547 s\n",
            "loss: 1.069 (epoch: 16, step: 350) // Avg time/img: 0.0549 s\n",
            "loss: 1.069 (epoch: 16, step: 400) // Avg time/img: 0.0547 s\n",
            "loss: 1.067 (epoch: 16, step: 450) // Avg time/img: 0.0550 s\n",
            "----- VALIDATING - EPOCH 16 -----\n",
            "VAL loss: 0.8613 (epoch: 16, step: 0) // Avg time/img: 0.0559 s\n",
            "VAL loss: 1.158 (epoch: 16, step: 50) // Avg time/img: 0.0348 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m32.83\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training1/model-016.pth (epoch: 16)\n",
            "save: ../save/enet_training1/model_best.pth (epoch: 16)\n",
            "----- TRAINING - EPOCH 17 -----\n",
            "LEARNING RATE:  1.1746189430880188e-05\n",
            "loss: 0.8075 (epoch: 17, step: 0) // Avg time/img: 0.0529 s\n",
            "loss: 1.077 (epoch: 17, step: 50) // Avg time/img: 0.0544 s\n",
            "loss: 1.07 (epoch: 17, step: 100) // Avg time/img: 0.0550 s\n",
            "loss: 1.053 (epoch: 17, step: 150) // Avg time/img: 0.0553 s\n",
            "loss: 1.039 (epoch: 17, step: 200) // Avg time/img: 0.0560 s\n",
            "loss: 1.025 (epoch: 17, step: 250) // Avg time/img: 0.0555 s\n",
            "loss: 1.023 (epoch: 17, step: 300) // Avg time/img: 0.0559 s\n",
            "loss: 1.023 (epoch: 17, step: 350) // Avg time/img: 0.0556 s\n",
            "loss: 1.025 (epoch: 17, step: 400) // Avg time/img: 0.0555 s\n",
            "loss: 1.019 (epoch: 17, step: 450) // Avg time/img: 0.0552 s\n",
            "----- VALIDATING - EPOCH 17 -----\n",
            "VAL loss: 0.8445 (epoch: 17, step: 0) // Avg time/img: 0.0481 s\n",
            "VAL loss: 1.118 (epoch: 17, step: 50) // Avg time/img: 0.0333 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m33.44\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training1/model-017.pth (epoch: 17)\n",
            "save: ../save/enet_training1/model_best.pth (epoch: 17)\n",
            "----- TRAINING - EPOCH 18 -----\n",
            "LEARNING RATE:  9.066760365683729e-06\n",
            "loss: 1.003 (epoch: 18, step: 0) // Avg time/img: 0.0772 s\n",
            "loss: 0.9728 (epoch: 18, step: 50) // Avg time/img: 0.0537 s\n",
            "loss: 0.9814 (epoch: 18, step: 100) // Avg time/img: 0.0543 s\n",
            "loss: 0.9928 (epoch: 18, step: 150) // Avg time/img: 0.0548 s\n",
            "loss: 0.9875 (epoch: 18, step: 200) // Avg time/img: 0.0546 s\n",
            "loss: 0.9903 (epoch: 18, step: 250) // Avg time/img: 0.0547 s\n",
            "loss: 0.987 (epoch: 18, step: 300) // Avg time/img: 0.0546 s\n",
            "loss: 0.9849 (epoch: 18, step: 350) // Avg time/img: 0.0546 s\n",
            "loss: 0.9821 (epoch: 18, step: 400) // Avg time/img: 0.0545 s\n",
            "loss: 0.9828 (epoch: 18, step: 450) // Avg time/img: 0.0548 s\n",
            "----- VALIDATING - EPOCH 18 -----\n",
            "VAL loss: 0.8267 (epoch: 18, step: 0) // Avg time/img: 0.0498 s\n",
            "VAL loss: 1.095 (epoch: 18, step: 50) // Avg time/img: 0.0350 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m33.71\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training1/model-018.pth (epoch: 18)\n",
            "save: ../save/enet_training1/model_best.pth (epoch: 18)\n",
            "----- TRAINING - EPOCH 19 -----\n",
            "LEARNING RATE:  6.294627058970836e-06\n",
            "loss: 0.8197 (epoch: 19, step: 0) // Avg time/img: 0.0633 s\n",
            "loss: 0.9671 (epoch: 19, step: 50) // Avg time/img: 0.0529 s\n",
            "loss: 0.984 (epoch: 19, step: 100) // Avg time/img: 0.0538 s\n",
            "loss: 0.9722 (epoch: 19, step: 150) // Avg time/img: 0.0541 s\n",
            "loss: 0.9649 (epoch: 19, step: 200) // Avg time/img: 0.0544 s\n",
            "loss: 0.9679 (epoch: 19, step: 250) // Avg time/img: 0.0540 s\n",
            "loss: 0.9711 (epoch: 19, step: 300) // Avg time/img: 0.0540 s\n",
            "loss: 0.9675 (epoch: 19, step: 350) // Avg time/img: 0.0540 s\n",
            "loss: 0.9665 (epoch: 19, step: 400) // Avg time/img: 0.0544 s\n",
            "loss: 0.9619 (epoch: 19, step: 450) // Avg time/img: 0.0546 s\n",
            "----- VALIDATING - EPOCH 19 -----\n",
            "VAL loss: 0.792 (epoch: 19, step: 0) // Avg time/img: 0.0560 s\n",
            "VAL loss: 1.067 (epoch: 19, step: 50) // Avg time/img: 0.0350 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m34.07\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training1/model-019.pth (epoch: 19)\n",
            "save: ../save/enet_training1/model_best.pth (epoch: 19)\n",
            "----- TRAINING - EPOCH 20 -----\n",
            "LEARNING RATE:  3.373207119183911e-06\n",
            "loss: 0.8169 (epoch: 20, step: 0) // Avg time/img: 0.0620 s\n",
            "loss: 0.9404 (epoch: 20, step: 50) // Avg time/img: 0.0537 s\n",
            "loss: 0.9419 (epoch: 20, step: 100) // Avg time/img: 0.0538 s\n",
            "loss: 0.9288 (epoch: 20, step: 150) // Avg time/img: 0.0547 s\n",
            "loss: 0.9415 (epoch: 20, step: 200) // Avg time/img: 0.0550 s\n",
            "loss: 0.952 (epoch: 20, step: 250) // Avg time/img: 0.0551 s\n",
            "loss: 0.9574 (epoch: 20, step: 300) // Avg time/img: 0.0550 s\n",
            "loss: 0.9569 (epoch: 20, step: 350) // Avg time/img: 0.0552 s\n",
            "loss: 0.9585 (epoch: 20, step: 400) // Avg time/img: 0.0552 s\n",
            "loss: 0.9473 (epoch: 20, step: 450) // Avg time/img: 0.0552 s\n",
            "----- VALIDATING - EPOCH 20 -----\n",
            "VAL loss: 0.7734 (epoch: 20, step: 0) // Avg time/img: 0.0648 s\n",
            "VAL loss: 1.056 (epoch: 20, step: 50) // Avg time/img: 0.0374 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m34.19\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training1/model-020.pth (epoch: 20)\n",
            "save: ../save/enet_training1/model_best.pth (epoch: 20)\n",
            "========== TRAINING FINISHED ===========\n",
            "updating: content/AnomalySegmentation/save/enet_training1/ (stored 0%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/automated_log.txt (deflated 66%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model.txt (deflated 96%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/enet.py (deflated 84%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/opts.txt (deflated 37%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model_best.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-002.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-001.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-003.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-006.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-010.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-005.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-007.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/best.txt (stored 0%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model_best.pth.tar (deflated 19%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-008.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-009.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/checkpoint.pth.tar (deflated 19%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-004.pth (deflated 18%)\n",
            "  adding: content/AnomalySegmentation/save/enet_training1/model-014.pth (deflated 18%)\n",
            "  adding: content/AnomalySegmentation/save/enet_training1/model-011.pth (deflated 18%)\n",
            "  adding: content/AnomalySegmentation/save/enet_training1/model-020.pth (deflated 18%)\n",
            "  adding: content/AnomalySegmentation/save/enet_training1/model-015.pth (deflated 18%)\n",
            "  adding: content/AnomalySegmentation/save/enet_training1/model-012.pth (deflated 18%)\n",
            "  adding: content/AnomalySegmentation/save/enet_training1/model-018.pth (deflated 18%)\n",
            "  adding: content/AnomalySegmentation/save/enet_training1/model-016.pth (deflated 18%)\n",
            "  adding: content/AnomalySegmentation/save/enet_training1/model-019.pth (deflated 18%)\n",
            "  adding: content/AnomalySegmentation/save/enet_training1/model-013.pth (deflated 18%)\n",
            "  adding: content/AnomalySegmentation/save/enet_training1/model-017.pth (deflated 18%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r save_enet_training1.zip /content/AnomalySegmentation/save/enet_training1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYLM7axZRkRx",
        "outputId": "5ffbe005-ee11-4208-aad9-d94a56bcf9d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updating: content/AnomalySegmentation/save/enet_training1/ (stored 0%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/automated_log.txt (deflated 66%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model.txt (deflated 96%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/enet.py (deflated 84%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/opts.txt (deflated 37%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model_best.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-002.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-001.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-003.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-006.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-010.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-005.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-007.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/best.txt (stored 0%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model_best.pth.tar (deflated 19%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-008.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-009.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/checkpoint.pth.tar (deflated 19%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-004.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-014.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-011.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-020.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-015.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-012.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-018.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-016.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-019.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-013.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-017.pth (deflated 18%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####BISENET"
      ],
      "metadata": {
        "id": "Cm1ITQbYM9n7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/AnomalySegmentation && git pull"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f80b9df-737a-492a-f51b-2daa5e46cb65",
        "collapsed": true,
        "id": "wyZLcBENT_Hy"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "remote: Enumerating objects: 7, done.\u001b[K\n",
            "remote: Counting objects:  14% (1/7)\u001b[K\rremote: Counting objects:  28% (2/7)\u001b[K\rremote: Counting objects:  42% (3/7)\u001b[K\rremote: Counting objects:  57% (4/7)\u001b[K\rremote: Counting objects:  71% (5/7)\u001b[K\rremote: Counting objects:  85% (6/7)\u001b[K\rremote: Counting objects: 100% (7/7)\u001b[K\rremote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1/1)\u001b[K\rremote: Compressing objects: 100% (1/1), done.\u001b[K\n",
            "remote: Total 4 (delta 3), reused 4 (delta 3), pack-reused 0 (from 0)\u001b[K\n",
            "Unpacking objects:  25% (1/4)\rUnpacking objects:  50% (2/4)\rUnpacking objects:  75% (3/4)\rUnpacking objects: 100% (4/4)\rUnpacking objects: 100% (4/4), 358 bytes | 179.00 KiB/s, done.\n",
            "From https://github.com/RonPlusSign/AnomalySegmentation\n",
            "   07c6057..bf241fd  main       -> origin/main\n",
            "Updating 07c6057..bf241fd\n",
            "Fast-forward\n",
            " train/main.py | 2 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " 1 file changed, 1 insertion(+), 1 deletion(-)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Finetune BiSeNet (20 epochs)\n",
        "!cd /content/AnomalySegmentation/train; python -W ignore main.py --savedir bisenet_training1 --datadir /content/cityscapes --model bisenet --cuda --num-epochs=20 --epochs-save=1 --FineTune --loadWeights=bisenetv1_pretrained.pth\n",
        "!zip -r save_bisenet_training1.zip /content/AnomalySegmentation/save/bisenet_training1"
      ],
      "metadata": {
        "id": "h-ftZ-p1Yn9s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f13ccc48-93d2-4527-e695-a693a2af691d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "odict_keys(['cp.resnet.conv1.weight', 'cp.resnet.bn1.weight', 'cp.resnet.bn1.bias', 'cp.resnet.bn1.running_mean', 'cp.resnet.bn1.running_var', 'cp.resnet.bn1.num_batches_tracked', 'cp.resnet.layer1.0.conv1.weight', 'cp.resnet.layer1.0.bn1.weight', 'cp.resnet.layer1.0.bn1.bias', 'cp.resnet.layer1.0.bn1.running_mean', 'cp.resnet.layer1.0.bn1.running_var', 'cp.resnet.layer1.0.bn1.num_batches_tracked', 'cp.resnet.layer1.0.conv2.weight', 'cp.resnet.layer1.0.bn2.weight', 'cp.resnet.layer1.0.bn2.bias', 'cp.resnet.layer1.0.bn2.running_mean', 'cp.resnet.layer1.0.bn2.running_var', 'cp.resnet.layer1.0.bn2.num_batches_tracked', 'cp.resnet.layer1.1.conv1.weight', 'cp.resnet.layer1.1.bn1.weight', 'cp.resnet.layer1.1.bn1.bias', 'cp.resnet.layer1.1.bn1.running_mean', 'cp.resnet.layer1.1.bn1.running_var', 'cp.resnet.layer1.1.bn1.num_batches_tracked', 'cp.resnet.layer1.1.conv2.weight', 'cp.resnet.layer1.1.bn2.weight', 'cp.resnet.layer1.1.bn2.bias', 'cp.resnet.layer1.1.bn2.running_mean', 'cp.resnet.layer1.1.bn2.running_var', 'cp.resnet.layer1.1.bn2.num_batches_tracked', 'cp.resnet.layer2.0.conv1.weight', 'cp.resnet.layer2.0.bn1.weight', 'cp.resnet.layer2.0.bn1.bias', 'cp.resnet.layer2.0.bn1.running_mean', 'cp.resnet.layer2.0.bn1.running_var', 'cp.resnet.layer2.0.bn1.num_batches_tracked', 'cp.resnet.layer2.0.conv2.weight', 'cp.resnet.layer2.0.bn2.weight', 'cp.resnet.layer2.0.bn2.bias', 'cp.resnet.layer2.0.bn2.running_mean', 'cp.resnet.layer2.0.bn2.running_var', 'cp.resnet.layer2.0.bn2.num_batches_tracked', 'cp.resnet.layer2.0.downsample.0.weight', 'cp.resnet.layer2.0.downsample.1.weight', 'cp.resnet.layer2.0.downsample.1.bias', 'cp.resnet.layer2.0.downsample.1.running_mean', 'cp.resnet.layer2.0.downsample.1.running_var', 'cp.resnet.layer2.0.downsample.1.num_batches_tracked', 'cp.resnet.layer2.1.conv1.weight', 'cp.resnet.layer2.1.bn1.weight', 'cp.resnet.layer2.1.bn1.bias', 'cp.resnet.layer2.1.bn1.running_mean', 'cp.resnet.layer2.1.bn1.running_var', 'cp.resnet.layer2.1.bn1.num_batches_tracked', 'cp.resnet.layer2.1.conv2.weight', 'cp.resnet.layer2.1.bn2.weight', 'cp.resnet.layer2.1.bn2.bias', 'cp.resnet.layer2.1.bn2.running_mean', 'cp.resnet.layer2.1.bn2.running_var', 'cp.resnet.layer2.1.bn2.num_batches_tracked', 'cp.resnet.layer3.0.conv1.weight', 'cp.resnet.layer3.0.bn1.weight', 'cp.resnet.layer3.0.bn1.bias', 'cp.resnet.layer3.0.bn1.running_mean', 'cp.resnet.layer3.0.bn1.running_var', 'cp.resnet.layer3.0.bn1.num_batches_tracked', 'cp.resnet.layer3.0.conv2.weight', 'cp.resnet.layer3.0.bn2.weight', 'cp.resnet.layer3.0.bn2.bias', 'cp.resnet.layer3.0.bn2.running_mean', 'cp.resnet.layer3.0.bn2.running_var', 'cp.resnet.layer3.0.bn2.num_batches_tracked', 'cp.resnet.layer3.0.downsample.0.weight', 'cp.resnet.layer3.0.downsample.1.weight', 'cp.resnet.layer3.0.downsample.1.bias', 'cp.resnet.layer3.0.downsample.1.running_mean', 'cp.resnet.layer3.0.downsample.1.running_var', 'cp.resnet.layer3.0.downsample.1.num_batches_tracked', 'cp.resnet.layer3.1.conv1.weight', 'cp.resnet.layer3.1.bn1.weight', 'cp.resnet.layer3.1.bn1.bias', 'cp.resnet.layer3.1.bn1.running_mean', 'cp.resnet.layer3.1.bn1.running_var', 'cp.resnet.layer3.1.bn1.num_batches_tracked', 'cp.resnet.layer3.1.conv2.weight', 'cp.resnet.layer3.1.bn2.weight', 'cp.resnet.layer3.1.bn2.bias', 'cp.resnet.layer3.1.bn2.running_mean', 'cp.resnet.layer3.1.bn2.running_var', 'cp.resnet.layer3.1.bn2.num_batches_tracked', 'cp.resnet.layer4.0.conv1.weight', 'cp.resnet.layer4.0.bn1.weight', 'cp.resnet.layer4.0.bn1.bias', 'cp.resnet.layer4.0.bn1.running_mean', 'cp.resnet.layer4.0.bn1.running_var', 'cp.resnet.layer4.0.bn1.num_batches_tracked', 'cp.resnet.layer4.0.conv2.weight', 'cp.resnet.layer4.0.bn2.weight', 'cp.resnet.layer4.0.bn2.bias', 'cp.resnet.layer4.0.bn2.running_mean', 'cp.resnet.layer4.0.bn2.running_var', 'cp.resnet.layer4.0.bn2.num_batches_tracked', 'cp.resnet.layer4.0.downsample.0.weight', 'cp.resnet.layer4.0.downsample.1.weight', 'cp.resnet.layer4.0.downsample.1.bias', 'cp.resnet.layer4.0.downsample.1.running_mean', 'cp.resnet.layer4.0.downsample.1.running_var', 'cp.resnet.layer4.0.downsample.1.num_batches_tracked', 'cp.resnet.layer4.1.conv1.weight', 'cp.resnet.layer4.1.bn1.weight', 'cp.resnet.layer4.1.bn1.bias', 'cp.resnet.layer4.1.bn1.running_mean', 'cp.resnet.layer4.1.bn1.running_var', 'cp.resnet.layer4.1.bn1.num_batches_tracked', 'cp.resnet.layer4.1.conv2.weight', 'cp.resnet.layer4.1.bn2.weight', 'cp.resnet.layer4.1.bn2.bias', 'cp.resnet.layer4.1.bn2.running_mean', 'cp.resnet.layer4.1.bn2.running_var', 'cp.resnet.layer4.1.bn2.num_batches_tracked', 'cp.arm16.conv.conv.weight', 'cp.arm16.conv.bn.weight', 'cp.arm16.conv.bn.bias', 'cp.arm16.conv.bn.running_mean', 'cp.arm16.conv.bn.running_var', 'cp.arm16.conv.bn.num_batches_tracked', 'cp.arm16.conv_atten.weight', 'cp.arm16.bn_atten.weight', 'cp.arm16.bn_atten.bias', 'cp.arm16.bn_atten.running_mean', 'cp.arm16.bn_atten.running_var', 'cp.arm16.bn_atten.num_batches_tracked', 'cp.arm32.conv.conv.weight', 'cp.arm32.conv.bn.weight', 'cp.arm32.conv.bn.bias', 'cp.arm32.conv.bn.running_mean', 'cp.arm32.conv.bn.running_var', 'cp.arm32.conv.bn.num_batches_tracked', 'cp.arm32.conv_atten.weight', 'cp.arm32.bn_atten.weight', 'cp.arm32.bn_atten.bias', 'cp.arm32.bn_atten.running_mean', 'cp.arm32.bn_atten.running_var', 'cp.arm32.bn_atten.num_batches_tracked', 'cp.conv_head32.conv.weight', 'cp.conv_head32.bn.weight', 'cp.conv_head32.bn.bias', 'cp.conv_head32.bn.running_mean', 'cp.conv_head32.bn.running_var', 'cp.conv_head32.bn.num_batches_tracked', 'cp.conv_head16.conv.weight', 'cp.conv_head16.bn.weight', 'cp.conv_head16.bn.bias', 'cp.conv_head16.bn.running_mean', 'cp.conv_head16.bn.running_var', 'cp.conv_head16.bn.num_batches_tracked', 'cp.conv_avg.conv.weight', 'cp.conv_avg.bn.weight', 'cp.conv_avg.bn.bias', 'cp.conv_avg.bn.running_mean', 'cp.conv_avg.bn.running_var', 'cp.conv_avg.bn.num_batches_tracked', 'sp.conv1.conv.weight', 'sp.conv1.bn.weight', 'sp.conv1.bn.bias', 'sp.conv1.bn.running_mean', 'sp.conv1.bn.running_var', 'sp.conv1.bn.num_batches_tracked', 'sp.conv2.conv.weight', 'sp.conv2.bn.weight', 'sp.conv2.bn.bias', 'sp.conv2.bn.running_mean', 'sp.conv2.bn.running_var', 'sp.conv2.bn.num_batches_tracked', 'sp.conv3.conv.weight', 'sp.conv3.bn.weight', 'sp.conv3.bn.bias', 'sp.conv3.bn.running_mean', 'sp.conv3.bn.running_var', 'sp.conv3.bn.num_batches_tracked', 'sp.conv_out.conv.weight', 'sp.conv_out.bn.weight', 'sp.conv_out.bn.bias', 'sp.conv_out.bn.running_mean', 'sp.conv_out.bn.running_var', 'sp.conv_out.bn.num_batches_tracked', 'ffm.convblk.conv.weight', 'ffm.convblk.bn.weight', 'ffm.convblk.bn.bias', 'ffm.convblk.bn.running_mean', 'ffm.convblk.bn.running_var', 'ffm.convblk.bn.num_batches_tracked', 'ffm.conv.weight', 'ffm.bn.weight', 'ffm.bn.bias', 'ffm.bn.running_mean', 'ffm.bn.running_var', 'ffm.bn.num_batches_tracked', 'conv_out.conv.conv.weight', 'conv_out.conv.bn.weight', 'conv_out.conv.bn.bias', 'conv_out.conv.bn.running_mean', 'conv_out.conv.bn.running_var', 'conv_out.conv.bn.num_batches_tracked', 'conv_out.conv_out.weight', 'conv_out.conv_out.bias', 'conv_out16.conv.conv.weight', 'conv_out16.conv.bn.weight', 'conv_out16.conv.bn.bias', 'conv_out16.conv.bn.running_mean', 'conv_out16.conv.bn.running_var', 'conv_out16.conv.bn.num_batches_tracked', 'conv_out16.conv_out.weight', 'conv_out16.conv_out.bias', 'conv_out32.conv.conv.weight', 'conv_out32.conv.bn.weight', 'conv_out32.conv.bn.bias', 'conv_out32.conv.bn.running_mean', 'conv_out32.conv.bn.running_var', 'conv_out32.conv.bn.num_batches_tracked', 'conv_out32.conv_out.weight', 'conv_out32.conv_out.bias'])\n",
            "odict_keys(['cp.resnet.conv1.weight', 'cp.resnet.bn1.weight', 'cp.resnet.bn1.bias', 'cp.resnet.bn1.running_mean', 'cp.resnet.bn1.running_var', 'cp.resnet.bn1.num_batches_tracked', 'cp.resnet.layer1.0.conv1.weight', 'cp.resnet.layer1.0.bn1.weight', 'cp.resnet.layer1.0.bn1.bias', 'cp.resnet.layer1.0.bn1.running_mean', 'cp.resnet.layer1.0.bn1.running_var', 'cp.resnet.layer1.0.bn1.num_batches_tracked', 'cp.resnet.layer1.0.conv2.weight', 'cp.resnet.layer1.0.bn2.weight', 'cp.resnet.layer1.0.bn2.bias', 'cp.resnet.layer1.0.bn2.running_mean', 'cp.resnet.layer1.0.bn2.running_var', 'cp.resnet.layer1.0.bn2.num_batches_tracked', 'cp.resnet.layer1.1.conv1.weight', 'cp.resnet.layer1.1.bn1.weight', 'cp.resnet.layer1.1.bn1.bias', 'cp.resnet.layer1.1.bn1.running_mean', 'cp.resnet.layer1.1.bn1.running_var', 'cp.resnet.layer1.1.bn1.num_batches_tracked', 'cp.resnet.layer1.1.conv2.weight', 'cp.resnet.layer1.1.bn2.weight', 'cp.resnet.layer1.1.bn2.bias', 'cp.resnet.layer1.1.bn2.running_mean', 'cp.resnet.layer1.1.bn2.running_var', 'cp.resnet.layer1.1.bn2.num_batches_tracked', 'cp.resnet.layer2.0.conv1.weight', 'cp.resnet.layer2.0.bn1.weight', 'cp.resnet.layer2.0.bn1.bias', 'cp.resnet.layer2.0.bn1.running_mean', 'cp.resnet.layer2.0.bn1.running_var', 'cp.resnet.layer2.0.bn1.num_batches_tracked', 'cp.resnet.layer2.0.conv2.weight', 'cp.resnet.layer2.0.bn2.weight', 'cp.resnet.layer2.0.bn2.bias', 'cp.resnet.layer2.0.bn2.running_mean', 'cp.resnet.layer2.0.bn2.running_var', 'cp.resnet.layer2.0.bn2.num_batches_tracked', 'cp.resnet.layer2.0.downsample.0.weight', 'cp.resnet.layer2.0.downsample.1.weight', 'cp.resnet.layer2.0.downsample.1.bias', 'cp.resnet.layer2.0.downsample.1.running_mean', 'cp.resnet.layer2.0.downsample.1.running_var', 'cp.resnet.layer2.0.downsample.1.num_batches_tracked', 'cp.resnet.layer2.1.conv1.weight', 'cp.resnet.layer2.1.bn1.weight', 'cp.resnet.layer2.1.bn1.bias', 'cp.resnet.layer2.1.bn1.running_mean', 'cp.resnet.layer2.1.bn1.running_var', 'cp.resnet.layer2.1.bn1.num_batches_tracked', 'cp.resnet.layer2.1.conv2.weight', 'cp.resnet.layer2.1.bn2.weight', 'cp.resnet.layer2.1.bn2.bias', 'cp.resnet.layer2.1.bn2.running_mean', 'cp.resnet.layer2.1.bn2.running_var', 'cp.resnet.layer2.1.bn2.num_batches_tracked', 'cp.resnet.layer3.0.conv1.weight', 'cp.resnet.layer3.0.bn1.weight', 'cp.resnet.layer3.0.bn1.bias', 'cp.resnet.layer3.0.bn1.running_mean', 'cp.resnet.layer3.0.bn1.running_var', 'cp.resnet.layer3.0.bn1.num_batches_tracked', 'cp.resnet.layer3.0.conv2.weight', 'cp.resnet.layer3.0.bn2.weight', 'cp.resnet.layer3.0.bn2.bias', 'cp.resnet.layer3.0.bn2.running_mean', 'cp.resnet.layer3.0.bn2.running_var', 'cp.resnet.layer3.0.bn2.num_batches_tracked', 'cp.resnet.layer3.0.downsample.0.weight', 'cp.resnet.layer3.0.downsample.1.weight', 'cp.resnet.layer3.0.downsample.1.bias', 'cp.resnet.layer3.0.downsample.1.running_mean', 'cp.resnet.layer3.0.downsample.1.running_var', 'cp.resnet.layer3.0.downsample.1.num_batches_tracked', 'cp.resnet.layer3.1.conv1.weight', 'cp.resnet.layer3.1.bn1.weight', 'cp.resnet.layer3.1.bn1.bias', 'cp.resnet.layer3.1.bn1.running_mean', 'cp.resnet.layer3.1.bn1.running_var', 'cp.resnet.layer3.1.bn1.num_batches_tracked', 'cp.resnet.layer3.1.conv2.weight', 'cp.resnet.layer3.1.bn2.weight', 'cp.resnet.layer3.1.bn2.bias', 'cp.resnet.layer3.1.bn2.running_mean', 'cp.resnet.layer3.1.bn2.running_var', 'cp.resnet.layer3.1.bn2.num_batches_tracked', 'cp.resnet.layer4.0.conv1.weight', 'cp.resnet.layer4.0.bn1.weight', 'cp.resnet.layer4.0.bn1.bias', 'cp.resnet.layer4.0.bn1.running_mean', 'cp.resnet.layer4.0.bn1.running_var', 'cp.resnet.layer4.0.bn1.num_batches_tracked', 'cp.resnet.layer4.0.conv2.weight', 'cp.resnet.layer4.0.bn2.weight', 'cp.resnet.layer4.0.bn2.bias', 'cp.resnet.layer4.0.bn2.running_mean', 'cp.resnet.layer4.0.bn2.running_var', 'cp.resnet.layer4.0.bn2.num_batches_tracked', 'cp.resnet.layer4.0.downsample.0.weight', 'cp.resnet.layer4.0.downsample.1.weight', 'cp.resnet.layer4.0.downsample.1.bias', 'cp.resnet.layer4.0.downsample.1.running_mean', 'cp.resnet.layer4.0.downsample.1.running_var', 'cp.resnet.layer4.0.downsample.1.num_batches_tracked', 'cp.resnet.layer4.1.conv1.weight', 'cp.resnet.layer4.1.bn1.weight', 'cp.resnet.layer4.1.bn1.bias', 'cp.resnet.layer4.1.bn1.running_mean', 'cp.resnet.layer4.1.bn1.running_var', 'cp.resnet.layer4.1.bn1.num_batches_tracked', 'cp.resnet.layer4.1.conv2.weight', 'cp.resnet.layer4.1.bn2.weight', 'cp.resnet.layer4.1.bn2.bias', 'cp.resnet.layer4.1.bn2.running_mean', 'cp.resnet.layer4.1.bn2.running_var', 'cp.resnet.layer4.1.bn2.num_batches_tracked', 'cp.arm16.conv.conv.weight', 'cp.arm16.conv.bn.weight', 'cp.arm16.conv.bn.bias', 'cp.arm16.conv.bn.running_mean', 'cp.arm16.conv.bn.running_var', 'cp.arm16.conv.bn.num_batches_tracked', 'cp.arm16.conv_atten.weight', 'cp.arm16.bn_atten.weight', 'cp.arm16.bn_atten.bias', 'cp.arm16.bn_atten.running_mean', 'cp.arm16.bn_atten.running_var', 'cp.arm16.bn_atten.num_batches_tracked', 'cp.arm32.conv.conv.weight', 'cp.arm32.conv.bn.weight', 'cp.arm32.conv.bn.bias', 'cp.arm32.conv.bn.running_mean', 'cp.arm32.conv.bn.running_var', 'cp.arm32.conv.bn.num_batches_tracked', 'cp.arm32.conv_atten.weight', 'cp.arm32.bn_atten.weight', 'cp.arm32.bn_atten.bias', 'cp.arm32.bn_atten.running_mean', 'cp.arm32.bn_atten.running_var', 'cp.arm32.bn_atten.num_batches_tracked', 'cp.conv_head32.conv.weight', 'cp.conv_head32.bn.weight', 'cp.conv_head32.bn.bias', 'cp.conv_head32.bn.running_mean', 'cp.conv_head32.bn.running_var', 'cp.conv_head32.bn.num_batches_tracked', 'cp.conv_head16.conv.weight', 'cp.conv_head16.bn.weight', 'cp.conv_head16.bn.bias', 'cp.conv_head16.bn.running_mean', 'cp.conv_head16.bn.running_var', 'cp.conv_head16.bn.num_batches_tracked', 'cp.conv_avg.conv.weight', 'cp.conv_avg.bn.weight', 'cp.conv_avg.bn.bias', 'cp.conv_avg.bn.running_mean', 'cp.conv_avg.bn.running_var', 'cp.conv_avg.bn.num_batches_tracked', 'sp.conv1.conv.weight', 'sp.conv1.bn.weight', 'sp.conv1.bn.bias', 'sp.conv1.bn.running_mean', 'sp.conv1.bn.running_var', 'sp.conv1.bn.num_batches_tracked', 'sp.conv2.conv.weight', 'sp.conv2.bn.weight', 'sp.conv2.bn.bias', 'sp.conv2.bn.running_mean', 'sp.conv2.bn.running_var', 'sp.conv2.bn.num_batches_tracked', 'sp.conv3.conv.weight', 'sp.conv3.bn.weight', 'sp.conv3.bn.bias', 'sp.conv3.bn.running_mean', 'sp.conv3.bn.running_var', 'sp.conv3.bn.num_batches_tracked', 'sp.conv_out.conv.weight', 'sp.conv_out.bn.weight', 'sp.conv_out.bn.bias', 'sp.conv_out.bn.running_mean', 'sp.conv_out.bn.running_var', 'sp.conv_out.bn.num_batches_tracked', 'ffm.convblk.conv.weight', 'ffm.convblk.bn.weight', 'ffm.convblk.bn.bias', 'ffm.convblk.bn.running_mean', 'ffm.convblk.bn.running_var', 'ffm.convblk.bn.num_batches_tracked', 'ffm.conv.weight', 'ffm.bn.weight', 'ffm.bn.bias', 'ffm.bn.running_mean', 'ffm.bn.running_var', 'ffm.bn.num_batches_tracked', 'conv_out.conv.conv.weight', 'conv_out.conv.bn.weight', 'conv_out.conv.bn.bias', 'conv_out.conv.bn.running_mean', 'conv_out.conv.bn.running_var', 'conv_out.conv.bn.num_batches_tracked', 'conv_out.conv_out.weight', 'conv_out.conv_out.bias', 'conv_out16.conv.conv.weight', 'conv_out16.conv.bn.weight', 'conv_out16.conv.bn.bias', 'conv_out16.conv.bn.running_mean', 'conv_out16.conv.bn.running_var', 'conv_out16.conv.bn.num_batches_tracked', 'conv_out16.conv_out.weight', 'conv_out16.conv_out.bias', 'conv_out32.conv.conv.weight', 'conv_out32.conv.bn.weight', 'conv_out32.conv.bn.bias', 'conv_out32.conv.bn.running_mean', 'conv_out32.conv.bn.running_var', 'conv_out32.conv.bn.num_batches_tracked', 'conv_out32.conv_out.weight', 'conv_out32.conv_out.bias'])\n",
            "conv_out.conv_out.weight in own_state\n",
            "Size mismatch for conv_out.conv_out.weight: torch.Size([20, 256, 1, 1]) vs torch.Size([19, 256, 1, 1])\n",
            "conv_out.conv_out.bias in own_state\n",
            "Size mismatch for conv_out.conv_out.bias: torch.Size([20]) vs torch.Size([19])\n",
            "conv_out16.conv_out.weight in own_state\n",
            "Size mismatch for conv_out16.conv_out.weight: torch.Size([20, 64, 1, 1]) vs torch.Size([19, 64, 1, 1])\n",
            "conv_out16.conv_out.bias in own_state\n",
            "Size mismatch for conv_out16.conv_out.bias: torch.Size([20]) vs torch.Size([19])\n",
            "conv_out32.conv_out.weight in own_state\n",
            "Size mismatch for conv_out32.conv_out.weight: torch.Size([20, 64, 1, 1]) vs torch.Size([19, 64, 1, 1])\n",
            "conv_out32.conv_out.bias in own_state\n",
            "Size mismatch for conv_out32.conv_out.bias: torch.Size([20]) vs torch.Size([19])\n",
            "Import Model bisenet with weights bisenetv1_pretrained.pth to FineTune\n",
            "========== TRAINING ===========\n",
            "/content/cityscapes/leftImg8bit/train\n",
            "/content/cityscapes/leftImg8bit/val\n",
            "<class '__main__.CrossEntropyLoss2d'>\n",
            "----- TRAINING - EPOCH 1 -----\n",
            "LEARNING RATE:  0.0025\n",
            "loss: 0.4135 (epoch: 1, step: 0) // Avg time/img: 0.5223 s\n",
            "loss: 0.3298 (epoch: 1, step: 50) // Avg time/img: 0.0438 s\n",
            "loss: 0.3058 (epoch: 1, step: 100) // Avg time/img: 0.0390 s\n",
            "loss: 0.2876 (epoch: 1, step: 150) // Avg time/img: 0.0375 s\n",
            "loss: 0.2787 (epoch: 1, step: 200) // Avg time/img: 0.0369 s\n",
            "loss: 0.2708 (epoch: 1, step: 250) // Avg time/img: 0.0365 s\n",
            "loss: 0.2649 (epoch: 1, step: 300) // Avg time/img: 0.0364 s\n",
            "loss: 0.2594 (epoch: 1, step: 350) // Avg time/img: 0.0362 s\n",
            "loss: 0.256 (epoch: 1, step: 400) // Avg time/img: 0.0362 s\n",
            "loss: 0.2529 (epoch: 1, step: 450) // Avg time/img: 0.0361 s\n",
            "----- VALIDATING - EPOCH 1 -----\n",
            "VAL loss: 0.1632 (epoch: 1, step: 0) // Avg time/img: 0.0359 s\n",
            "VAL loss: 0.3407 (epoch: 1, step: 50) // Avg time/img: 0.0223 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m65.55\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/bisenet_training1/model-001.pth (epoch: 1)\n",
            "save: ../save/bisenet_training1/model_best.pth (epoch: 1)\n",
            "----- TRAINING - EPOCH 2 -----\n",
            "LEARNING RATE:  0.0023872134540537495\n",
            "loss: 0.2414 (epoch: 2, step: 0) // Avg time/img: 0.0425 s\n",
            "loss: 0.2249 (epoch: 2, step: 50) // Avg time/img: 0.0361 s\n",
            "loss: 0.2301 (epoch: 2, step: 100) // Avg time/img: 0.0355 s\n",
            "loss: 0.2275 (epoch: 2, step: 150) // Avg time/img: 0.0354 s\n",
            "loss: 0.2262 (epoch: 2, step: 200) // Avg time/img: 0.0355 s\n",
            "loss: 0.2259 (epoch: 2, step: 250) // Avg time/img: 0.0356 s\n",
            "loss: 0.2256 (epoch: 2, step: 300) // Avg time/img: 0.0355 s\n",
            "loss: 0.2255 (epoch: 2, step: 350) // Avg time/img: 0.0355 s\n",
            "loss: 0.2256 (epoch: 2, step: 400) // Avg time/img: 0.0355 s\n",
            "loss: 0.2261 (epoch: 2, step: 450) // Avg time/img: 0.0355 s\n",
            "----- VALIDATING - EPOCH 2 -----\n",
            "VAL loss: 0.1606 (epoch: 2, step: 0) // Avg time/img: 0.0290 s\n",
            "VAL loss: 0.3336 (epoch: 2, step: 50) // Avg time/img: 0.0217 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m65.41\u001b[0m %\n",
            "save: ../save/bisenet_training1/model-002.pth (epoch: 2)\n",
            "----- TRAINING - EPOCH 3 -----\n",
            "LEARNING RATE:  0.0022738314402074057\n",
            "loss: 0.1743 (epoch: 3, step: 0) // Avg time/img: 0.0431 s\n",
            "loss: 0.2214 (epoch: 3, step: 50) // Avg time/img: 0.0356 s\n",
            "loss: 0.221 (epoch: 3, step: 100) // Avg time/img: 0.0356 s\n",
            "loss: 0.2208 (epoch: 3, step: 150) // Avg time/img: 0.0357 s\n",
            "loss: 0.2177 (epoch: 3, step: 200) // Avg time/img: 0.0357 s\n",
            "loss: 0.2175 (epoch: 3, step: 250) // Avg time/img: 0.0357 s\n",
            "loss: 0.2194 (epoch: 3, step: 300) // Avg time/img: 0.0358 s\n",
            "loss: 0.2205 (epoch: 3, step: 350) // Avg time/img: 0.0358 s\n",
            "loss: 0.2193 (epoch: 3, step: 400) // Avg time/img: 0.0358 s\n",
            "loss: 0.2197 (epoch: 3, step: 450) // Avg time/img: 0.0356 s\n",
            "----- VALIDATING - EPOCH 3 -----\n",
            "VAL loss: 0.1585 (epoch: 3, step: 0) // Avg time/img: 0.0334 s\n",
            "VAL loss: 0.329 (epoch: 3, step: 50) // Avg time/img: 0.0221 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m65.52\u001b[0m %\n",
            "save: ../save/bisenet_training1/model-003.pth (epoch: 3)\n",
            "----- TRAINING - EPOCH 4 -----\n",
            "LEARNING RATE:  0.0021598174307570477\n",
            "loss: 0.1685 (epoch: 4, step: 0) // Avg time/img: 0.0411 s\n",
            "loss: 0.2234 (epoch: 4, step: 50) // Avg time/img: 0.0359 s\n",
            "loss: 0.219 (epoch: 4, step: 100) // Avg time/img: 0.0354 s\n",
            "loss: 0.2168 (epoch: 4, step: 150) // Avg time/img: 0.0355 s\n",
            "loss: 0.2165 (epoch: 4, step: 200) // Avg time/img: 0.0356 s\n",
            "loss: 0.2163 (epoch: 4, step: 250) // Avg time/img: 0.0355 s\n",
            "loss: 0.2171 (epoch: 4, step: 300) // Avg time/img: 0.0356 s\n",
            "loss: 0.2179 (epoch: 4, step: 350) // Avg time/img: 0.0355 s\n",
            "loss: 0.2172 (epoch: 4, step: 400) // Avg time/img: 0.0354 s\n",
            "loss: 0.2177 (epoch: 4, step: 450) // Avg time/img: 0.0355 s\n",
            "----- VALIDATING - EPOCH 4 -----\n",
            "VAL loss: 0.1574 (epoch: 4, step: 0) // Avg time/img: 0.0295 s\n",
            "VAL loss: 0.3274 (epoch: 4, step: 50) // Avg time/img: 0.0215 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m65.54\u001b[0m %\n",
            "save: ../save/bisenet_training1/model-004.pth (epoch: 4)\n",
            "----- TRAINING - EPOCH 5 -----\n",
            "LEARNING RATE:  0.002045130365127146\n",
            "loss: 0.2186 (epoch: 5, step: 0) // Avg time/img: 0.0397 s\n",
            "loss: 0.2124 (epoch: 5, step: 50) // Avg time/img: 0.0352 s\n",
            "loss: 0.214 (epoch: 5, step: 100) // Avg time/img: 0.0353 s\n",
            "loss: 0.2166 (epoch: 5, step: 150) // Avg time/img: 0.0352 s\n",
            "loss: 0.217 (epoch: 5, step: 200) // Avg time/img: 0.0353 s\n",
            "loss: 0.2162 (epoch: 5, step: 250) // Avg time/img: 0.0353 s\n",
            "loss: 0.2159 (epoch: 5, step: 300) // Avg time/img: 0.0354 s\n",
            "loss: 0.2166 (epoch: 5, step: 350) // Avg time/img: 0.0355 s\n",
            "loss: 0.2163 (epoch: 5, step: 400) // Avg time/img: 0.0356 s\n",
            "loss: 0.2159 (epoch: 5, step: 450) // Avg time/img: 0.0356 s\n",
            "----- VALIDATING - EPOCH 5 -----\n",
            "VAL loss: 0.156 (epoch: 5, step: 0) // Avg time/img: 0.0352 s\n",
            "VAL loss: 0.3222 (epoch: 5, step: 50) // Avg time/img: 0.0222 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m65.76\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/bisenet_training1/model-005.pth (epoch: 5)\n",
            "save: ../save/bisenet_training1/model_best.pth (epoch: 5)\n",
            "----- TRAINING - EPOCH 6 -----\n",
            "LEARNING RATE:  0.0019297237668089262\n",
            "loss: 0.2863 (epoch: 6, step: 0) // Avg time/img: 0.0442 s\n",
            "loss: 0.2075 (epoch: 6, step: 50) // Avg time/img: 0.0361 s\n",
            "loss: 0.2091 (epoch: 6, step: 100) // Avg time/img: 0.0356 s\n",
            "loss: 0.2093 (epoch: 6, step: 150) // Avg time/img: 0.0354 s\n",
            "loss: 0.2104 (epoch: 6, step: 200) // Avg time/img: 0.0356 s\n",
            "loss: 0.2115 (epoch: 6, step: 250) // Avg time/img: 0.0355 s\n",
            "loss: 0.2119 (epoch: 6, step: 300) // Avg time/img: 0.0355 s\n",
            "loss: 0.2116 (epoch: 6, step: 350) // Avg time/img: 0.0355 s\n",
            "loss: 0.2126 (epoch: 6, step: 400) // Avg time/img: 0.0355 s\n",
            "loss: 0.2138 (epoch: 6, step: 450) // Avg time/img: 0.0354 s\n",
            "----- VALIDATING - EPOCH 6 -----\n",
            "VAL loss: 0.1564 (epoch: 6, step: 0) // Avg time/img: 0.0370 s\n",
            "VAL loss: 0.3225 (epoch: 6, step: 50) // Avg time/img: 0.0215 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m65.89\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/bisenet_training1/model-006.pth (epoch: 6)\n",
            "save: ../save/bisenet_training1/model_best.pth (epoch: 6)\n",
            "----- TRAINING - EPOCH 7 -----\n",
            "LEARNING RATE:  0.0018135446173430498\n",
            "loss: 0.2585 (epoch: 7, step: 0) // Avg time/img: 0.0420 s\n",
            "loss: 0.2074 (epoch: 7, step: 50) // Avg time/img: 0.0358 s\n",
            "loss: 0.2106 (epoch: 7, step: 100) // Avg time/img: 0.0356 s\n",
            "loss: 0.2117 (epoch: 7, step: 150) // Avg time/img: 0.0356 s\n",
            "loss: 0.2117 (epoch: 7, step: 200) // Avg time/img: 0.0356 s\n",
            "loss: 0.2128 (epoch: 7, step: 250) // Avg time/img: 0.0357 s\n",
            "loss: 0.2109 (epoch: 7, step: 300) // Avg time/img: 0.0355 s\n",
            "loss: 0.2104 (epoch: 7, step: 350) // Avg time/img: 0.0357 s\n",
            "loss: 0.2113 (epoch: 7, step: 400) // Avg time/img: 0.0357 s\n",
            "loss: 0.2118 (epoch: 7, step: 450) // Avg time/img: 0.0358 s\n",
            "----- VALIDATING - EPOCH 7 -----\n",
            "VAL loss: 0.1558 (epoch: 7, step: 0) // Avg time/img: 0.0326 s\n",
            "VAL loss: 0.3235 (epoch: 7, step: 50) // Avg time/img: 0.0222 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m65.54\u001b[0m %\n",
            "save: ../save/bisenet_training1/model-007.pth (epoch: 7)\n",
            "----- TRAINING - EPOCH 8 -----\n",
            "LEARNING RATE:  0.0016965318981453127\n",
            "loss: 0.1711 (epoch: 8, step: 0) // Avg time/img: 0.0424 s\n",
            "loss: 0.2119 (epoch: 8, step: 50) // Avg time/img: 0.0361 s\n",
            "loss: 0.2098 (epoch: 8, step: 100) // Avg time/img: 0.0359 s\n",
            "loss: 0.2081 (epoch: 8, step: 150) // Avg time/img: 0.0356 s\n",
            "loss: 0.2071 (epoch: 8, step: 200) // Avg time/img: 0.0355 s\n",
            "loss: 0.2081 (epoch: 8, step: 250) // Avg time/img: 0.0356 s\n",
            "loss: 0.2087 (epoch: 8, step: 300) // Avg time/img: 0.0354 s\n",
            "loss: 0.2094 (epoch: 8, step: 350) // Avg time/img: 0.0355 s\n",
            "loss: 0.21 (epoch: 8, step: 400) // Avg time/img: 0.0356 s\n",
            "loss: 0.211 (epoch: 8, step: 450) // Avg time/img: 0.0356 s\n",
            "----- VALIDATING - EPOCH 8 -----\n",
            "VAL loss: 0.1525 (epoch: 8, step: 0) // Avg time/img: 0.0369 s\n",
            "VAL loss: 0.3211 (epoch: 8, step: 50) // Avg time/img: 0.0216 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m65.88\u001b[0m %\n",
            "save: ../save/bisenet_training1/model-008.pth (epoch: 8)\n",
            "----- TRAINING - EPOCH 9 -----\n",
            "LEARNING RATE:  0.0015786146687233882\n",
            "loss: 0.2422 (epoch: 9, step: 0) // Avg time/img: 0.0475 s\n",
            "loss: 0.2092 (epoch: 9, step: 50) // Avg time/img: 0.0353 s\n",
            "loss: 0.2116 (epoch: 9, step: 100) // Avg time/img: 0.0354 s\n",
            "loss: 0.2135 (epoch: 9, step: 150) // Avg time/img: 0.0357 s\n",
            "loss: 0.2141 (epoch: 9, step: 200) // Avg time/img: 0.0356 s\n",
            "loss: 0.2124 (epoch: 9, step: 250) // Avg time/img: 0.0356 s\n",
            "loss: 0.2117 (epoch: 9, step: 300) // Avg time/img: 0.0356 s\n",
            "loss: 0.2122 (epoch: 9, step: 350) // Avg time/img: 0.0355 s\n",
            "loss: 0.2128 (epoch: 9, step: 400) // Avg time/img: 0.0356 s\n",
            "loss: 0.2121 (epoch: 9, step: 450) // Avg time/img: 0.0355 s\n",
            "----- VALIDATING - EPOCH 9 -----\n",
            "VAL loss: 0.1554 (epoch: 9, step: 0) // Avg time/img: 0.0423 s\n",
            "VAL loss: 0.325 (epoch: 9, step: 50) // Avg time/img: 0.0227 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m65.64\u001b[0m %\n",
            "save: ../save/bisenet_training1/model-009.pth (epoch: 9)\n",
            "----- TRAINING - EPOCH 10 -----\n",
            "LEARNING RATE:  0.0014597094822999506\n",
            "loss: 0.1769 (epoch: 10, step: 0) // Avg time/img: 0.0488 s\n",
            "loss: 0.2227 (epoch: 10, step: 50) // Avg time/img: 0.0361 s\n",
            "loss: 0.2171 (epoch: 10, step: 100) // Avg time/img: 0.0358 s\n",
            "loss: 0.216 (epoch: 10, step: 150) // Avg time/img: 0.0357 s\n",
            "loss: 0.215 (epoch: 10, step: 200) // Avg time/img: 0.0356 s\n",
            "loss: 0.214 (epoch: 10, step: 250) // Avg time/img: 0.0358 s\n",
            "loss: 0.2138 (epoch: 10, step: 300) // Avg time/img: 0.0357 s\n",
            "loss: 0.2132 (epoch: 10, step: 350) // Avg time/img: 0.0356 s\n",
            "loss: 0.2114 (epoch: 10, step: 400) // Avg time/img: 0.0357 s\n",
            "loss: 0.211 (epoch: 10, step: 450) // Avg time/img: 0.0356 s\n",
            "----- VALIDATING - EPOCH 10 -----\n",
            "VAL loss: 0.1543 (epoch: 10, step: 0) // Avg time/img: 0.0315 s\n",
            "VAL loss: 0.3228 (epoch: 10, step: 50) // Avg time/img: 0.0233 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m65.71\u001b[0m %\n",
            "save: ../save/bisenet_training1/model-010.pth (epoch: 10)\n",
            "----- TRAINING - EPOCH 11 -----\n",
            "LEARNING RATE:  0.0013397168281703664\n",
            "loss: 0.1687 (epoch: 11, step: 0) // Avg time/img: 0.0443 s\n",
            "loss: 0.2149 (epoch: 11, step: 50) // Avg time/img: 0.0371 s\n",
            "loss: 0.2125 (epoch: 11, step: 100) // Avg time/img: 0.0362 s\n",
            "loss: 0.2104 (epoch: 11, step: 150) // Avg time/img: 0.0359 s\n",
            "loss: 0.2095 (epoch: 11, step: 200) // Avg time/img: 0.0360 s\n",
            "loss: 0.2107 (epoch: 11, step: 250) // Avg time/img: 0.0359 s\n",
            "loss: 0.2111 (epoch: 11, step: 300) // Avg time/img: 0.0360 s\n",
            "loss: 0.2103 (epoch: 11, step: 350) // Avg time/img: 0.0360 s\n",
            "loss: 0.2094 (epoch: 11, step: 400) // Avg time/img: 0.0359 s\n",
            "loss: 0.2096 (epoch: 11, step: 450) // Avg time/img: 0.0360 s\n",
            "----- VALIDATING - EPOCH 11 -----\n",
            "VAL loss: 0.1542 (epoch: 11, step: 0) // Avg time/img: 0.0316 s\n",
            "VAL loss: 0.32 (epoch: 11, step: 50) // Avg time/img: 0.0223 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m65.99\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/bisenet_training1/model-011.pth (epoch: 11)\n",
            "save: ../save/bisenet_training1/model_best.pth (epoch: 11)\n",
            "----- TRAINING - EPOCH 12 -----\n",
            "LEARNING RATE:  0.0012185160979474884\n",
            "loss: 0.1901 (epoch: 12, step: 0) // Avg time/img: 0.0421 s\n",
            "loss: 0.2048 (epoch: 12, step: 50) // Avg time/img: 0.0360 s\n",
            "loss: 0.2075 (epoch: 12, step: 100) // Avg time/img: 0.0355 s\n",
            "loss: 0.2087 (epoch: 12, step: 150) // Avg time/img: 0.0352 s\n",
            "loss: 0.2099 (epoch: 12, step: 200) // Avg time/img: 0.0354 s\n",
            "loss: 0.2087 (epoch: 12, step: 250) // Avg time/img: 0.0354 s\n",
            "loss: 0.2089 (epoch: 12, step: 300) // Avg time/img: 0.0355 s\n",
            "loss: 0.2104 (epoch: 12, step: 350) // Avg time/img: 0.0355 s\n",
            "loss: 0.2107 (epoch: 12, step: 400) // Avg time/img: 0.0354 s\n",
            "loss: 0.2108 (epoch: 12, step: 450) // Avg time/img: 0.0355 s\n",
            "----- VALIDATING - EPOCH 12 -----\n",
            "VAL loss: 0.1537 (epoch: 12, step: 0) // Avg time/img: 0.0315 s\n",
            "VAL loss: 0.3193 (epoch: 12, step: 50) // Avg time/img: 0.0224 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m66.09\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/bisenet_training1/model-012.pth (epoch: 12)\n",
            "save: ../save/bisenet_training1/model_best.pth (epoch: 12)\n",
            "----- TRAINING - EPOCH 13 -----\n",
            "LEARNING RATE:  0.0010959582263852174\n",
            "loss: 0.1624 (epoch: 13, step: 0) // Avg time/img: 0.0457 s\n",
            "loss: 0.2108 (epoch: 13, step: 50) // Avg time/img: 0.0357 s\n",
            "loss: 0.2099 (epoch: 13, step: 100) // Avg time/img: 0.0353 s\n",
            "loss: 0.2107 (epoch: 13, step: 150) // Avg time/img: 0.0350 s\n",
            "loss: 0.2084 (epoch: 13, step: 200) // Avg time/img: 0.0351 s\n",
            "loss: 0.2089 (epoch: 13, step: 250) // Avg time/img: 0.0353 s\n",
            "loss: 0.2091 (epoch: 13, step: 300) // Avg time/img: 0.0354 s\n",
            "loss: 0.2094 (epoch: 13, step: 350) // Avg time/img: 0.0354 s\n",
            "loss: 0.2092 (epoch: 13, step: 400) // Avg time/img: 0.0354 s\n",
            "loss: 0.2093 (epoch: 13, step: 450) // Avg time/img: 0.0355 s\n",
            "----- VALIDATING - EPOCH 13 -----\n",
            "VAL loss: 0.1536 (epoch: 13, step: 0) // Avg time/img: 0.0320 s\n",
            "VAL loss: 0.3193 (epoch: 13, step: 50) // Avg time/img: 0.0221 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m65.97\u001b[0m %\n",
            "save: ../save/bisenet_training1/model-013.pth (epoch: 13)\n",
            "----- TRAINING - EPOCH 14 -----\n",
            "LEARNING RATE:  0.0009718544969969087\n",
            "loss: 0.2078 (epoch: 14, step: 0) // Avg time/img: 0.0454 s\n",
            "loss: 0.2052 (epoch: 14, step: 50) // Avg time/img: 0.0349 s\n",
            "loss: 0.2044 (epoch: 14, step: 100) // Avg time/img: 0.0350 s\n",
            "loss: 0.2077 (epoch: 14, step: 150) // Avg time/img: 0.0350 s\n",
            "loss: 0.206 (epoch: 14, step: 200) // Avg time/img: 0.0351 s\n",
            "loss: 0.2077 (epoch: 14, step: 250) // Avg time/img: 0.0354 s\n",
            "loss: 0.2067 (epoch: 14, step: 300) // Avg time/img: 0.0354 s\n",
            "loss: 0.2076 (epoch: 14, step: 350) // Avg time/img: 0.0355 s\n",
            "loss: 0.2078 (epoch: 14, step: 400) // Avg time/img: 0.0354 s\n",
            "loss: 0.2074 (epoch: 14, step: 450) // Avg time/img: 0.0355 s\n",
            "----- VALIDATING - EPOCH 14 -----\n",
            "VAL loss: 0.153 (epoch: 14, step: 0) // Avg time/img: 0.0332 s\n",
            "VAL loss: 0.3172 (epoch: 14, step: 50) // Avg time/img: 0.0218 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m66.11\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/bisenet_training1/model-014.pth (epoch: 14)\n",
            "save: ../save/bisenet_training1/model_best.pth (epoch: 14)\n",
            "----- TRAINING - EPOCH 15 -----\n",
            "LEARNING RATE:  0.0008459586547541247\n",
            "loss: 0.2014 (epoch: 15, step: 0) // Avg time/img: 0.0463 s\n",
            "loss: 0.2115 (epoch: 15, step: 50) // Avg time/img: 0.0356 s\n",
            "loss: 0.2117 (epoch: 15, step: 100) // Avg time/img: 0.0354 s\n",
            "loss: 0.2107 (epoch: 15, step: 150) // Avg time/img: 0.0353 s\n",
            "loss: 0.2101 (epoch: 15, step: 200) // Avg time/img: 0.0355 s\n",
            "loss: 0.2095 (epoch: 15, step: 250) // Avg time/img: 0.0355 s\n",
            "loss: 0.2104 (epoch: 15, step: 300) // Avg time/img: 0.0355 s\n",
            "loss: 0.209 (epoch: 15, step: 350) // Avg time/img: 0.0356 s\n",
            "loss: 0.2081 (epoch: 15, step: 400) // Avg time/img: 0.0356 s\n",
            "loss: 0.2083 (epoch: 15, step: 450) // Avg time/img: 0.0357 s\n",
            "----- VALIDATING - EPOCH 15 -----\n",
            "VAL loss: 0.1524 (epoch: 15, step: 0) // Avg time/img: 0.0346 s\n",
            "VAL loss: 0.3183 (epoch: 15, step: 50) // Avg time/img: 0.0228 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m65.96\u001b[0m %\n",
            "save: ../save/bisenet_training1/model-015.pth (epoch: 15)\n",
            "----- TRAINING - EPOCH 16 -----\n",
            "LEARNING RATE:  0.0007179364718731469\n",
            "loss: 0.3048 (epoch: 16, step: 0) // Avg time/img: 0.0425 s\n",
            "loss: 0.2035 (epoch: 16, step: 50) // Avg time/img: 0.0361 s\n",
            "loss: 0.2108 (epoch: 16, step: 100) // Avg time/img: 0.0363 s\n",
            "loss: 0.211 (epoch: 16, step: 150) // Avg time/img: 0.0361 s\n",
            "loss: 0.2099 (epoch: 16, step: 200) // Avg time/img: 0.0359 s\n",
            "loss: 0.2112 (epoch: 16, step: 250) // Avg time/img: 0.0358 s\n",
            "loss: 0.2081 (epoch: 16, step: 300) // Avg time/img: 0.0357 s\n",
            "loss: 0.2086 (epoch: 16, step: 350) // Avg time/img: 0.0357 s\n",
            "loss: 0.2085 (epoch: 16, step: 400) // Avg time/img: 0.0359 s\n",
            "loss: 0.2085 (epoch: 16, step: 450) // Avg time/img: 0.0358 s\n",
            "----- VALIDATING - EPOCH 16 -----\n",
            "VAL loss: 0.1535 (epoch: 16, step: 0) // Avg time/img: 0.0299 s\n",
            "VAL loss: 0.3203 (epoch: 16, step: 50) // Avg time/img: 0.0224 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m66.11\u001b[0m %\n",
            "save: ../save/bisenet_training1/model-016.pth (epoch: 16)\n",
            "----- TRAINING - EPOCH 17 -----\n",
            "LEARNING RATE:  0.0005873094715440094\n",
            "loss: 0.2015 (epoch: 17, step: 0) // Avg time/img: 0.0442 s\n",
            "loss: 0.2058 (epoch: 17, step: 50) // Avg time/img: 0.0352 s\n",
            "loss: 0.2023 (epoch: 17, step: 100) // Avg time/img: 0.0354 s\n",
            "loss: 0.2049 (epoch: 17, step: 150) // Avg time/img: 0.0355 s\n",
            "loss: 0.209 (epoch: 17, step: 200) // Avg time/img: 0.0355 s\n",
            "loss: 0.2062 (epoch: 17, step: 250) // Avg time/img: 0.0357 s\n",
            "loss: 0.2065 (epoch: 17, step: 300) // Avg time/img: 0.0357 s\n",
            "loss: 0.2067 (epoch: 17, step: 350) // Avg time/img: 0.0356 s\n",
            "loss: 0.208 (epoch: 17, step: 400) // Avg time/img: 0.0356 s\n",
            "loss: 0.2091 (epoch: 17, step: 450) // Avg time/img: 0.0356 s\n",
            "----- VALIDATING - EPOCH 17 -----\n",
            "VAL loss: 0.1522 (epoch: 17, step: 0) // Avg time/img: 0.0317 s\n",
            "VAL loss: 0.3166 (epoch: 17, step: 50) // Avg time/img: 0.0230 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m66.01\u001b[0m %\n",
            "save: ../save/bisenet_training1/model-017.pth (epoch: 17)\n",
            "----- TRAINING - EPOCH 18 -----\n",
            "LEARNING RATE:  0.0004533380182841864\n",
            "loss: 0.3121 (epoch: 18, step: 0) // Avg time/img: 0.0388 s\n",
            "loss: 0.2069 (epoch: 18, step: 50) // Avg time/img: 0.0361 s\n",
            "loss: 0.2094 (epoch: 18, step: 100) // Avg time/img: 0.0356 s\n",
            "loss: 0.2082 (epoch: 18, step: 150) // Avg time/img: 0.0358 s\n",
            "loss: 0.2103 (epoch: 18, step: 200) // Avg time/img: 0.0356 s\n",
            "loss: 0.2088 (epoch: 18, step: 250) // Avg time/img: 0.0357 s\n",
            "loss: 0.2087 (epoch: 18, step: 300) // Avg time/img: 0.0359 s\n",
            "loss: 0.2089 (epoch: 18, step: 350) // Avg time/img: 0.0357 s\n",
            "loss: 0.2083 (epoch: 18, step: 400) // Avg time/img: 0.0358 s\n",
            "loss: 0.2086 (epoch: 18, step: 450) // Avg time/img: 0.0357 s\n",
            "----- VALIDATING - EPOCH 18 -----\n",
            "VAL loss: 0.1513 (epoch: 18, step: 0) // Avg time/img: 0.0351 s\n",
            "VAL loss: 0.3182 (epoch: 18, step: 50) // Avg time/img: 0.0224 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m66.04\u001b[0m %\n",
            "save: ../save/bisenet_training1/model-018.pth (epoch: 18)\n",
            "----- TRAINING - EPOCH 19 -----\n",
            "LEARNING RATE:  0.00031473135294854176\n",
            "loss: 0.1791 (epoch: 19, step: 0) // Avg time/img: 0.0426 s\n",
            "loss: 0.2055 (epoch: 19, step: 50) // Avg time/img: 0.0357 s\n",
            "loss: 0.2094 (epoch: 19, step: 100) // Avg time/img: 0.0355 s\n",
            "loss: 0.2082 (epoch: 19, step: 150) // Avg time/img: 0.0355 s\n",
            "loss: 0.2088 (epoch: 19, step: 200) // Avg time/img: 0.0358 s\n",
            "loss: 0.2079 (epoch: 19, step: 250) // Avg time/img: 0.0357 s\n",
            "loss: 0.2084 (epoch: 19, step: 300) // Avg time/img: 0.0357 s\n",
            "loss: 0.2084 (epoch: 19, step: 350) // Avg time/img: 0.0357 s\n",
            "loss: 0.2074 (epoch: 19, step: 400) // Avg time/img: 0.0357 s\n",
            "loss: 0.2067 (epoch: 19, step: 450) // Avg time/img: 0.0357 s\n",
            "----- VALIDATING - EPOCH 19 -----\n",
            "VAL loss: 0.1514 (epoch: 19, step: 0) // Avg time/img: 0.0315 s\n",
            "VAL loss: 0.3149 (epoch: 19, step: 50) // Avg time/img: 0.0218 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m66.26\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/bisenet_training1/model-019.pth (epoch: 19)\n",
            "save: ../save/bisenet_training1/model_best.pth (epoch: 19)\n",
            "----- TRAINING - EPOCH 20 -----\n",
            "LEARNING RATE:  0.00016866035595919555\n",
            "loss: 0.188 (epoch: 20, step: 0) // Avg time/img: 0.0450 s\n",
            "loss: 0.2009 (epoch: 20, step: 50) // Avg time/img: 0.0351 s\n",
            "loss: 0.2046 (epoch: 20, step: 100) // Avg time/img: 0.0353 s\n",
            "loss: 0.2019 (epoch: 20, step: 150) // Avg time/img: 0.0354 s\n",
            "loss: 0.2026 (epoch: 20, step: 200) // Avg time/img: 0.0353 s\n",
            "loss: 0.2035 (epoch: 20, step: 250) // Avg time/img: 0.0355 s\n",
            "loss: 0.2047 (epoch: 20, step: 300) // Avg time/img: 0.0354 s\n",
            "loss: 0.2058 (epoch: 20, step: 350) // Avg time/img: 0.0353 s\n",
            "loss: 0.2065 (epoch: 20, step: 400) // Avg time/img: 0.0355 s\n",
            "loss: 0.2072 (epoch: 20, step: 450) // Avg time/img: 0.0355 s\n",
            "----- VALIDATING - EPOCH 20 -----\n",
            "VAL loss: 0.1508 (epoch: 20, step: 0) // Avg time/img: 0.0293 s\n",
            "VAL loss: 0.3149 (epoch: 20, step: 50) // Avg time/img: 0.0229 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m66.37\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/bisenet_training1/model-020.pth (epoch: 20)\n",
            "save: ../save/bisenet_training1/model_best.pth (epoch: 20)\n",
            "========== TRAINING FINISHED ===========\n",
            "updating: content/AnomalySegmentation/save/bisenet_training1/ (stored 0%)\n",
            "updating: content/AnomalySegmentation/save/bisenet_training1/automated_log.txt (deflated 66%)\n",
            "updating: content/AnomalySegmentation/save/bisenet_training1/bisenet.py (deflated 82%)\n",
            "updating: content/AnomalySegmentation/save/bisenet_training1/model-010.pth (deflated 7%)\n",
            "updating: content/AnomalySegmentation/save/bisenet_training1/model.txt (deflated 91%)\n",
            "updating: content/AnomalySegmentation/save/bisenet_training1/opts.txt (deflated 37%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training1/model-014.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training1/model_best.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training1/model-002.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training1/model-011.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training1/model-001.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training1/model-003.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training1/model-020.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training1/model-006.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training1/model-015.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training1/model-005.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training1/model-007.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training1/model-012.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training1/model-018.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training1/best.txt (stored 0%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training1/model_best.pth.tar (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training1/model-016.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training1/model-019.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training1/model-013.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training1/model-008.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training1/model-017.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training1/model-009.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training1/checkpoint.pth.tar (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training1/model-004.pth (deflated 7%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation"
      ],
      "metadata": {
        "id": "mMrPCZ56IShf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "no_execute = False\n",
        "just_once = False\n",
        "\n",
        "for net in [\"erfnet\", \"enet\", \"bisenet\"]:\n",
        "  print(\"----------------------------\")\n",
        "  for dataset_dir in ['RoadAnomaly21', 'RoadObsticle21', 'FS_LostFound_full', 'fs_static', 'RoadAnomaly']:\n",
        "\n",
        "    if no_execute:\n",
        "      break\n",
        "\n",
        "    load_dir = f'/content/AnomalySegmentation/save/{net}_training1'\n",
        "    weights = f'/model_best.pth'\n",
        "    format_file = os.listdir(f'/content/Validation_Dataset/{dataset_dir}/images')[0].split(\".\")[1]\n",
        "    input =f'/content/Validation_Dataset/{dataset_dir}/images/\\*.{format_file}'\n",
        "    print(f\"\\nDataset: {dataset_dir} net: {net}\")\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "      !python -W ignore /content/AnomalySegmentation/eval/evalAnomaly.py --input {input} --void --model {net} --loadDir {load_dir} --loadWeights {weights} | tail -n 2\n",
        "    else:\n",
        "      !python -W ignore /content/AnomalySegmentation/eval/evalAnomaly.py --input {input} --void --model {net} --loadDir {load_dir} --loadWeights {weights} --cpu | tail -n 2\n",
        "\n",
        "    if just_once:\n",
        "      no_execute = True\n",
        "      just_once = False"
      ],
      "metadata": {
        "id": "cchB40LlIT9a",
        "outputId": "30674a33-f5d0-47a4-a215-15cf17f372ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------\n",
            "\n",
            "Dataset: RoadAnomaly21 net: erfnet\n",
            "AUPRC score: 23.066538132595678\n",
            "FPR@TPR95: 83.00218872131218\n",
            "\n",
            "Dataset: RoadObsticle21 net: erfnet\n",
            "AUPRC score: 1.2479619783498568\n",
            "FPR@TPR95: 98.77720541475112\n",
            "\n",
            "Dataset: FS_LostFound_full net: erfnet\n",
            "AUPRC score: 3.9670465669000987\n",
            "FPR@TPR95: 37.15986952996774\n",
            "\n",
            "Dataset: fs_static net: erfnet\n",
            "AUPRC score: 12.255619101525266\n",
            "FPR@TPR95: 82.25043369836908\n",
            "\n",
            "Dataset: RoadAnomaly net: erfnet\n",
            "AUPRC score: 10.108372222814976\n",
            "FPR@TPR95: 97.9176876261683\n",
            "----------------------------\n",
            "\n",
            "Dataset: RoadAnomaly21 net: enet\n",
            "AUPRC score: 17.710540775846635\n",
            "FPR@TPR95: 93.9867224686995\n",
            "\n",
            "Dataset: RoadObsticle21 net: enet\n",
            "AUPRC score: 1.3713287968833945\n",
            "FPR@TPR95: 91.93726483525218\n",
            "\n",
            "Dataset: FS_LostFound_full net: enet\n",
            "AUPRC score: 0.9859109956888955\n",
            "FPR@TPR95: 60.28533261005652\n",
            "\n",
            "Dataset: fs_static net: enet\n",
            "AUPRC score: 7.72351282202996\n",
            "FPR@TPR95: 78.34549796336687\n",
            "\n",
            "Dataset: RoadAnomaly net: enet\n",
            "AUPRC score: 12.788489618794117\n",
            "FPR@TPR95: 86.93573427839908\n",
            "----------------------------\n",
            "\n",
            "Dataset: RoadAnomaly21 net: bisenet\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n",
            "100% 44.7M/44.7M [00:00<00:00, 352MB/s]\n",
            "AUPRC score: 21.81055194591118\n",
            "FPR@TPR95: 91.87575225697388\n",
            "\n",
            "Dataset: RoadObsticle21 net: bisenet\n",
            "AUPRC score: 10.298027408646206\n",
            "FPR@TPR95: 44.90818638296744\n",
            "\n",
            "Dataset: FS_LostFound_full net: bisenet\n",
            "AUPRC score: 3.5811069764660606\n",
            "FPR@TPR95: 44.204070137081445\n",
            "\n",
            "Dataset: fs_static net: bisenet\n",
            "AUPRC score: 9.766178585596988\n",
            "FPR@TPR95: 50.448052314843984\n",
            "\n",
            "Dataset: RoadAnomaly net: bisenet\n",
            "AUPRC score: 12.153410762011895\n",
            "FPR@TPR95: 91.2516594411978\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**MAHALANOBIS**"
      ],
      "metadata": {
        "id": "uVugUIVbxMSq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/AnomalySegmentation && git pull -q"
      ],
      "metadata": {
        "id": "vZiXsIPdxnUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datadir = '/content/cityscapes'\n",
        "loadWeights = '/save/erfnet_training1/model_best.pth'\n",
        "loadDir = '/content/AnomalySegmentation'\n",
        "\n",
        "!python /content/AnomalySegmentation/eval/mahalanobis.py --datadir {datadir} --model erfnet --loadDir {loadDir} --loadWeights {loadWeights}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S70ucntWxy4F",
        "outputId": "c68c1ca4-0d67-4ada-ba91-af794e5ea2fc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model: erfnet\n",
            "Loading weights: /content/AnomalySegmentation/save/erfnet_training1/model_best.pth\n",
            "/content/cityscapes/leftImg8bit/train\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/content/AnomalySegmentation/eval/mahalanobis.py:133: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model = load_my_state_dict(model, torch.load(weightspath, map_location=lambda storage, loc: storage))\n",
            "odict_keys(['module.encoder.initial_block.conv.weight', 'module.encoder.initial_block.conv.bias', 'module.encoder.initial_block.bn.weight', 'module.encoder.initial_block.bn.bias', 'module.encoder.initial_block.bn.running_mean', 'module.encoder.initial_block.bn.running_var', 'module.encoder.initial_block.bn.num_batches_tracked', 'module.encoder.layers.0.conv.weight', 'module.encoder.layers.0.conv.bias', 'module.encoder.layers.0.bn.weight', 'module.encoder.layers.0.bn.bias', 'module.encoder.layers.0.bn.running_mean', 'module.encoder.layers.0.bn.running_var', 'module.encoder.layers.0.bn.num_batches_tracked', 'module.encoder.layers.1.conv3x1_1.weight', 'module.encoder.layers.1.conv3x1_1.bias', 'module.encoder.layers.1.conv1x3_1.weight', 'module.encoder.layers.1.conv1x3_1.bias', 'module.encoder.layers.1.bn1.weight', 'module.encoder.layers.1.bn1.bias', 'module.encoder.layers.1.bn1.running_mean', 'module.encoder.layers.1.bn1.running_var', 'module.encoder.layers.1.bn1.num_batches_tracked', 'module.encoder.layers.1.conv3x1_2.weight', 'module.encoder.layers.1.conv3x1_2.bias', 'module.encoder.layers.1.conv1x3_2.weight', 'module.encoder.layers.1.conv1x3_2.bias', 'module.encoder.layers.1.bn2.weight', 'module.encoder.layers.1.bn2.bias', 'module.encoder.layers.1.bn2.running_mean', 'module.encoder.layers.1.bn2.running_var', 'module.encoder.layers.1.bn2.num_batches_tracked', 'module.encoder.layers.2.conv3x1_1.weight', 'module.encoder.layers.2.conv3x1_1.bias', 'module.encoder.layers.2.conv1x3_1.weight', 'module.encoder.layers.2.conv1x3_1.bias', 'module.encoder.layers.2.bn1.weight', 'module.encoder.layers.2.bn1.bias', 'module.encoder.layers.2.bn1.running_mean', 'module.encoder.layers.2.bn1.running_var', 'module.encoder.layers.2.bn1.num_batches_tracked', 'module.encoder.layers.2.conv3x1_2.weight', 'module.encoder.layers.2.conv3x1_2.bias', 'module.encoder.layers.2.conv1x3_2.weight', 'module.encoder.layers.2.conv1x3_2.bias', 'module.encoder.layers.2.bn2.weight', 'module.encoder.layers.2.bn2.bias', 'module.encoder.layers.2.bn2.running_mean', 'module.encoder.layers.2.bn2.running_var', 'module.encoder.layers.2.bn2.num_batches_tracked', 'module.encoder.layers.3.conv3x1_1.weight', 'module.encoder.layers.3.conv3x1_1.bias', 'module.encoder.layers.3.conv1x3_1.weight', 'module.encoder.layers.3.conv1x3_1.bias', 'module.encoder.layers.3.bn1.weight', 'module.encoder.layers.3.bn1.bias', 'module.encoder.layers.3.bn1.running_mean', 'module.encoder.layers.3.bn1.running_var', 'module.encoder.layers.3.bn1.num_batches_tracked', 'module.encoder.layers.3.conv3x1_2.weight', 'module.encoder.layers.3.conv3x1_2.bias', 'module.encoder.layers.3.conv1x3_2.weight', 'module.encoder.layers.3.conv1x3_2.bias', 'module.encoder.layers.3.bn2.weight', 'module.encoder.layers.3.bn2.bias', 'module.encoder.layers.3.bn2.running_mean', 'module.encoder.layers.3.bn2.running_var', 'module.encoder.layers.3.bn2.num_batches_tracked', 'module.encoder.layers.4.conv3x1_1.weight', 'module.encoder.layers.4.conv3x1_1.bias', 'module.encoder.layers.4.conv1x3_1.weight', 'module.encoder.layers.4.conv1x3_1.bias', 'module.encoder.layers.4.bn1.weight', 'module.encoder.layers.4.bn1.bias', 'module.encoder.layers.4.bn1.running_mean', 'module.encoder.layers.4.bn1.running_var', 'module.encoder.layers.4.bn1.num_batches_tracked', 'module.encoder.layers.4.conv3x1_2.weight', 'module.encoder.layers.4.conv3x1_2.bias', 'module.encoder.layers.4.conv1x3_2.weight', 'module.encoder.layers.4.conv1x3_2.bias', 'module.encoder.layers.4.bn2.weight', 'module.encoder.layers.4.bn2.bias', 'module.encoder.layers.4.bn2.running_mean', 'module.encoder.layers.4.bn2.running_var', 'module.encoder.layers.4.bn2.num_batches_tracked', 'module.encoder.layers.5.conv3x1_1.weight', 'module.encoder.layers.5.conv3x1_1.bias', 'module.encoder.layers.5.conv1x3_1.weight', 'module.encoder.layers.5.conv1x3_1.bias', 'module.encoder.layers.5.bn1.weight', 'module.encoder.layers.5.bn1.bias', 'module.encoder.layers.5.bn1.running_mean', 'module.encoder.layers.5.bn1.running_var', 'module.encoder.layers.5.bn1.num_batches_tracked', 'module.encoder.layers.5.conv3x1_2.weight', 'module.encoder.layers.5.conv3x1_2.bias', 'module.encoder.layers.5.conv1x3_2.weight', 'module.encoder.layers.5.conv1x3_2.bias', 'module.encoder.layers.5.bn2.weight', 'module.encoder.layers.5.bn2.bias', 'module.encoder.layers.5.bn2.running_mean', 'module.encoder.layers.5.bn2.running_var', 'module.encoder.layers.5.bn2.num_batches_tracked', 'module.encoder.layers.6.conv.weight', 'module.encoder.layers.6.conv.bias', 'module.encoder.layers.6.bn.weight', 'module.encoder.layers.6.bn.bias', 'module.encoder.layers.6.bn.running_mean', 'module.encoder.layers.6.bn.running_var', 'module.encoder.layers.6.bn.num_batches_tracked', 'module.encoder.layers.7.conv3x1_1.weight', 'module.encoder.layers.7.conv3x1_1.bias', 'module.encoder.layers.7.conv1x3_1.weight', 'module.encoder.layers.7.conv1x3_1.bias', 'module.encoder.layers.7.bn1.weight', 'module.encoder.layers.7.bn1.bias', 'module.encoder.layers.7.bn1.running_mean', 'module.encoder.layers.7.bn1.running_var', 'module.encoder.layers.7.bn1.num_batches_tracked', 'module.encoder.layers.7.conv3x1_2.weight', 'module.encoder.layers.7.conv3x1_2.bias', 'module.encoder.layers.7.conv1x3_2.weight', 'module.encoder.layers.7.conv1x3_2.bias', 'module.encoder.layers.7.bn2.weight', 'module.encoder.layers.7.bn2.bias', 'module.encoder.layers.7.bn2.running_mean', 'module.encoder.layers.7.bn2.running_var', 'module.encoder.layers.7.bn2.num_batches_tracked', 'module.encoder.layers.8.conv3x1_1.weight', 'module.encoder.layers.8.conv3x1_1.bias', 'module.encoder.layers.8.conv1x3_1.weight', 'module.encoder.layers.8.conv1x3_1.bias', 'module.encoder.layers.8.bn1.weight', 'module.encoder.layers.8.bn1.bias', 'module.encoder.layers.8.bn1.running_mean', 'module.encoder.layers.8.bn1.running_var', 'module.encoder.layers.8.bn1.num_batches_tracked', 'module.encoder.layers.8.conv3x1_2.weight', 'module.encoder.layers.8.conv3x1_2.bias', 'module.encoder.layers.8.conv1x3_2.weight', 'module.encoder.layers.8.conv1x3_2.bias', 'module.encoder.layers.8.bn2.weight', 'module.encoder.layers.8.bn2.bias', 'module.encoder.layers.8.bn2.running_mean', 'module.encoder.layers.8.bn2.running_var', 'module.encoder.layers.8.bn2.num_batches_tracked', 'module.encoder.layers.9.conv3x1_1.weight', 'module.encoder.layers.9.conv3x1_1.bias', 'module.encoder.layers.9.conv1x3_1.weight', 'module.encoder.layers.9.conv1x3_1.bias', 'module.encoder.layers.9.bn1.weight', 'module.encoder.layers.9.bn1.bias', 'module.encoder.layers.9.bn1.running_mean', 'module.encoder.layers.9.bn1.running_var', 'module.encoder.layers.9.bn1.num_batches_tracked', 'module.encoder.layers.9.conv3x1_2.weight', 'module.encoder.layers.9.conv3x1_2.bias', 'module.encoder.layers.9.conv1x3_2.weight', 'module.encoder.layers.9.conv1x3_2.bias', 'module.encoder.layers.9.bn2.weight', 'module.encoder.layers.9.bn2.bias', 'module.encoder.layers.9.bn2.running_mean', 'module.encoder.layers.9.bn2.running_var', 'module.encoder.layers.9.bn2.num_batches_tracked', 'module.encoder.layers.10.conv3x1_1.weight', 'module.encoder.layers.10.conv3x1_1.bias', 'module.encoder.layers.10.conv1x3_1.weight', 'module.encoder.layers.10.conv1x3_1.bias', 'module.encoder.layers.10.bn1.weight', 'module.encoder.layers.10.bn1.bias', 'module.encoder.layers.10.bn1.running_mean', 'module.encoder.layers.10.bn1.running_var', 'module.encoder.layers.10.bn1.num_batches_tracked', 'module.encoder.layers.10.conv3x1_2.weight', 'module.encoder.layers.10.conv3x1_2.bias', 'module.encoder.layers.10.conv1x3_2.weight', 'module.encoder.layers.10.conv1x3_2.bias', 'module.encoder.layers.10.bn2.weight', 'module.encoder.layers.10.bn2.bias', 'module.encoder.layers.10.bn2.running_mean', 'module.encoder.layers.10.bn2.running_var', 'module.encoder.layers.10.bn2.num_batches_tracked', 'module.encoder.layers.11.conv3x1_1.weight', 'module.encoder.layers.11.conv3x1_1.bias', 'module.encoder.layers.11.conv1x3_1.weight', 'module.encoder.layers.11.conv1x3_1.bias', 'module.encoder.layers.11.bn1.weight', 'module.encoder.layers.11.bn1.bias', 'module.encoder.layers.11.bn1.running_mean', 'module.encoder.layers.11.bn1.running_var', 'module.encoder.layers.11.bn1.num_batches_tracked', 'module.encoder.layers.11.conv3x1_2.weight', 'module.encoder.layers.11.conv3x1_2.bias', 'module.encoder.layers.11.conv1x3_2.weight', 'module.encoder.layers.11.conv1x3_2.bias', 'module.encoder.layers.11.bn2.weight', 'module.encoder.layers.11.bn2.bias', 'module.encoder.layers.11.bn2.running_mean', 'module.encoder.layers.11.bn2.running_var', 'module.encoder.layers.11.bn2.num_batches_tracked', 'module.encoder.layers.12.conv3x1_1.weight', 'module.encoder.layers.12.conv3x1_1.bias', 'module.encoder.layers.12.conv1x3_1.weight', 'module.encoder.layers.12.conv1x3_1.bias', 'module.encoder.layers.12.bn1.weight', 'module.encoder.layers.12.bn1.bias', 'module.encoder.layers.12.bn1.running_mean', 'module.encoder.layers.12.bn1.running_var', 'module.encoder.layers.12.bn1.num_batches_tracked', 'module.encoder.layers.12.conv3x1_2.weight', 'module.encoder.layers.12.conv3x1_2.bias', 'module.encoder.layers.12.conv1x3_2.weight', 'module.encoder.layers.12.conv1x3_2.bias', 'module.encoder.layers.12.bn2.weight', 'module.encoder.layers.12.bn2.bias', 'module.encoder.layers.12.bn2.running_mean', 'module.encoder.layers.12.bn2.running_var', 'module.encoder.layers.12.bn2.num_batches_tracked', 'module.encoder.layers.13.conv3x1_1.weight', 'module.encoder.layers.13.conv3x1_1.bias', 'module.encoder.layers.13.conv1x3_1.weight', 'module.encoder.layers.13.conv1x3_1.bias', 'module.encoder.layers.13.bn1.weight', 'module.encoder.layers.13.bn1.bias', 'module.encoder.layers.13.bn1.running_mean', 'module.encoder.layers.13.bn1.running_var', 'module.encoder.layers.13.bn1.num_batches_tracked', 'module.encoder.layers.13.conv3x1_2.weight', 'module.encoder.layers.13.conv3x1_2.bias', 'module.encoder.layers.13.conv1x3_2.weight', 'module.encoder.layers.13.conv1x3_2.bias', 'module.encoder.layers.13.bn2.weight', 'module.encoder.layers.13.bn2.bias', 'module.encoder.layers.13.bn2.running_mean', 'module.encoder.layers.13.bn2.running_var', 'module.encoder.layers.13.bn2.num_batches_tracked', 'module.encoder.layers.14.conv3x1_1.weight', 'module.encoder.layers.14.conv3x1_1.bias', 'module.encoder.layers.14.conv1x3_1.weight', 'module.encoder.layers.14.conv1x3_1.bias', 'module.encoder.layers.14.bn1.weight', 'module.encoder.layers.14.bn1.bias', 'module.encoder.layers.14.bn1.running_mean', 'module.encoder.layers.14.bn1.running_var', 'module.encoder.layers.14.bn1.num_batches_tracked', 'module.encoder.layers.14.conv3x1_2.weight', 'module.encoder.layers.14.conv3x1_2.bias', 'module.encoder.layers.14.conv1x3_2.weight', 'module.encoder.layers.14.conv1x3_2.bias', 'module.encoder.layers.14.bn2.weight', 'module.encoder.layers.14.bn2.bias', 'module.encoder.layers.14.bn2.running_mean', 'module.encoder.layers.14.bn2.running_var', 'module.encoder.layers.14.bn2.num_batches_tracked', 'module.encoder.output_conv.weight', 'module.encoder.output_conv.bias', 'module.decoder.layers.0.conv.weight', 'module.decoder.layers.0.conv.bias', 'module.decoder.layers.0.bn.weight', 'module.decoder.layers.0.bn.bias', 'module.decoder.layers.0.bn.running_mean', 'module.decoder.layers.0.bn.running_var', 'module.decoder.layers.0.bn.num_batches_tracked', 'module.decoder.layers.1.conv3x1_1.weight', 'module.decoder.layers.1.conv3x1_1.bias', 'module.decoder.layers.1.conv1x3_1.weight', 'module.decoder.layers.1.conv1x3_1.bias', 'module.decoder.layers.1.bn1.weight', 'module.decoder.layers.1.bn1.bias', 'module.decoder.layers.1.bn1.running_mean', 'module.decoder.layers.1.bn1.running_var', 'module.decoder.layers.1.bn1.num_batches_tracked', 'module.decoder.layers.1.conv3x1_2.weight', 'module.decoder.layers.1.conv3x1_2.bias', 'module.decoder.layers.1.conv1x3_2.weight', 'module.decoder.layers.1.conv1x3_2.bias', 'module.decoder.layers.1.bn2.weight', 'module.decoder.layers.1.bn2.bias', 'module.decoder.layers.1.bn2.running_mean', 'module.decoder.layers.1.bn2.running_var', 'module.decoder.layers.1.bn2.num_batches_tracked', 'module.decoder.layers.2.conv3x1_1.weight', 'module.decoder.layers.2.conv3x1_1.bias', 'module.decoder.layers.2.conv1x3_1.weight', 'module.decoder.layers.2.conv1x3_1.bias', 'module.decoder.layers.2.bn1.weight', 'module.decoder.layers.2.bn1.bias', 'module.decoder.layers.2.bn1.running_mean', 'module.decoder.layers.2.bn1.running_var', 'module.decoder.layers.2.bn1.num_batches_tracked', 'module.decoder.layers.2.conv3x1_2.weight', 'module.decoder.layers.2.conv3x1_2.bias', 'module.decoder.layers.2.conv1x3_2.weight', 'module.decoder.layers.2.conv1x3_2.bias', 'module.decoder.layers.2.bn2.weight', 'module.decoder.layers.2.bn2.bias', 'module.decoder.layers.2.bn2.running_mean', 'module.decoder.layers.2.bn2.running_var', 'module.decoder.layers.2.bn2.num_batches_tracked', 'module.decoder.layers.3.conv.weight', 'module.decoder.layers.3.conv.bias', 'module.decoder.layers.3.bn.weight', 'module.decoder.layers.3.bn.bias', 'module.decoder.layers.3.bn.running_mean', 'module.decoder.layers.3.bn.running_var', 'module.decoder.layers.3.bn.num_batches_tracked', 'module.decoder.layers.4.conv3x1_1.weight', 'module.decoder.layers.4.conv3x1_1.bias', 'module.decoder.layers.4.conv1x3_1.weight', 'module.decoder.layers.4.conv1x3_1.bias', 'module.decoder.layers.4.bn1.weight', 'module.decoder.layers.4.bn1.bias', 'module.decoder.layers.4.bn1.running_mean', 'module.decoder.layers.4.bn1.running_var', 'module.decoder.layers.4.bn1.num_batches_tracked', 'module.decoder.layers.4.conv3x1_2.weight', 'module.decoder.layers.4.conv3x1_2.bias', 'module.decoder.layers.4.conv1x3_2.weight', 'module.decoder.layers.4.conv1x3_2.bias', 'module.decoder.layers.4.bn2.weight', 'module.decoder.layers.4.bn2.bias', 'module.decoder.layers.4.bn2.running_mean', 'module.decoder.layers.4.bn2.running_var', 'module.decoder.layers.4.bn2.num_batches_tracked', 'module.decoder.layers.5.conv3x1_1.weight', 'module.decoder.layers.5.conv3x1_1.bias', 'module.decoder.layers.5.conv1x3_1.weight', 'module.decoder.layers.5.conv1x3_1.bias', 'module.decoder.layers.5.bn1.weight', 'module.decoder.layers.5.bn1.bias', 'module.decoder.layers.5.bn1.running_mean', 'module.decoder.layers.5.bn1.running_var', 'module.decoder.layers.5.bn1.num_batches_tracked', 'module.decoder.layers.5.conv3x1_2.weight', 'module.decoder.layers.5.conv3x1_2.bias', 'module.decoder.layers.5.conv1x3_2.weight', 'module.decoder.layers.5.conv1x3_2.bias', 'module.decoder.layers.5.bn2.weight', 'module.decoder.layers.5.bn2.bias', 'module.decoder.layers.5.bn2.running_mean', 'module.decoder.layers.5.bn2.running_var', 'module.decoder.layers.5.bn2.num_batches_tracked', 'module.decoder.output_conv.weight', 'module.decoder.output_conv.bias'])\n",
            "odict_keys(['module.encoder.initial_block.conv.weight', 'module.encoder.initial_block.conv.bias', 'module.encoder.initial_block.bn.weight', 'module.encoder.initial_block.bn.bias', 'module.encoder.initial_block.bn.running_mean', 'module.encoder.initial_block.bn.running_var', 'module.encoder.initial_block.bn.num_batches_tracked', 'module.encoder.layers.0.conv.weight', 'module.encoder.layers.0.conv.bias', 'module.encoder.layers.0.bn.weight', 'module.encoder.layers.0.bn.bias', 'module.encoder.layers.0.bn.running_mean', 'module.encoder.layers.0.bn.running_var', 'module.encoder.layers.0.bn.num_batches_tracked', 'module.encoder.layers.1.conv3x1_1.weight', 'module.encoder.layers.1.conv3x1_1.bias', 'module.encoder.layers.1.conv1x3_1.weight', 'module.encoder.layers.1.conv1x3_1.bias', 'module.encoder.layers.1.bn1.weight', 'module.encoder.layers.1.bn1.bias', 'module.encoder.layers.1.bn1.running_mean', 'module.encoder.layers.1.bn1.running_var', 'module.encoder.layers.1.bn1.num_batches_tracked', 'module.encoder.layers.1.conv3x1_2.weight', 'module.encoder.layers.1.conv3x1_2.bias', 'module.encoder.layers.1.conv1x3_2.weight', 'module.encoder.layers.1.conv1x3_2.bias', 'module.encoder.layers.1.bn2.weight', 'module.encoder.layers.1.bn2.bias', 'module.encoder.layers.1.bn2.running_mean', 'module.encoder.layers.1.bn2.running_var', 'module.encoder.layers.1.bn2.num_batches_tracked', 'module.encoder.layers.2.conv3x1_1.weight', 'module.encoder.layers.2.conv3x1_1.bias', 'module.encoder.layers.2.conv1x3_1.weight', 'module.encoder.layers.2.conv1x3_1.bias', 'module.encoder.layers.2.bn1.weight', 'module.encoder.layers.2.bn1.bias', 'module.encoder.layers.2.bn1.running_mean', 'module.encoder.layers.2.bn1.running_var', 'module.encoder.layers.2.bn1.num_batches_tracked', 'module.encoder.layers.2.conv3x1_2.weight', 'module.encoder.layers.2.conv3x1_2.bias', 'module.encoder.layers.2.conv1x3_2.weight', 'module.encoder.layers.2.conv1x3_2.bias', 'module.encoder.layers.2.bn2.weight', 'module.encoder.layers.2.bn2.bias', 'module.encoder.layers.2.bn2.running_mean', 'module.encoder.layers.2.bn2.running_var', 'module.encoder.layers.2.bn2.num_batches_tracked', 'module.encoder.layers.3.conv3x1_1.weight', 'module.encoder.layers.3.conv3x1_1.bias', 'module.encoder.layers.3.conv1x3_1.weight', 'module.encoder.layers.3.conv1x3_1.bias', 'module.encoder.layers.3.bn1.weight', 'module.encoder.layers.3.bn1.bias', 'module.encoder.layers.3.bn1.running_mean', 'module.encoder.layers.3.bn1.running_var', 'module.encoder.layers.3.bn1.num_batches_tracked', 'module.encoder.layers.3.conv3x1_2.weight', 'module.encoder.layers.3.conv3x1_2.bias', 'module.encoder.layers.3.conv1x3_2.weight', 'module.encoder.layers.3.conv1x3_2.bias', 'module.encoder.layers.3.bn2.weight', 'module.encoder.layers.3.bn2.bias', 'module.encoder.layers.3.bn2.running_mean', 'module.encoder.layers.3.bn2.running_var', 'module.encoder.layers.3.bn2.num_batches_tracked', 'module.encoder.layers.4.conv3x1_1.weight', 'module.encoder.layers.4.conv3x1_1.bias', 'module.encoder.layers.4.conv1x3_1.weight', 'module.encoder.layers.4.conv1x3_1.bias', 'module.encoder.layers.4.bn1.weight', 'module.encoder.layers.4.bn1.bias', 'module.encoder.layers.4.bn1.running_mean', 'module.encoder.layers.4.bn1.running_var', 'module.encoder.layers.4.bn1.num_batches_tracked', 'module.encoder.layers.4.conv3x1_2.weight', 'module.encoder.layers.4.conv3x1_2.bias', 'module.encoder.layers.4.conv1x3_2.weight', 'module.encoder.layers.4.conv1x3_2.bias', 'module.encoder.layers.4.bn2.weight', 'module.encoder.layers.4.bn2.bias', 'module.encoder.layers.4.bn2.running_mean', 'module.encoder.layers.4.bn2.running_var', 'module.encoder.layers.4.bn2.num_batches_tracked', 'module.encoder.layers.5.conv3x1_1.weight', 'module.encoder.layers.5.conv3x1_1.bias', 'module.encoder.layers.5.conv1x3_1.weight', 'module.encoder.layers.5.conv1x3_1.bias', 'module.encoder.layers.5.bn1.weight', 'module.encoder.layers.5.bn1.bias', 'module.encoder.layers.5.bn1.running_mean', 'module.encoder.layers.5.bn1.running_var', 'module.encoder.layers.5.bn1.num_batches_tracked', 'module.encoder.layers.5.conv3x1_2.weight', 'module.encoder.layers.5.conv3x1_2.bias', 'module.encoder.layers.5.conv1x3_2.weight', 'module.encoder.layers.5.conv1x3_2.bias', 'module.encoder.layers.5.bn2.weight', 'module.encoder.layers.5.bn2.bias', 'module.encoder.layers.5.bn2.running_mean', 'module.encoder.layers.5.bn2.running_var', 'module.encoder.layers.5.bn2.num_batches_tracked', 'module.encoder.layers.6.conv.weight', 'module.encoder.layers.6.conv.bias', 'module.encoder.layers.6.bn.weight', 'module.encoder.layers.6.bn.bias', 'module.encoder.layers.6.bn.running_mean', 'module.encoder.layers.6.bn.running_var', 'module.encoder.layers.6.bn.num_batches_tracked', 'module.encoder.layers.7.conv3x1_1.weight', 'module.encoder.layers.7.conv3x1_1.bias', 'module.encoder.layers.7.conv1x3_1.weight', 'module.encoder.layers.7.conv1x3_1.bias', 'module.encoder.layers.7.bn1.weight', 'module.encoder.layers.7.bn1.bias', 'module.encoder.layers.7.bn1.running_mean', 'module.encoder.layers.7.bn1.running_var', 'module.encoder.layers.7.bn1.num_batches_tracked', 'module.encoder.layers.7.conv3x1_2.weight', 'module.encoder.layers.7.conv3x1_2.bias', 'module.encoder.layers.7.conv1x3_2.weight', 'module.encoder.layers.7.conv1x3_2.bias', 'module.encoder.layers.7.bn2.weight', 'module.encoder.layers.7.bn2.bias', 'module.encoder.layers.7.bn2.running_mean', 'module.encoder.layers.7.bn2.running_var', 'module.encoder.layers.7.bn2.num_batches_tracked', 'module.encoder.layers.8.conv3x1_1.weight', 'module.encoder.layers.8.conv3x1_1.bias', 'module.encoder.layers.8.conv1x3_1.weight', 'module.encoder.layers.8.conv1x3_1.bias', 'module.encoder.layers.8.bn1.weight', 'module.encoder.layers.8.bn1.bias', 'module.encoder.layers.8.bn1.running_mean', 'module.encoder.layers.8.bn1.running_var', 'module.encoder.layers.8.bn1.num_batches_tracked', 'module.encoder.layers.8.conv3x1_2.weight', 'module.encoder.layers.8.conv3x1_2.bias', 'module.encoder.layers.8.conv1x3_2.weight', 'module.encoder.layers.8.conv1x3_2.bias', 'module.encoder.layers.8.bn2.weight', 'module.encoder.layers.8.bn2.bias', 'module.encoder.layers.8.bn2.running_mean', 'module.encoder.layers.8.bn2.running_var', 'module.encoder.layers.8.bn2.num_batches_tracked', 'module.encoder.layers.9.conv3x1_1.weight', 'module.encoder.layers.9.conv3x1_1.bias', 'module.encoder.layers.9.conv1x3_1.weight', 'module.encoder.layers.9.conv1x3_1.bias', 'module.encoder.layers.9.bn1.weight', 'module.encoder.layers.9.bn1.bias', 'module.encoder.layers.9.bn1.running_mean', 'module.encoder.layers.9.bn1.running_var', 'module.encoder.layers.9.bn1.num_batches_tracked', 'module.encoder.layers.9.conv3x1_2.weight', 'module.encoder.layers.9.conv3x1_2.bias', 'module.encoder.layers.9.conv1x3_2.weight', 'module.encoder.layers.9.conv1x3_2.bias', 'module.encoder.layers.9.bn2.weight', 'module.encoder.layers.9.bn2.bias', 'module.encoder.layers.9.bn2.running_mean', 'module.encoder.layers.9.bn2.running_var', 'module.encoder.layers.9.bn2.num_batches_tracked', 'module.encoder.layers.10.conv3x1_1.weight', 'module.encoder.layers.10.conv3x1_1.bias', 'module.encoder.layers.10.conv1x3_1.weight', 'module.encoder.layers.10.conv1x3_1.bias', 'module.encoder.layers.10.bn1.weight', 'module.encoder.layers.10.bn1.bias', 'module.encoder.layers.10.bn1.running_mean', 'module.encoder.layers.10.bn1.running_var', 'module.encoder.layers.10.bn1.num_batches_tracked', 'module.encoder.layers.10.conv3x1_2.weight', 'module.encoder.layers.10.conv3x1_2.bias', 'module.encoder.layers.10.conv1x3_2.weight', 'module.encoder.layers.10.conv1x3_2.bias', 'module.encoder.layers.10.bn2.weight', 'module.encoder.layers.10.bn2.bias', 'module.encoder.layers.10.bn2.running_mean', 'module.encoder.layers.10.bn2.running_var', 'module.encoder.layers.10.bn2.num_batches_tracked', 'module.encoder.layers.11.conv3x1_1.weight', 'module.encoder.layers.11.conv3x1_1.bias', 'module.encoder.layers.11.conv1x3_1.weight', 'module.encoder.layers.11.conv1x3_1.bias', 'module.encoder.layers.11.bn1.weight', 'module.encoder.layers.11.bn1.bias', 'module.encoder.layers.11.bn1.running_mean', 'module.encoder.layers.11.bn1.running_var', 'module.encoder.layers.11.bn1.num_batches_tracked', 'module.encoder.layers.11.conv3x1_2.weight', 'module.encoder.layers.11.conv3x1_2.bias', 'module.encoder.layers.11.conv1x3_2.weight', 'module.encoder.layers.11.conv1x3_2.bias', 'module.encoder.layers.11.bn2.weight', 'module.encoder.layers.11.bn2.bias', 'module.encoder.layers.11.bn2.running_mean', 'module.encoder.layers.11.bn2.running_var', 'module.encoder.layers.11.bn2.num_batches_tracked', 'module.encoder.layers.12.conv3x1_1.weight', 'module.encoder.layers.12.conv3x1_1.bias', 'module.encoder.layers.12.conv1x3_1.weight', 'module.encoder.layers.12.conv1x3_1.bias', 'module.encoder.layers.12.bn1.weight', 'module.encoder.layers.12.bn1.bias', 'module.encoder.layers.12.bn1.running_mean', 'module.encoder.layers.12.bn1.running_var', 'module.encoder.layers.12.bn1.num_batches_tracked', 'module.encoder.layers.12.conv3x1_2.weight', 'module.encoder.layers.12.conv3x1_2.bias', 'module.encoder.layers.12.conv1x3_2.weight', 'module.encoder.layers.12.conv1x3_2.bias', 'module.encoder.layers.12.bn2.weight', 'module.encoder.layers.12.bn2.bias', 'module.encoder.layers.12.bn2.running_mean', 'module.encoder.layers.12.bn2.running_var', 'module.encoder.layers.12.bn2.num_batches_tracked', 'module.encoder.layers.13.conv3x1_1.weight', 'module.encoder.layers.13.conv3x1_1.bias', 'module.encoder.layers.13.conv1x3_1.weight', 'module.encoder.layers.13.conv1x3_1.bias', 'module.encoder.layers.13.bn1.weight', 'module.encoder.layers.13.bn1.bias', 'module.encoder.layers.13.bn1.running_mean', 'module.encoder.layers.13.bn1.running_var', 'module.encoder.layers.13.bn1.num_batches_tracked', 'module.encoder.layers.13.conv3x1_2.weight', 'module.encoder.layers.13.conv3x1_2.bias', 'module.encoder.layers.13.conv1x3_2.weight', 'module.encoder.layers.13.conv1x3_2.bias', 'module.encoder.layers.13.bn2.weight', 'module.encoder.layers.13.bn2.bias', 'module.encoder.layers.13.bn2.running_mean', 'module.encoder.layers.13.bn2.running_var', 'module.encoder.layers.13.bn2.num_batches_tracked', 'module.encoder.layers.14.conv3x1_1.weight', 'module.encoder.layers.14.conv3x1_1.bias', 'module.encoder.layers.14.conv1x3_1.weight', 'module.encoder.layers.14.conv1x3_1.bias', 'module.encoder.layers.14.bn1.weight', 'module.encoder.layers.14.bn1.bias', 'module.encoder.layers.14.bn1.running_mean', 'module.encoder.layers.14.bn1.running_var', 'module.encoder.layers.14.bn1.num_batches_tracked', 'module.encoder.layers.14.conv3x1_2.weight', 'module.encoder.layers.14.conv3x1_2.bias', 'module.encoder.layers.14.conv1x3_2.weight', 'module.encoder.layers.14.conv1x3_2.bias', 'module.encoder.layers.14.bn2.weight', 'module.encoder.layers.14.bn2.bias', 'module.encoder.layers.14.bn2.running_mean', 'module.encoder.layers.14.bn2.running_var', 'module.encoder.layers.14.bn2.num_batches_tracked', 'module.encoder.output_conv.weight', 'module.encoder.output_conv.bias', 'module.decoder.layers.0.conv.weight', 'module.decoder.layers.0.conv.bias', 'module.decoder.layers.0.bn.weight', 'module.decoder.layers.0.bn.bias', 'module.decoder.layers.0.bn.running_mean', 'module.decoder.layers.0.bn.running_var', 'module.decoder.layers.0.bn.num_batches_tracked', 'module.decoder.layers.1.conv3x1_1.weight', 'module.decoder.layers.1.conv3x1_1.bias', 'module.decoder.layers.1.conv1x3_1.weight', 'module.decoder.layers.1.conv1x3_1.bias', 'module.decoder.layers.1.bn1.weight', 'module.decoder.layers.1.bn1.bias', 'module.decoder.layers.1.bn1.running_mean', 'module.decoder.layers.1.bn1.running_var', 'module.decoder.layers.1.bn1.num_batches_tracked', 'module.decoder.layers.1.conv3x1_2.weight', 'module.decoder.layers.1.conv3x1_2.bias', 'module.decoder.layers.1.conv1x3_2.weight', 'module.decoder.layers.1.conv1x3_2.bias', 'module.decoder.layers.1.bn2.weight', 'module.decoder.layers.1.bn2.bias', 'module.decoder.layers.1.bn2.running_mean', 'module.decoder.layers.1.bn2.running_var', 'module.decoder.layers.1.bn2.num_batches_tracked', 'module.decoder.layers.2.conv3x1_1.weight', 'module.decoder.layers.2.conv3x1_1.bias', 'module.decoder.layers.2.conv1x3_1.weight', 'module.decoder.layers.2.conv1x3_1.bias', 'module.decoder.layers.2.bn1.weight', 'module.decoder.layers.2.bn1.bias', 'module.decoder.layers.2.bn1.running_mean', 'module.decoder.layers.2.bn1.running_var', 'module.decoder.layers.2.bn1.num_batches_tracked', 'module.decoder.layers.2.conv3x1_2.weight', 'module.decoder.layers.2.conv3x1_2.bias', 'module.decoder.layers.2.conv1x3_2.weight', 'module.decoder.layers.2.conv1x3_2.bias', 'module.decoder.layers.2.bn2.weight', 'module.decoder.layers.2.bn2.bias', 'module.decoder.layers.2.bn2.running_mean', 'module.decoder.layers.2.bn2.running_var', 'module.decoder.layers.2.bn2.num_batches_tracked', 'module.decoder.layers.3.conv.weight', 'module.decoder.layers.3.conv.bias', 'module.decoder.layers.3.bn.weight', 'module.decoder.layers.3.bn.bias', 'module.decoder.layers.3.bn.running_mean', 'module.decoder.layers.3.bn.running_var', 'module.decoder.layers.3.bn.num_batches_tracked', 'module.decoder.layers.4.conv3x1_1.weight', 'module.decoder.layers.4.conv3x1_1.bias', 'module.decoder.layers.4.conv1x3_1.weight', 'module.decoder.layers.4.conv1x3_1.bias', 'module.decoder.layers.4.bn1.weight', 'module.decoder.layers.4.bn1.bias', 'module.decoder.layers.4.bn1.running_mean', 'module.decoder.layers.4.bn1.running_var', 'module.decoder.layers.4.bn1.num_batches_tracked', 'module.decoder.layers.4.conv3x1_2.weight', 'module.decoder.layers.4.conv3x1_2.bias', 'module.decoder.layers.4.conv1x3_2.weight', 'module.decoder.layers.4.conv1x3_2.bias', 'module.decoder.layers.4.bn2.weight', 'module.decoder.layers.4.bn2.bias', 'module.decoder.layers.4.bn2.running_mean', 'module.decoder.layers.4.bn2.running_var', 'module.decoder.layers.4.bn2.num_batches_tracked', 'module.decoder.layers.5.conv3x1_1.weight', 'module.decoder.layers.5.conv3x1_1.bias', 'module.decoder.layers.5.conv1x3_1.weight', 'module.decoder.layers.5.conv1x3_1.bias', 'module.decoder.layers.5.bn1.weight', 'module.decoder.layers.5.bn1.bias', 'module.decoder.layers.5.bn1.running_mean', 'module.decoder.layers.5.bn1.running_var', 'module.decoder.layers.5.bn1.num_batches_tracked', 'module.decoder.layers.5.conv3x1_2.weight', 'module.decoder.layers.5.conv3x1_2.bias', 'module.decoder.layers.5.conv1x3_2.weight', 'module.decoder.layers.5.conv1x3_2.bias', 'module.decoder.layers.5.bn2.weight', 'module.decoder.layers.5.bn2.bias', 'module.decoder.layers.5.bn2.running_mean', 'module.decoder.layers.5.bn2.running_var', 'module.decoder.layers.5.bn2.num_batches_tracked', 'module.decoder.output_conv.weight', 'module.decoder.output_conv.bias'])\n",
            "Model and weights LOADED successfully\n",
            "100% 2975/2975 [07:42<00:00,  6.44it/s]\n",
            "Mean per class: (20,)\n",
            "Mean output saved as '/content/AnomalySegmentation/save/mean_cityscapes_erfnet.npy'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datadir = '/content/cityscapes'\n",
        "loadWeights = '/save/erfnet_training1/model_best.pth'\n",
        "loadDir = '/content/AnomalySegmentation'\n",
        "mean = \"/save/mean_cityscapes_erfnet.npy\"\n",
        "\n",
        "!python /content/AnomalySegmentation/eval/mahalanobis.py --datadir {datadir} --model erfnet --loadDir {loadDir} --loadWeights {loadWeights} --mean {mean} --num-workers 2"
      ],
      "metadata": {
        "id": "h5yP-t0skBGb",
        "outputId": "eb760714-4e45-416f-f608-a37f04439536",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model: erfnet\n",
            "Loading weights: /content/AnomalySegmentation/save/erfnet_training1/model_best.pth\n",
            "pre_computed_mean torch.Size([20])\n",
            "/content/cityscapes/leftImg8bit/train\n",
            "/content/AnomalySegmentation/eval/mahalanobis.py:133: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model = load_my_state_dict(model, torch.load(weightspath, map_location=lambda storage, loc: storage))\n",
            "odict_keys(['module.encoder.initial_block.conv.weight', 'module.encoder.initial_block.conv.bias', 'module.encoder.initial_block.bn.weight', 'module.encoder.initial_block.bn.bias', 'module.encoder.initial_block.bn.running_mean', 'module.encoder.initial_block.bn.running_var', 'module.encoder.initial_block.bn.num_batches_tracked', 'module.encoder.layers.0.conv.weight', 'module.encoder.layers.0.conv.bias', 'module.encoder.layers.0.bn.weight', 'module.encoder.layers.0.bn.bias', 'module.encoder.layers.0.bn.running_mean', 'module.encoder.layers.0.bn.running_var', 'module.encoder.layers.0.bn.num_batches_tracked', 'module.encoder.layers.1.conv3x1_1.weight', 'module.encoder.layers.1.conv3x1_1.bias', 'module.encoder.layers.1.conv1x3_1.weight', 'module.encoder.layers.1.conv1x3_1.bias', 'module.encoder.layers.1.bn1.weight', 'module.encoder.layers.1.bn1.bias', 'module.encoder.layers.1.bn1.running_mean', 'module.encoder.layers.1.bn1.running_var', 'module.encoder.layers.1.bn1.num_batches_tracked', 'module.encoder.layers.1.conv3x1_2.weight', 'module.encoder.layers.1.conv3x1_2.bias', 'module.encoder.layers.1.conv1x3_2.weight', 'module.encoder.layers.1.conv1x3_2.bias', 'module.encoder.layers.1.bn2.weight', 'module.encoder.layers.1.bn2.bias', 'module.encoder.layers.1.bn2.running_mean', 'module.encoder.layers.1.bn2.running_var', 'module.encoder.layers.1.bn2.num_batches_tracked', 'module.encoder.layers.2.conv3x1_1.weight', 'module.encoder.layers.2.conv3x1_1.bias', 'module.encoder.layers.2.conv1x3_1.weight', 'module.encoder.layers.2.conv1x3_1.bias', 'module.encoder.layers.2.bn1.weight', 'module.encoder.layers.2.bn1.bias', 'module.encoder.layers.2.bn1.running_mean', 'module.encoder.layers.2.bn1.running_var', 'module.encoder.layers.2.bn1.num_batches_tracked', 'module.encoder.layers.2.conv3x1_2.weight', 'module.encoder.layers.2.conv3x1_2.bias', 'module.encoder.layers.2.conv1x3_2.weight', 'module.encoder.layers.2.conv1x3_2.bias', 'module.encoder.layers.2.bn2.weight', 'module.encoder.layers.2.bn2.bias', 'module.encoder.layers.2.bn2.running_mean', 'module.encoder.layers.2.bn2.running_var', 'module.encoder.layers.2.bn2.num_batches_tracked', 'module.encoder.layers.3.conv3x1_1.weight', 'module.encoder.layers.3.conv3x1_1.bias', 'module.encoder.layers.3.conv1x3_1.weight', 'module.encoder.layers.3.conv1x3_1.bias', 'module.encoder.layers.3.bn1.weight', 'module.encoder.layers.3.bn1.bias', 'module.encoder.layers.3.bn1.running_mean', 'module.encoder.layers.3.bn1.running_var', 'module.encoder.layers.3.bn1.num_batches_tracked', 'module.encoder.layers.3.conv3x1_2.weight', 'module.encoder.layers.3.conv3x1_2.bias', 'module.encoder.layers.3.conv1x3_2.weight', 'module.encoder.layers.3.conv1x3_2.bias', 'module.encoder.layers.3.bn2.weight', 'module.encoder.layers.3.bn2.bias', 'module.encoder.layers.3.bn2.running_mean', 'module.encoder.layers.3.bn2.running_var', 'module.encoder.layers.3.bn2.num_batches_tracked', 'module.encoder.layers.4.conv3x1_1.weight', 'module.encoder.layers.4.conv3x1_1.bias', 'module.encoder.layers.4.conv1x3_1.weight', 'module.encoder.layers.4.conv1x3_1.bias', 'module.encoder.layers.4.bn1.weight', 'module.encoder.layers.4.bn1.bias', 'module.encoder.layers.4.bn1.running_mean', 'module.encoder.layers.4.bn1.running_var', 'module.encoder.layers.4.bn1.num_batches_tracked', 'module.encoder.layers.4.conv3x1_2.weight', 'module.encoder.layers.4.conv3x1_2.bias', 'module.encoder.layers.4.conv1x3_2.weight', 'module.encoder.layers.4.conv1x3_2.bias', 'module.encoder.layers.4.bn2.weight', 'module.encoder.layers.4.bn2.bias', 'module.encoder.layers.4.bn2.running_mean', 'module.encoder.layers.4.bn2.running_var', 'module.encoder.layers.4.bn2.num_batches_tracked', 'module.encoder.layers.5.conv3x1_1.weight', 'module.encoder.layers.5.conv3x1_1.bias', 'module.encoder.layers.5.conv1x3_1.weight', 'module.encoder.layers.5.conv1x3_1.bias', 'module.encoder.layers.5.bn1.weight', 'module.encoder.layers.5.bn1.bias', 'module.encoder.layers.5.bn1.running_mean', 'module.encoder.layers.5.bn1.running_var', 'module.encoder.layers.5.bn1.num_batches_tracked', 'module.encoder.layers.5.conv3x1_2.weight', 'module.encoder.layers.5.conv3x1_2.bias', 'module.encoder.layers.5.conv1x3_2.weight', 'module.encoder.layers.5.conv1x3_2.bias', 'module.encoder.layers.5.bn2.weight', 'module.encoder.layers.5.bn2.bias', 'module.encoder.layers.5.bn2.running_mean', 'module.encoder.layers.5.bn2.running_var', 'module.encoder.layers.5.bn2.num_batches_tracked', 'module.encoder.layers.6.conv.weight', 'module.encoder.layers.6.conv.bias', 'module.encoder.layers.6.bn.weight', 'module.encoder.layers.6.bn.bias', 'module.encoder.layers.6.bn.running_mean', 'module.encoder.layers.6.bn.running_var', 'module.encoder.layers.6.bn.num_batches_tracked', 'module.encoder.layers.7.conv3x1_1.weight', 'module.encoder.layers.7.conv3x1_1.bias', 'module.encoder.layers.7.conv1x3_1.weight', 'module.encoder.layers.7.conv1x3_1.bias', 'module.encoder.layers.7.bn1.weight', 'module.encoder.layers.7.bn1.bias', 'module.encoder.layers.7.bn1.running_mean', 'module.encoder.layers.7.bn1.running_var', 'module.encoder.layers.7.bn1.num_batches_tracked', 'module.encoder.layers.7.conv3x1_2.weight', 'module.encoder.layers.7.conv3x1_2.bias', 'module.encoder.layers.7.conv1x3_2.weight', 'module.encoder.layers.7.conv1x3_2.bias', 'module.encoder.layers.7.bn2.weight', 'module.encoder.layers.7.bn2.bias', 'module.encoder.layers.7.bn2.running_mean', 'module.encoder.layers.7.bn2.running_var', 'module.encoder.layers.7.bn2.num_batches_tracked', 'module.encoder.layers.8.conv3x1_1.weight', 'module.encoder.layers.8.conv3x1_1.bias', 'module.encoder.layers.8.conv1x3_1.weight', 'module.encoder.layers.8.conv1x3_1.bias', 'module.encoder.layers.8.bn1.weight', 'module.encoder.layers.8.bn1.bias', 'module.encoder.layers.8.bn1.running_mean', 'module.encoder.layers.8.bn1.running_var', 'module.encoder.layers.8.bn1.num_batches_tracked', 'module.encoder.layers.8.conv3x1_2.weight', 'module.encoder.layers.8.conv3x1_2.bias', 'module.encoder.layers.8.conv1x3_2.weight', 'module.encoder.layers.8.conv1x3_2.bias', 'module.encoder.layers.8.bn2.weight', 'module.encoder.layers.8.bn2.bias', 'module.encoder.layers.8.bn2.running_mean', 'module.encoder.layers.8.bn2.running_var', 'module.encoder.layers.8.bn2.num_batches_tracked', 'module.encoder.layers.9.conv3x1_1.weight', 'module.encoder.layers.9.conv3x1_1.bias', 'module.encoder.layers.9.conv1x3_1.weight', 'module.encoder.layers.9.conv1x3_1.bias', 'module.encoder.layers.9.bn1.weight', 'module.encoder.layers.9.bn1.bias', 'module.encoder.layers.9.bn1.running_mean', 'module.encoder.layers.9.bn1.running_var', 'module.encoder.layers.9.bn1.num_batches_tracked', 'module.encoder.layers.9.conv3x1_2.weight', 'module.encoder.layers.9.conv3x1_2.bias', 'module.encoder.layers.9.conv1x3_2.weight', 'module.encoder.layers.9.conv1x3_2.bias', 'module.encoder.layers.9.bn2.weight', 'module.encoder.layers.9.bn2.bias', 'module.encoder.layers.9.bn2.running_mean', 'module.encoder.layers.9.bn2.running_var', 'module.encoder.layers.9.bn2.num_batches_tracked', 'module.encoder.layers.10.conv3x1_1.weight', 'module.encoder.layers.10.conv3x1_1.bias', 'module.encoder.layers.10.conv1x3_1.weight', 'module.encoder.layers.10.conv1x3_1.bias', 'module.encoder.layers.10.bn1.weight', 'module.encoder.layers.10.bn1.bias', 'module.encoder.layers.10.bn1.running_mean', 'module.encoder.layers.10.bn1.running_var', 'module.encoder.layers.10.bn1.num_batches_tracked', 'module.encoder.layers.10.conv3x1_2.weight', 'module.encoder.layers.10.conv3x1_2.bias', 'module.encoder.layers.10.conv1x3_2.weight', 'module.encoder.layers.10.conv1x3_2.bias', 'module.encoder.layers.10.bn2.weight', 'module.encoder.layers.10.bn2.bias', 'module.encoder.layers.10.bn2.running_mean', 'module.encoder.layers.10.bn2.running_var', 'module.encoder.layers.10.bn2.num_batches_tracked', 'module.encoder.layers.11.conv3x1_1.weight', 'module.encoder.layers.11.conv3x1_1.bias', 'module.encoder.layers.11.conv1x3_1.weight', 'module.encoder.layers.11.conv1x3_1.bias', 'module.encoder.layers.11.bn1.weight', 'module.encoder.layers.11.bn1.bias', 'module.encoder.layers.11.bn1.running_mean', 'module.encoder.layers.11.bn1.running_var', 'module.encoder.layers.11.bn1.num_batches_tracked', 'module.encoder.layers.11.conv3x1_2.weight', 'module.encoder.layers.11.conv3x1_2.bias', 'module.encoder.layers.11.conv1x3_2.weight', 'module.encoder.layers.11.conv1x3_2.bias', 'module.encoder.layers.11.bn2.weight', 'module.encoder.layers.11.bn2.bias', 'module.encoder.layers.11.bn2.running_mean', 'module.encoder.layers.11.bn2.running_var', 'module.encoder.layers.11.bn2.num_batches_tracked', 'module.encoder.layers.12.conv3x1_1.weight', 'module.encoder.layers.12.conv3x1_1.bias', 'module.encoder.layers.12.conv1x3_1.weight', 'module.encoder.layers.12.conv1x3_1.bias', 'module.encoder.layers.12.bn1.weight', 'module.encoder.layers.12.bn1.bias', 'module.encoder.layers.12.bn1.running_mean', 'module.encoder.layers.12.bn1.running_var', 'module.encoder.layers.12.bn1.num_batches_tracked', 'module.encoder.layers.12.conv3x1_2.weight', 'module.encoder.layers.12.conv3x1_2.bias', 'module.encoder.layers.12.conv1x3_2.weight', 'module.encoder.layers.12.conv1x3_2.bias', 'module.encoder.layers.12.bn2.weight', 'module.encoder.layers.12.bn2.bias', 'module.encoder.layers.12.bn2.running_mean', 'module.encoder.layers.12.bn2.running_var', 'module.encoder.layers.12.bn2.num_batches_tracked', 'module.encoder.layers.13.conv3x1_1.weight', 'module.encoder.layers.13.conv3x1_1.bias', 'module.encoder.layers.13.conv1x3_1.weight', 'module.encoder.layers.13.conv1x3_1.bias', 'module.encoder.layers.13.bn1.weight', 'module.encoder.layers.13.bn1.bias', 'module.encoder.layers.13.bn1.running_mean', 'module.encoder.layers.13.bn1.running_var', 'module.encoder.layers.13.bn1.num_batches_tracked', 'module.encoder.layers.13.conv3x1_2.weight', 'module.encoder.layers.13.conv3x1_2.bias', 'module.encoder.layers.13.conv1x3_2.weight', 'module.encoder.layers.13.conv1x3_2.bias', 'module.encoder.layers.13.bn2.weight', 'module.encoder.layers.13.bn2.bias', 'module.encoder.layers.13.bn2.running_mean', 'module.encoder.layers.13.bn2.running_var', 'module.encoder.layers.13.bn2.num_batches_tracked', 'module.encoder.layers.14.conv3x1_1.weight', 'module.encoder.layers.14.conv3x1_1.bias', 'module.encoder.layers.14.conv1x3_1.weight', 'module.encoder.layers.14.conv1x3_1.bias', 'module.encoder.layers.14.bn1.weight', 'module.encoder.layers.14.bn1.bias', 'module.encoder.layers.14.bn1.running_mean', 'module.encoder.layers.14.bn1.running_var', 'module.encoder.layers.14.bn1.num_batches_tracked', 'module.encoder.layers.14.conv3x1_2.weight', 'module.encoder.layers.14.conv3x1_2.bias', 'module.encoder.layers.14.conv1x3_2.weight', 'module.encoder.layers.14.conv1x3_2.bias', 'module.encoder.layers.14.bn2.weight', 'module.encoder.layers.14.bn2.bias', 'module.encoder.layers.14.bn2.running_mean', 'module.encoder.layers.14.bn2.running_var', 'module.encoder.layers.14.bn2.num_batches_tracked', 'module.encoder.output_conv.weight', 'module.encoder.output_conv.bias', 'module.decoder.layers.0.conv.weight', 'module.decoder.layers.0.conv.bias', 'module.decoder.layers.0.bn.weight', 'module.decoder.layers.0.bn.bias', 'module.decoder.layers.0.bn.running_mean', 'module.decoder.layers.0.bn.running_var', 'module.decoder.layers.0.bn.num_batches_tracked', 'module.decoder.layers.1.conv3x1_1.weight', 'module.decoder.layers.1.conv3x1_1.bias', 'module.decoder.layers.1.conv1x3_1.weight', 'module.decoder.layers.1.conv1x3_1.bias', 'module.decoder.layers.1.bn1.weight', 'module.decoder.layers.1.bn1.bias', 'module.decoder.layers.1.bn1.running_mean', 'module.decoder.layers.1.bn1.running_var', 'module.decoder.layers.1.bn1.num_batches_tracked', 'module.decoder.layers.1.conv3x1_2.weight', 'module.decoder.layers.1.conv3x1_2.bias', 'module.decoder.layers.1.conv1x3_2.weight', 'module.decoder.layers.1.conv1x3_2.bias', 'module.decoder.layers.1.bn2.weight', 'module.decoder.layers.1.bn2.bias', 'module.decoder.layers.1.bn2.running_mean', 'module.decoder.layers.1.bn2.running_var', 'module.decoder.layers.1.bn2.num_batches_tracked', 'module.decoder.layers.2.conv3x1_1.weight', 'module.decoder.layers.2.conv3x1_1.bias', 'module.decoder.layers.2.conv1x3_1.weight', 'module.decoder.layers.2.conv1x3_1.bias', 'module.decoder.layers.2.bn1.weight', 'module.decoder.layers.2.bn1.bias', 'module.decoder.layers.2.bn1.running_mean', 'module.decoder.layers.2.bn1.running_var', 'module.decoder.layers.2.bn1.num_batches_tracked', 'module.decoder.layers.2.conv3x1_2.weight', 'module.decoder.layers.2.conv3x1_2.bias', 'module.decoder.layers.2.conv1x3_2.weight', 'module.decoder.layers.2.conv1x3_2.bias', 'module.decoder.layers.2.bn2.weight', 'module.decoder.layers.2.bn2.bias', 'module.decoder.layers.2.bn2.running_mean', 'module.decoder.layers.2.bn2.running_var', 'module.decoder.layers.2.bn2.num_batches_tracked', 'module.decoder.layers.3.conv.weight', 'module.decoder.layers.3.conv.bias', 'module.decoder.layers.3.bn.weight', 'module.decoder.layers.3.bn.bias', 'module.decoder.layers.3.bn.running_mean', 'module.decoder.layers.3.bn.running_var', 'module.decoder.layers.3.bn.num_batches_tracked', 'module.decoder.layers.4.conv3x1_1.weight', 'module.decoder.layers.4.conv3x1_1.bias', 'module.decoder.layers.4.conv1x3_1.weight', 'module.decoder.layers.4.conv1x3_1.bias', 'module.decoder.layers.4.bn1.weight', 'module.decoder.layers.4.bn1.bias', 'module.decoder.layers.4.bn1.running_mean', 'module.decoder.layers.4.bn1.running_var', 'module.decoder.layers.4.bn1.num_batches_tracked', 'module.decoder.layers.4.conv3x1_2.weight', 'module.decoder.layers.4.conv3x1_2.bias', 'module.decoder.layers.4.conv1x3_2.weight', 'module.decoder.layers.4.conv1x3_2.bias', 'module.decoder.layers.4.bn2.weight', 'module.decoder.layers.4.bn2.bias', 'module.decoder.layers.4.bn2.running_mean', 'module.decoder.layers.4.bn2.running_var', 'module.decoder.layers.4.bn2.num_batches_tracked', 'module.decoder.layers.5.conv3x1_1.weight', 'module.decoder.layers.5.conv3x1_1.bias', 'module.decoder.layers.5.conv1x3_1.weight', 'module.decoder.layers.5.conv1x3_1.bias', 'module.decoder.layers.5.bn1.weight', 'module.decoder.layers.5.bn1.bias', 'module.decoder.layers.5.bn1.running_mean', 'module.decoder.layers.5.bn1.running_var', 'module.decoder.layers.5.bn1.num_batches_tracked', 'module.decoder.layers.5.conv3x1_2.weight', 'module.decoder.layers.5.conv3x1_2.bias', 'module.decoder.layers.5.conv1x3_2.weight', 'module.decoder.layers.5.conv1x3_2.bias', 'module.decoder.layers.5.bn2.weight', 'module.decoder.layers.5.bn2.bias', 'module.decoder.layers.5.bn2.running_mean', 'module.decoder.layers.5.bn2.running_var', 'module.decoder.layers.5.bn2.num_batches_tracked', 'module.decoder.output_conv.weight', 'module.decoder.output_conv.bias'])\n",
            "odict_keys(['module.encoder.initial_block.conv.weight', 'module.encoder.initial_block.conv.bias', 'module.encoder.initial_block.bn.weight', 'module.encoder.initial_block.bn.bias', 'module.encoder.initial_block.bn.running_mean', 'module.encoder.initial_block.bn.running_var', 'module.encoder.initial_block.bn.num_batches_tracked', 'module.encoder.layers.0.conv.weight', 'module.encoder.layers.0.conv.bias', 'module.encoder.layers.0.bn.weight', 'module.encoder.layers.0.bn.bias', 'module.encoder.layers.0.bn.running_mean', 'module.encoder.layers.0.bn.running_var', 'module.encoder.layers.0.bn.num_batches_tracked', 'module.encoder.layers.1.conv3x1_1.weight', 'module.encoder.layers.1.conv3x1_1.bias', 'module.encoder.layers.1.conv1x3_1.weight', 'module.encoder.layers.1.conv1x3_1.bias', 'module.encoder.layers.1.bn1.weight', 'module.encoder.layers.1.bn1.bias', 'module.encoder.layers.1.bn1.running_mean', 'module.encoder.layers.1.bn1.running_var', 'module.encoder.layers.1.bn1.num_batches_tracked', 'module.encoder.layers.1.conv3x1_2.weight', 'module.encoder.layers.1.conv3x1_2.bias', 'module.encoder.layers.1.conv1x3_2.weight', 'module.encoder.layers.1.conv1x3_2.bias', 'module.encoder.layers.1.bn2.weight', 'module.encoder.layers.1.bn2.bias', 'module.encoder.layers.1.bn2.running_mean', 'module.encoder.layers.1.bn2.running_var', 'module.encoder.layers.1.bn2.num_batches_tracked', 'module.encoder.layers.2.conv3x1_1.weight', 'module.encoder.layers.2.conv3x1_1.bias', 'module.encoder.layers.2.conv1x3_1.weight', 'module.encoder.layers.2.conv1x3_1.bias', 'module.encoder.layers.2.bn1.weight', 'module.encoder.layers.2.bn1.bias', 'module.encoder.layers.2.bn1.running_mean', 'module.encoder.layers.2.bn1.running_var', 'module.encoder.layers.2.bn1.num_batches_tracked', 'module.encoder.layers.2.conv3x1_2.weight', 'module.encoder.layers.2.conv3x1_2.bias', 'module.encoder.layers.2.conv1x3_2.weight', 'module.encoder.layers.2.conv1x3_2.bias', 'module.encoder.layers.2.bn2.weight', 'module.encoder.layers.2.bn2.bias', 'module.encoder.layers.2.bn2.running_mean', 'module.encoder.layers.2.bn2.running_var', 'module.encoder.layers.2.bn2.num_batches_tracked', 'module.encoder.layers.3.conv3x1_1.weight', 'module.encoder.layers.3.conv3x1_1.bias', 'module.encoder.layers.3.conv1x3_1.weight', 'module.encoder.layers.3.conv1x3_1.bias', 'module.encoder.layers.3.bn1.weight', 'module.encoder.layers.3.bn1.bias', 'module.encoder.layers.3.bn1.running_mean', 'module.encoder.layers.3.bn1.running_var', 'module.encoder.layers.3.bn1.num_batches_tracked', 'module.encoder.layers.3.conv3x1_2.weight', 'module.encoder.layers.3.conv3x1_2.bias', 'module.encoder.layers.3.conv1x3_2.weight', 'module.encoder.layers.3.conv1x3_2.bias', 'module.encoder.layers.3.bn2.weight', 'module.encoder.layers.3.bn2.bias', 'module.encoder.layers.3.bn2.running_mean', 'module.encoder.layers.3.bn2.running_var', 'module.encoder.layers.3.bn2.num_batches_tracked', 'module.encoder.layers.4.conv3x1_1.weight', 'module.encoder.layers.4.conv3x1_1.bias', 'module.encoder.layers.4.conv1x3_1.weight', 'module.encoder.layers.4.conv1x3_1.bias', 'module.encoder.layers.4.bn1.weight', 'module.encoder.layers.4.bn1.bias', 'module.encoder.layers.4.bn1.running_mean', 'module.encoder.layers.4.bn1.running_var', 'module.encoder.layers.4.bn1.num_batches_tracked', 'module.encoder.layers.4.conv3x1_2.weight', 'module.encoder.layers.4.conv3x1_2.bias', 'module.encoder.layers.4.conv1x3_2.weight', 'module.encoder.layers.4.conv1x3_2.bias', 'module.encoder.layers.4.bn2.weight', 'module.encoder.layers.4.bn2.bias', 'module.encoder.layers.4.bn2.running_mean', 'module.encoder.layers.4.bn2.running_var', 'module.encoder.layers.4.bn2.num_batches_tracked', 'module.encoder.layers.5.conv3x1_1.weight', 'module.encoder.layers.5.conv3x1_1.bias', 'module.encoder.layers.5.conv1x3_1.weight', 'module.encoder.layers.5.conv1x3_1.bias', 'module.encoder.layers.5.bn1.weight', 'module.encoder.layers.5.bn1.bias', 'module.encoder.layers.5.bn1.running_mean', 'module.encoder.layers.5.bn1.running_var', 'module.encoder.layers.5.bn1.num_batches_tracked', 'module.encoder.layers.5.conv3x1_2.weight', 'module.encoder.layers.5.conv3x1_2.bias', 'module.encoder.layers.5.conv1x3_2.weight', 'module.encoder.layers.5.conv1x3_2.bias', 'module.encoder.layers.5.bn2.weight', 'module.encoder.layers.5.bn2.bias', 'module.encoder.layers.5.bn2.running_mean', 'module.encoder.layers.5.bn2.running_var', 'module.encoder.layers.5.bn2.num_batches_tracked', 'module.encoder.layers.6.conv.weight', 'module.encoder.layers.6.conv.bias', 'module.encoder.layers.6.bn.weight', 'module.encoder.layers.6.bn.bias', 'module.encoder.layers.6.bn.running_mean', 'module.encoder.layers.6.bn.running_var', 'module.encoder.layers.6.bn.num_batches_tracked', 'module.encoder.layers.7.conv3x1_1.weight', 'module.encoder.layers.7.conv3x1_1.bias', 'module.encoder.layers.7.conv1x3_1.weight', 'module.encoder.layers.7.conv1x3_1.bias', 'module.encoder.layers.7.bn1.weight', 'module.encoder.layers.7.bn1.bias', 'module.encoder.layers.7.bn1.running_mean', 'module.encoder.layers.7.bn1.running_var', 'module.encoder.layers.7.bn1.num_batches_tracked', 'module.encoder.layers.7.conv3x1_2.weight', 'module.encoder.layers.7.conv3x1_2.bias', 'module.encoder.layers.7.conv1x3_2.weight', 'module.encoder.layers.7.conv1x3_2.bias', 'module.encoder.layers.7.bn2.weight', 'module.encoder.layers.7.bn2.bias', 'module.encoder.layers.7.bn2.running_mean', 'module.encoder.layers.7.bn2.running_var', 'module.encoder.layers.7.bn2.num_batches_tracked', 'module.encoder.layers.8.conv3x1_1.weight', 'module.encoder.layers.8.conv3x1_1.bias', 'module.encoder.layers.8.conv1x3_1.weight', 'module.encoder.layers.8.conv1x3_1.bias', 'module.encoder.layers.8.bn1.weight', 'module.encoder.layers.8.bn1.bias', 'module.encoder.layers.8.bn1.running_mean', 'module.encoder.layers.8.bn1.running_var', 'module.encoder.layers.8.bn1.num_batches_tracked', 'module.encoder.layers.8.conv3x1_2.weight', 'module.encoder.layers.8.conv3x1_2.bias', 'module.encoder.layers.8.conv1x3_2.weight', 'module.encoder.layers.8.conv1x3_2.bias', 'module.encoder.layers.8.bn2.weight', 'module.encoder.layers.8.bn2.bias', 'module.encoder.layers.8.bn2.running_mean', 'module.encoder.layers.8.bn2.running_var', 'module.encoder.layers.8.bn2.num_batches_tracked', 'module.encoder.layers.9.conv3x1_1.weight', 'module.encoder.layers.9.conv3x1_1.bias', 'module.encoder.layers.9.conv1x3_1.weight', 'module.encoder.layers.9.conv1x3_1.bias', 'module.encoder.layers.9.bn1.weight', 'module.encoder.layers.9.bn1.bias', 'module.encoder.layers.9.bn1.running_mean', 'module.encoder.layers.9.bn1.running_var', 'module.encoder.layers.9.bn1.num_batches_tracked', 'module.encoder.layers.9.conv3x1_2.weight', 'module.encoder.layers.9.conv3x1_2.bias', 'module.encoder.layers.9.conv1x3_2.weight', 'module.encoder.layers.9.conv1x3_2.bias', 'module.encoder.layers.9.bn2.weight', 'module.encoder.layers.9.bn2.bias', 'module.encoder.layers.9.bn2.running_mean', 'module.encoder.layers.9.bn2.running_var', 'module.encoder.layers.9.bn2.num_batches_tracked', 'module.encoder.layers.10.conv3x1_1.weight', 'module.encoder.layers.10.conv3x1_1.bias', 'module.encoder.layers.10.conv1x3_1.weight', 'module.encoder.layers.10.conv1x3_1.bias', 'module.encoder.layers.10.bn1.weight', 'module.encoder.layers.10.bn1.bias', 'module.encoder.layers.10.bn1.running_mean', 'module.encoder.layers.10.bn1.running_var', 'module.encoder.layers.10.bn1.num_batches_tracked', 'module.encoder.layers.10.conv3x1_2.weight', 'module.encoder.layers.10.conv3x1_2.bias', 'module.encoder.layers.10.conv1x3_2.weight', 'module.encoder.layers.10.conv1x3_2.bias', 'module.encoder.layers.10.bn2.weight', 'module.encoder.layers.10.bn2.bias', 'module.encoder.layers.10.bn2.running_mean', 'module.encoder.layers.10.bn2.running_var', 'module.encoder.layers.10.bn2.num_batches_tracked', 'module.encoder.layers.11.conv3x1_1.weight', 'module.encoder.layers.11.conv3x1_1.bias', 'module.encoder.layers.11.conv1x3_1.weight', 'module.encoder.layers.11.conv1x3_1.bias', 'module.encoder.layers.11.bn1.weight', 'module.encoder.layers.11.bn1.bias', 'module.encoder.layers.11.bn1.running_mean', 'module.encoder.layers.11.bn1.running_var', 'module.encoder.layers.11.bn1.num_batches_tracked', 'module.encoder.layers.11.conv3x1_2.weight', 'module.encoder.layers.11.conv3x1_2.bias', 'module.encoder.layers.11.conv1x3_2.weight', 'module.encoder.layers.11.conv1x3_2.bias', 'module.encoder.layers.11.bn2.weight', 'module.encoder.layers.11.bn2.bias', 'module.encoder.layers.11.bn2.running_mean', 'module.encoder.layers.11.bn2.running_var', 'module.encoder.layers.11.bn2.num_batches_tracked', 'module.encoder.layers.12.conv3x1_1.weight', 'module.encoder.layers.12.conv3x1_1.bias', 'module.encoder.layers.12.conv1x3_1.weight', 'module.encoder.layers.12.conv1x3_1.bias', 'module.encoder.layers.12.bn1.weight', 'module.encoder.layers.12.bn1.bias', 'module.encoder.layers.12.bn1.running_mean', 'module.encoder.layers.12.bn1.running_var', 'module.encoder.layers.12.bn1.num_batches_tracked', 'module.encoder.layers.12.conv3x1_2.weight', 'module.encoder.layers.12.conv3x1_2.bias', 'module.encoder.layers.12.conv1x3_2.weight', 'module.encoder.layers.12.conv1x3_2.bias', 'module.encoder.layers.12.bn2.weight', 'module.encoder.layers.12.bn2.bias', 'module.encoder.layers.12.bn2.running_mean', 'module.encoder.layers.12.bn2.running_var', 'module.encoder.layers.12.bn2.num_batches_tracked', 'module.encoder.layers.13.conv3x1_1.weight', 'module.encoder.layers.13.conv3x1_1.bias', 'module.encoder.layers.13.conv1x3_1.weight', 'module.encoder.layers.13.conv1x3_1.bias', 'module.encoder.layers.13.bn1.weight', 'module.encoder.layers.13.bn1.bias', 'module.encoder.layers.13.bn1.running_mean', 'module.encoder.layers.13.bn1.running_var', 'module.encoder.layers.13.bn1.num_batches_tracked', 'module.encoder.layers.13.conv3x1_2.weight', 'module.encoder.layers.13.conv3x1_2.bias', 'module.encoder.layers.13.conv1x3_2.weight', 'module.encoder.layers.13.conv1x3_2.bias', 'module.encoder.layers.13.bn2.weight', 'module.encoder.layers.13.bn2.bias', 'module.encoder.layers.13.bn2.running_mean', 'module.encoder.layers.13.bn2.running_var', 'module.encoder.layers.13.bn2.num_batches_tracked', 'module.encoder.layers.14.conv3x1_1.weight', 'module.encoder.layers.14.conv3x1_1.bias', 'module.encoder.layers.14.conv1x3_1.weight', 'module.encoder.layers.14.conv1x3_1.bias', 'module.encoder.layers.14.bn1.weight', 'module.encoder.layers.14.bn1.bias', 'module.encoder.layers.14.bn1.running_mean', 'module.encoder.layers.14.bn1.running_var', 'module.encoder.layers.14.bn1.num_batches_tracked', 'module.encoder.layers.14.conv3x1_2.weight', 'module.encoder.layers.14.conv3x1_2.bias', 'module.encoder.layers.14.conv1x3_2.weight', 'module.encoder.layers.14.conv1x3_2.bias', 'module.encoder.layers.14.bn2.weight', 'module.encoder.layers.14.bn2.bias', 'module.encoder.layers.14.bn2.running_mean', 'module.encoder.layers.14.bn2.running_var', 'module.encoder.layers.14.bn2.num_batches_tracked', 'module.encoder.output_conv.weight', 'module.encoder.output_conv.bias', 'module.decoder.layers.0.conv.weight', 'module.decoder.layers.0.conv.bias', 'module.decoder.layers.0.bn.weight', 'module.decoder.layers.0.bn.bias', 'module.decoder.layers.0.bn.running_mean', 'module.decoder.layers.0.bn.running_var', 'module.decoder.layers.0.bn.num_batches_tracked', 'module.decoder.layers.1.conv3x1_1.weight', 'module.decoder.layers.1.conv3x1_1.bias', 'module.decoder.layers.1.conv1x3_1.weight', 'module.decoder.layers.1.conv1x3_1.bias', 'module.decoder.layers.1.bn1.weight', 'module.decoder.layers.1.bn1.bias', 'module.decoder.layers.1.bn1.running_mean', 'module.decoder.layers.1.bn1.running_var', 'module.decoder.layers.1.bn1.num_batches_tracked', 'module.decoder.layers.1.conv3x1_2.weight', 'module.decoder.layers.1.conv3x1_2.bias', 'module.decoder.layers.1.conv1x3_2.weight', 'module.decoder.layers.1.conv1x3_2.bias', 'module.decoder.layers.1.bn2.weight', 'module.decoder.layers.1.bn2.bias', 'module.decoder.layers.1.bn2.running_mean', 'module.decoder.layers.1.bn2.running_var', 'module.decoder.layers.1.bn2.num_batches_tracked', 'module.decoder.layers.2.conv3x1_1.weight', 'module.decoder.layers.2.conv3x1_1.bias', 'module.decoder.layers.2.conv1x3_1.weight', 'module.decoder.layers.2.conv1x3_1.bias', 'module.decoder.layers.2.bn1.weight', 'module.decoder.layers.2.bn1.bias', 'module.decoder.layers.2.bn1.running_mean', 'module.decoder.layers.2.bn1.running_var', 'module.decoder.layers.2.bn1.num_batches_tracked', 'module.decoder.layers.2.conv3x1_2.weight', 'module.decoder.layers.2.conv3x1_2.bias', 'module.decoder.layers.2.conv1x3_2.weight', 'module.decoder.layers.2.conv1x3_2.bias', 'module.decoder.layers.2.bn2.weight', 'module.decoder.layers.2.bn2.bias', 'module.decoder.layers.2.bn2.running_mean', 'module.decoder.layers.2.bn2.running_var', 'module.decoder.layers.2.bn2.num_batches_tracked', 'module.decoder.layers.3.conv.weight', 'module.decoder.layers.3.conv.bias', 'module.decoder.layers.3.bn.weight', 'module.decoder.layers.3.bn.bias', 'module.decoder.layers.3.bn.running_mean', 'module.decoder.layers.3.bn.running_var', 'module.decoder.layers.3.bn.num_batches_tracked', 'module.decoder.layers.4.conv3x1_1.weight', 'module.decoder.layers.4.conv3x1_1.bias', 'module.decoder.layers.4.conv1x3_1.weight', 'module.decoder.layers.4.conv1x3_1.bias', 'module.decoder.layers.4.bn1.weight', 'module.decoder.layers.4.bn1.bias', 'module.decoder.layers.4.bn1.running_mean', 'module.decoder.layers.4.bn1.running_var', 'module.decoder.layers.4.bn1.num_batches_tracked', 'module.decoder.layers.4.conv3x1_2.weight', 'module.decoder.layers.4.conv3x1_2.bias', 'module.decoder.layers.4.conv1x3_2.weight', 'module.decoder.layers.4.conv1x3_2.bias', 'module.decoder.layers.4.bn2.weight', 'module.decoder.layers.4.bn2.bias', 'module.decoder.layers.4.bn2.running_mean', 'module.decoder.layers.4.bn2.running_var', 'module.decoder.layers.4.bn2.num_batches_tracked', 'module.decoder.layers.5.conv3x1_1.weight', 'module.decoder.layers.5.conv3x1_1.bias', 'module.decoder.layers.5.conv1x3_1.weight', 'module.decoder.layers.5.conv1x3_1.bias', 'module.decoder.layers.5.bn1.weight', 'module.decoder.layers.5.bn1.bias', 'module.decoder.layers.5.bn1.running_mean', 'module.decoder.layers.5.bn1.running_var', 'module.decoder.layers.5.bn1.num_batches_tracked', 'module.decoder.layers.5.conv3x1_2.weight', 'module.decoder.layers.5.conv3x1_2.bias', 'module.decoder.layers.5.conv1x3_2.weight', 'module.decoder.layers.5.conv1x3_2.bias', 'module.decoder.layers.5.bn2.weight', 'module.decoder.layers.5.bn2.bias', 'module.decoder.layers.5.bn2.running_mean', 'module.decoder.layers.5.bn2.running_var', 'module.decoder.layers.5.bn2.num_batches_tracked', 'module.decoder.output_conv.weight', 'module.decoder.output_conv.bias'])\n",
            "Model and weights LOADED successfully\n",
            "100% 2975/2975 [08:05<00:00,  6.13it/s]\n",
            "Covariance matrix: torch.Size([20, 20])\n",
            "Covariance matrice saved as '/content/AnomalySegmentation/save/cov_matrix_erfnet.npy'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print cov matrice and mean\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "cov_matrix_file = \"/content/AnomalySegmentation/save/cov_matrix_erfnet.npy\"\n",
        "mean_vector_file = \"/content/AnomalySegmentation/save/mean_cityscapes_erfnet.npy\"\n",
        "\n",
        "try:\n",
        "    cov_matrix = np.load(cov_matrix_file)\n",
        "    mean_vector = np.load(mean_vector_file)\n",
        "\n",
        "    print(\"Covariance Matrix:\\n\", cov_matrix)\n",
        "    print(\"\\nMean Vector:\\n\", mean_vector)\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cov_matrix, annot=False, cmap='viridis', fmt=\".2f\")\n",
        "    plt.title('Covariance Matrix Heatmap')\n",
        "    plt.xlabel('Features')\n",
        "    plt.ylabel('Features')\n",
        "    plt.show()\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Could not find covariance matrix file at {cov_matrix_file} or mean vector file at {mean_vector_file}.\")\n",
        "    print(\"Make sure the mahalanobis.py script has created and saved these files correctly.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_7XI7B7daNWF",
        "outputId": "84e94c5d-1fc8-408d-98ff-0b78d7f5559f",
        "collapsed": true
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Covariance Matrix:\n",
            " [[ 2.28502348e-01 -2.29301881e-02 -8.00867602e-02 -3.58177256e-03\n",
            "  -4.05418687e-03 -5.64318895e-03 -9.11850424e-04 -2.48282240e-03\n",
            "  -5.42139038e-02 -5.44829993e-03 -1.44906538e-02 -4.56483429e-03\n",
            "  -5.29712182e-04 -2.44116783e-02 -1.12906913e-03 -9.11794952e-04\n",
            "  -8.34311184e-04 -4.93511441e-04 -1.78034033e-03 -3.52807183e-06]\n",
            " [-2.29301881e-02  5.71689494e-02 -1.38173578e-02 -3.34985787e-04\n",
            "  -5.10512909e-04 -7.06194085e-04 -1.62800890e-04 -4.41754964e-04\n",
            "  -9.53663699e-03 -4.54843539e-04 -2.58168695e-03 -6.09819603e-04\n",
            "  -8.98436483e-05 -4.23260499e-03 -2.00247625e-04 -1.85394834e-04\n",
            "  -1.48221952e-04 -7.10798340e-05 -1.54829992e-04  6.20527274e-08]\n",
            " [-8.00867602e-02 -1.38173578e-02  1.57491878e-01 -1.92788488e-03\n",
            "  -2.15187436e-03 -1.21724431e-03 -2.31238213e-04 -7.43032841e-04\n",
            "  -2.67761387e-02 -3.66315385e-03 -7.71120470e-03 -2.19010515e-03\n",
            "  -2.80451495e-04 -1.38787096e-02 -6.06737682e-04 -5.76157123e-04\n",
            "  -4.38446616e-04 -2.94140860e-04 -8.98503466e-04 -2.53055850e-06]\n",
            " [-3.58177256e-03 -3.34985787e-04 -1.92788488e-03  7.88964238e-03\n",
            "   1.94895314e-04 -6.98038493e-05 -2.39948713e-05 -5.69227777e-05\n",
            "  -1.23633735e-03  1.37569368e-04 -3.89741967e-04 -7.90807608e-05\n",
            "  -7.04072045e-06 -5.70417440e-04  3.86211796e-05 -1.57840659e-05\n",
            "  -2.36521646e-05  7.36518050e-05 -1.80250281e-05  1.05686854e-06]\n",
            " [-4.05418687e-03 -5.10512909e-04 -2.15187436e-03  1.94895314e-04\n",
            "   8.88562668e-03 -5.59892669e-06 -2.22679992e-05 -3.24552420e-05\n",
            "  -1.38909370e-03  1.77821610e-04 -4.41278535e-04 -1.47077690e-05\n",
            "   5.80422920e-06 -6.87093183e-04 -2.73981623e-05  6.55016606e-07\n",
            "  -2.44915191e-05  1.75889375e-06  9.31484974e-05  1.25103838e-06]\n",
            " [-5.64318895e-03 -7.06194085e-04 -1.21724431e-03 -6.98038493e-05\n",
            "  -5.59892669e-06  1.00834798e-02  1.14642564e-04  1.11369780e-04\n",
            "  -1.14987895e-03 -1.43266632e-04 -4.14025330e-04 -5.94105004e-05\n",
            "  -1.41436940e-05 -8.42093548e-04 -3.35692966e-05 -2.81954035e-05\n",
            "  -2.07347948e-05 -1.30809358e-05  5.02430848e-05  7.14480450e-07]\n",
            " [-9.11850424e-04 -1.62800890e-04 -2.31238213e-04 -2.39948713e-05\n",
            "  -2.22679992e-05  1.14642564e-04  1.70467398e-03  2.74837639e-05\n",
            "  -1.78851667e-04 -3.90568312e-05 -7.35998401e-05 -1.77963811e-05\n",
            "  -2.81500024e-06 -1.60826763e-04 -6.69368364e-06  2.64602045e-06\n",
            "  -2.59882222e-06 -3.52860002e-06 -1.15725334e-05  4.60851126e-08]\n",
            " [-2.48282240e-03 -4.41754964e-04 -7.43032841e-04 -5.69227777e-05\n",
            "  -3.24552420e-05  1.11369780e-04  2.74837639e-05  5.20999217e-03\n",
            "  -7.38827104e-04 -1.00206096e-04 -2.27493481e-04 -5.89895353e-05\n",
            "  -1.38248936e-06 -3.94209492e-04 -4.65768790e-06 -1.10175633e-05\n",
            "  -1.53849978e-05 -9.46756063e-06 -3.03622473e-05  1.42970720e-07]\n",
            " [-5.42139038e-02 -9.53663699e-03 -2.67761387e-02 -1.23633735e-03\n",
            "  -1.38909370e-03 -1.14987895e-03 -1.78851667e-04 -7.38827104e-04\n",
            "   1.15474574e-01 -2.11996678e-03 -4.80486918e-03 -1.71792216e-03\n",
            "  -1.98122478e-04 -9.38855018e-03 -4.48290899e-04 -3.78447789e-04\n",
            "  -3.29278148e-04 -2.08422585e-04 -6.59411598e-04 -1.67296241e-06]\n",
            " [-5.44829993e-03 -4.54843539e-04 -3.66315385e-03  1.37569368e-04\n",
            "   1.77821610e-04 -1.43266632e-04 -3.90568312e-05 -1.00206096e-04\n",
            "  -2.11996678e-03  1.33194840e-02 -6.60847116e-04 -1.33501322e-04\n",
            "  -3.44581781e-06 -9.01580614e-04 -4.67518839e-05  2.93556914e-05\n",
            "  -3.95505740e-05  4.32746165e-05  4.51930428e-05  1.78522373e-06]\n",
            " [-1.44906538e-02 -2.58168695e-03 -7.71120470e-03 -3.89741967e-04\n",
            "  -4.41278535e-04 -4.14025330e-04 -7.35998401e-05 -2.27493481e-04\n",
            "  -4.80486918e-03 -6.60847116e-04  3.55072953e-02 -4.93827683e-04\n",
            "  -5.66534945e-05 -2.59785564e-03 -1.19435048e-04 -1.07133019e-04\n",
            "  -8.79117142e-05 -5.86957940e-05 -1.89415470e-04 -9.65945674e-07]\n",
            " [-4.56483429e-03 -6.09819603e-04 -2.19010515e-03 -7.90807608e-05\n",
            "  -1.47077690e-05 -5.94105004e-05 -1.77963811e-05 -5.89895353e-05\n",
            "  -1.71792216e-03 -1.33501322e-04 -4.93827683e-04  1.05898613e-02\n",
            "   6.20679857e-05 -6.80533994e-04 -3.49643415e-05 -2.36539636e-05\n",
            "  -2.24684863e-05  1.35431801e-05  3.56377168e-05  5.14588692e-07]\n",
            " [-5.29712182e-04 -8.98436483e-05 -2.80451495e-04 -7.04072045e-06\n",
            "   5.80422920e-06 -1.41436940e-05 -2.81500024e-06 -1.38248936e-06\n",
            "  -1.98122478e-04 -3.44581781e-06 -5.66534945e-05  6.20679857e-05\n",
            "   1.02451874e-03 -7.64182114e-05  5.00803480e-06  8.19839806e-06\n",
            "  -1.89027776e-06  3.43666325e-05  1.21753641e-04  2.01889122e-07]\n",
            " [-2.44116783e-02 -4.23260499e-03 -1.38787096e-02 -5.70417440e-04\n",
            "  -6.87093183e-04 -8.42093548e-04 -1.60826763e-04 -3.94209492e-04\n",
            "  -9.38855018e-03 -9.01580614e-04 -2.59785564e-03 -6.80533994e-04\n",
            "  -7.64182114e-05  5.94302341e-02 -1.10340407e-04 -9.19078157e-05\n",
            "  -1.48734456e-04 -2.99772219e-05 -2.27226497e-04  4.58076755e-07]\n",
            " [-1.12906913e-03 -2.00247625e-04 -6.06737682e-04  3.86211796e-05\n",
            "  -2.73981623e-05 -3.35692966e-05 -6.69368364e-06 -4.65768790e-06\n",
            "  -4.48290899e-04 -4.67518839e-05 -1.19435048e-04 -3.49643415e-05\n",
            "   5.00803480e-06 -1.10340407e-04  2.69719306e-03  2.42379519e-05\n",
            "  -3.13811574e-06  1.67350336e-05 -1.10627798e-05  5.65676544e-07]\n",
            " [-9.11794952e-04 -1.85394834e-04 -5.76157123e-04 -1.57840659e-05\n",
            "   6.55016606e-07 -2.81954035e-05  2.64602045e-06 -1.10175633e-05\n",
            "  -3.78447789e-04  2.93556914e-05 -1.07133019e-04 -2.36539636e-05\n",
            "   8.19839806e-06 -9.19078157e-05  2.42379519e-05  2.23372388e-03\n",
            "   1.68781644e-05  1.49772141e-05 -1.97676309e-06  7.94705215e-07]\n",
            " [-8.34311184e-04 -1.48221952e-04 -4.38446616e-04 -2.36521646e-05\n",
            "  -2.44915191e-05 -2.07347948e-05 -2.59882222e-06 -1.53849978e-05\n",
            "  -3.29278148e-04 -3.95505740e-05 -8.79117142e-05 -2.24684863e-05\n",
            "  -1.89027776e-06 -1.48734456e-04 -3.13811574e-06  1.68781644e-05\n",
            "   2.12815357e-03 -1.85863871e-06 -2.56860108e-06  2.29894454e-07]\n",
            " [-4.93511441e-04 -7.10798340e-05 -2.94140860e-04  7.36518050e-05\n",
            "   1.75889375e-06 -1.30809358e-05 -3.52860002e-06 -9.46756063e-06\n",
            "  -2.08422585e-04  4.32746165e-05 -5.86957940e-05  1.35431801e-05\n",
            "   3.43666325e-05 -2.99772219e-05  1.67350336e-05  1.49772141e-05\n",
            "  -1.85863871e-06  9.25860368e-04  5.92305296e-05  3.65792715e-07]\n",
            " [-1.78034033e-03 -1.54829992e-04 -8.98503466e-04 -1.80250281e-05\n",
            "   9.31484974e-05  5.02430848e-05 -1.15725334e-05 -3.03622473e-05\n",
            "  -6.59411598e-04  4.51930428e-05 -1.89415470e-04  3.56377168e-05\n",
            "   1.21753641e-04 -2.27226497e-04 -1.10627798e-05 -1.97676309e-06\n",
            "  -2.56860108e-06  5.92305296e-05  3.57960723e-03  4.79217022e-07]\n",
            " [-3.52807183e-06  6.20527274e-08 -2.53055850e-06  1.05686854e-06\n",
            "   1.25103838e-06  7.14480450e-07  4.60851126e-08  1.42970720e-07\n",
            "  -1.67296241e-06  1.78522373e-06 -9.65945674e-07  5.14588692e-07\n",
            "   2.01889122e-07  4.58076755e-07  5.65676544e-07  7.94705215e-07\n",
            "   2.29894454e-07  3.65792715e-07  4.79217022e-07  2.89810931e-08]]\n",
            "\n",
            "Mean Vector:\n",
            " [3.76644236e-01 6.73552851e-02 2.12839909e-01 1.01585100e-02\n",
            " 1.14667107e-02 1.52445702e-02 2.44878432e-03 6.71762418e-03\n",
            " 1.44292249e-01 1.74334217e-02 3.86047953e-02 1.28242236e-02\n",
            " 1.53285868e-03 6.73732674e-02 3.26919459e-03 2.85805582e-03\n",
            " 2.40420317e-03 1.53234450e-03 4.96272784e-03 3.73634395e-05]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x800 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "no_execute = False\n",
        "just_once = False\n",
        "\n",
        "for dataset_dir in ['RoadAnomaly21', 'RoadObsticle21', 'FS_LostFound_full', 'fs_static', 'RoadAnomaly']:\n",
        "  if no_execute:\n",
        "    break\n",
        "  net = \"erfnet\"\n",
        "  method = \"Mahalanobis\"\n",
        "  load_dir = f'/content/AnomalySegmentation/save/erfnet_training1'\n",
        "  weights = f'/model_best.pth'\n",
        "  format_file = os.listdir(f'/content/Validation_Dataset/{dataset_dir}/images')[0].split(\".\")[1]\n",
        "  input =f'/content/Validation_Dataset/{dataset_dir}/images/\\*.{format_file}'\n",
        "  print(f\"\\nDataset: {dataset_dir} method: {method}\")\n",
        "\n",
        "  if torch.cuda.is_available():\n",
        "    !python -W ignore /content/AnomalySegmentation/eval/evalAnomaly.py --input {input} --method  {method} --model {net} --loadDir {load_dir} --loadWeights {weights}\n",
        "  else:\n",
        "    !python -W ignore /content/AnomalySegmentation/eval/evalAnomaly.py --input {input} --method  {method} --model {net} --loadDir {load_dir} --loadWeights {weights} -cpu | tail -n 2\n",
        "  if just_once:\n",
        "    no_execute = True\n",
        "    just_once = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8Ne14rpyCsD",
        "outputId": "39b1a7fe-5257-47f1-d168-9a98416fb942"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dataset: RoadAnomaly21 method: Mahalanobis\n",
            "Loading model: /content/AnomalySegmentation/save/erfnet_training1/erfnet.py\n",
            "Loading weights: /content/AnomalySegmentation/save/erfnet_training1/model_best.pth\n",
            "Model and weights LOADED successfully\n",
            "mean shape:  (20,)\n",
            "cov shape:  (20, 20)\n",
            "Computing Mahalanobis for /content/Validation_Dataset/RoadAnomaly21/images/2.png\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/AnomalySegmentation/eval/evalAnomaly.py\", line 301, in <module>\n",
            "    main()\n",
            "  File \"/content/AnomalySegmentation/eval/evalAnomaly.py\", line 232, in main\n",
            "    anomaly_result = mahalanobis_score(F.softmax(result, dim=0), means, cov_inv)\n",
            "  File \"/content/AnomalySegmentation/eval/evalAnomaly.py\", line 118, in mahalanobis_score\n",
            "    if score > max_scores[h, w]:  # Keep the max score for each pixel\n",
            "KeyboardInterrupt\n",
            "\n",
            "Dataset: RoadObsticle21 method: Mahalanobis\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/AnomalySegmentation/eval/evalAnomaly.py\", line 18, in <module>\n",
            "    from torchvision.transforms import Compose, ToTensor, Normalize, Resize\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torchvision/__init__.py\", line 10, in <module>\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torchvision/models/__init__.py\", line 2, in <module>\n",
            "    from .convnext import *\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torchvision/models/convnext.py\", line 8, in <module>\n",
            "    from ..ops.misc import Conv2dNormActivation, Permute\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torchvision/ops/__init__.py\", line 23, in <module>\n",
            "    from .poolers import MultiScaleRoIAlign\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torchvision/ops/poolers.py\", line 10, in <module>\n",
            "    from .roi_align import roi_align\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torchvision/ops/roi_align.py\", line 7, in <module>\n",
            "    from torch._dynamo.utils import is_compile_supported\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/__init__.py\", line 3, in <module>\n",
            "    from . import convert_frame, eval_frame, resume_execution\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 31, in <module>\n",
            "    from torch._dynamo.utils import CompileTimeInstructionCounter\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py\", line 62, in <module>\n",
            "    import torch.fx.experimental.symbolic_shapes\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/symbolic_shapes.py\", line 65, in <module>\n",
            "    from torch.utils._sympy.functions import (\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_sympy/functions.py\", line 7, in <module>\n",
            "    import sympy\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sympy/__init__.py\", line 30, in <module>\n",
            "    from sympy.core.cache import lazy_function\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sympy/core/__init__.py\", line 4, in <module>\n",
            "    from .sympify import sympify, SympifyError\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sympy/core/sympify.py\", line 620, in <module>\n",
            "    from .basic import Basic\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sympy/core/basic.py\", line 126, in <module>\n",
            "    class Basic(Printable):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sympy/core/basic.py\", line 2091, in Basic\n",
            "    def _exec_constructor_postprocessors(cls, obj):\n",
            "KeyboardInterrupt\n",
            "^C\n",
            "\n",
            "Dataset: FS_LostFound_full method: Mahalanobis\n",
            "Loading model: /content/AnomalySegmentation/save/erfnet_training1/erfnet.py\n",
            "Loading weights: /content/AnomalySegmentation/save/erfnet_training1/model_best.pth\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/AnomalySegmentation/eval/evalAnomaly.py\", line 301, in <module>\n",
            "    main()\n",
            "  File \"/content/AnomalySegmentation/eval/evalAnomaly.py\", line 163, in main\n",
            "    model = ERFNet(NUM_CLASSES).to(device)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n",
            "    return self._apply(convert)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
            "    module._apply(fn)\n",
            "  [Previous line repeated 1 more time]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n",
            "    param_applied = fn(param)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1328, in convert\n",
            "    dtype if t.is_floating_point() or t.is_complex() else None,\n",
            "KeyboardInterrupt\n",
            "\n",
            "Dataset: fs_static method: Mahalanobis\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/AnomalySegmentation/eval/evalAnomaly.py\", line 5, in <module>\n",
            "    import torch\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2486, in <module>\n",
            "    from torch import _meta_registrations\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_meta_registrations.py\", line 10, in <module>\n",
            "    from torch._decomp import (\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_decomp/__init__.py\", line 198, in <module>\n",
            "  File \"/usr/lib/python3.10/typing.py\", line 309, in inner\n",
            "    return cached(*args, **kwds)\n",
            "  File \"/usr/lib/python3.10/typing.py\", line 403, in __getitem__\n",
            "    return self._getitem(self, parameters)\n",
            "  File \"/usr/lib/python3.10/typing.py\", line 521, in Union\n",
            "    return _UnionGenericAlias(self, parameters)\n",
            "  File \"/usr/lib/python3.10/typing.py\", line 1025, in __init__\n",
            "    self.__parameters__ = _collect_type_vars(params, typevar_types=_typevar_types)\n",
            "  File \"/usr/lib/python3.10/typing.py\", line 987, in __setattr__\n",
            "    if _is_dunder(attr) or attr in {'_name', '_inst', '_nparams',\n",
            "  File \"/usr/lib/python3.10/typing.py\", line 936, in _is_dunder\n",
            "    return attr.startswith('__') and attr.endswith('__')\n",
            "KeyboardInterrupt\n",
            "^C\n",
            "\n",
            "Dataset: RoadAnomaly method: Mahalanobis\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/AnomalySegmentation/eval/evalAnomaly.py\", line 12, in <module>\n",
            "    from ood_metrics import fpr_at_95_tpr, calc_metrics, plot_roc, plot_pr,plot_barcode\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ood_metrics/__init__.py\", line 1, in <module>\n",
            "    from .metrics import *\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ood_metrics/metrics.py\", line 2, in <module>\n",
            "    from sklearn.metrics import auc, precision_recall_curve, roc_curve\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/__init__.py\", line 73, in <module>\n",
            "    from .base import clone  # noqa: E402\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 19, in <module>\n",
            "    from .utils._estimator_html_repr import _HTMLDocumentationLinkMixin, estimator_html_repr\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/__init__.py\", line 15, in <module>\n",
            "    from ._chunking import gen_batches, gen_even_slices\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_chunking.py\", line 11, in <module>\n",
            "    from ._param_validation import Interval, validate_params\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\", line 17, in <module>\n",
            "    from .validation import _is_arraylike_not_scalar\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\", line 21, in <module>\n",
            "    from ..utils._array_api import _asarray_with_order, _is_numpy_namespace, get_namespace\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_array_api.py\", line 17, in <module>\n",
            "    from .fixes import parse_version\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/fixes.py\", line 20, in <module>\n",
            "    import pandas as pd\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/__init__.py\", line 49, in <module>\n",
            "    from pandas.core.api import (\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/core/api.py\", line 28, in <module>\n",
            "    from pandas.core.arrays import Categorical\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/core/arrays/__init__.py\", line 1, in <module>\n",
            "    from pandas.core.arrays.arrow import ArrowExtensionArray\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/core/arrays/arrow/__init__.py\", line 1, in <module>\n",
            "    from pandas.core.arrays.arrow.accessors import (\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/core/arrays/arrow/accessors.py\", line 23, in <module>\n",
            "    import pyarrow.compute as pc\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pyarrow/compute.py\", line 336, in <module>\n",
            "    _make_global_functions()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pyarrow/compute.py\", line 333, in _make_global_functions\n",
            "    g[cpp_name] = g[name] = _wrap_function(name, func)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pyarrow/compute.py\", line 304, in _wrap_function\n",
            "    return _decorate_compute_function(wrapper, name, func, options_class)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pyarrow/compute.py\", line 170, in _decorate_compute_function\n",
            "    options_class_doc = _scrape_options_class_doc(options_class)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pyarrow/compute.py\", line 117, in _scrape_options_class_doc\n",
            "    doc = docscrape.NumpyDocString(options_class.__doc__)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pyarrow/vendored/docscrape.py\", line 151, in __init__\n",
            "    self._parsed_data = copy.deepcopy(self.sections)\n",
            "  File \"/usr/lib/python3.10/copy.py\", line 146, in deepcopy\n",
            "    y = copier(x, memo)\n",
            "  File \"/usr/lib/python3.10/copy.py\", line 231, in _deepcopy_dict\n",
            "    y[deepcopy(key, memo)] = deepcopy(value, memo)\n",
            "  File \"/usr/lib/python3.10/copy.py\", line 177, in deepcopy\n",
            "    _keep_alive(x, memo) # Make sure x lives at least as long as d\n",
            "  File \"/usr/lib/python3.10/copy.py\", line 254, in _keep_alive\n",
            "    memo[id(memo)].append(x)\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}