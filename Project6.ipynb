{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypo8OBRZ-1p3"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/RonPlusSign/AnomalySegmentation/blob/main/Project6.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUjRrYEW8-uz"
      },
      "source": [
        "# **Anomaly Segmentation Project 6**\n",
        "##*Andrea Delli, Christian Dellisanti, Giorgia Modi*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4x3MajLNeXhX"
      },
      "source": [
        "##**Dataset Preparation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yorO9_xVX2bJ"
      },
      "outputs": [],
      "source": [
        "!rm -rf /content/AnomalySegmentation\n",
        "!rm -rf AnomalySegmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_tj8W3BeVPo",
        "outputId": "a131b038-1f07-41a0-a27a-2b6b231a9693"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already up to date.\n"
          ]
        }
      ],
      "source": [
        "!pip  install -q numpy matplotlib Pillow torchvision visdom ood_metrics icecream cityscapesscripts tqdm #triton\n",
        "\n",
        "import sys, os\n",
        "if not os.path.isfile('/content/Validation_Dataset.zip'):\n",
        "  !gdown 12YJq48XkCxQHjN3CmLc-zM5dThSak4Ta\n",
        "if not os.path.isdir('/content/Validation_Dataset'):\n",
        "  !unzip -q Validation_Dataset.zip\n",
        "if not os.path.isdir('/content/AnomalySegmentation'):\n",
        "  #!git clone https://github.com/shyam671/AnomalySegmentation_CourseProjectBaseCode.git\n",
        "  #token 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\n",
        "  !git clone -b main https://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@github.com/RonPlusSign/AnomalySegmentation.git\n",
        "!cd /content/AnomalySegmentation && git pull\n",
        "#!cd /content/AnomalySegmentation && git checkout main"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dataset download"
      ],
      "metadata": {
        "id": "oRWyQO_-Iiuz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3w0jewGkphY"
      },
      "outputs": [],
      "source": [
        "import  os\n",
        "# s306027@studenti.polito.it\n",
        "# %mR+g$L\\~5U03O9)IZ-_\n",
        "createLabel = True\n",
        "fast_download = True\n",
        "super_fast_download = True\n",
        "if super_fast_download:\n",
        "  !gdown 1-fjLAk4_-GkixW1-GP_cYDBUmhnVbApL\n",
        "  !unzip -q cityscapes.zip\n",
        "  !mv  ./content/cityscapes /content/cityscapes\n",
        "  !rm -rf ./content\n",
        "else:\n",
        "  if not os.path.isdir('/content/cityscapes'):\n",
        "    !mkdir /content/cityscapes\n",
        "\n",
        "  if not os.path.isfile('/content/cityscapes/gtFine_trainvaltest.zip'):\n",
        "    if fast_download:\n",
        "      !gdown 1J31rnVd33GBt-IYGYqC9mv73q7vc55pw -O /content/cityscapes/\n",
        "    else:\n",
        "      !csDownload gtFine_trainvaltest.zip -d /content/cityscapes/\n",
        "\n",
        "  if not os.path.isdir('/content/cityscapes/gtFine'):\n",
        "    !unzip -q /content/cityscapes/gtFine_trainvaltest.zip -d /content/cityscapes/\n",
        "    createLabel = True\n",
        "    !rm /content/cityscapes/README\n",
        "    !rm /content/cityscapes/license.txt\n",
        "\n",
        "  if not os.path.isfile('/content/cityscapes/leftImg8bit_trainvaltest.zip'):\n",
        "    if fast_download:\n",
        "      #https://drive.google.com/file/d/1m8Y3Zc6vG11Q9SxW7Be5EGXTDq4s4RlJ/view?usp=sharing\n",
        "      !gdown 1m8Y3Zc6vG11Q9SxW7Be5EGXTDq4s4RlJ -O /content/cityscapes/\n",
        "    else:\n",
        "      !csDownload leftImg8bit_trainvaltest.zip -d /content/cityscapes/\n",
        "\n",
        "  if not os.path.isdir('/content/cityscapes/leftImg8bit'):\n",
        "    !unzip -q /content/cityscapes/leftImg8bit_trainvaltest.zip -d /content/cityscapes/\n",
        "    createLabel = True\n",
        "    !rm /content/cityscapes/README\n",
        "    !rm /content/cityscapes/license.txt\n",
        "\n",
        "  if createLabel:\n",
        "    os.environ['CITYSCAPES_DATASET'] = '/content/cityscapes/'\n",
        "    !csCreateTrainIdLabelImgs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAmA1igJhHhR"
      },
      "source": [
        "##**mIoU**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAeuewkbhM3p",
        "outputId": "91464ffe-a919-48c1-cf3e-c3a79a673dcb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "498 val/munster/munster_000172_000019_leftImg8bit.png\n",
            "499 val/munster/munster_000173_000019_leftImg8bit.png\n",
            "-------------MSP-------------------\n",
            "---------------------------------------\n",
            "Took  80.77754092216492 seconds\n",
            "=======================================\n",
            "Per-Class IoU:\n",
            "\u001b[0m97.62\u001b[0m Road\n",
            "\u001b[0m81.37\u001b[0m sidewalk\n",
            "\u001b[0m90.77\u001b[0m building\n",
            "\u001b[0m49.43\u001b[0m wall\n",
            "\u001b[0m54.93\u001b[0m fence\n",
            "\u001b[0m60.81\u001b[0m pole\n",
            "\u001b[0m62.60\u001b[0m traffic light\n",
            "\u001b[0m72.32\u001b[0m traffic sign\n",
            "\u001b[0m91.35\u001b[0m vegetation\n",
            "\u001b[0m60.97\u001b[0m terrain\n",
            "\u001b[0m93.38\u001b[0m sky\n",
            "\u001b[0m76.11\u001b[0m person\n",
            "\u001b[0m53.45\u001b[0m rider\n",
            "\u001b[0m92.91\u001b[0m car\n",
            "\u001b[0m72.78\u001b[0m truck\n",
            "\u001b[0m78.87\u001b[0m bus\n",
            "\u001b[0m63.86\u001b[0m train\n",
            "\u001b[0m46.41\u001b[0m motorcycle\n",
            "\u001b[0m71.89\u001b[0m bicycle\n",
            "=======================================\n",
            "MEAN IoU:  \u001b[0m72.20\u001b[0m %\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  !python -W ignore /content/AnomalySegmentation/eval/eval_iou.py --loadDir /content/AnomalySegmentation/trained_models/ --datadir /content/cityscapes/  | tail -n 28\n",
        "else:\n",
        "  !python -W ignore /content/AnomalySegmentation/eval/eval_iou.py  --loadDir  /content/AnomalySegmentation/trained_models/ --datadir /content/cityscapes/  --cpu | tail -n 28"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RZTrDS4Mysu"
      },
      "source": [
        "##**Anomaly Inference**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9afwM8zdM7_l",
        "outputId": "80d0aaaa-35f3-4bb8-a9fb-b46181f31866"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------\n",
            "\n",
            "Dataset: RoadAnomaly21 method: MSP\n",
            "AUPRC score: 29.100168300581203\n",
            "FPR@TPR95: 62.51075321069286\n",
            "\n",
            "Dataset: RoadAnomaly21 method: MaxLogit\n",
            "AUPRC score: 38.31957797222208\n",
            "FPR@TPR95: 59.3370558914899\n",
            "\n",
            "Dataset: RoadAnomaly21 method: MaxEntropy\n",
            "AUPRC score: 31.005102648344756\n",
            "FPR@TPR95: 62.593151130093226\n",
            "----------------------------\n",
            "\n",
            "Dataset: RoadObsticle21 method: MSP\n",
            "AUPRC score: 2.7116243119338366\n",
            "FPR@TPR95: 64.9739786894368\n",
            "\n",
            "Dataset: RoadObsticle21 method: MaxLogit\n",
            "AUPRC score: 4.626567617520253\n",
            "FPR@TPR95: 48.443439151949555\n",
            "\n",
            "Dataset: RoadObsticle21 method: MaxEntropy\n",
            "AUPRC score: 3.051560023478638\n",
            "FPR@TPR95: 65.59968252759046\n",
            "----------------------------\n",
            "\n",
            "Dataset: FS_LostFound_full method: MSP\n",
            "AUPRC score: 1.747872547607269\n",
            "FPR@TPR95: 50.76348570192957\n",
            "\n",
            "Dataset: FS_LostFound_full method: MaxLogit\n",
            "AUPRC score: 3.3014401015087245\n",
            "FPR@TPR95: 45.494876929038305\n",
            "\n",
            "Dataset: FS_LostFound_full method: MaxEntropy\n",
            "AUPRC score: 2.581709137723009\n",
            "FPR@TPR95: 50.368099783135676\n",
            "----------------------------\n",
            "\n",
            "Dataset: fs_static method: MSP\n",
            "AUPRC score: 7.4700433549050915\n",
            "FPR@TPR95: 41.82346831776172\n",
            "\n",
            "Dataset: fs_static method: MaxLogit\n",
            "AUPRC score: 9.498677970785756\n",
            "FPR@TPR95: 40.3000747567442\n",
            "\n",
            "Dataset: fs_static method: MaxEntropy\n",
            "AUPRC score: 8.82636607633996\n",
            "FPR@TPR95: 41.52332673090571\n",
            "----------------------------\n",
            "\n",
            "Dataset: RoadAnomaly method: MSP\n",
            "AUPRC score: 12.426265849563665\n",
            "FPR@TPR95: 82.49244029880458\n",
            "\n",
            "Dataset: RoadAnomaly method: MaxLogit\n",
            "AUPRC score: 15.581983301641019\n",
            "FPR@TPR95: 73.24766535735604\n",
            "\n",
            "Dataset: RoadAnomaly method: MaxEntropy\n",
            "AUPRC score: 12.678035094227063\n",
            "FPR@TPR95: 82.63192451735861\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "no_execute = False\n",
        "just_once = False\n",
        "\n",
        "for dataset_dir in ['RoadAnomaly21', 'RoadObsticle21', 'FS_LostFound_full', 'fs_static', 'RoadAnomaly']:\n",
        "  print(\"----------------------------\")\n",
        "  for method in [\"MSP\", \"MaxLogit\", \"MaxEntropy\"]:\n",
        "\n",
        "    if no_execute:\n",
        "      break\n",
        "\n",
        "    format_file = os.listdir(f'/content/Validation_Dataset/{dataset_dir}/images')[0].split(\".\")[1]\n",
        "    input =f'/content/Validation_Dataset/{dataset_dir}/images/\\*.{format_file}'\n",
        "    print(f\"\\nDataset: {dataset_dir} method: {method}\")\n",
        "    save_plots_dir = f'/content/AnomalySegmentation/plots/{dataset_dir}_{method}'\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "      !python -W ignore /content/AnomalySegmentation/eval/evalAnomaly.py --input {input} --method {method} --save-plots-dir {save_plots_dir} | tail -n 2\n",
        "    else:\n",
        "      !python -W ignore /content/AnomalySegmentation/eval/evalAnomaly.py --input {input} --method {method} --save-plots-dir {save_plots_dir} --cpu | tail -n 2\n",
        "\n",
        "    if just_once:\n",
        "      no_execute = True\n",
        "      just_once = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhQWIx8rklfO"
      },
      "source": [
        "##**Temperature Scaling**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7DsE7oO1n9G"
      },
      "source": [
        "**Anomaly Inference with temperature**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Zu-dIEeqFLq3",
        "outputId": "47825080-3d12-479f-c2c2-51ca15ecab1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------\n",
            "\n",
            "Dataset: RoadAnomaly21 method : MSP Temperature: 0.5\n",
            "AUPRC score: 27.060833635879618\n",
            "FPR@TPR95: 62.730810427606734\n",
            "\n",
            "Dataset: RoadAnomaly21 method : MSP Temperature: 0.75\n",
            "AUPRC score: 28.156063054348103\n",
            "FPR@TPR95: 62.478737323984326\n",
            "\n",
            "Dataset: RoadAnomaly21 method : MSP Temperature: 1.1\n",
            "AUPRC score: 29.40955379121979\n",
            "FPR@TPR95: 62.58986549662704\n",
            "----------------------------\n",
            "\n",
            "Dataset: RoadObsticle21 method : MSP Temperature: 0.5\n",
            "AUPRC score: 2.4195519558429823\n",
            "FPR@TPR95: 63.22544524787239\n",
            "\n",
            "Dataset: RoadObsticle21 method : MSP Temperature: 0.75\n",
            "AUPRC score: 2.5668802249367677\n",
            "FPR@TPR95: 64.05285534718263\n",
            "\n",
            "Dataset: RoadObsticle21 method : MSP Temperature: 1.1\n",
            "AUPRC score: 2.7658075767433776\n",
            "FPR@TPR95: 65.52358106228223\n",
            "----------------------------\n",
            "\n",
            "Dataset: FS_LostFound_full method : MSP Temperature: 0.5\n",
            "AUPRC score: 1.2802500246431052\n",
            "FPR@TPR95: 66.73710676943257\n",
            "\n",
            "Dataset: FS_LostFound_full method : MSP Temperature: 0.75\n",
            "AUPRC score: 1.4927065686510383\n",
            "FPR@TPR95: 51.848262648332636\n",
            "\n",
            "Dataset: FS_LostFound_full method : MSP Temperature: 1.1\n",
            "AUPRC score: 1.8596703140506141\n",
            "FPR@TPR95: 50.38650128754133\n",
            "----------------------------\n",
            "\n",
            "Dataset: fs_static method : MSP Temperature: 0.5\n",
            "AUPRC score: 6.6011970066164665\n",
            "FPR@TPR95: 43.47565874225287\n",
            "\n",
            "Dataset: fs_static method : MSP Temperature: 0.75\n",
            "AUPRC score: 6.99079114995491\n",
            "FPR@TPR95: 42.49329123307483\n",
            "\n",
            "Dataset: fs_static method : MSP Temperature: 1.1\n",
            "AUPRC score: 7.686696846804934\n",
            "FPR@TPR95: 41.586844199987\n",
            "----------------------------\n",
            "\n",
            "Dataset: RoadAnomaly method : MSP Temperature: 0.5\n",
            "AUPRC score: 12.187681345765725\n",
            "FPR@TPR95: 82.02224728951396\n",
            "\n",
            "Dataset: RoadAnomaly method : MSP Temperature: 0.75\n",
            "AUPRC score: 12.319186617225913\n",
            "FPR@TPR95: 82.28451947325927\n",
            "\n",
            "Dataset: RoadAnomaly method : MSP Temperature: 1.1\n",
            "AUPRC score: 12.465779148190585\n",
            "FPR@TPR95: 82.62125003163526\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "no_execute = False\n",
        "just_once = False\n",
        "for dataset_dir in ['RoadAnomaly21', 'RoadObsticle21', 'FS_LostFound_full', 'fs_static', 'RoadAnomaly']:\n",
        "  print(\"----------------------------\")\n",
        "  for t in [0.5, 0.75, 1.1]:\n",
        "    if no_execute:\n",
        "        break\n",
        "\n",
        "    format_file = os.listdir(f'/content/Validation_Dataset/{dataset_dir}/images')[0].split(\".\")[1]\n",
        "    input =f'/content/Validation_Dataset/{dataset_dir}/images/\\*.{format_file}'\n",
        "    print(f\"\\nDataset: {dataset_dir}, method: MSP, Temperature: {t}\")\n",
        "    if torch.cuda.is_available():\n",
        "      !python -W ignore /content/AnomalySegmentation/eval/evalAnomaly.py --input {input} --method 'MSP' --temperature {t} | tail -n 2\n",
        "    else:\n",
        "      !python -W ignore /content/AnomalySegmentation/eval/evalAnomaly.py --input {input} --method 'MSP' --cpu --temperature {t} | tail -n 2\n",
        "\n",
        "    if just_once:\n",
        "      no_execute = True\n",
        "      just_once = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nY-OAYlIjGaG"
      },
      "source": [
        "## **Void Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3bSxCH-7WT7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b932771a-d879-47d2-e64d-19ba95331ae4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "----- Fine-tuning erfnet for VOID classification -----\n",
            "odict_keys(['module.encoder.initial_block.conv.weight', 'module.encoder.initial_block.conv.bias', 'module.encoder.initial_block.bn.weight', 'module.encoder.initial_block.bn.bias', 'module.encoder.initial_block.bn.running_mean', 'module.encoder.initial_block.bn.running_var', 'module.encoder.layers.0.conv.weight', 'module.encoder.layers.0.conv.bias', 'module.encoder.layers.0.bn.weight', 'module.encoder.layers.0.bn.bias', 'module.encoder.layers.0.bn.running_mean', 'module.encoder.layers.0.bn.running_var', 'module.encoder.layers.1.conv3x1_1.weight', 'module.encoder.layers.1.conv3x1_1.bias', 'module.encoder.layers.1.conv1x3_1.weight', 'module.encoder.layers.1.conv1x3_1.bias', 'module.encoder.layers.1.conv3x1_2.weight', 'module.encoder.layers.1.conv3x1_2.bias', 'module.encoder.layers.1.conv1x3_2.weight', 'module.encoder.layers.1.conv1x3_2.bias', 'module.encoder.layers.1.bn1.weight', 'module.encoder.layers.1.bn1.bias', 'module.encoder.layers.1.bn1.running_mean', 'module.encoder.layers.1.bn1.running_var', 'module.encoder.layers.1.bn2.weight', 'module.encoder.layers.1.bn2.bias', 'module.encoder.layers.1.bn2.running_mean', 'module.encoder.layers.1.bn2.running_var', 'module.encoder.layers.2.conv3x1_1.weight', 'module.encoder.layers.2.conv3x1_1.bias', 'module.encoder.layers.2.conv1x3_1.weight', 'module.encoder.layers.2.conv1x3_1.bias', 'module.encoder.layers.2.conv3x1_2.weight', 'module.encoder.layers.2.conv3x1_2.bias', 'module.encoder.layers.2.conv1x3_2.weight', 'module.encoder.layers.2.conv1x3_2.bias', 'module.encoder.layers.2.bn1.weight', 'module.encoder.layers.2.bn1.bias', 'module.encoder.layers.2.bn1.running_mean', 'module.encoder.layers.2.bn1.running_var', 'module.encoder.layers.2.bn2.weight', 'module.encoder.layers.2.bn2.bias', 'module.encoder.layers.2.bn2.running_mean', 'module.encoder.layers.2.bn2.running_var', 'module.encoder.layers.3.conv3x1_1.weight', 'module.encoder.layers.3.conv3x1_1.bias', 'module.encoder.layers.3.conv1x3_1.weight', 'module.encoder.layers.3.conv1x3_1.bias', 'module.encoder.layers.3.conv3x1_2.weight', 'module.encoder.layers.3.conv3x1_2.bias', 'module.encoder.layers.3.conv1x3_2.weight', 'module.encoder.layers.3.conv1x3_2.bias', 'module.encoder.layers.3.bn1.weight', 'module.encoder.layers.3.bn1.bias', 'module.encoder.layers.3.bn1.running_mean', 'module.encoder.layers.3.bn1.running_var', 'module.encoder.layers.3.bn2.weight', 'module.encoder.layers.3.bn2.bias', 'module.encoder.layers.3.bn2.running_mean', 'module.encoder.layers.3.bn2.running_var', 'module.encoder.layers.4.conv3x1_1.weight', 'module.encoder.layers.4.conv3x1_1.bias', 'module.encoder.layers.4.conv1x3_1.weight', 'module.encoder.layers.4.conv1x3_1.bias', 'module.encoder.layers.4.conv3x1_2.weight', 'module.encoder.layers.4.conv3x1_2.bias', 'module.encoder.layers.4.conv1x3_2.weight', 'module.encoder.layers.4.conv1x3_2.bias', 'module.encoder.layers.4.bn1.weight', 'module.encoder.layers.4.bn1.bias', 'module.encoder.layers.4.bn1.running_mean', 'module.encoder.layers.4.bn1.running_var', 'module.encoder.layers.4.bn2.weight', 'module.encoder.layers.4.bn2.bias', 'module.encoder.layers.4.bn2.running_mean', 'module.encoder.layers.4.bn2.running_var', 'module.encoder.layers.5.conv3x1_1.weight', 'module.encoder.layers.5.conv3x1_1.bias', 'module.encoder.layers.5.conv1x3_1.weight', 'module.encoder.layers.5.conv1x3_1.bias', 'module.encoder.layers.5.conv3x1_2.weight', 'module.encoder.layers.5.conv3x1_2.bias', 'module.encoder.layers.5.conv1x3_2.weight', 'module.encoder.layers.5.conv1x3_2.bias', 'module.encoder.layers.5.bn1.weight', 'module.encoder.layers.5.bn1.bias', 'module.encoder.layers.5.bn1.running_mean', 'module.encoder.layers.5.bn1.running_var', 'module.encoder.layers.5.bn2.weight', 'module.encoder.layers.5.bn2.bias', 'module.encoder.layers.5.bn2.running_mean', 'module.encoder.layers.5.bn2.running_var', 'module.encoder.layers.6.conv.weight', 'module.encoder.layers.6.conv.bias', 'module.encoder.layers.6.bn.weight', 'module.encoder.layers.6.bn.bias', 'module.encoder.layers.6.bn.running_mean', 'module.encoder.layers.6.bn.running_var', 'module.encoder.layers.7.conv3x1_1.weight', 'module.encoder.layers.7.conv3x1_1.bias', 'module.encoder.layers.7.conv1x3_1.weight', 'module.encoder.layers.7.conv1x3_1.bias', 'module.encoder.layers.7.conv3x1_2.weight', 'module.encoder.layers.7.conv3x1_2.bias', 'module.encoder.layers.7.conv1x3_2.weight', 'module.encoder.layers.7.conv1x3_2.bias', 'module.encoder.layers.7.bn1.weight', 'module.encoder.layers.7.bn1.bias', 'module.encoder.layers.7.bn1.running_mean', 'module.encoder.layers.7.bn1.running_var', 'module.encoder.layers.7.bn2.weight', 'module.encoder.layers.7.bn2.bias', 'module.encoder.layers.7.bn2.running_mean', 'module.encoder.layers.7.bn2.running_var', 'module.encoder.layers.8.conv3x1_1.weight', 'module.encoder.layers.8.conv3x1_1.bias', 'module.encoder.layers.8.conv1x3_1.weight', 'module.encoder.layers.8.conv1x3_1.bias', 'module.encoder.layers.8.conv3x1_2.weight', 'module.encoder.layers.8.conv3x1_2.bias', 'module.encoder.layers.8.conv1x3_2.weight', 'module.encoder.layers.8.conv1x3_2.bias', 'module.encoder.layers.8.bn1.weight', 'module.encoder.layers.8.bn1.bias', 'module.encoder.layers.8.bn1.running_mean', 'module.encoder.layers.8.bn1.running_var', 'module.encoder.layers.8.bn2.weight', 'module.encoder.layers.8.bn2.bias', 'module.encoder.layers.8.bn2.running_mean', 'module.encoder.layers.8.bn2.running_var', 'module.encoder.layers.9.conv3x1_1.weight', 'module.encoder.layers.9.conv3x1_1.bias', 'module.encoder.layers.9.conv1x3_1.weight', 'module.encoder.layers.9.conv1x3_1.bias', 'module.encoder.layers.9.conv3x1_2.weight', 'module.encoder.layers.9.conv3x1_2.bias', 'module.encoder.layers.9.conv1x3_2.weight', 'module.encoder.layers.9.conv1x3_2.bias', 'module.encoder.layers.9.bn1.weight', 'module.encoder.layers.9.bn1.bias', 'module.encoder.layers.9.bn1.running_mean', 'module.encoder.layers.9.bn1.running_var', 'module.encoder.layers.9.bn2.weight', 'module.encoder.layers.9.bn2.bias', 'module.encoder.layers.9.bn2.running_mean', 'module.encoder.layers.9.bn2.running_var', 'module.encoder.layers.10.conv3x1_1.weight', 'module.encoder.layers.10.conv3x1_1.bias', 'module.encoder.layers.10.conv1x3_1.weight', 'module.encoder.layers.10.conv1x3_1.bias', 'module.encoder.layers.10.conv3x1_2.weight', 'module.encoder.layers.10.conv3x1_2.bias', 'module.encoder.layers.10.conv1x3_2.weight', 'module.encoder.layers.10.conv1x3_2.bias', 'module.encoder.layers.10.bn1.weight', 'module.encoder.layers.10.bn1.bias', 'module.encoder.layers.10.bn1.running_mean', 'module.encoder.layers.10.bn1.running_var', 'module.encoder.layers.10.bn2.weight', 'module.encoder.layers.10.bn2.bias', 'module.encoder.layers.10.bn2.running_mean', 'module.encoder.layers.10.bn2.running_var', 'module.encoder.layers.11.conv3x1_1.weight', 'module.encoder.layers.11.conv3x1_1.bias', 'module.encoder.layers.11.conv1x3_1.weight', 'module.encoder.layers.11.conv1x3_1.bias', 'module.encoder.layers.11.conv3x1_2.weight', 'module.encoder.layers.11.conv3x1_2.bias', 'module.encoder.layers.11.conv1x3_2.weight', 'module.encoder.layers.11.conv1x3_2.bias', 'module.encoder.layers.11.bn1.weight', 'module.encoder.layers.11.bn1.bias', 'module.encoder.layers.11.bn1.running_mean', 'module.encoder.layers.11.bn1.running_var', 'module.encoder.layers.11.bn2.weight', 'module.encoder.layers.11.bn2.bias', 'module.encoder.layers.11.bn2.running_mean', 'module.encoder.layers.11.bn2.running_var', 'module.encoder.layers.12.conv3x1_1.weight', 'module.encoder.layers.12.conv3x1_1.bias', 'module.encoder.layers.12.conv1x3_1.weight', 'module.encoder.layers.12.conv1x3_1.bias', 'module.encoder.layers.12.conv3x1_2.weight', 'module.encoder.layers.12.conv3x1_2.bias', 'module.encoder.layers.12.conv1x3_2.weight', 'module.encoder.layers.12.conv1x3_2.bias', 'module.encoder.layers.12.bn1.weight', 'module.encoder.layers.12.bn1.bias', 'module.encoder.layers.12.bn1.running_mean', 'module.encoder.layers.12.bn1.running_var', 'module.encoder.layers.12.bn2.weight', 'module.encoder.layers.12.bn2.bias', 'module.encoder.layers.12.bn2.running_mean', 'module.encoder.layers.12.bn2.running_var', 'module.encoder.layers.13.conv3x1_1.weight', 'module.encoder.layers.13.conv3x1_1.bias', 'module.encoder.layers.13.conv1x3_1.weight', 'module.encoder.layers.13.conv1x3_1.bias', 'module.encoder.layers.13.conv3x1_2.weight', 'module.encoder.layers.13.conv3x1_2.bias', 'module.encoder.layers.13.conv1x3_2.weight', 'module.encoder.layers.13.conv1x3_2.bias', 'module.encoder.layers.13.bn1.weight', 'module.encoder.layers.13.bn1.bias', 'module.encoder.layers.13.bn1.running_mean', 'module.encoder.layers.13.bn1.running_var', 'module.encoder.layers.13.bn2.weight', 'module.encoder.layers.13.bn2.bias', 'module.encoder.layers.13.bn2.running_mean', 'module.encoder.layers.13.bn2.running_var', 'module.encoder.layers.14.conv3x1_1.weight', 'module.encoder.layers.14.conv3x1_1.bias', 'module.encoder.layers.14.conv1x3_1.weight', 'module.encoder.layers.14.conv1x3_1.bias', 'module.encoder.layers.14.conv3x1_2.weight', 'module.encoder.layers.14.conv3x1_2.bias', 'module.encoder.layers.14.conv1x3_2.weight', 'module.encoder.layers.14.conv1x3_2.bias', 'module.encoder.layers.14.bn1.weight', 'module.encoder.layers.14.bn1.bias', 'module.encoder.layers.14.bn1.running_mean', 'module.encoder.layers.14.bn1.running_var', 'module.encoder.layers.14.bn2.weight', 'module.encoder.layers.14.bn2.bias', 'module.encoder.layers.14.bn2.running_mean', 'module.encoder.layers.14.bn2.running_var', 'module.decoder.layers.0.conv.weight', 'module.decoder.layers.0.conv.bias', 'module.decoder.layers.0.bn.weight', 'module.decoder.layers.0.bn.bias', 'module.decoder.layers.0.bn.running_mean', 'module.decoder.layers.0.bn.running_var', 'module.decoder.layers.1.conv3x1_1.weight', 'module.decoder.layers.1.conv3x1_1.bias', 'module.decoder.layers.1.conv1x3_1.weight', 'module.decoder.layers.1.conv1x3_1.bias', 'module.decoder.layers.1.bn1.weight', 'module.decoder.layers.1.bn1.bias', 'module.decoder.layers.1.bn1.running_mean', 'module.decoder.layers.1.bn1.running_var', 'module.decoder.layers.1.conv3x1_2.weight', 'module.decoder.layers.1.conv3x1_2.bias', 'module.decoder.layers.1.conv1x3_2.weight', 'module.decoder.layers.1.conv1x3_2.bias', 'module.decoder.layers.1.bn2.weight', 'module.decoder.layers.1.bn2.bias', 'module.decoder.layers.1.bn2.running_mean', 'module.decoder.layers.1.bn2.running_var', 'module.decoder.layers.2.conv3x1_1.weight', 'module.decoder.layers.2.conv3x1_1.bias', 'module.decoder.layers.2.conv1x3_1.weight', 'module.decoder.layers.2.conv1x3_1.bias', 'module.decoder.layers.2.bn1.weight', 'module.decoder.layers.2.bn1.bias', 'module.decoder.layers.2.bn1.running_mean', 'module.decoder.layers.2.bn1.running_var', 'module.decoder.layers.2.conv3x1_2.weight', 'module.decoder.layers.2.conv3x1_2.bias', 'module.decoder.layers.2.conv1x3_2.weight', 'module.decoder.layers.2.conv1x3_2.bias', 'module.decoder.layers.2.bn2.weight', 'module.decoder.layers.2.bn2.bias', 'module.decoder.layers.2.bn2.running_mean', 'module.decoder.layers.2.bn2.running_var', 'module.decoder.layers.3.conv.weight', 'module.decoder.layers.3.conv.bias', 'module.decoder.layers.3.bn.weight', 'module.decoder.layers.3.bn.bias', 'module.decoder.layers.3.bn.running_mean', 'module.decoder.layers.3.bn.running_var', 'module.decoder.layers.4.conv3x1_1.weight', 'module.decoder.layers.4.conv3x1_1.bias', 'module.decoder.layers.4.conv1x3_1.weight', 'module.decoder.layers.4.conv1x3_1.bias', 'module.decoder.layers.4.bn1.weight', 'module.decoder.layers.4.bn1.bias', 'module.decoder.layers.4.bn1.running_mean', 'module.decoder.layers.4.bn1.running_var', 'module.decoder.layers.4.conv3x1_2.weight', 'module.decoder.layers.4.conv3x1_2.bias', 'module.decoder.layers.4.conv1x3_2.weight', 'module.decoder.layers.4.conv1x3_2.bias', 'module.decoder.layers.4.bn2.weight', 'module.decoder.layers.4.bn2.bias', 'module.decoder.layers.4.bn2.running_mean', 'module.decoder.layers.4.bn2.running_var', 'module.decoder.layers.5.conv3x1_1.weight', 'module.decoder.layers.5.conv3x1_1.bias', 'module.decoder.layers.5.conv1x3_1.weight', 'module.decoder.layers.5.conv1x3_1.bias', 'module.decoder.layers.5.bn1.weight', 'module.decoder.layers.5.bn1.bias', 'module.decoder.layers.5.bn1.running_mean', 'module.decoder.layers.5.bn1.running_var', 'module.decoder.layers.5.conv3x1_2.weight', 'module.decoder.layers.5.conv3x1_2.bias', 'module.decoder.layers.5.conv1x3_2.weight', 'module.decoder.layers.5.conv1x3_2.bias', 'module.decoder.layers.5.bn2.weight', 'module.decoder.layers.5.bn2.bias', 'module.decoder.layers.5.bn2.running_mean', 'module.decoder.layers.5.bn2.running_var', 'module.decoder.output_conv.weight', 'module.decoder.output_conv.bias'])\n",
            "odict_keys(['encoder.initial_block.conv.weight', 'encoder.initial_block.conv.bias', 'encoder.initial_block.bn.weight', 'encoder.initial_block.bn.bias', 'encoder.initial_block.bn.running_mean', 'encoder.initial_block.bn.running_var', 'encoder.initial_block.bn.num_batches_tracked', 'encoder.layers.0.conv.weight', 'encoder.layers.0.conv.bias', 'encoder.layers.0.bn.weight', 'encoder.layers.0.bn.bias', 'encoder.layers.0.bn.running_mean', 'encoder.layers.0.bn.running_var', 'encoder.layers.0.bn.num_batches_tracked', 'encoder.layers.1.conv3x1_1.weight', 'encoder.layers.1.conv3x1_1.bias', 'encoder.layers.1.conv1x3_1.weight', 'encoder.layers.1.conv1x3_1.bias', 'encoder.layers.1.bn1.weight', 'encoder.layers.1.bn1.bias', 'encoder.layers.1.bn1.running_mean', 'encoder.layers.1.bn1.running_var', 'encoder.layers.1.bn1.num_batches_tracked', 'encoder.layers.1.conv3x1_2.weight', 'encoder.layers.1.conv3x1_2.bias', 'encoder.layers.1.conv1x3_2.weight', 'encoder.layers.1.conv1x3_2.bias', 'encoder.layers.1.bn2.weight', 'encoder.layers.1.bn2.bias', 'encoder.layers.1.bn2.running_mean', 'encoder.layers.1.bn2.running_var', 'encoder.layers.1.bn2.num_batches_tracked', 'encoder.layers.2.conv3x1_1.weight', 'encoder.layers.2.conv3x1_1.bias', 'encoder.layers.2.conv1x3_1.weight', 'encoder.layers.2.conv1x3_1.bias', 'encoder.layers.2.bn1.weight', 'encoder.layers.2.bn1.bias', 'encoder.layers.2.bn1.running_mean', 'encoder.layers.2.bn1.running_var', 'encoder.layers.2.bn1.num_batches_tracked', 'encoder.layers.2.conv3x1_2.weight', 'encoder.layers.2.conv3x1_2.bias', 'encoder.layers.2.conv1x3_2.weight', 'encoder.layers.2.conv1x3_2.bias', 'encoder.layers.2.bn2.weight', 'encoder.layers.2.bn2.bias', 'encoder.layers.2.bn2.running_mean', 'encoder.layers.2.bn2.running_var', 'encoder.layers.2.bn2.num_batches_tracked', 'encoder.layers.3.conv3x1_1.weight', 'encoder.layers.3.conv3x1_1.bias', 'encoder.layers.3.conv1x3_1.weight', 'encoder.layers.3.conv1x3_1.bias', 'encoder.layers.3.bn1.weight', 'encoder.layers.3.bn1.bias', 'encoder.layers.3.bn1.running_mean', 'encoder.layers.3.bn1.running_var', 'encoder.layers.3.bn1.num_batches_tracked', 'encoder.layers.3.conv3x1_2.weight', 'encoder.layers.3.conv3x1_2.bias', 'encoder.layers.3.conv1x3_2.weight', 'encoder.layers.3.conv1x3_2.bias', 'encoder.layers.3.bn2.weight', 'encoder.layers.3.bn2.bias', 'encoder.layers.3.bn2.running_mean', 'encoder.layers.3.bn2.running_var', 'encoder.layers.3.bn2.num_batches_tracked', 'encoder.layers.4.conv3x1_1.weight', 'encoder.layers.4.conv3x1_1.bias', 'encoder.layers.4.conv1x3_1.weight', 'encoder.layers.4.conv1x3_1.bias', 'encoder.layers.4.bn1.weight', 'encoder.layers.4.bn1.bias', 'encoder.layers.4.bn1.running_mean', 'encoder.layers.4.bn1.running_var', 'encoder.layers.4.bn1.num_batches_tracked', 'encoder.layers.4.conv3x1_2.weight', 'encoder.layers.4.conv3x1_2.bias', 'encoder.layers.4.conv1x3_2.weight', 'encoder.layers.4.conv1x3_2.bias', 'encoder.layers.4.bn2.weight', 'encoder.layers.4.bn2.bias', 'encoder.layers.4.bn2.running_mean', 'encoder.layers.4.bn2.running_var', 'encoder.layers.4.bn2.num_batches_tracked', 'encoder.layers.5.conv3x1_1.weight', 'encoder.layers.5.conv3x1_1.bias', 'encoder.layers.5.conv1x3_1.weight', 'encoder.layers.5.conv1x3_1.bias', 'encoder.layers.5.bn1.weight', 'encoder.layers.5.bn1.bias', 'encoder.layers.5.bn1.running_mean', 'encoder.layers.5.bn1.running_var', 'encoder.layers.5.bn1.num_batches_tracked', 'encoder.layers.5.conv3x1_2.weight', 'encoder.layers.5.conv3x1_2.bias', 'encoder.layers.5.conv1x3_2.weight', 'encoder.layers.5.conv1x3_2.bias', 'encoder.layers.5.bn2.weight', 'encoder.layers.5.bn2.bias', 'encoder.layers.5.bn2.running_mean', 'encoder.layers.5.bn2.running_var', 'encoder.layers.5.bn2.num_batches_tracked', 'encoder.layers.6.conv.weight', 'encoder.layers.6.conv.bias', 'encoder.layers.6.bn.weight', 'encoder.layers.6.bn.bias', 'encoder.layers.6.bn.running_mean', 'encoder.layers.6.bn.running_var', 'encoder.layers.6.bn.num_batches_tracked', 'encoder.layers.7.conv3x1_1.weight', 'encoder.layers.7.conv3x1_1.bias', 'encoder.layers.7.conv1x3_1.weight', 'encoder.layers.7.conv1x3_1.bias', 'encoder.layers.7.bn1.weight', 'encoder.layers.7.bn1.bias', 'encoder.layers.7.bn1.running_mean', 'encoder.layers.7.bn1.running_var', 'encoder.layers.7.bn1.num_batches_tracked', 'encoder.layers.7.conv3x1_2.weight', 'encoder.layers.7.conv3x1_2.bias', 'encoder.layers.7.conv1x3_2.weight', 'encoder.layers.7.conv1x3_2.bias', 'encoder.layers.7.bn2.weight', 'encoder.layers.7.bn2.bias', 'encoder.layers.7.bn2.running_mean', 'encoder.layers.7.bn2.running_var', 'encoder.layers.7.bn2.num_batches_tracked', 'encoder.layers.8.conv3x1_1.weight', 'encoder.layers.8.conv3x1_1.bias', 'encoder.layers.8.conv1x3_1.weight', 'encoder.layers.8.conv1x3_1.bias', 'encoder.layers.8.bn1.weight', 'encoder.layers.8.bn1.bias', 'encoder.layers.8.bn1.running_mean', 'encoder.layers.8.bn1.running_var', 'encoder.layers.8.bn1.num_batches_tracked', 'encoder.layers.8.conv3x1_2.weight', 'encoder.layers.8.conv3x1_2.bias', 'encoder.layers.8.conv1x3_2.weight', 'encoder.layers.8.conv1x3_2.bias', 'encoder.layers.8.bn2.weight', 'encoder.layers.8.bn2.bias', 'encoder.layers.8.bn2.running_mean', 'encoder.layers.8.bn2.running_var', 'encoder.layers.8.bn2.num_batches_tracked', 'encoder.layers.9.conv3x1_1.weight', 'encoder.layers.9.conv3x1_1.bias', 'encoder.layers.9.conv1x3_1.weight', 'encoder.layers.9.conv1x3_1.bias', 'encoder.layers.9.bn1.weight', 'encoder.layers.9.bn1.bias', 'encoder.layers.9.bn1.running_mean', 'encoder.layers.9.bn1.running_var', 'encoder.layers.9.bn1.num_batches_tracked', 'encoder.layers.9.conv3x1_2.weight', 'encoder.layers.9.conv3x1_2.bias', 'encoder.layers.9.conv1x3_2.weight', 'encoder.layers.9.conv1x3_2.bias', 'encoder.layers.9.bn2.weight', 'encoder.layers.9.bn2.bias', 'encoder.layers.9.bn2.running_mean', 'encoder.layers.9.bn2.running_var', 'encoder.layers.9.bn2.num_batches_tracked', 'encoder.layers.10.conv3x1_1.weight', 'encoder.layers.10.conv3x1_1.bias', 'encoder.layers.10.conv1x3_1.weight', 'encoder.layers.10.conv1x3_1.bias', 'encoder.layers.10.bn1.weight', 'encoder.layers.10.bn1.bias', 'encoder.layers.10.bn1.running_mean', 'encoder.layers.10.bn1.running_var', 'encoder.layers.10.bn1.num_batches_tracked', 'encoder.layers.10.conv3x1_2.weight', 'encoder.layers.10.conv3x1_2.bias', 'encoder.layers.10.conv1x3_2.weight', 'encoder.layers.10.conv1x3_2.bias', 'encoder.layers.10.bn2.weight', 'encoder.layers.10.bn2.bias', 'encoder.layers.10.bn2.running_mean', 'encoder.layers.10.bn2.running_var', 'encoder.layers.10.bn2.num_batches_tracked', 'encoder.layers.11.conv3x1_1.weight', 'encoder.layers.11.conv3x1_1.bias', 'encoder.layers.11.conv1x3_1.weight', 'encoder.layers.11.conv1x3_1.bias', 'encoder.layers.11.bn1.weight', 'encoder.layers.11.bn1.bias', 'encoder.layers.11.bn1.running_mean', 'encoder.layers.11.bn1.running_var', 'encoder.layers.11.bn1.num_batches_tracked', 'encoder.layers.11.conv3x1_2.weight', 'encoder.layers.11.conv3x1_2.bias', 'encoder.layers.11.conv1x3_2.weight', 'encoder.layers.11.conv1x3_2.bias', 'encoder.layers.11.bn2.weight', 'encoder.layers.11.bn2.bias', 'encoder.layers.11.bn2.running_mean', 'encoder.layers.11.bn2.running_var', 'encoder.layers.11.bn2.num_batches_tracked', 'encoder.layers.12.conv3x1_1.weight', 'encoder.layers.12.conv3x1_1.bias', 'encoder.layers.12.conv1x3_1.weight', 'encoder.layers.12.conv1x3_1.bias', 'encoder.layers.12.bn1.weight', 'encoder.layers.12.bn1.bias', 'encoder.layers.12.bn1.running_mean', 'encoder.layers.12.bn1.running_var', 'encoder.layers.12.bn1.num_batches_tracked', 'encoder.layers.12.conv3x1_2.weight', 'encoder.layers.12.conv3x1_2.bias', 'encoder.layers.12.conv1x3_2.weight', 'encoder.layers.12.conv1x3_2.bias', 'encoder.layers.12.bn2.weight', 'encoder.layers.12.bn2.bias', 'encoder.layers.12.bn2.running_mean', 'encoder.layers.12.bn2.running_var', 'encoder.layers.12.bn2.num_batches_tracked', 'encoder.layers.13.conv3x1_1.weight', 'encoder.layers.13.conv3x1_1.bias', 'encoder.layers.13.conv1x3_1.weight', 'encoder.layers.13.conv1x3_1.bias', 'encoder.layers.13.bn1.weight', 'encoder.layers.13.bn1.bias', 'encoder.layers.13.bn1.running_mean', 'encoder.layers.13.bn1.running_var', 'encoder.layers.13.bn1.num_batches_tracked', 'encoder.layers.13.conv3x1_2.weight', 'encoder.layers.13.conv3x1_2.bias', 'encoder.layers.13.conv1x3_2.weight', 'encoder.layers.13.conv1x3_2.bias', 'encoder.layers.13.bn2.weight', 'encoder.layers.13.bn2.bias', 'encoder.layers.13.bn2.running_mean', 'encoder.layers.13.bn2.running_var', 'encoder.layers.13.bn2.num_batches_tracked', 'encoder.layers.14.conv3x1_1.weight', 'encoder.layers.14.conv3x1_1.bias', 'encoder.layers.14.conv1x3_1.weight', 'encoder.layers.14.conv1x3_1.bias', 'encoder.layers.14.bn1.weight', 'encoder.layers.14.bn1.bias', 'encoder.layers.14.bn1.running_mean', 'encoder.layers.14.bn1.running_var', 'encoder.layers.14.bn1.num_batches_tracked', 'encoder.layers.14.conv3x1_2.weight', 'encoder.layers.14.conv3x1_2.bias', 'encoder.layers.14.conv1x3_2.weight', 'encoder.layers.14.conv1x3_2.bias', 'encoder.layers.14.bn2.weight', 'encoder.layers.14.bn2.bias', 'encoder.layers.14.bn2.running_mean', 'encoder.layers.14.bn2.running_var', 'encoder.layers.14.bn2.num_batches_tracked', 'encoder.output_conv.weight', 'encoder.output_conv.bias', 'decoder.layers.0.conv.weight', 'decoder.layers.0.conv.bias', 'decoder.layers.0.bn.weight', 'decoder.layers.0.bn.bias', 'decoder.layers.0.bn.running_mean', 'decoder.layers.0.bn.running_var', 'decoder.layers.0.bn.num_batches_tracked', 'decoder.layers.1.conv3x1_1.weight', 'decoder.layers.1.conv3x1_1.bias', 'decoder.layers.1.conv1x3_1.weight', 'decoder.layers.1.conv1x3_1.bias', 'decoder.layers.1.bn1.weight', 'decoder.layers.1.bn1.bias', 'decoder.layers.1.bn1.running_mean', 'decoder.layers.1.bn1.running_var', 'decoder.layers.1.bn1.num_batches_tracked', 'decoder.layers.1.conv3x1_2.weight', 'decoder.layers.1.conv3x1_2.bias', 'decoder.layers.1.conv1x3_2.weight', 'decoder.layers.1.conv1x3_2.bias', 'decoder.layers.1.bn2.weight', 'decoder.layers.1.bn2.bias', 'decoder.layers.1.bn2.running_mean', 'decoder.layers.1.bn2.running_var', 'decoder.layers.1.bn2.num_batches_tracked', 'decoder.layers.2.conv3x1_1.weight', 'decoder.layers.2.conv3x1_1.bias', 'decoder.layers.2.conv1x3_1.weight', 'decoder.layers.2.conv1x3_1.bias', 'decoder.layers.2.bn1.weight', 'decoder.layers.2.bn1.bias', 'decoder.layers.2.bn1.running_mean', 'decoder.layers.2.bn1.running_var', 'decoder.layers.2.bn1.num_batches_tracked', 'decoder.layers.2.conv3x1_2.weight', 'decoder.layers.2.conv3x1_2.bias', 'decoder.layers.2.conv1x3_2.weight', 'decoder.layers.2.conv1x3_2.bias', 'decoder.layers.2.bn2.weight', 'decoder.layers.2.bn2.bias', 'decoder.layers.2.bn2.running_mean', 'decoder.layers.2.bn2.running_var', 'decoder.layers.2.bn2.num_batches_tracked', 'decoder.layers.3.conv.weight', 'decoder.layers.3.conv.bias', 'decoder.layers.3.bn.weight', 'decoder.layers.3.bn.bias', 'decoder.layers.3.bn.running_mean', 'decoder.layers.3.bn.running_var', 'decoder.layers.3.bn.num_batches_tracked', 'decoder.layers.4.conv3x1_1.weight', 'decoder.layers.4.conv3x1_1.bias', 'decoder.layers.4.conv1x3_1.weight', 'decoder.layers.4.conv1x3_1.bias', 'decoder.layers.4.bn1.weight', 'decoder.layers.4.bn1.bias', 'decoder.layers.4.bn1.running_mean', 'decoder.layers.4.bn1.running_var', 'decoder.layers.4.bn1.num_batches_tracked', 'decoder.layers.4.conv3x1_2.weight', 'decoder.layers.4.conv3x1_2.bias', 'decoder.layers.4.conv1x3_2.weight', 'decoder.layers.4.conv1x3_2.bias', 'decoder.layers.4.bn2.weight', 'decoder.layers.4.bn2.bias', 'decoder.layers.4.bn2.running_mean', 'decoder.layers.4.bn2.running_var', 'decoder.layers.4.bn2.num_batches_tracked', 'decoder.layers.5.conv3x1_1.weight', 'decoder.layers.5.conv3x1_1.bias', 'decoder.layers.5.conv1x3_1.weight', 'decoder.layers.5.conv1x3_1.bias', 'decoder.layers.5.bn1.weight', 'decoder.layers.5.bn1.bias', 'decoder.layers.5.bn1.running_mean', 'decoder.layers.5.bn1.running_var', 'decoder.layers.5.bn1.num_batches_tracked', 'decoder.layers.5.conv3x1_2.weight', 'decoder.layers.5.conv3x1_2.bias', 'decoder.layers.5.conv1x3_2.weight', 'decoder.layers.5.conv1x3_2.bias', 'decoder.layers.5.bn2.weight', 'decoder.layers.5.bn2.bias', 'decoder.layers.5.bn2.running_mean', 'decoder.layers.5.bn2.running_var', 'decoder.layers.5.bn2.num_batches_tracked', 'decoder.output_conv.weight', 'decoder.output_conv.bias'])\n",
            "Import Model erfnet with weights erfnet_pretrained.pth to FineTune\n",
            "========== TRAINING ===========\n",
            "/content/cityscapes/leftImg8bit/train\n",
            "/content/cityscapes/leftImg8bit/val\n",
            "----- TRAINING - EPOCH 1 -----\n",
            "LEARNING RATE:  5e-05\n",
            "loss: 0.326 (epoch: 1, step: 0) // Avg time/img: 0.4121 s\n",
            "loss: 0.4037 (epoch: 1, step: 50) // Avg time/img: 0.0468 s\n",
            "loss: 0.3878 (epoch: 1, step: 100) // Avg time/img: 0.0436 s\n",
            "loss: 0.3854 (epoch: 1, step: 150) // Avg time/img: 0.0425 s\n",
            "loss: 0.3806 (epoch: 1, step: 200) // Avg time/img: 0.0422 s\n",
            "loss: 0.3821 (epoch: 1, step: 250) // Avg time/img: 0.0420 s\n",
            "loss: 0.3818 (epoch: 1, step: 300) // Avg time/img: 0.0421 s\n",
            "loss: 0.3768 (epoch: 1, step: 350) // Avg time/img: 0.0421 s\n",
            "loss: 0.3736 (epoch: 1, step: 400) // Avg time/img: 0.0420 s\n",
            "loss: 0.3724 (epoch: 1, step: 450) // Avg time/img: 0.0420 s\n",
            "----- VALIDATING - EPOCH 1 -----\n",
            "VAL loss: 0.4843 (epoch: 1, step: 0) // Avg time/img: 0.0493 s\n",
            "VAL loss: 0.577 (epoch: 1, step: 50) // Avg time/img: 0.0334 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.91\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/erfnet_training_void/model-001.pth (epoch: 1)\n",
            "save: ../save/erfnet_training_void/model_best.pth (epoch: 1)\n",
            "----- TRAINING - EPOCH 2 -----\n",
            "LEARNING RATE:  4.774426908107499e-05\n",
            "loss: 0.311 (epoch: 2, step: 0) // Avg time/img: 0.0516 s\n",
            "loss: 0.3439 (epoch: 2, step: 50) // Avg time/img: 0.0425 s\n",
            "loss: 0.3386 (epoch: 2, step: 100) // Avg time/img: 0.0425 s\n",
            "loss: 0.339 (epoch: 2, step: 150) // Avg time/img: 0.0422 s\n",
            "loss: 0.3382 (epoch: 2, step: 200) // Avg time/img: 0.0422 s\n",
            "loss: 0.3397 (epoch: 2, step: 250) // Avg time/img: 0.0423 s\n",
            "loss: 0.3386 (epoch: 2, step: 300) // Avg time/img: 0.0422 s\n",
            "loss: 0.3352 (epoch: 2, step: 350) // Avg time/img: 0.0423 s\n",
            "loss: 0.3339 (epoch: 2, step: 400) // Avg time/img: 0.0421 s\n",
            "loss: 0.3302 (epoch: 2, step: 450) // Avg time/img: 0.0422 s\n",
            "----- VALIDATING - EPOCH 2 -----\n",
            "VAL loss: 0.4263 (epoch: 2, step: 0) // Avg time/img: 0.0446 s\n",
            "VAL loss: 0.5277 (epoch: 2, step: 50) // Avg time/img: 0.0330 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.76\u001b[0m %\n",
            "save: ../save/erfnet_training_void/model-002.pth (epoch: 2)\n",
            "----- TRAINING - EPOCH 3 -----\n",
            "LEARNING RATE:  4.547662880414811e-05\n",
            "loss: 0.3206 (epoch: 3, step: 0) // Avg time/img: 0.0542 s\n",
            "loss: 0.3058 (epoch: 3, step: 50) // Avg time/img: 0.0418 s\n",
            "loss: 0.3016 (epoch: 3, step: 100) // Avg time/img: 0.0419 s\n",
            "loss: 0.2986 (epoch: 3, step: 150) // Avg time/img: 0.0417 s\n",
            "loss: 0.2986 (epoch: 3, step: 200) // Avg time/img: 0.0416 s\n",
            "loss: 0.2964 (epoch: 3, step: 250) // Avg time/img: 0.0418 s\n",
            "loss: 0.297 (epoch: 3, step: 300) // Avg time/img: 0.0419 s\n",
            "loss: 0.2949 (epoch: 3, step: 350) // Avg time/img: 0.0418 s\n",
            "loss: 0.2933 (epoch: 3, step: 400) // Avg time/img: 0.0419 s\n",
            "loss: 0.2915 (epoch: 3, step: 450) // Avg time/img: 0.0419 s\n",
            "----- VALIDATING - EPOCH 3 -----\n",
            "VAL loss: 0.4135 (epoch: 3, step: 0) // Avg time/img: 0.0496 s\n",
            "VAL loss: 0.4814 (epoch: 3, step: 50) // Avg time/img: 0.0338 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.51\u001b[0m %\n",
            "save: ../save/erfnet_training_void/model-003.pth (epoch: 3)\n",
            "----- TRAINING - EPOCH 4 -----\n",
            "LEARNING RATE:  4.319634861514096e-05\n",
            "loss: 0.3269 (epoch: 4, step: 0) // Avg time/img: 0.0562 s\n",
            "loss: 0.2721 (epoch: 4, step: 50) // Avg time/img: 0.0423 s\n",
            "loss: 0.2695 (epoch: 4, step: 100) // Avg time/img: 0.0418 s\n",
            "loss: 0.2701 (epoch: 4, step: 150) // Avg time/img: 0.0418 s\n",
            "loss: 0.2663 (epoch: 4, step: 200) // Avg time/img: 0.0420 s\n",
            "loss: 0.2635 (epoch: 4, step: 250) // Avg time/img: 0.0419 s\n",
            "loss: 0.2622 (epoch: 4, step: 300) // Avg time/img: 0.0420 s\n",
            "loss: 0.2608 (epoch: 4, step: 350) // Avg time/img: 0.0420 s\n",
            "loss: 0.2594 (epoch: 4, step: 400) // Avg time/img: 0.0419 s\n",
            "loss: 0.2596 (epoch: 4, step: 450) // Avg time/img: 0.0420 s\n",
            "----- VALIDATING - EPOCH 4 -----\n",
            "VAL loss: 0.3695 (epoch: 4, step: 0) // Avg time/img: 0.0444 s\n",
            "VAL loss: 0.4497 (epoch: 4, step: 50) // Avg time/img: 0.0335 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.66\u001b[0m %\n",
            "save: ../save/erfnet_training_void/model-004.pth (epoch: 4)\n",
            "----- TRAINING - EPOCH 5 -----\n",
            "LEARNING RATE:  4.090260730254292e-05\n",
            "loss: 0.355 (epoch: 5, step: 0) // Avg time/img: 0.0489 s\n",
            "loss: 0.2361 (epoch: 5, step: 50) // Avg time/img: 0.0426 s\n",
            "loss: 0.242 (epoch: 5, step: 100) // Avg time/img: 0.0423 s\n",
            "loss: 0.2429 (epoch: 5, step: 150) // Avg time/img: 0.0423 s\n",
            "loss: 0.2402 (epoch: 5, step: 200) // Avg time/img: 0.0424 s\n",
            "loss: 0.2428 (epoch: 5, step: 250) // Avg time/img: 0.0421 s\n",
            "loss: 0.2425 (epoch: 5, step: 300) // Avg time/img: 0.0422 s\n",
            "loss: 0.2413 (epoch: 5, step: 350) // Avg time/img: 0.0422 s\n",
            "loss: 0.2422 (epoch: 5, step: 400) // Avg time/img: 0.0421 s\n",
            "loss: 0.2411 (epoch: 5, step: 450) // Avg time/img: 0.0422 s\n",
            "----- VALIDATING - EPOCH 5 -----\n",
            "VAL loss: 0.3604 (epoch: 5, step: 0) // Avg time/img: 0.0472 s\n",
            "VAL loss: 0.4355 (epoch: 5, step: 50) // Avg time/img: 0.0329 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.65\u001b[0m %\n",
            "save: ../save/erfnet_training_void/model-005.pth (epoch: 5)\n",
            "----- TRAINING - EPOCH 6 -----\n",
            "LEARNING RATE:  3.859447533617852e-05\n",
            "loss: 0.2253 (epoch: 6, step: 0) // Avg time/img: 0.0498 s\n",
            "loss: 0.2457 (epoch: 6, step: 50) // Avg time/img: 0.0425 s\n",
            "loss: 0.2416 (epoch: 6, step: 100) // Avg time/img: 0.0424 s\n",
            "loss: 0.237 (epoch: 6, step: 150) // Avg time/img: 0.0422 s\n",
            "loss: 0.2356 (epoch: 6, step: 200) // Avg time/img: 0.0422 s\n",
            "loss: 0.2336 (epoch: 6, step: 250) // Avg time/img: 0.0422 s\n",
            "loss: 0.2343 (epoch: 6, step: 300) // Avg time/img: 0.0422 s\n",
            "loss: 0.2331 (epoch: 6, step: 350) // Avg time/img: 0.0422 s\n",
            "loss: 0.233 (epoch: 6, step: 400) // Avg time/img: 0.0422 s\n",
            "loss: 0.2324 (epoch: 6, step: 450) // Avg time/img: 0.0422 s\n",
            "----- VALIDATING - EPOCH 6 -----\n",
            "VAL loss: 0.3295 (epoch: 6, step: 0) // Avg time/img: 0.0485 s\n",
            "VAL loss: 0.422 (epoch: 6, step: 50) // Avg time/img: 0.0335 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.73\u001b[0m %\n",
            "save: ../save/erfnet_training_void/model-006.pth (epoch: 6)\n",
            "----- TRAINING - EPOCH 7 -----\n",
            "LEARNING RATE:  3.6270892346861e-05\n",
            "loss: 0.1908 (epoch: 7, step: 0) // Avg time/img: 0.0516 s\n",
            "loss: 0.2258 (epoch: 7, step: 50) // Avg time/img: 0.0422 s\n",
            "loss: 0.2271 (epoch: 7, step: 100) // Avg time/img: 0.0421 s\n",
            "loss: 0.2284 (epoch: 7, step: 150) // Avg time/img: 0.0420 s\n",
            "loss: 0.2289 (epoch: 7, step: 200) // Avg time/img: 0.0420 s\n",
            "loss: 0.2296 (epoch: 7, step: 250) // Avg time/img: 0.0423 s\n",
            "loss: 0.2287 (epoch: 7, step: 300) // Avg time/img: 0.0422 s\n",
            "loss: 0.2285 (epoch: 7, step: 350) // Avg time/img: 0.0423 s\n",
            "loss: 0.2272 (epoch: 7, step: 400) // Avg time/img: 0.0423 s\n",
            "loss: 0.2269 (epoch: 7, step: 450) // Avg time/img: 0.0422 s\n",
            "----- VALIDATING - EPOCH 7 -----\n",
            "VAL loss: 0.3283 (epoch: 7, step: 0) // Avg time/img: 0.0513 s\n",
            "VAL loss: 0.4156 (epoch: 7, step: 50) // Avg time/img: 0.0335 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.60\u001b[0m %\n",
            "save: ../save/erfnet_training_void/model-007.pth (epoch: 7)\n",
            "----- TRAINING - EPOCH 8 -----\n",
            "LEARNING RATE:  3.393063796290625e-05\n",
            "loss: 0.2905 (epoch: 8, step: 0) // Avg time/img: 0.0537 s\n",
            "loss: 0.2197 (epoch: 8, step: 50) // Avg time/img: 0.0416 s\n",
            "loss: 0.219 (epoch: 8, step: 100) // Avg time/img: 0.0414 s\n",
            "loss: 0.2224 (epoch: 8, step: 150) // Avg time/img: 0.0415 s\n",
            "loss: 0.2211 (epoch: 8, step: 200) // Avg time/img: 0.0415 s\n",
            "loss: 0.2215 (epoch: 8, step: 250) // Avg time/img: 0.0417 s\n",
            "loss: 0.2219 (epoch: 8, step: 300) // Avg time/img: 0.0419 s\n",
            "loss: 0.222 (epoch: 8, step: 350) // Avg time/img: 0.0419 s\n",
            "loss: 0.2218 (epoch: 8, step: 400) // Avg time/img: 0.0420 s\n",
            "loss: 0.2218 (epoch: 8, step: 450) // Avg time/img: 0.0419 s\n",
            "----- VALIDATING - EPOCH 8 -----\n",
            "VAL loss: 0.3166 (epoch: 8, step: 0) // Avg time/img: 0.0485 s\n",
            "VAL loss: 0.4134 (epoch: 8, step: 50) // Avg time/img: 0.0332 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.80\u001b[0m %\n",
            "save: ../save/erfnet_training_void/model-008.pth (epoch: 8)\n",
            "----- TRAINING - EPOCH 9 -----\n",
            "LEARNING RATE:  3.157229337446777e-05\n",
            "loss: 0.1933 (epoch: 9, step: 0) // Avg time/img: 0.0498 s\n",
            "loss: 0.2247 (epoch: 9, step: 50) // Avg time/img: 0.0434 s\n",
            "loss: 0.226 (epoch: 9, step: 100) // Avg time/img: 0.0426 s\n",
            "loss: 0.2231 (epoch: 9, step: 150) // Avg time/img: 0.0425 s\n",
            "loss: 0.2229 (epoch: 9, step: 200) // Avg time/img: 0.0425 s\n",
            "loss: 0.221 (epoch: 9, step: 250) // Avg time/img: 0.0424 s\n",
            "loss: 0.2201 (epoch: 9, step: 300) // Avg time/img: 0.0424 s\n",
            "loss: 0.2189 (epoch: 9, step: 350) // Avg time/img: 0.0424 s\n",
            "loss: 0.2182 (epoch: 9, step: 400) // Avg time/img: 0.0424 s\n",
            "loss: 0.2195 (epoch: 9, step: 450) // Avg time/img: 0.0424 s\n",
            "----- VALIDATING - EPOCH 9 -----\n",
            "VAL loss: 0.3145 (epoch: 9, step: 0) // Avg time/img: 0.0508 s\n",
            "VAL loss: 0.4046 (epoch: 9, step: 50) // Avg time/img: 0.0335 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.85\u001b[0m %\n",
            "save: ../save/erfnet_training_void/model-009.pth (epoch: 9)\n",
            "----- TRAINING - EPOCH 10 -----\n",
            "LEARNING RATE:  2.9194189645999014e-05\n",
            "loss: 0.1822 (epoch: 10, step: 0) // Avg time/img: 0.0517 s\n",
            "loss: 0.2173 (epoch: 10, step: 50) // Avg time/img: 0.0426 s\n",
            "loss: 0.2177 (epoch: 10, step: 100) // Avg time/img: 0.0419 s\n",
            "loss: 0.2175 (epoch: 10, step: 150) // Avg time/img: 0.0417 s\n",
            "loss: 0.2182 (epoch: 10, step: 200) // Avg time/img: 0.0421 s\n",
            "loss: 0.2187 (epoch: 10, step: 250) // Avg time/img: 0.0420 s\n",
            "loss: 0.2177 (epoch: 10, step: 300) // Avg time/img: 0.0420 s\n",
            "loss: 0.2174 (epoch: 10, step: 350) // Avg time/img: 0.0420 s\n",
            "loss: 0.2174 (epoch: 10, step: 400) // Avg time/img: 0.0420 s\n",
            "loss: 0.217 (epoch: 10, step: 450) // Avg time/img: 0.0421 s\n",
            "----- VALIDATING - EPOCH 10 -----\n",
            "VAL loss: 0.319 (epoch: 10, step: 0) // Avg time/img: 0.0461 s\n",
            "VAL loss: 0.3999 (epoch: 10, step: 50) // Avg time/img: 0.0331 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.46\u001b[0m %\n",
            "save: ../save/erfnet_training_void/model-010.pth (epoch: 10)\n",
            "----- TRAINING - EPOCH 11 -----\n",
            "LEARNING RATE:  2.679433656340733e-05\n",
            "loss: 0.1791 (epoch: 11, step: 0) // Avg time/img: 0.0476 s\n",
            "loss: 0.2232 (epoch: 11, step: 50) // Avg time/img: 0.0432 s\n",
            "loss: 0.2188 (epoch: 11, step: 100) // Avg time/img: 0.0430 s\n",
            "loss: 0.2168 (epoch: 11, step: 150) // Avg time/img: 0.0426 s\n",
            "loss: 0.2127 (epoch: 11, step: 200) // Avg time/img: 0.0427 s\n",
            "loss: 0.2125 (epoch: 11, step: 250) // Avg time/img: 0.0427 s\n",
            "loss: 0.2141 (epoch: 11, step: 300) // Avg time/img: 0.0426 s\n",
            "loss: 0.2146 (epoch: 11, step: 350) // Avg time/img: 0.0425 s\n",
            "loss: 0.2152 (epoch: 11, step: 400) // Avg time/img: 0.0424 s\n",
            "loss: 0.2151 (epoch: 11, step: 450) // Avg time/img: 0.0424 s\n",
            "----- VALIDATING - EPOCH 11 -----\n",
            "VAL loss: 0.3172 (epoch: 11, step: 0) // Avg time/img: 0.0457 s\n",
            "VAL loss: 0.3991 (epoch: 11, step: 50) // Avg time/img: 0.0333 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.82\u001b[0m %\n",
            "save: ../save/erfnet_training_void/model-011.pth (epoch: 11)\n",
            "----- TRAINING - EPOCH 12 -----\n",
            "LEARNING RATE:  2.437032195894977e-05\n",
            "loss: 0.2036 (epoch: 12, step: 0) // Avg time/img: 0.0504 s\n",
            "loss: 0.2164 (epoch: 12, step: 50) // Avg time/img: 0.0427 s\n",
            "loss: 0.2157 (epoch: 12, step: 100) // Avg time/img: 0.0420 s\n",
            "loss: 0.2138 (epoch: 12, step: 150) // Avg time/img: 0.0424 s\n",
            "loss: 0.2127 (epoch: 12, step: 200) // Avg time/img: 0.0422 s\n",
            "loss: 0.2114 (epoch: 12, step: 250) // Avg time/img: 0.0423 s\n",
            "loss: 0.2118 (epoch: 12, step: 300) // Avg time/img: 0.0421 s\n",
            "loss: 0.2109 (epoch: 12, step: 350) // Avg time/img: 0.0421 s\n",
            "loss: 0.2116 (epoch: 12, step: 400) // Avg time/img: 0.0422 s\n",
            "loss: 0.2125 (epoch: 12, step: 450) // Avg time/img: 0.0422 s\n",
            "----- VALIDATING - EPOCH 12 -----\n",
            "VAL loss: 0.3135 (epoch: 12, step: 0) // Avg time/img: 0.0516 s\n",
            "VAL loss: 0.3944 (epoch: 12, step: 50) // Avg time/img: 0.0334 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.65\u001b[0m %\n",
            "save: ../save/erfnet_training_void/model-012.pth (epoch: 12)\n",
            "----- TRAINING - EPOCH 13 -----\n",
            "LEARNING RATE:  2.191916452770435e-05\n",
            "loss: 0.1953 (epoch: 13, step: 0) // Avg time/img: 0.0508 s\n",
            "loss: 0.2212 (epoch: 13, step: 50) // Avg time/img: 0.0426 s\n",
            "loss: 0.2178 (epoch: 13, step: 100) // Avg time/img: 0.0422 s\n",
            "loss: 0.2153 (epoch: 13, step: 150) // Avg time/img: 0.0421 s\n",
            "loss: 0.2128 (epoch: 13, step: 200) // Avg time/img: 0.0423 s\n",
            "loss: 0.2129 (epoch: 13, step: 250) // Avg time/img: 0.0421 s\n",
            "loss: 0.2121 (epoch: 13, step: 300) // Avg time/img: 0.0420 s\n",
            "loss: 0.2105 (epoch: 13, step: 350) // Avg time/img: 0.0421 s\n",
            "loss: 0.2118 (epoch: 13, step: 400) // Avg time/img: 0.0420 s\n",
            "loss: 0.2119 (epoch: 13, step: 450) // Avg time/img: 0.0422 s\n",
            "----- VALIDATING - EPOCH 13 -----\n",
            "VAL loss: 0.3088 (epoch: 13, step: 0) // Avg time/img: 0.0436 s\n",
            "VAL loss: 0.3911 (epoch: 13, step: 50) // Avg time/img: 0.0336 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.67\u001b[0m %\n",
            "save: ../save/erfnet_training_void/model-013.pth (epoch: 13)\n",
            "----- TRAINING - EPOCH 14 -----\n",
            "LEARNING RATE:  1.9437089939938174e-05\n",
            "loss: 0.2577 (epoch: 14, step: 0) // Avg time/img: 0.0477 s\n",
            "loss: 0.2116 (epoch: 14, step: 50) // Avg time/img: 0.0422 s\n",
            "loss: 0.2079 (epoch: 14, step: 100) // Avg time/img: 0.0426 s\n",
            "loss: 0.2098 (epoch: 14, step: 150) // Avg time/img: 0.0422 s\n",
            "loss: 0.2098 (epoch: 14, step: 200) // Avg time/img: 0.0423 s\n",
            "loss: 0.2111 (epoch: 14, step: 250) // Avg time/img: 0.0422 s\n",
            "loss: 0.212 (epoch: 14, step: 300) // Avg time/img: 0.0422 s\n",
            "loss: 0.212 (epoch: 14, step: 350) // Avg time/img: 0.0422 s\n",
            "loss: 0.2116 (epoch: 14, step: 400) // Avg time/img: 0.0422 s\n",
            "loss: 0.2103 (epoch: 14, step: 450) // Avg time/img: 0.0422 s\n",
            "----- VALIDATING - EPOCH 14 -----\n",
            "VAL loss: 0.326 (epoch: 14, step: 0) // Avg time/img: 0.0519 s\n",
            "VAL loss: 0.3921 (epoch: 14, step: 50) // Avg time/img: 0.0333 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.89\u001b[0m %\n",
            "save: ../save/erfnet_training_void/model-014.pth (epoch: 14)\n",
            "----- TRAINING - EPOCH 15 -----\n",
            "LEARNING RATE:  1.6919173095082493e-05\n",
            "loss: 0.1789 (epoch: 15, step: 0) // Avg time/img: 0.0474 s\n",
            "loss: 0.2163 (epoch: 15, step: 50) // Avg time/img: 0.0421 s\n",
            "loss: 0.2146 (epoch: 15, step: 100) // Avg time/img: 0.0421 s\n",
            "loss: 0.2115 (epoch: 15, step: 150) // Avg time/img: 0.0423 s\n",
            "loss: 0.2129 (epoch: 15, step: 200) // Avg time/img: 0.0421 s\n",
            "loss: 0.2126 (epoch: 15, step: 250) // Avg time/img: 0.0421 s\n",
            "loss: 0.212 (epoch: 15, step: 300) // Avg time/img: 0.0421 s\n",
            "loss: 0.2115 (epoch: 15, step: 350) // Avg time/img: 0.0421 s\n",
            "loss: 0.211 (epoch: 15, step: 400) // Avg time/img: 0.0421 s\n",
            "loss: 0.2105 (epoch: 15, step: 450) // Avg time/img: 0.0421 s\n",
            "----- VALIDATING - EPOCH 15 -----\n",
            "VAL loss: 0.3146 (epoch: 15, step: 0) // Avg time/img: 0.0527 s\n",
            "VAL loss: 0.3892 (epoch: 15, step: 50) // Avg time/img: 0.0333 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.44\u001b[0m %\n",
            "save: ../save/erfnet_training_void/model-015.pth (epoch: 15)\n",
            "----- TRAINING - EPOCH 16 -----\n",
            "LEARNING RATE:  1.4358729437462937e-05\n",
            "loss: 0.1668 (epoch: 16, step: 0) // Avg time/img: 0.0584 s\n",
            "loss: 0.2033 (epoch: 16, step: 50) // Avg time/img: 0.0426 s\n",
            "loss: 0.2067 (epoch: 16, step: 100) // Avg time/img: 0.0420 s\n",
            "loss: 0.2092 (epoch: 16, step: 150) // Avg time/img: 0.0421 s\n",
            "loss: 0.2112 (epoch: 16, step: 200) // Avg time/img: 0.0422 s\n",
            "loss: 0.2098 (epoch: 16, step: 250) // Avg time/img: 0.0421 s\n",
            "loss: 0.2094 (epoch: 16, step: 300) // Avg time/img: 0.0422 s\n",
            "loss: 0.2092 (epoch: 16, step: 350) // Avg time/img: 0.0421 s\n",
            "loss: 0.21 (epoch: 16, step: 400) // Avg time/img: 0.0422 s\n",
            "loss: 0.2094 (epoch: 16, step: 450) // Avg time/img: 0.0421 s\n",
            "----- VALIDATING - EPOCH 16 -----\n",
            "VAL loss: 0.3133 (epoch: 16, step: 0) // Avg time/img: 0.0459 s\n",
            "VAL loss: 0.391 (epoch: 16, step: 50) // Avg time/img: 0.0335 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.76\u001b[0m %\n",
            "save: ../save/erfnet_training_void/model-016.pth (epoch: 16)\n",
            "----- TRAINING - EPOCH 17 -----\n",
            "LEARNING RATE:  1.1746189430880188e-05\n",
            "loss: 0.2328 (epoch: 17, step: 0) // Avg time/img: 0.0541 s\n",
            "loss: 0.2081 (epoch: 17, step: 50) // Avg time/img: 0.0431 s\n",
            "loss: 0.2107 (epoch: 17, step: 100) // Avg time/img: 0.0424 s\n",
            "loss: 0.2099 (epoch: 17, step: 150) // Avg time/img: 0.0421 s\n",
            "loss: 0.2102 (epoch: 17, step: 200) // Avg time/img: 0.0422 s\n",
            "loss: 0.2105 (epoch: 17, step: 250) // Avg time/img: 0.0420 s\n",
            "loss: 0.2106 (epoch: 17, step: 300) // Avg time/img: 0.0420 s\n",
            "loss: 0.2105 (epoch: 17, step: 350) // Avg time/img: 0.0421 s\n",
            "loss: 0.2096 (epoch: 17, step: 400) // Avg time/img: 0.0421 s\n",
            "loss: 0.2087 (epoch: 17, step: 450) // Avg time/img: 0.0421 s\n",
            "----- VALIDATING - EPOCH 17 -----\n",
            "VAL loss: 0.3048 (epoch: 17, step: 0) // Avg time/img: 0.0480 s\n",
            "VAL loss: 0.3925 (epoch: 17, step: 50) // Avg time/img: 0.0333 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.62\u001b[0m %\n",
            "save: ../save/erfnet_training_void/model-017.pth (epoch: 17)\n",
            "----- TRAINING - EPOCH 18 -----\n",
            "LEARNING RATE:  9.066760365683729e-06\n",
            "loss: 0.1615 (epoch: 18, step: 0) // Avg time/img: 0.0492 s\n",
            "loss: 0.2073 (epoch: 18, step: 50) // Avg time/img: 0.0425 s\n",
            "loss: 0.208 (epoch: 18, step: 100) // Avg time/img: 0.0424 s\n",
            "loss: 0.2075 (epoch: 18, step: 150) // Avg time/img: 0.0423 s\n",
            "loss: 0.2064 (epoch: 18, step: 200) // Avg time/img: 0.0423 s\n",
            "loss: 0.2083 (epoch: 18, step: 250) // Avg time/img: 0.0423 s\n",
            "loss: 0.2074 (epoch: 18, step: 300) // Avg time/img: 0.0423 s\n",
            "loss: 0.2067 (epoch: 18, step: 350) // Avg time/img: 0.0423 s\n",
            "loss: 0.207 (epoch: 18, step: 400) // Avg time/img: 0.0423 s\n",
            "loss: 0.2075 (epoch: 18, step: 450) // Avg time/img: 0.0424 s\n",
            "----- VALIDATING - EPOCH 18 -----\n",
            "VAL loss: 0.3039 (epoch: 18, step: 0) // Avg time/img: 0.0490 s\n",
            "VAL loss: 0.3887 (epoch: 18, step: 50) // Avg time/img: 0.0331 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.78\u001b[0m %\n",
            "save: ../save/erfnet_training_void/model-018.pth (epoch: 18)\n",
            "----- TRAINING - EPOCH 19 -----\n",
            "LEARNING RATE:  6.294627058970836e-06\n",
            "loss: 0.2163 (epoch: 19, step: 0) // Avg time/img: 0.0500 s\n",
            "loss: 0.2055 (epoch: 19, step: 50) // Avg time/img: 0.0422 s\n",
            "loss: 0.2071 (epoch: 19, step: 100) // Avg time/img: 0.0420 s\n",
            "loss: 0.2061 (epoch: 19, step: 150) // Avg time/img: 0.0421 s\n",
            "loss: 0.2076 (epoch: 19, step: 200) // Avg time/img: 0.0419 s\n",
            "loss: 0.2095 (epoch: 19, step: 250) // Avg time/img: 0.0421 s\n",
            "loss: 0.2081 (epoch: 19, step: 300) // Avg time/img: 0.0421 s\n",
            "loss: 0.208 (epoch: 19, step: 350) // Avg time/img: 0.0421 s\n",
            "loss: 0.2081 (epoch: 19, step: 400) // Avg time/img: 0.0422 s\n",
            "loss: 0.2082 (epoch: 19, step: 450) // Avg time/img: 0.0422 s\n",
            "----- VALIDATING - EPOCH 19 -----\n",
            "VAL loss: 0.307 (epoch: 19, step: 0) // Avg time/img: 0.0416 s\n",
            "VAL loss: 0.386 (epoch: 19, step: 50) // Avg time/img: 0.0336 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.94\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/erfnet_training_void/model-019.pth (epoch: 19)\n",
            "save: ../save/erfnet_training_void/model_best.pth (epoch: 19)\n",
            "----- TRAINING - EPOCH 20 -----\n",
            "LEARNING RATE:  3.373207119183911e-06\n",
            "loss: 0.2421 (epoch: 20, step: 0) // Avg time/img: 0.0452 s\n",
            "loss: 0.2051 (epoch: 20, step: 50) // Avg time/img: 0.0431 s\n",
            "loss: 0.203 (epoch: 20, step: 100) // Avg time/img: 0.0424 s\n",
            "loss: 0.2044 (epoch: 20, step: 150) // Avg time/img: 0.0421 s\n",
            "loss: 0.2073 (epoch: 20, step: 200) // Avg time/img: 0.0421 s\n",
            "loss: 0.2073 (epoch: 20, step: 250) // Avg time/img: 0.0421 s\n",
            "loss: 0.2075 (epoch: 20, step: 300) // Avg time/img: 0.0423 s\n",
            "loss: 0.2071 (epoch: 20, step: 350) // Avg time/img: 0.0424 s\n",
            "loss: 0.2078 (epoch: 20, step: 400) // Avg time/img: 0.0424 s\n",
            "loss: 0.2079 (epoch: 20, step: 450) // Avg time/img: 0.0424 s\n",
            "----- VALIDATING - EPOCH 20 -----\n",
            "VAL loss: 0.2996 (epoch: 20, step: 0) // Avg time/img: 0.0454 s\n",
            "VAL loss: 0.3889 (epoch: 20, step: 50) // Avg time/img: 0.0332 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.86\u001b[0m %\n",
            "save: ../save/erfnet_training_void/model-020.pth (epoch: 20)\n",
            "========== TRAINING FINISHED ===========\n",
            "Model saved in /content/AnomalySegmentation/save/erfnet_training_void\n",
            "updating: content/AnomalySegmentation/save/erfnet_training_void/ (stored 0%)\n",
            "updating: content/AnomalySegmentation/save/erfnet_training_void/opts.txt (deflated 37%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_void/model-005.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_void/best.txt (stored 0%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_void/model.txt (deflated 92%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_void/model-015.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_void/model-013.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_void/model-012.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_void/model_best.pth.tar (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_void/model-014.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_void/model-004.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_void/model-018.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_void/model-009.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_void/model-010.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_void/model_best.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_void/model-017.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_void/model-007.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_void/model-020.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_void/model-016.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_void/automated_log.txt (deflated 66%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_void/model-006.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_void/model-001.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_void/model-003.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_void/checkpoint.pth.tar (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_void/model-008.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_void/model-019.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_void/model-002.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_void/model-011.pth (deflated 10%)\n",
            "\n",
            "\n",
            "----- Fine-tuning enet for VOID classification -----\n",
            "odict_keys(['initial_block.main_branch.weight', 'initial_block.batch_norm.weight', 'initial_block.batch_norm.bias', 'initial_block.batch_norm.running_mean', 'initial_block.batch_norm.running_var', 'initial_block.batch_norm.num_batches_tracked', 'initial_block.out_activation.weight', 'downsample1_0.ext_conv1.0.weight', 'downsample1_0.ext_conv1.1.weight', 'downsample1_0.ext_conv1.1.bias', 'downsample1_0.ext_conv1.1.running_mean', 'downsample1_0.ext_conv1.1.running_var', 'downsample1_0.ext_conv1.1.num_batches_tracked', 'downsample1_0.ext_conv1.2.weight', 'downsample1_0.ext_conv2.0.weight', 'downsample1_0.ext_conv2.1.weight', 'downsample1_0.ext_conv2.1.bias', 'downsample1_0.ext_conv2.1.running_mean', 'downsample1_0.ext_conv2.1.running_var', 'downsample1_0.ext_conv2.1.num_batches_tracked', 'downsample1_0.ext_conv2.2.weight', 'downsample1_0.ext_conv3.0.weight', 'downsample1_0.ext_conv3.1.weight', 'downsample1_0.ext_conv3.1.bias', 'downsample1_0.ext_conv3.1.running_mean', 'downsample1_0.ext_conv3.1.running_var', 'downsample1_0.ext_conv3.1.num_batches_tracked', 'downsample1_0.ext_conv3.2.weight', 'downsample1_0.out_activation.weight', 'regular1_1.ext_conv1.0.weight', 'regular1_1.ext_conv1.1.weight', 'regular1_1.ext_conv1.1.bias', 'regular1_1.ext_conv1.1.running_mean', 'regular1_1.ext_conv1.1.running_var', 'regular1_1.ext_conv1.1.num_batches_tracked', 'regular1_1.ext_conv1.2.weight', 'regular1_1.ext_conv2.0.weight', 'regular1_1.ext_conv2.1.weight', 'regular1_1.ext_conv2.1.bias', 'regular1_1.ext_conv2.1.running_mean', 'regular1_1.ext_conv2.1.running_var', 'regular1_1.ext_conv2.1.num_batches_tracked', 'regular1_1.ext_conv2.2.weight', 'regular1_1.ext_conv3.0.weight', 'regular1_1.ext_conv3.1.weight', 'regular1_1.ext_conv3.1.bias', 'regular1_1.ext_conv3.1.running_mean', 'regular1_1.ext_conv3.1.running_var', 'regular1_1.ext_conv3.1.num_batches_tracked', 'regular1_1.ext_conv3.2.weight', 'regular1_1.out_activation.weight', 'regular1_2.ext_conv1.0.weight', 'regular1_2.ext_conv1.1.weight', 'regular1_2.ext_conv1.1.bias', 'regular1_2.ext_conv1.1.running_mean', 'regular1_2.ext_conv1.1.running_var', 'regular1_2.ext_conv1.1.num_batches_tracked', 'regular1_2.ext_conv1.2.weight', 'regular1_2.ext_conv2.0.weight', 'regular1_2.ext_conv2.1.weight', 'regular1_2.ext_conv2.1.bias', 'regular1_2.ext_conv2.1.running_mean', 'regular1_2.ext_conv2.1.running_var', 'regular1_2.ext_conv2.1.num_batches_tracked', 'regular1_2.ext_conv2.2.weight', 'regular1_2.ext_conv3.0.weight', 'regular1_2.ext_conv3.1.weight', 'regular1_2.ext_conv3.1.bias', 'regular1_2.ext_conv3.1.running_mean', 'regular1_2.ext_conv3.1.running_var', 'regular1_2.ext_conv3.1.num_batches_tracked', 'regular1_2.ext_conv3.2.weight', 'regular1_2.out_activation.weight', 'regular1_3.ext_conv1.0.weight', 'regular1_3.ext_conv1.1.weight', 'regular1_3.ext_conv1.1.bias', 'regular1_3.ext_conv1.1.running_mean', 'regular1_3.ext_conv1.1.running_var', 'regular1_3.ext_conv1.1.num_batches_tracked', 'regular1_3.ext_conv1.2.weight', 'regular1_3.ext_conv2.0.weight', 'regular1_3.ext_conv2.1.weight', 'regular1_3.ext_conv2.1.bias', 'regular1_3.ext_conv2.1.running_mean', 'regular1_3.ext_conv2.1.running_var', 'regular1_3.ext_conv2.1.num_batches_tracked', 'regular1_3.ext_conv2.2.weight', 'regular1_3.ext_conv3.0.weight', 'regular1_3.ext_conv3.1.weight', 'regular1_3.ext_conv3.1.bias', 'regular1_3.ext_conv3.1.running_mean', 'regular1_3.ext_conv3.1.running_var', 'regular1_3.ext_conv3.1.num_batches_tracked', 'regular1_3.ext_conv3.2.weight', 'regular1_3.out_activation.weight', 'regular1_4.ext_conv1.0.weight', 'regular1_4.ext_conv1.1.weight', 'regular1_4.ext_conv1.1.bias', 'regular1_4.ext_conv1.1.running_mean', 'regular1_4.ext_conv1.1.running_var', 'regular1_4.ext_conv1.1.num_batches_tracked', 'regular1_4.ext_conv1.2.weight', 'regular1_4.ext_conv2.0.weight', 'regular1_4.ext_conv2.1.weight', 'regular1_4.ext_conv2.1.bias', 'regular1_4.ext_conv2.1.running_mean', 'regular1_4.ext_conv2.1.running_var', 'regular1_4.ext_conv2.1.num_batches_tracked', 'regular1_4.ext_conv2.2.weight', 'regular1_4.ext_conv3.0.weight', 'regular1_4.ext_conv3.1.weight', 'regular1_4.ext_conv3.1.bias', 'regular1_4.ext_conv3.1.running_mean', 'regular1_4.ext_conv3.1.running_var', 'regular1_4.ext_conv3.1.num_batches_tracked', 'regular1_4.ext_conv3.2.weight', 'regular1_4.out_activation.weight', 'downsample2_0.ext_conv1.0.weight', 'downsample2_0.ext_conv1.1.weight', 'downsample2_0.ext_conv1.1.bias', 'downsample2_0.ext_conv1.1.running_mean', 'downsample2_0.ext_conv1.1.running_var', 'downsample2_0.ext_conv1.1.num_batches_tracked', 'downsample2_0.ext_conv1.2.weight', 'downsample2_0.ext_conv2.0.weight', 'downsample2_0.ext_conv2.1.weight', 'downsample2_0.ext_conv2.1.bias', 'downsample2_0.ext_conv2.1.running_mean', 'downsample2_0.ext_conv2.1.running_var', 'downsample2_0.ext_conv2.1.num_batches_tracked', 'downsample2_0.ext_conv2.2.weight', 'downsample2_0.ext_conv3.0.weight', 'downsample2_0.ext_conv3.1.weight', 'downsample2_0.ext_conv3.1.bias', 'downsample2_0.ext_conv3.1.running_mean', 'downsample2_0.ext_conv3.1.running_var', 'downsample2_0.ext_conv3.1.num_batches_tracked', 'downsample2_0.ext_conv3.2.weight', 'downsample2_0.out_activation.weight', 'regular2_1.ext_conv1.0.weight', 'regular2_1.ext_conv1.1.weight', 'regular2_1.ext_conv1.1.bias', 'regular2_1.ext_conv1.1.running_mean', 'regular2_1.ext_conv1.1.running_var', 'regular2_1.ext_conv1.1.num_batches_tracked', 'regular2_1.ext_conv1.2.weight', 'regular2_1.ext_conv2.0.weight', 'regular2_1.ext_conv2.1.weight', 'regular2_1.ext_conv2.1.bias', 'regular2_1.ext_conv2.1.running_mean', 'regular2_1.ext_conv2.1.running_var', 'regular2_1.ext_conv2.1.num_batches_tracked', 'regular2_1.ext_conv2.2.weight', 'regular2_1.ext_conv3.0.weight', 'regular2_1.ext_conv3.1.weight', 'regular2_1.ext_conv3.1.bias', 'regular2_1.ext_conv3.1.running_mean', 'regular2_1.ext_conv3.1.running_var', 'regular2_1.ext_conv3.1.num_batches_tracked', 'regular2_1.ext_conv3.2.weight', 'regular2_1.out_activation.weight', 'dilated2_2.ext_conv1.0.weight', 'dilated2_2.ext_conv1.1.weight', 'dilated2_2.ext_conv1.1.bias', 'dilated2_2.ext_conv1.1.running_mean', 'dilated2_2.ext_conv1.1.running_var', 'dilated2_2.ext_conv1.1.num_batches_tracked', 'dilated2_2.ext_conv1.2.weight', 'dilated2_2.ext_conv2.0.weight', 'dilated2_2.ext_conv2.1.weight', 'dilated2_2.ext_conv2.1.bias', 'dilated2_2.ext_conv2.1.running_mean', 'dilated2_2.ext_conv2.1.running_var', 'dilated2_2.ext_conv2.1.num_batches_tracked', 'dilated2_2.ext_conv2.2.weight', 'dilated2_2.ext_conv3.0.weight', 'dilated2_2.ext_conv3.1.weight', 'dilated2_2.ext_conv3.1.bias', 'dilated2_2.ext_conv3.1.running_mean', 'dilated2_2.ext_conv3.1.running_var', 'dilated2_2.ext_conv3.1.num_batches_tracked', 'dilated2_2.ext_conv3.2.weight', 'dilated2_2.out_activation.weight', 'asymmetric2_3.ext_conv1.0.weight', 'asymmetric2_3.ext_conv1.1.weight', 'asymmetric2_3.ext_conv1.1.bias', 'asymmetric2_3.ext_conv1.1.running_mean', 'asymmetric2_3.ext_conv1.1.running_var', 'asymmetric2_3.ext_conv1.1.num_batches_tracked', 'asymmetric2_3.ext_conv1.2.weight', 'asymmetric2_3.ext_conv2.0.weight', 'asymmetric2_3.ext_conv2.1.weight', 'asymmetric2_3.ext_conv2.1.bias', 'asymmetric2_3.ext_conv2.1.running_mean', 'asymmetric2_3.ext_conv2.1.running_var', 'asymmetric2_3.ext_conv2.1.num_batches_tracked', 'asymmetric2_3.ext_conv2.2.weight', 'asymmetric2_3.ext_conv2.3.weight', 'asymmetric2_3.ext_conv2.4.weight', 'asymmetric2_3.ext_conv2.4.bias', 'asymmetric2_3.ext_conv2.4.running_mean', 'asymmetric2_3.ext_conv2.4.running_var', 'asymmetric2_3.ext_conv2.4.num_batches_tracked', 'asymmetric2_3.ext_conv2.5.weight', 'asymmetric2_3.ext_conv3.0.weight', 'asymmetric2_3.ext_conv3.1.weight', 'asymmetric2_3.ext_conv3.1.bias', 'asymmetric2_3.ext_conv3.1.running_mean', 'asymmetric2_3.ext_conv3.1.running_var', 'asymmetric2_3.ext_conv3.1.num_batches_tracked', 'asymmetric2_3.ext_conv3.2.weight', 'asymmetric2_3.out_activation.weight', 'dilated2_4.ext_conv1.0.weight', 'dilated2_4.ext_conv1.1.weight', 'dilated2_4.ext_conv1.1.bias', 'dilated2_4.ext_conv1.1.running_mean', 'dilated2_4.ext_conv1.1.running_var', 'dilated2_4.ext_conv1.1.num_batches_tracked', 'dilated2_4.ext_conv1.2.weight', 'dilated2_4.ext_conv2.0.weight', 'dilated2_4.ext_conv2.1.weight', 'dilated2_4.ext_conv2.1.bias', 'dilated2_4.ext_conv2.1.running_mean', 'dilated2_4.ext_conv2.1.running_var', 'dilated2_4.ext_conv2.1.num_batches_tracked', 'dilated2_4.ext_conv2.2.weight', 'dilated2_4.ext_conv3.0.weight', 'dilated2_4.ext_conv3.1.weight', 'dilated2_4.ext_conv3.1.bias', 'dilated2_4.ext_conv3.1.running_mean', 'dilated2_4.ext_conv3.1.running_var', 'dilated2_4.ext_conv3.1.num_batches_tracked', 'dilated2_4.ext_conv3.2.weight', 'dilated2_4.out_activation.weight', 'regular2_5.ext_conv1.0.weight', 'regular2_5.ext_conv1.1.weight', 'regular2_5.ext_conv1.1.bias', 'regular2_5.ext_conv1.1.running_mean', 'regular2_5.ext_conv1.1.running_var', 'regular2_5.ext_conv1.1.num_batches_tracked', 'regular2_5.ext_conv1.2.weight', 'regular2_5.ext_conv2.0.weight', 'regular2_5.ext_conv2.1.weight', 'regular2_5.ext_conv2.1.bias', 'regular2_5.ext_conv2.1.running_mean', 'regular2_5.ext_conv2.1.running_var', 'regular2_5.ext_conv2.1.num_batches_tracked', 'regular2_5.ext_conv2.2.weight', 'regular2_5.ext_conv3.0.weight', 'regular2_5.ext_conv3.1.weight', 'regular2_5.ext_conv3.1.bias', 'regular2_5.ext_conv3.1.running_mean', 'regular2_5.ext_conv3.1.running_var', 'regular2_5.ext_conv3.1.num_batches_tracked', 'regular2_5.ext_conv3.2.weight', 'regular2_5.out_activation.weight', 'dilated2_6.ext_conv1.0.weight', 'dilated2_6.ext_conv1.1.weight', 'dilated2_6.ext_conv1.1.bias', 'dilated2_6.ext_conv1.1.running_mean', 'dilated2_6.ext_conv1.1.running_var', 'dilated2_6.ext_conv1.1.num_batches_tracked', 'dilated2_6.ext_conv1.2.weight', 'dilated2_6.ext_conv2.0.weight', 'dilated2_6.ext_conv2.1.weight', 'dilated2_6.ext_conv2.1.bias', 'dilated2_6.ext_conv2.1.running_mean', 'dilated2_6.ext_conv2.1.running_var', 'dilated2_6.ext_conv2.1.num_batches_tracked', 'dilated2_6.ext_conv2.2.weight', 'dilated2_6.ext_conv3.0.weight', 'dilated2_6.ext_conv3.1.weight', 'dilated2_6.ext_conv3.1.bias', 'dilated2_6.ext_conv3.1.running_mean', 'dilated2_6.ext_conv3.1.running_var', 'dilated2_6.ext_conv3.1.num_batches_tracked', 'dilated2_6.ext_conv3.2.weight', 'dilated2_6.out_activation.weight', 'asymmetric2_7.ext_conv1.0.weight', 'asymmetric2_7.ext_conv1.1.weight', 'asymmetric2_7.ext_conv1.1.bias', 'asymmetric2_7.ext_conv1.1.running_mean', 'asymmetric2_7.ext_conv1.1.running_var', 'asymmetric2_7.ext_conv1.1.num_batches_tracked', 'asymmetric2_7.ext_conv1.2.weight', 'asymmetric2_7.ext_conv2.0.weight', 'asymmetric2_7.ext_conv2.1.weight', 'asymmetric2_7.ext_conv2.1.bias', 'asymmetric2_7.ext_conv2.1.running_mean', 'asymmetric2_7.ext_conv2.1.running_var', 'asymmetric2_7.ext_conv2.1.num_batches_tracked', 'asymmetric2_7.ext_conv2.2.weight', 'asymmetric2_7.ext_conv2.3.weight', 'asymmetric2_7.ext_conv2.4.weight', 'asymmetric2_7.ext_conv2.4.bias', 'asymmetric2_7.ext_conv2.4.running_mean', 'asymmetric2_7.ext_conv2.4.running_var', 'asymmetric2_7.ext_conv2.4.num_batches_tracked', 'asymmetric2_7.ext_conv2.5.weight', 'asymmetric2_7.ext_conv3.0.weight', 'asymmetric2_7.ext_conv3.1.weight', 'asymmetric2_7.ext_conv3.1.bias', 'asymmetric2_7.ext_conv3.1.running_mean', 'asymmetric2_7.ext_conv3.1.running_var', 'asymmetric2_7.ext_conv3.1.num_batches_tracked', 'asymmetric2_7.ext_conv3.2.weight', 'asymmetric2_7.out_activation.weight', 'dilated2_8.ext_conv1.0.weight', 'dilated2_8.ext_conv1.1.weight', 'dilated2_8.ext_conv1.1.bias', 'dilated2_8.ext_conv1.1.running_mean', 'dilated2_8.ext_conv1.1.running_var', 'dilated2_8.ext_conv1.1.num_batches_tracked', 'dilated2_8.ext_conv1.2.weight', 'dilated2_8.ext_conv2.0.weight', 'dilated2_8.ext_conv2.1.weight', 'dilated2_8.ext_conv2.1.bias', 'dilated2_8.ext_conv2.1.running_mean', 'dilated2_8.ext_conv2.1.running_var', 'dilated2_8.ext_conv2.1.num_batches_tracked', 'dilated2_8.ext_conv2.2.weight', 'dilated2_8.ext_conv3.0.weight', 'dilated2_8.ext_conv3.1.weight', 'dilated2_8.ext_conv3.1.bias', 'dilated2_8.ext_conv3.1.running_mean', 'dilated2_8.ext_conv3.1.running_var', 'dilated2_8.ext_conv3.1.num_batches_tracked', 'dilated2_8.ext_conv3.2.weight', 'dilated2_8.out_activation.weight', 'regular3_0.ext_conv1.0.weight', 'regular3_0.ext_conv1.1.weight', 'regular3_0.ext_conv1.1.bias', 'regular3_0.ext_conv1.1.running_mean', 'regular3_0.ext_conv1.1.running_var', 'regular3_0.ext_conv1.1.num_batches_tracked', 'regular3_0.ext_conv1.2.weight', 'regular3_0.ext_conv2.0.weight', 'regular3_0.ext_conv2.1.weight', 'regular3_0.ext_conv2.1.bias', 'regular3_0.ext_conv2.1.running_mean', 'regular3_0.ext_conv2.1.running_var', 'regular3_0.ext_conv2.1.num_batches_tracked', 'regular3_0.ext_conv2.2.weight', 'regular3_0.ext_conv3.0.weight', 'regular3_0.ext_conv3.1.weight', 'regular3_0.ext_conv3.1.bias', 'regular3_0.ext_conv3.1.running_mean', 'regular3_0.ext_conv3.1.running_var', 'regular3_0.ext_conv3.1.num_batches_tracked', 'regular3_0.ext_conv3.2.weight', 'regular3_0.out_activation.weight', 'dilated3_1.ext_conv1.0.weight', 'dilated3_1.ext_conv1.1.weight', 'dilated3_1.ext_conv1.1.bias', 'dilated3_1.ext_conv1.1.running_mean', 'dilated3_1.ext_conv1.1.running_var', 'dilated3_1.ext_conv1.1.num_batches_tracked', 'dilated3_1.ext_conv1.2.weight', 'dilated3_1.ext_conv2.0.weight', 'dilated3_1.ext_conv2.1.weight', 'dilated3_1.ext_conv2.1.bias', 'dilated3_1.ext_conv2.1.running_mean', 'dilated3_1.ext_conv2.1.running_var', 'dilated3_1.ext_conv2.1.num_batches_tracked', 'dilated3_1.ext_conv2.2.weight', 'dilated3_1.ext_conv3.0.weight', 'dilated3_1.ext_conv3.1.weight', 'dilated3_1.ext_conv3.1.bias', 'dilated3_1.ext_conv3.1.running_mean', 'dilated3_1.ext_conv3.1.running_var', 'dilated3_1.ext_conv3.1.num_batches_tracked', 'dilated3_1.ext_conv3.2.weight', 'dilated3_1.out_activation.weight', 'asymmetric3_2.ext_conv1.0.weight', 'asymmetric3_2.ext_conv1.1.weight', 'asymmetric3_2.ext_conv1.1.bias', 'asymmetric3_2.ext_conv1.1.running_mean', 'asymmetric3_2.ext_conv1.1.running_var', 'asymmetric3_2.ext_conv1.1.num_batches_tracked', 'asymmetric3_2.ext_conv1.2.weight', 'asymmetric3_2.ext_conv2.0.weight', 'asymmetric3_2.ext_conv2.1.weight', 'asymmetric3_2.ext_conv2.1.bias', 'asymmetric3_2.ext_conv2.1.running_mean', 'asymmetric3_2.ext_conv2.1.running_var', 'asymmetric3_2.ext_conv2.1.num_batches_tracked', 'asymmetric3_2.ext_conv2.2.weight', 'asymmetric3_2.ext_conv2.3.weight', 'asymmetric3_2.ext_conv2.4.weight', 'asymmetric3_2.ext_conv2.4.bias', 'asymmetric3_2.ext_conv2.4.running_mean', 'asymmetric3_2.ext_conv2.4.running_var', 'asymmetric3_2.ext_conv2.4.num_batches_tracked', 'asymmetric3_2.ext_conv2.5.weight', 'asymmetric3_2.ext_conv3.0.weight', 'asymmetric3_2.ext_conv3.1.weight', 'asymmetric3_2.ext_conv3.1.bias', 'asymmetric3_2.ext_conv3.1.running_mean', 'asymmetric3_2.ext_conv3.1.running_var', 'asymmetric3_2.ext_conv3.1.num_batches_tracked', 'asymmetric3_2.ext_conv3.2.weight', 'asymmetric3_2.out_activation.weight', 'dilated3_3.ext_conv1.0.weight', 'dilated3_3.ext_conv1.1.weight', 'dilated3_3.ext_conv1.1.bias', 'dilated3_3.ext_conv1.1.running_mean', 'dilated3_3.ext_conv1.1.running_var', 'dilated3_3.ext_conv1.1.num_batches_tracked', 'dilated3_3.ext_conv1.2.weight', 'dilated3_3.ext_conv2.0.weight', 'dilated3_3.ext_conv2.1.weight', 'dilated3_3.ext_conv2.1.bias', 'dilated3_3.ext_conv2.1.running_mean', 'dilated3_3.ext_conv2.1.running_var', 'dilated3_3.ext_conv2.1.num_batches_tracked', 'dilated3_3.ext_conv2.2.weight', 'dilated3_3.ext_conv3.0.weight', 'dilated3_3.ext_conv3.1.weight', 'dilated3_3.ext_conv3.1.bias', 'dilated3_3.ext_conv3.1.running_mean', 'dilated3_3.ext_conv3.1.running_var', 'dilated3_3.ext_conv3.1.num_batches_tracked', 'dilated3_3.ext_conv3.2.weight', 'dilated3_3.out_activation.weight', 'regular3_4.ext_conv1.0.weight', 'regular3_4.ext_conv1.1.weight', 'regular3_4.ext_conv1.1.bias', 'regular3_4.ext_conv1.1.running_mean', 'regular3_4.ext_conv1.1.running_var', 'regular3_4.ext_conv1.1.num_batches_tracked', 'regular3_4.ext_conv1.2.weight', 'regular3_4.ext_conv2.0.weight', 'regular3_4.ext_conv2.1.weight', 'regular3_4.ext_conv2.1.bias', 'regular3_4.ext_conv2.1.running_mean', 'regular3_4.ext_conv2.1.running_var', 'regular3_4.ext_conv2.1.num_batches_tracked', 'regular3_4.ext_conv2.2.weight', 'regular3_4.ext_conv3.0.weight', 'regular3_4.ext_conv3.1.weight', 'regular3_4.ext_conv3.1.bias', 'regular3_4.ext_conv3.1.running_mean', 'regular3_4.ext_conv3.1.running_var', 'regular3_4.ext_conv3.1.num_batches_tracked', 'regular3_4.ext_conv3.2.weight', 'regular3_4.out_activation.weight', 'dilated3_5.ext_conv1.0.weight', 'dilated3_5.ext_conv1.1.weight', 'dilated3_5.ext_conv1.1.bias', 'dilated3_5.ext_conv1.1.running_mean', 'dilated3_5.ext_conv1.1.running_var', 'dilated3_5.ext_conv1.1.num_batches_tracked', 'dilated3_5.ext_conv1.2.weight', 'dilated3_5.ext_conv2.0.weight', 'dilated3_5.ext_conv2.1.weight', 'dilated3_5.ext_conv2.1.bias', 'dilated3_5.ext_conv2.1.running_mean', 'dilated3_5.ext_conv2.1.running_var', 'dilated3_5.ext_conv2.1.num_batches_tracked', 'dilated3_5.ext_conv2.2.weight', 'dilated3_5.ext_conv3.0.weight', 'dilated3_5.ext_conv3.1.weight', 'dilated3_5.ext_conv3.1.bias', 'dilated3_5.ext_conv3.1.running_mean', 'dilated3_5.ext_conv3.1.running_var', 'dilated3_5.ext_conv3.1.num_batches_tracked', 'dilated3_5.ext_conv3.2.weight', 'dilated3_5.out_activation.weight', 'asymmetric3_6.ext_conv1.0.weight', 'asymmetric3_6.ext_conv1.1.weight', 'asymmetric3_6.ext_conv1.1.bias', 'asymmetric3_6.ext_conv1.1.running_mean', 'asymmetric3_6.ext_conv1.1.running_var', 'asymmetric3_6.ext_conv1.1.num_batches_tracked', 'asymmetric3_6.ext_conv1.2.weight', 'asymmetric3_6.ext_conv2.0.weight', 'asymmetric3_6.ext_conv2.1.weight', 'asymmetric3_6.ext_conv2.1.bias', 'asymmetric3_6.ext_conv2.1.running_mean', 'asymmetric3_6.ext_conv2.1.running_var', 'asymmetric3_6.ext_conv2.1.num_batches_tracked', 'asymmetric3_6.ext_conv2.2.weight', 'asymmetric3_6.ext_conv2.3.weight', 'asymmetric3_6.ext_conv2.4.weight', 'asymmetric3_6.ext_conv2.4.bias', 'asymmetric3_6.ext_conv2.4.running_mean', 'asymmetric3_6.ext_conv2.4.running_var', 'asymmetric3_6.ext_conv2.4.num_batches_tracked', 'asymmetric3_6.ext_conv2.5.weight', 'asymmetric3_6.ext_conv3.0.weight', 'asymmetric3_6.ext_conv3.1.weight', 'asymmetric3_6.ext_conv3.1.bias', 'asymmetric3_6.ext_conv3.1.running_mean', 'asymmetric3_6.ext_conv3.1.running_var', 'asymmetric3_6.ext_conv3.1.num_batches_tracked', 'asymmetric3_6.ext_conv3.2.weight', 'asymmetric3_6.out_activation.weight', 'dilated3_7.ext_conv1.0.weight', 'dilated3_7.ext_conv1.1.weight', 'dilated3_7.ext_conv1.1.bias', 'dilated3_7.ext_conv1.1.running_mean', 'dilated3_7.ext_conv1.1.running_var', 'dilated3_7.ext_conv1.1.num_batches_tracked', 'dilated3_7.ext_conv1.2.weight', 'dilated3_7.ext_conv2.0.weight', 'dilated3_7.ext_conv2.1.weight', 'dilated3_7.ext_conv2.1.bias', 'dilated3_7.ext_conv2.1.running_mean', 'dilated3_7.ext_conv2.1.running_var', 'dilated3_7.ext_conv2.1.num_batches_tracked', 'dilated3_7.ext_conv2.2.weight', 'dilated3_7.ext_conv3.0.weight', 'dilated3_7.ext_conv3.1.weight', 'dilated3_7.ext_conv3.1.bias', 'dilated3_7.ext_conv3.1.running_mean', 'dilated3_7.ext_conv3.1.running_var', 'dilated3_7.ext_conv3.1.num_batches_tracked', 'dilated3_7.ext_conv3.2.weight', 'dilated3_7.out_activation.weight', 'upsample4_0.main_conv1.0.weight', 'upsample4_0.main_conv1.1.weight', 'upsample4_0.main_conv1.1.bias', 'upsample4_0.main_conv1.1.running_mean', 'upsample4_0.main_conv1.1.running_var', 'upsample4_0.main_conv1.1.num_batches_tracked', 'upsample4_0.ext_conv1.0.weight', 'upsample4_0.ext_conv1.1.weight', 'upsample4_0.ext_conv1.1.bias', 'upsample4_0.ext_conv1.1.running_mean', 'upsample4_0.ext_conv1.1.running_var', 'upsample4_0.ext_conv1.1.num_batches_tracked', 'upsample4_0.ext_tconv1.weight', 'upsample4_0.ext_tconv1_bnorm.weight', 'upsample4_0.ext_tconv1_bnorm.bias', 'upsample4_0.ext_tconv1_bnorm.running_mean', 'upsample4_0.ext_tconv1_bnorm.running_var', 'upsample4_0.ext_tconv1_bnorm.num_batches_tracked', 'upsample4_0.ext_conv2.0.weight', 'upsample4_0.ext_conv2.1.weight', 'upsample4_0.ext_conv2.1.bias', 'upsample4_0.ext_conv2.1.running_mean', 'upsample4_0.ext_conv2.1.running_var', 'upsample4_0.ext_conv2.1.num_batches_tracked', 'regular4_1.ext_conv1.0.weight', 'regular4_1.ext_conv1.1.weight', 'regular4_1.ext_conv1.1.bias', 'regular4_1.ext_conv1.1.running_mean', 'regular4_1.ext_conv1.1.running_var', 'regular4_1.ext_conv1.1.num_batches_tracked', 'regular4_1.ext_conv2.0.weight', 'regular4_1.ext_conv2.1.weight', 'regular4_1.ext_conv2.1.bias', 'regular4_1.ext_conv2.1.running_mean', 'regular4_1.ext_conv2.1.running_var', 'regular4_1.ext_conv2.1.num_batches_tracked', 'regular4_1.ext_conv3.0.weight', 'regular4_1.ext_conv3.1.weight', 'regular4_1.ext_conv3.1.bias', 'regular4_1.ext_conv3.1.running_mean', 'regular4_1.ext_conv3.1.running_var', 'regular4_1.ext_conv3.1.num_batches_tracked', 'regular4_2.ext_conv1.0.weight', 'regular4_2.ext_conv1.1.weight', 'regular4_2.ext_conv1.1.bias', 'regular4_2.ext_conv1.1.running_mean', 'regular4_2.ext_conv1.1.running_var', 'regular4_2.ext_conv1.1.num_batches_tracked', 'regular4_2.ext_conv2.0.weight', 'regular4_2.ext_conv2.1.weight', 'regular4_2.ext_conv2.1.bias', 'regular4_2.ext_conv2.1.running_mean', 'regular4_2.ext_conv2.1.running_var', 'regular4_2.ext_conv2.1.num_batches_tracked', 'regular4_2.ext_conv3.0.weight', 'regular4_2.ext_conv3.1.weight', 'regular4_2.ext_conv3.1.bias', 'regular4_2.ext_conv3.1.running_mean', 'regular4_2.ext_conv3.1.running_var', 'regular4_2.ext_conv3.1.num_batches_tracked', 'upsample5_0.main_conv1.0.weight', 'upsample5_0.main_conv1.1.weight', 'upsample5_0.main_conv1.1.bias', 'upsample5_0.main_conv1.1.running_mean', 'upsample5_0.main_conv1.1.running_var', 'upsample5_0.main_conv1.1.num_batches_tracked', 'upsample5_0.ext_conv1.0.weight', 'upsample5_0.ext_conv1.1.weight', 'upsample5_0.ext_conv1.1.bias', 'upsample5_0.ext_conv1.1.running_mean', 'upsample5_0.ext_conv1.1.running_var', 'upsample5_0.ext_conv1.1.num_batches_tracked', 'upsample5_0.ext_tconv1.weight', 'upsample5_0.ext_tconv1_bnorm.weight', 'upsample5_0.ext_tconv1_bnorm.bias', 'upsample5_0.ext_tconv1_bnorm.running_mean', 'upsample5_0.ext_tconv1_bnorm.running_var', 'upsample5_0.ext_tconv1_bnorm.num_batches_tracked', 'upsample5_0.ext_conv2.0.weight', 'upsample5_0.ext_conv2.1.weight', 'upsample5_0.ext_conv2.1.bias', 'upsample5_0.ext_conv2.1.running_mean', 'upsample5_0.ext_conv2.1.running_var', 'upsample5_0.ext_conv2.1.num_batches_tracked', 'regular5_1.ext_conv1.0.weight', 'regular5_1.ext_conv1.1.weight', 'regular5_1.ext_conv1.1.bias', 'regular5_1.ext_conv1.1.running_mean', 'regular5_1.ext_conv1.1.running_var', 'regular5_1.ext_conv1.1.num_batches_tracked', 'regular5_1.ext_conv2.0.weight', 'regular5_1.ext_conv2.1.weight', 'regular5_1.ext_conv2.1.bias', 'regular5_1.ext_conv2.1.running_mean', 'regular5_1.ext_conv2.1.running_var', 'regular5_1.ext_conv2.1.num_batches_tracked', 'regular5_1.ext_conv3.0.weight', 'regular5_1.ext_conv3.1.weight', 'regular5_1.ext_conv3.1.bias', 'regular5_1.ext_conv3.1.running_mean', 'regular5_1.ext_conv3.1.running_var', 'regular5_1.ext_conv3.1.num_batches_tracked', 'transposed_conv.weight'])\n",
            "odict_keys(['initial_block.main_branch.weight', 'initial_block.batch_norm.weight', 'initial_block.batch_norm.bias', 'initial_block.batch_norm.running_mean', 'initial_block.batch_norm.running_var', 'initial_block.batch_norm.num_batches_tracked', 'initial_block.out_activation.weight', 'downsample1_0.ext_conv1.0.weight', 'downsample1_0.ext_conv1.1.weight', 'downsample1_0.ext_conv1.1.bias', 'downsample1_0.ext_conv1.1.running_mean', 'downsample1_0.ext_conv1.1.running_var', 'downsample1_0.ext_conv1.1.num_batches_tracked', 'downsample1_0.ext_conv1.2.weight', 'downsample1_0.ext_conv2.0.weight', 'downsample1_0.ext_conv2.1.weight', 'downsample1_0.ext_conv2.1.bias', 'downsample1_0.ext_conv2.1.running_mean', 'downsample1_0.ext_conv2.1.running_var', 'downsample1_0.ext_conv2.1.num_batches_tracked', 'downsample1_0.ext_conv2.2.weight', 'downsample1_0.ext_conv3.0.weight', 'downsample1_0.ext_conv3.1.weight', 'downsample1_0.ext_conv3.1.bias', 'downsample1_0.ext_conv3.1.running_mean', 'downsample1_0.ext_conv3.1.running_var', 'downsample1_0.ext_conv3.1.num_batches_tracked', 'downsample1_0.ext_conv3.2.weight', 'downsample1_0.out_activation.weight', 'regular1_1.ext_conv1.0.weight', 'regular1_1.ext_conv1.1.weight', 'regular1_1.ext_conv1.1.bias', 'regular1_1.ext_conv1.1.running_mean', 'regular1_1.ext_conv1.1.running_var', 'regular1_1.ext_conv1.1.num_batches_tracked', 'regular1_1.ext_conv1.2.weight', 'regular1_1.ext_conv2.0.weight', 'regular1_1.ext_conv2.1.weight', 'regular1_1.ext_conv2.1.bias', 'regular1_1.ext_conv2.1.running_mean', 'regular1_1.ext_conv2.1.running_var', 'regular1_1.ext_conv2.1.num_batches_tracked', 'regular1_1.ext_conv2.2.weight', 'regular1_1.ext_conv3.0.weight', 'regular1_1.ext_conv3.1.weight', 'regular1_1.ext_conv3.1.bias', 'regular1_1.ext_conv3.1.running_mean', 'regular1_1.ext_conv3.1.running_var', 'regular1_1.ext_conv3.1.num_batches_tracked', 'regular1_1.ext_conv3.2.weight', 'regular1_1.out_activation.weight', 'regular1_2.ext_conv1.0.weight', 'regular1_2.ext_conv1.1.weight', 'regular1_2.ext_conv1.1.bias', 'regular1_2.ext_conv1.1.running_mean', 'regular1_2.ext_conv1.1.running_var', 'regular1_2.ext_conv1.1.num_batches_tracked', 'regular1_2.ext_conv1.2.weight', 'regular1_2.ext_conv2.0.weight', 'regular1_2.ext_conv2.1.weight', 'regular1_2.ext_conv2.1.bias', 'regular1_2.ext_conv2.1.running_mean', 'regular1_2.ext_conv2.1.running_var', 'regular1_2.ext_conv2.1.num_batches_tracked', 'regular1_2.ext_conv2.2.weight', 'regular1_2.ext_conv3.0.weight', 'regular1_2.ext_conv3.1.weight', 'regular1_2.ext_conv3.1.bias', 'regular1_2.ext_conv3.1.running_mean', 'regular1_2.ext_conv3.1.running_var', 'regular1_2.ext_conv3.1.num_batches_tracked', 'regular1_2.ext_conv3.2.weight', 'regular1_2.out_activation.weight', 'regular1_3.ext_conv1.0.weight', 'regular1_3.ext_conv1.1.weight', 'regular1_3.ext_conv1.1.bias', 'regular1_3.ext_conv1.1.running_mean', 'regular1_3.ext_conv1.1.running_var', 'regular1_3.ext_conv1.1.num_batches_tracked', 'regular1_3.ext_conv1.2.weight', 'regular1_3.ext_conv2.0.weight', 'regular1_3.ext_conv2.1.weight', 'regular1_3.ext_conv2.1.bias', 'regular1_3.ext_conv2.1.running_mean', 'regular1_3.ext_conv2.1.running_var', 'regular1_3.ext_conv2.1.num_batches_tracked', 'regular1_3.ext_conv2.2.weight', 'regular1_3.ext_conv3.0.weight', 'regular1_3.ext_conv3.1.weight', 'regular1_3.ext_conv3.1.bias', 'regular1_3.ext_conv3.1.running_mean', 'regular1_3.ext_conv3.1.running_var', 'regular1_3.ext_conv3.1.num_batches_tracked', 'regular1_3.ext_conv3.2.weight', 'regular1_3.out_activation.weight', 'regular1_4.ext_conv1.0.weight', 'regular1_4.ext_conv1.1.weight', 'regular1_4.ext_conv1.1.bias', 'regular1_4.ext_conv1.1.running_mean', 'regular1_4.ext_conv1.1.running_var', 'regular1_4.ext_conv1.1.num_batches_tracked', 'regular1_4.ext_conv1.2.weight', 'regular1_4.ext_conv2.0.weight', 'regular1_4.ext_conv2.1.weight', 'regular1_4.ext_conv2.1.bias', 'regular1_4.ext_conv2.1.running_mean', 'regular1_4.ext_conv2.1.running_var', 'regular1_4.ext_conv2.1.num_batches_tracked', 'regular1_4.ext_conv2.2.weight', 'regular1_4.ext_conv3.0.weight', 'regular1_4.ext_conv3.1.weight', 'regular1_4.ext_conv3.1.bias', 'regular1_4.ext_conv3.1.running_mean', 'regular1_4.ext_conv3.1.running_var', 'regular1_4.ext_conv3.1.num_batches_tracked', 'regular1_4.ext_conv3.2.weight', 'regular1_4.out_activation.weight', 'downsample2_0.ext_conv1.0.weight', 'downsample2_0.ext_conv1.1.weight', 'downsample2_0.ext_conv1.1.bias', 'downsample2_0.ext_conv1.1.running_mean', 'downsample2_0.ext_conv1.1.running_var', 'downsample2_0.ext_conv1.1.num_batches_tracked', 'downsample2_0.ext_conv1.2.weight', 'downsample2_0.ext_conv2.0.weight', 'downsample2_0.ext_conv2.1.weight', 'downsample2_0.ext_conv2.1.bias', 'downsample2_0.ext_conv2.1.running_mean', 'downsample2_0.ext_conv2.1.running_var', 'downsample2_0.ext_conv2.1.num_batches_tracked', 'downsample2_0.ext_conv2.2.weight', 'downsample2_0.ext_conv3.0.weight', 'downsample2_0.ext_conv3.1.weight', 'downsample2_0.ext_conv3.1.bias', 'downsample2_0.ext_conv3.1.running_mean', 'downsample2_0.ext_conv3.1.running_var', 'downsample2_0.ext_conv3.1.num_batches_tracked', 'downsample2_0.ext_conv3.2.weight', 'downsample2_0.out_activation.weight', 'regular2_1.ext_conv1.0.weight', 'regular2_1.ext_conv1.1.weight', 'regular2_1.ext_conv1.1.bias', 'regular2_1.ext_conv1.1.running_mean', 'regular2_1.ext_conv1.1.running_var', 'regular2_1.ext_conv1.1.num_batches_tracked', 'regular2_1.ext_conv1.2.weight', 'regular2_1.ext_conv2.0.weight', 'regular2_1.ext_conv2.1.weight', 'regular2_1.ext_conv2.1.bias', 'regular2_1.ext_conv2.1.running_mean', 'regular2_1.ext_conv2.1.running_var', 'regular2_1.ext_conv2.1.num_batches_tracked', 'regular2_1.ext_conv2.2.weight', 'regular2_1.ext_conv3.0.weight', 'regular2_1.ext_conv3.1.weight', 'regular2_1.ext_conv3.1.bias', 'regular2_1.ext_conv3.1.running_mean', 'regular2_1.ext_conv3.1.running_var', 'regular2_1.ext_conv3.1.num_batches_tracked', 'regular2_1.ext_conv3.2.weight', 'regular2_1.out_activation.weight', 'dilated2_2.ext_conv1.0.weight', 'dilated2_2.ext_conv1.1.weight', 'dilated2_2.ext_conv1.1.bias', 'dilated2_2.ext_conv1.1.running_mean', 'dilated2_2.ext_conv1.1.running_var', 'dilated2_2.ext_conv1.1.num_batches_tracked', 'dilated2_2.ext_conv1.2.weight', 'dilated2_2.ext_conv2.0.weight', 'dilated2_2.ext_conv2.1.weight', 'dilated2_2.ext_conv2.1.bias', 'dilated2_2.ext_conv2.1.running_mean', 'dilated2_2.ext_conv2.1.running_var', 'dilated2_2.ext_conv2.1.num_batches_tracked', 'dilated2_2.ext_conv2.2.weight', 'dilated2_2.ext_conv3.0.weight', 'dilated2_2.ext_conv3.1.weight', 'dilated2_2.ext_conv3.1.bias', 'dilated2_2.ext_conv3.1.running_mean', 'dilated2_2.ext_conv3.1.running_var', 'dilated2_2.ext_conv3.1.num_batches_tracked', 'dilated2_2.ext_conv3.2.weight', 'dilated2_2.out_activation.weight', 'asymmetric2_3.ext_conv1.0.weight', 'asymmetric2_3.ext_conv1.1.weight', 'asymmetric2_3.ext_conv1.1.bias', 'asymmetric2_3.ext_conv1.1.running_mean', 'asymmetric2_3.ext_conv1.1.running_var', 'asymmetric2_3.ext_conv1.1.num_batches_tracked', 'asymmetric2_3.ext_conv1.2.weight', 'asymmetric2_3.ext_conv2.0.weight', 'asymmetric2_3.ext_conv2.1.weight', 'asymmetric2_3.ext_conv2.1.bias', 'asymmetric2_3.ext_conv2.1.running_mean', 'asymmetric2_3.ext_conv2.1.running_var', 'asymmetric2_3.ext_conv2.1.num_batches_tracked', 'asymmetric2_3.ext_conv2.2.weight', 'asymmetric2_3.ext_conv2.3.weight', 'asymmetric2_3.ext_conv2.4.weight', 'asymmetric2_3.ext_conv2.4.bias', 'asymmetric2_3.ext_conv2.4.running_mean', 'asymmetric2_3.ext_conv2.4.running_var', 'asymmetric2_3.ext_conv2.4.num_batches_tracked', 'asymmetric2_3.ext_conv2.5.weight', 'asymmetric2_3.ext_conv3.0.weight', 'asymmetric2_3.ext_conv3.1.weight', 'asymmetric2_3.ext_conv3.1.bias', 'asymmetric2_3.ext_conv3.1.running_mean', 'asymmetric2_3.ext_conv3.1.running_var', 'asymmetric2_3.ext_conv3.1.num_batches_tracked', 'asymmetric2_3.ext_conv3.2.weight', 'asymmetric2_3.out_activation.weight', 'dilated2_4.ext_conv1.0.weight', 'dilated2_4.ext_conv1.1.weight', 'dilated2_4.ext_conv1.1.bias', 'dilated2_4.ext_conv1.1.running_mean', 'dilated2_4.ext_conv1.1.running_var', 'dilated2_4.ext_conv1.1.num_batches_tracked', 'dilated2_4.ext_conv1.2.weight', 'dilated2_4.ext_conv2.0.weight', 'dilated2_4.ext_conv2.1.weight', 'dilated2_4.ext_conv2.1.bias', 'dilated2_4.ext_conv2.1.running_mean', 'dilated2_4.ext_conv2.1.running_var', 'dilated2_4.ext_conv2.1.num_batches_tracked', 'dilated2_4.ext_conv2.2.weight', 'dilated2_4.ext_conv3.0.weight', 'dilated2_4.ext_conv3.1.weight', 'dilated2_4.ext_conv3.1.bias', 'dilated2_4.ext_conv3.1.running_mean', 'dilated2_4.ext_conv3.1.running_var', 'dilated2_4.ext_conv3.1.num_batches_tracked', 'dilated2_4.ext_conv3.2.weight', 'dilated2_4.out_activation.weight', 'regular2_5.ext_conv1.0.weight', 'regular2_5.ext_conv1.1.weight', 'regular2_5.ext_conv1.1.bias', 'regular2_5.ext_conv1.1.running_mean', 'regular2_5.ext_conv1.1.running_var', 'regular2_5.ext_conv1.1.num_batches_tracked', 'regular2_5.ext_conv1.2.weight', 'regular2_5.ext_conv2.0.weight', 'regular2_5.ext_conv2.1.weight', 'regular2_5.ext_conv2.1.bias', 'regular2_5.ext_conv2.1.running_mean', 'regular2_5.ext_conv2.1.running_var', 'regular2_5.ext_conv2.1.num_batches_tracked', 'regular2_5.ext_conv2.2.weight', 'regular2_5.ext_conv3.0.weight', 'regular2_5.ext_conv3.1.weight', 'regular2_5.ext_conv3.1.bias', 'regular2_5.ext_conv3.1.running_mean', 'regular2_5.ext_conv3.1.running_var', 'regular2_5.ext_conv3.1.num_batches_tracked', 'regular2_5.ext_conv3.2.weight', 'regular2_5.out_activation.weight', 'dilated2_6.ext_conv1.0.weight', 'dilated2_6.ext_conv1.1.weight', 'dilated2_6.ext_conv1.1.bias', 'dilated2_6.ext_conv1.1.running_mean', 'dilated2_6.ext_conv1.1.running_var', 'dilated2_6.ext_conv1.1.num_batches_tracked', 'dilated2_6.ext_conv1.2.weight', 'dilated2_6.ext_conv2.0.weight', 'dilated2_6.ext_conv2.1.weight', 'dilated2_6.ext_conv2.1.bias', 'dilated2_6.ext_conv2.1.running_mean', 'dilated2_6.ext_conv2.1.running_var', 'dilated2_6.ext_conv2.1.num_batches_tracked', 'dilated2_6.ext_conv2.2.weight', 'dilated2_6.ext_conv3.0.weight', 'dilated2_6.ext_conv3.1.weight', 'dilated2_6.ext_conv3.1.bias', 'dilated2_6.ext_conv3.1.running_mean', 'dilated2_6.ext_conv3.1.running_var', 'dilated2_6.ext_conv3.1.num_batches_tracked', 'dilated2_6.ext_conv3.2.weight', 'dilated2_6.out_activation.weight', 'asymmetric2_7.ext_conv1.0.weight', 'asymmetric2_7.ext_conv1.1.weight', 'asymmetric2_7.ext_conv1.1.bias', 'asymmetric2_7.ext_conv1.1.running_mean', 'asymmetric2_7.ext_conv1.1.running_var', 'asymmetric2_7.ext_conv1.1.num_batches_tracked', 'asymmetric2_7.ext_conv1.2.weight', 'asymmetric2_7.ext_conv2.0.weight', 'asymmetric2_7.ext_conv2.1.weight', 'asymmetric2_7.ext_conv2.1.bias', 'asymmetric2_7.ext_conv2.1.running_mean', 'asymmetric2_7.ext_conv2.1.running_var', 'asymmetric2_7.ext_conv2.1.num_batches_tracked', 'asymmetric2_7.ext_conv2.2.weight', 'asymmetric2_7.ext_conv2.3.weight', 'asymmetric2_7.ext_conv2.4.weight', 'asymmetric2_7.ext_conv2.4.bias', 'asymmetric2_7.ext_conv2.4.running_mean', 'asymmetric2_7.ext_conv2.4.running_var', 'asymmetric2_7.ext_conv2.4.num_batches_tracked', 'asymmetric2_7.ext_conv2.5.weight', 'asymmetric2_7.ext_conv3.0.weight', 'asymmetric2_7.ext_conv3.1.weight', 'asymmetric2_7.ext_conv3.1.bias', 'asymmetric2_7.ext_conv3.1.running_mean', 'asymmetric2_7.ext_conv3.1.running_var', 'asymmetric2_7.ext_conv3.1.num_batches_tracked', 'asymmetric2_7.ext_conv3.2.weight', 'asymmetric2_7.out_activation.weight', 'dilated2_8.ext_conv1.0.weight', 'dilated2_8.ext_conv1.1.weight', 'dilated2_8.ext_conv1.1.bias', 'dilated2_8.ext_conv1.1.running_mean', 'dilated2_8.ext_conv1.1.running_var', 'dilated2_8.ext_conv1.1.num_batches_tracked', 'dilated2_8.ext_conv1.2.weight', 'dilated2_8.ext_conv2.0.weight', 'dilated2_8.ext_conv2.1.weight', 'dilated2_8.ext_conv2.1.bias', 'dilated2_8.ext_conv2.1.running_mean', 'dilated2_8.ext_conv2.1.running_var', 'dilated2_8.ext_conv2.1.num_batches_tracked', 'dilated2_8.ext_conv2.2.weight', 'dilated2_8.ext_conv3.0.weight', 'dilated2_8.ext_conv3.1.weight', 'dilated2_8.ext_conv3.1.bias', 'dilated2_8.ext_conv3.1.running_mean', 'dilated2_8.ext_conv3.1.running_var', 'dilated2_8.ext_conv3.1.num_batches_tracked', 'dilated2_8.ext_conv3.2.weight', 'dilated2_8.out_activation.weight', 'regular3_0.ext_conv1.0.weight', 'regular3_0.ext_conv1.1.weight', 'regular3_0.ext_conv1.1.bias', 'regular3_0.ext_conv1.1.running_mean', 'regular3_0.ext_conv1.1.running_var', 'regular3_0.ext_conv1.1.num_batches_tracked', 'regular3_0.ext_conv1.2.weight', 'regular3_0.ext_conv2.0.weight', 'regular3_0.ext_conv2.1.weight', 'regular3_0.ext_conv2.1.bias', 'regular3_0.ext_conv2.1.running_mean', 'regular3_0.ext_conv2.1.running_var', 'regular3_0.ext_conv2.1.num_batches_tracked', 'regular3_0.ext_conv2.2.weight', 'regular3_0.ext_conv3.0.weight', 'regular3_0.ext_conv3.1.weight', 'regular3_0.ext_conv3.1.bias', 'regular3_0.ext_conv3.1.running_mean', 'regular3_0.ext_conv3.1.running_var', 'regular3_0.ext_conv3.1.num_batches_tracked', 'regular3_0.ext_conv3.2.weight', 'regular3_0.out_activation.weight', 'dilated3_1.ext_conv1.0.weight', 'dilated3_1.ext_conv1.1.weight', 'dilated3_1.ext_conv1.1.bias', 'dilated3_1.ext_conv1.1.running_mean', 'dilated3_1.ext_conv1.1.running_var', 'dilated3_1.ext_conv1.1.num_batches_tracked', 'dilated3_1.ext_conv1.2.weight', 'dilated3_1.ext_conv2.0.weight', 'dilated3_1.ext_conv2.1.weight', 'dilated3_1.ext_conv2.1.bias', 'dilated3_1.ext_conv2.1.running_mean', 'dilated3_1.ext_conv2.1.running_var', 'dilated3_1.ext_conv2.1.num_batches_tracked', 'dilated3_1.ext_conv2.2.weight', 'dilated3_1.ext_conv3.0.weight', 'dilated3_1.ext_conv3.1.weight', 'dilated3_1.ext_conv3.1.bias', 'dilated3_1.ext_conv3.1.running_mean', 'dilated3_1.ext_conv3.1.running_var', 'dilated3_1.ext_conv3.1.num_batches_tracked', 'dilated3_1.ext_conv3.2.weight', 'dilated3_1.out_activation.weight', 'asymmetric3_2.ext_conv1.0.weight', 'asymmetric3_2.ext_conv1.1.weight', 'asymmetric3_2.ext_conv1.1.bias', 'asymmetric3_2.ext_conv1.1.running_mean', 'asymmetric3_2.ext_conv1.1.running_var', 'asymmetric3_2.ext_conv1.1.num_batches_tracked', 'asymmetric3_2.ext_conv1.2.weight', 'asymmetric3_2.ext_conv2.0.weight', 'asymmetric3_2.ext_conv2.1.weight', 'asymmetric3_2.ext_conv2.1.bias', 'asymmetric3_2.ext_conv2.1.running_mean', 'asymmetric3_2.ext_conv2.1.running_var', 'asymmetric3_2.ext_conv2.1.num_batches_tracked', 'asymmetric3_2.ext_conv2.2.weight', 'asymmetric3_2.ext_conv2.3.weight', 'asymmetric3_2.ext_conv2.4.weight', 'asymmetric3_2.ext_conv2.4.bias', 'asymmetric3_2.ext_conv2.4.running_mean', 'asymmetric3_2.ext_conv2.4.running_var', 'asymmetric3_2.ext_conv2.4.num_batches_tracked', 'asymmetric3_2.ext_conv2.5.weight', 'asymmetric3_2.ext_conv3.0.weight', 'asymmetric3_2.ext_conv3.1.weight', 'asymmetric3_2.ext_conv3.1.bias', 'asymmetric3_2.ext_conv3.1.running_mean', 'asymmetric3_2.ext_conv3.1.running_var', 'asymmetric3_2.ext_conv3.1.num_batches_tracked', 'asymmetric3_2.ext_conv3.2.weight', 'asymmetric3_2.out_activation.weight', 'dilated3_3.ext_conv1.0.weight', 'dilated3_3.ext_conv1.1.weight', 'dilated3_3.ext_conv1.1.bias', 'dilated3_3.ext_conv1.1.running_mean', 'dilated3_3.ext_conv1.1.running_var', 'dilated3_3.ext_conv1.1.num_batches_tracked', 'dilated3_3.ext_conv1.2.weight', 'dilated3_3.ext_conv2.0.weight', 'dilated3_3.ext_conv2.1.weight', 'dilated3_3.ext_conv2.1.bias', 'dilated3_3.ext_conv2.1.running_mean', 'dilated3_3.ext_conv2.1.running_var', 'dilated3_3.ext_conv2.1.num_batches_tracked', 'dilated3_3.ext_conv2.2.weight', 'dilated3_3.ext_conv3.0.weight', 'dilated3_3.ext_conv3.1.weight', 'dilated3_3.ext_conv3.1.bias', 'dilated3_3.ext_conv3.1.running_mean', 'dilated3_3.ext_conv3.1.running_var', 'dilated3_3.ext_conv3.1.num_batches_tracked', 'dilated3_3.ext_conv3.2.weight', 'dilated3_3.out_activation.weight', 'regular3_4.ext_conv1.0.weight', 'regular3_4.ext_conv1.1.weight', 'regular3_4.ext_conv1.1.bias', 'regular3_4.ext_conv1.1.running_mean', 'regular3_4.ext_conv1.1.running_var', 'regular3_4.ext_conv1.1.num_batches_tracked', 'regular3_4.ext_conv1.2.weight', 'regular3_4.ext_conv2.0.weight', 'regular3_4.ext_conv2.1.weight', 'regular3_4.ext_conv2.1.bias', 'regular3_4.ext_conv2.1.running_mean', 'regular3_4.ext_conv2.1.running_var', 'regular3_4.ext_conv2.1.num_batches_tracked', 'regular3_4.ext_conv2.2.weight', 'regular3_4.ext_conv3.0.weight', 'regular3_4.ext_conv3.1.weight', 'regular3_4.ext_conv3.1.bias', 'regular3_4.ext_conv3.1.running_mean', 'regular3_4.ext_conv3.1.running_var', 'regular3_4.ext_conv3.1.num_batches_tracked', 'regular3_4.ext_conv3.2.weight', 'regular3_4.out_activation.weight', 'dilated3_5.ext_conv1.0.weight', 'dilated3_5.ext_conv1.1.weight', 'dilated3_5.ext_conv1.1.bias', 'dilated3_5.ext_conv1.1.running_mean', 'dilated3_5.ext_conv1.1.running_var', 'dilated3_5.ext_conv1.1.num_batches_tracked', 'dilated3_5.ext_conv1.2.weight', 'dilated3_5.ext_conv2.0.weight', 'dilated3_5.ext_conv2.1.weight', 'dilated3_5.ext_conv2.1.bias', 'dilated3_5.ext_conv2.1.running_mean', 'dilated3_5.ext_conv2.1.running_var', 'dilated3_5.ext_conv2.1.num_batches_tracked', 'dilated3_5.ext_conv2.2.weight', 'dilated3_5.ext_conv3.0.weight', 'dilated3_5.ext_conv3.1.weight', 'dilated3_5.ext_conv3.1.bias', 'dilated3_5.ext_conv3.1.running_mean', 'dilated3_5.ext_conv3.1.running_var', 'dilated3_5.ext_conv3.1.num_batches_tracked', 'dilated3_5.ext_conv3.2.weight', 'dilated3_5.out_activation.weight', 'asymmetric3_6.ext_conv1.0.weight', 'asymmetric3_6.ext_conv1.1.weight', 'asymmetric3_6.ext_conv1.1.bias', 'asymmetric3_6.ext_conv1.1.running_mean', 'asymmetric3_6.ext_conv1.1.running_var', 'asymmetric3_6.ext_conv1.1.num_batches_tracked', 'asymmetric3_6.ext_conv1.2.weight', 'asymmetric3_6.ext_conv2.0.weight', 'asymmetric3_6.ext_conv2.1.weight', 'asymmetric3_6.ext_conv2.1.bias', 'asymmetric3_6.ext_conv2.1.running_mean', 'asymmetric3_6.ext_conv2.1.running_var', 'asymmetric3_6.ext_conv2.1.num_batches_tracked', 'asymmetric3_6.ext_conv2.2.weight', 'asymmetric3_6.ext_conv2.3.weight', 'asymmetric3_6.ext_conv2.4.weight', 'asymmetric3_6.ext_conv2.4.bias', 'asymmetric3_6.ext_conv2.4.running_mean', 'asymmetric3_6.ext_conv2.4.running_var', 'asymmetric3_6.ext_conv2.4.num_batches_tracked', 'asymmetric3_6.ext_conv2.5.weight', 'asymmetric3_6.ext_conv3.0.weight', 'asymmetric3_6.ext_conv3.1.weight', 'asymmetric3_6.ext_conv3.1.bias', 'asymmetric3_6.ext_conv3.1.running_mean', 'asymmetric3_6.ext_conv3.1.running_var', 'asymmetric3_6.ext_conv3.1.num_batches_tracked', 'asymmetric3_6.ext_conv3.2.weight', 'asymmetric3_6.out_activation.weight', 'dilated3_7.ext_conv1.0.weight', 'dilated3_7.ext_conv1.1.weight', 'dilated3_7.ext_conv1.1.bias', 'dilated3_7.ext_conv1.1.running_mean', 'dilated3_7.ext_conv1.1.running_var', 'dilated3_7.ext_conv1.1.num_batches_tracked', 'dilated3_7.ext_conv1.2.weight', 'dilated3_7.ext_conv2.0.weight', 'dilated3_7.ext_conv2.1.weight', 'dilated3_7.ext_conv2.1.bias', 'dilated3_7.ext_conv2.1.running_mean', 'dilated3_7.ext_conv2.1.running_var', 'dilated3_7.ext_conv2.1.num_batches_tracked', 'dilated3_7.ext_conv2.2.weight', 'dilated3_7.ext_conv3.0.weight', 'dilated3_7.ext_conv3.1.weight', 'dilated3_7.ext_conv3.1.bias', 'dilated3_7.ext_conv3.1.running_mean', 'dilated3_7.ext_conv3.1.running_var', 'dilated3_7.ext_conv3.1.num_batches_tracked', 'dilated3_7.ext_conv3.2.weight', 'dilated3_7.out_activation.weight', 'upsample4_0.main_conv1.0.weight', 'upsample4_0.main_conv1.1.weight', 'upsample4_0.main_conv1.1.bias', 'upsample4_0.main_conv1.1.running_mean', 'upsample4_0.main_conv1.1.running_var', 'upsample4_0.main_conv1.1.num_batches_tracked', 'upsample4_0.ext_conv1.0.weight', 'upsample4_0.ext_conv1.1.weight', 'upsample4_0.ext_conv1.1.bias', 'upsample4_0.ext_conv1.1.running_mean', 'upsample4_0.ext_conv1.1.running_var', 'upsample4_0.ext_conv1.1.num_batches_tracked', 'upsample4_0.ext_tconv1.weight', 'upsample4_0.ext_tconv1_bnorm.weight', 'upsample4_0.ext_tconv1_bnorm.bias', 'upsample4_0.ext_tconv1_bnorm.running_mean', 'upsample4_0.ext_tconv1_bnorm.running_var', 'upsample4_0.ext_tconv1_bnorm.num_batches_tracked', 'upsample4_0.ext_conv2.0.weight', 'upsample4_0.ext_conv2.1.weight', 'upsample4_0.ext_conv2.1.bias', 'upsample4_0.ext_conv2.1.running_mean', 'upsample4_0.ext_conv2.1.running_var', 'upsample4_0.ext_conv2.1.num_batches_tracked', 'regular4_1.ext_conv1.0.weight', 'regular4_1.ext_conv1.1.weight', 'regular4_1.ext_conv1.1.bias', 'regular4_1.ext_conv1.1.running_mean', 'regular4_1.ext_conv1.1.running_var', 'regular4_1.ext_conv1.1.num_batches_tracked', 'regular4_1.ext_conv2.0.weight', 'regular4_1.ext_conv2.1.weight', 'regular4_1.ext_conv2.1.bias', 'regular4_1.ext_conv2.1.running_mean', 'regular4_1.ext_conv2.1.running_var', 'regular4_1.ext_conv2.1.num_batches_tracked', 'regular4_1.ext_conv3.0.weight', 'regular4_1.ext_conv3.1.weight', 'regular4_1.ext_conv3.1.bias', 'regular4_1.ext_conv3.1.running_mean', 'regular4_1.ext_conv3.1.running_var', 'regular4_1.ext_conv3.1.num_batches_tracked', 'regular4_2.ext_conv1.0.weight', 'regular4_2.ext_conv1.1.weight', 'regular4_2.ext_conv1.1.bias', 'regular4_2.ext_conv1.1.running_mean', 'regular4_2.ext_conv1.1.running_var', 'regular4_2.ext_conv1.1.num_batches_tracked', 'regular4_2.ext_conv2.0.weight', 'regular4_2.ext_conv2.1.weight', 'regular4_2.ext_conv2.1.bias', 'regular4_2.ext_conv2.1.running_mean', 'regular4_2.ext_conv2.1.running_var', 'regular4_2.ext_conv2.1.num_batches_tracked', 'regular4_2.ext_conv3.0.weight', 'regular4_2.ext_conv3.1.weight', 'regular4_2.ext_conv3.1.bias', 'regular4_2.ext_conv3.1.running_mean', 'regular4_2.ext_conv3.1.running_var', 'regular4_2.ext_conv3.1.num_batches_tracked', 'upsample5_0.main_conv1.0.weight', 'upsample5_0.main_conv1.1.weight', 'upsample5_0.main_conv1.1.bias', 'upsample5_0.main_conv1.1.running_mean', 'upsample5_0.main_conv1.1.running_var', 'upsample5_0.main_conv1.1.num_batches_tracked', 'upsample5_0.ext_conv1.0.weight', 'upsample5_0.ext_conv1.1.weight', 'upsample5_0.ext_conv1.1.bias', 'upsample5_0.ext_conv1.1.running_mean', 'upsample5_0.ext_conv1.1.running_var', 'upsample5_0.ext_conv1.1.num_batches_tracked', 'upsample5_0.ext_tconv1.weight', 'upsample5_0.ext_tconv1_bnorm.weight', 'upsample5_0.ext_tconv1_bnorm.bias', 'upsample5_0.ext_tconv1_bnorm.running_mean', 'upsample5_0.ext_tconv1_bnorm.running_var', 'upsample5_0.ext_tconv1_bnorm.num_batches_tracked', 'upsample5_0.ext_conv2.0.weight', 'upsample5_0.ext_conv2.1.weight', 'upsample5_0.ext_conv2.1.bias', 'upsample5_0.ext_conv2.1.running_mean', 'upsample5_0.ext_conv2.1.running_var', 'upsample5_0.ext_conv2.1.num_batches_tracked', 'regular5_1.ext_conv1.0.weight', 'regular5_1.ext_conv1.1.weight', 'regular5_1.ext_conv1.1.bias', 'regular5_1.ext_conv1.1.running_mean', 'regular5_1.ext_conv1.1.running_var', 'regular5_1.ext_conv1.1.num_batches_tracked', 'regular5_1.ext_conv2.0.weight', 'regular5_1.ext_conv2.1.weight', 'regular5_1.ext_conv2.1.bias', 'regular5_1.ext_conv2.1.running_mean', 'regular5_1.ext_conv2.1.running_var', 'regular5_1.ext_conv2.1.num_batches_tracked', 'regular5_1.ext_conv3.0.weight', 'regular5_1.ext_conv3.1.weight', 'regular5_1.ext_conv3.1.bias', 'regular5_1.ext_conv3.1.running_mean', 'regular5_1.ext_conv3.1.running_var', 'regular5_1.ext_conv3.1.num_batches_tracked', 'transposed_conv.weight'])\n",
            "Import Model enet with weights enet_pretrained.pth to FineTune\n",
            "========== TRAINING ===========\n",
            "/content/cityscapes/leftImg8bit/train\n",
            "/content/cityscapes/leftImg8bit/val\n",
            "----- TRAINING - EPOCH 1 -----\n",
            "LEARNING RATE:  5e-05\n",
            "loss: 9.39 (epoch: 1, step: 0) // Avg time/img: 0.4163 s\n",
            "loss: 10.87 (epoch: 1, step: 50) // Avg time/img: 0.0617 s\n",
            "loss: 10.75 (epoch: 1, step: 100) // Avg time/img: 0.0585 s\n",
            "loss: 10.6 (epoch: 1, step: 150) // Avg time/img: 0.0566 s\n",
            "loss: 10.51 (epoch: 1, step: 200) // Avg time/img: 0.0565 s\n",
            "loss: 10.37 (epoch: 1, step: 250) // Avg time/img: 0.0558 s\n",
            "loss: 10.25 (epoch: 1, step: 300) // Avg time/img: 0.0559 s\n",
            "loss: 10.16 (epoch: 1, step: 350) // Avg time/img: 0.0554 s\n",
            "loss: 10.05 (epoch: 1, step: 400) // Avg time/img: 0.0553 s\n",
            "loss: 9.959 (epoch: 1, step: 450) // Avg time/img: 0.0550 s\n",
            "----- VALIDATING - EPOCH 1 -----\n",
            "VAL loss: 8.166 (epoch: 1, step: 0) // Avg time/img: 0.0505 s\n",
            "VAL loss: 8.062 (epoch: 1, step: 50) // Avg time/img: 0.0330 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m0.73\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training_void/model-001.pth (epoch: 1)\n",
            "save: ../save/enet_training_void/model_best.pth (epoch: 1)\n",
            "----- TRAINING - EPOCH 2 -----\n",
            "LEARNING RATE:  5e-05\n",
            "loss: 7.616 (epoch: 2, step: 0) // Avg time/img: 0.0522 s\n",
            "loss: 8.678 (epoch: 2, step: 50) // Avg time/img: 0.0548 s\n",
            "loss: 8.574 (epoch: 2, step: 100) // Avg time/img: 0.0538 s\n",
            "loss: 8.428 (epoch: 2, step: 150) // Avg time/img: 0.0532 s\n",
            "loss: 8.277 (epoch: 2, step: 200) // Avg time/img: 0.0536 s\n",
            "loss: 8.192 (epoch: 2, step: 250) // Avg time/img: 0.0538 s\n",
            "loss: 8.078 (epoch: 2, step: 300) // Avg time/img: 0.0540 s\n",
            "loss: 7.964 (epoch: 2, step: 350) // Avg time/img: 0.0543 s\n",
            "loss: 7.842 (epoch: 2, step: 400) // Avg time/img: 0.0542 s\n",
            "loss: 7.74 (epoch: 2, step: 450) // Avg time/img: 0.0545 s\n",
            "----- VALIDATING - EPOCH 2 -----\n",
            "VAL loss: 6.201 (epoch: 2, step: 0) // Avg time/img: 0.0523 s\n",
            "VAL loss: 6.125 (epoch: 2, step: 50) // Avg time/img: 0.0340 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m0.74\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training_void/model-002.pth (epoch: 2)\n",
            "save: ../save/enet_training_void/model_best.pth (epoch: 2)\n",
            "----- TRAINING - EPOCH 3 -----\n",
            "LEARNING RATE:  5e-05\n",
            "loss: 6.903 (epoch: 3, step: 0) // Avg time/img: 0.0579 s\n",
            "loss: 6.435 (epoch: 3, step: 50) // Avg time/img: 0.0429 s\n",
            "loss: 6.309 (epoch: 3, step: 100) // Avg time/img: 0.0431 s\n",
            "loss: 6.228 (epoch: 3, step: 150) // Avg time/img: 0.0428 s\n",
            "loss: 6.123 (epoch: 3, step: 200) // Avg time/img: 0.0428 s\n",
            "loss: 6.012 (epoch: 3, step: 250) // Avg time/img: 0.0427 s\n",
            "loss: 5.928 (epoch: 3, step: 300) // Avg time/img: 0.0425 s\n",
            "loss: 5.837 (epoch: 3, step: 350) // Avg time/img: 0.0425 s\n",
            "loss: 5.743 (epoch: 3, step: 400) // Avg time/img: 0.0428 s\n",
            "loss: 5.654 (epoch: 3, step: 450) // Avg time/img: 0.0427 s\n",
            "----- VALIDATING - EPOCH 3 -----\n",
            "VAL loss: 4.445 (epoch: 3, step: 0) // Avg time/img: 0.0496 s\n",
            "VAL loss: 4.414 (epoch: 3, step: 50) // Avg time/img: 0.0326 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m1.34\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training_void/model-003.pth (epoch: 3)\n",
            "save: ../save/enet_training_void/model_best.pth (epoch: 3)\n",
            "----- TRAINING - EPOCH 4 -----\n",
            "LEARNING RATE:  5e-05\n",
            "loss: 4.65 (epoch: 4, step: 0) // Avg time/img: 0.0544 s\n",
            "loss: 4.588 (epoch: 4, step: 50) // Avg time/img: 0.0556 s\n",
            "loss: 4.535 (epoch: 4, step: 100) // Avg time/img: 0.0550 s\n",
            "loss: 4.513 (epoch: 4, step: 150) // Avg time/img: 0.0546 s\n",
            "loss: 4.437 (epoch: 4, step: 200) // Avg time/img: 0.0547 s\n",
            "loss: 4.361 (epoch: 4, step: 250) // Avg time/img: 0.0546 s\n",
            "loss: 4.299 (epoch: 4, step: 300) // Avg time/img: 0.0544 s\n",
            "loss: 4.227 (epoch: 4, step: 350) // Avg time/img: 0.0545 s\n",
            "loss: 4.162 (epoch: 4, step: 400) // Avg time/img: 0.0544 s\n",
            "loss: 4.107 (epoch: 4, step: 450) // Avg time/img: 0.0544 s\n",
            "----- VALIDATING - EPOCH 4 -----\n",
            "VAL loss: 3.306 (epoch: 4, step: 0) // Avg time/img: 0.0576 s\n",
            "VAL loss: 3.291 (epoch: 4, step: 50) // Avg time/img: 0.0280 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m1.60\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training_void/model-004.pth (epoch: 4)\n",
            "save: ../save/enet_training_void/model_best.pth (epoch: 4)\n",
            "----- TRAINING - EPOCH 5 -----\n",
            "LEARNING RATE:  5e-05\n",
            "loss: 3.704 (epoch: 5, step: 0) // Avg time/img: 0.0549 s\n",
            "loss: 3.344 (epoch: 5, step: 50) // Avg time/img: 0.0525 s\n",
            "loss: 3.326 (epoch: 5, step: 100) // Avg time/img: 0.0527 s\n",
            "loss: 3.281 (epoch: 5, step: 150) // Avg time/img: 0.0532 s\n",
            "loss: 3.21 (epoch: 5, step: 200) // Avg time/img: 0.0531 s\n",
            "loss: 3.172 (epoch: 5, step: 250) // Avg time/img: 0.0536 s\n",
            "loss: 3.121 (epoch: 5, step: 300) // Avg time/img: 0.0535 s\n",
            "loss: 3.066 (epoch: 5, step: 350) // Avg time/img: 0.0535 s\n",
            "loss: 3.023 (epoch: 5, step: 400) // Avg time/img: 0.0536 s\n",
            "loss: 2.973 (epoch: 5, step: 450) // Avg time/img: 0.0537 s\n",
            "----- VALIDATING - EPOCH 5 -----\n",
            "VAL loss: 2.456 (epoch: 5, step: 0) // Avg time/img: 0.0522 s\n",
            "VAL loss: 2.45 (epoch: 5, step: 50) // Avg time/img: 0.0325 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m2.30\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training_void/model-005.pth (epoch: 5)\n",
            "save: ../save/enet_training_void/model_best.pth (epoch: 5)\n",
            "----- TRAINING - EPOCH 6 -----\n",
            "LEARNING RATE:  5e-05\n",
            "loss: 2.46 (epoch: 6, step: 0) // Avg time/img: 0.0861 s\n",
            "loss: 2.483 (epoch: 6, step: 50) // Avg time/img: 0.0559 s\n",
            "loss: 2.419 (epoch: 6, step: 100) // Avg time/img: 0.0549 s\n",
            "loss: 2.37 (epoch: 6, step: 150) // Avg time/img: 0.0551 s\n",
            "loss: 2.349 (epoch: 6, step: 200) // Avg time/img: 0.0554 s\n",
            "loss: 2.33 (epoch: 6, step: 250) // Avg time/img: 0.0552 s\n",
            "loss: 2.302 (epoch: 6, step: 300) // Avg time/img: 0.0555 s\n",
            "loss: 2.275 (epoch: 6, step: 350) // Avg time/img: 0.0551 s\n",
            "loss: 2.245 (epoch: 6, step: 400) // Avg time/img: 0.0551 s\n",
            "loss: 2.218 (epoch: 6, step: 450) // Avg time/img: 0.0551 s\n",
            "----- VALIDATING - EPOCH 6 -----\n",
            "VAL loss: 1.903 (epoch: 6, step: 0) // Avg time/img: 0.0355 s\n",
            "VAL loss: 1.911 (epoch: 6, step: 50) // Avg time/img: 0.0311 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m3.13\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training_void/model-006.pth (epoch: 6)\n",
            "save: ../save/enet_training_void/model_best.pth (epoch: 6)\n",
            "----- TRAINING - EPOCH 7 -----\n",
            "LEARNING RATE:  5e-06\n",
            "loss: 1.504 (epoch: 7, step: 0) // Avg time/img: 0.0626 s\n",
            "loss: 1.94 (epoch: 7, step: 50) // Avg time/img: 0.0523 s\n",
            "loss: 1.924 (epoch: 7, step: 100) // Avg time/img: 0.0544 s\n",
            "loss: 1.931 (epoch: 7, step: 150) // Avg time/img: 0.0543 s\n",
            "loss: 1.922 (epoch: 7, step: 200) // Avg time/img: 0.0546 s\n",
            "loss: 1.922 (epoch: 7, step: 250) // Avg time/img: 0.0544 s\n",
            "loss: 1.921 (epoch: 7, step: 300) // Avg time/img: 0.0544 s\n",
            "loss: 1.918 (epoch: 7, step: 350) // Avg time/img: 0.0548 s\n",
            "loss: 1.916 (epoch: 7, step: 400) // Avg time/img: 0.0548 s\n",
            "loss: 1.911 (epoch: 7, step: 450) // Avg time/img: 0.0549 s\n",
            "----- VALIDATING - EPOCH 7 -----\n",
            "VAL loss: 1.844 (epoch: 7, step: 0) // Avg time/img: 0.0411 s\n",
            "VAL loss: 1.854 (epoch: 7, step: 50) // Avg time/img: 0.0342 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m3.18\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training_void/model-007.pth (epoch: 7)\n",
            "save: ../save/enet_training_void/model_best.pth (epoch: 7)\n",
            "----- TRAINING - EPOCH 8 -----\n",
            "LEARNING RATE:  5e-06\n",
            "loss: 2.133 (epoch: 8, step: 0) // Avg time/img: 0.0591 s\n",
            "loss: 1.856 (epoch: 8, step: 50) // Avg time/img: 0.0531 s\n",
            "loss: 1.895 (epoch: 8, step: 100) // Avg time/img: 0.0545 s\n",
            "loss: 1.89 (epoch: 8, step: 150) // Avg time/img: 0.0542 s\n",
            "loss: 1.878 (epoch: 8, step: 200) // Avg time/img: 0.0542 s\n",
            "loss: 1.876 (epoch: 8, step: 250) // Avg time/img: 0.0545 s\n",
            "loss: 1.87 (epoch: 8, step: 300) // Avg time/img: 0.0546 s\n",
            "loss: 1.872 (epoch: 8, step: 350) // Avg time/img: 0.0545 s\n",
            "loss: 1.868 (epoch: 8, step: 400) // Avg time/img: 0.0545 s\n",
            "loss: 1.868 (epoch: 8, step: 450) // Avg time/img: 0.0543 s\n",
            "----- VALIDATING - EPOCH 8 -----\n",
            "VAL loss: 1.781 (epoch: 8, step: 0) // Avg time/img: 0.0557 s\n",
            "VAL loss: 1.795 (epoch: 8, step: 50) // Avg time/img: 0.0328 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m3.20\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training_void/model-008.pth (epoch: 8)\n",
            "save: ../save/enet_training_void/model_best.pth (epoch: 8)\n",
            "----- TRAINING - EPOCH 9 -----\n",
            "LEARNING RATE:  5e-06\n",
            "loss: 1.645 (epoch: 9, step: 0) // Avg time/img: 0.0853 s\n",
            "loss: 1.805 (epoch: 9, step: 50) // Avg time/img: 0.0546 s\n",
            "loss: 1.814 (epoch: 9, step: 100) // Avg time/img: 0.0538 s\n",
            "loss: 1.821 (epoch: 9, step: 150) // Avg time/img: 0.0547 s\n",
            "loss: 1.815 (epoch: 9, step: 200) // Avg time/img: 0.0545 s\n",
            "loss: 1.815 (epoch: 9, step: 250) // Avg time/img: 0.0547 s\n",
            "loss: 1.812 (epoch: 9, step: 300) // Avg time/img: 0.0547 s\n",
            "loss: 1.811 (epoch: 9, step: 350) // Avg time/img: 0.0544 s\n",
            "loss: 1.814 (epoch: 9, step: 400) // Avg time/img: 0.0545 s\n",
            "loss: 1.814 (epoch: 9, step: 450) // Avg time/img: 0.0544 s\n",
            "----- VALIDATING - EPOCH 9 -----\n",
            "VAL loss: 1.79 (epoch: 9, step: 0) // Avg time/img: 0.0570 s\n",
            "VAL loss: 1.804 (epoch: 9, step: 50) // Avg time/img: 0.0335 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m3.18\u001b[0m %\n",
            "save: ../save/enet_training_void/model-009.pth (epoch: 9)\n",
            "----- TRAINING - EPOCH 10 -----\n",
            "LEARNING RATE:  5e-06\n",
            "loss: 1.802 (epoch: 10, step: 0) // Avg time/img: 0.0554 s\n",
            "loss: 1.802 (epoch: 10, step: 50) // Avg time/img: 0.0552 s\n",
            "loss: 1.788 (epoch: 10, step: 100) // Avg time/img: 0.0538 s\n",
            "loss: 1.794 (epoch: 10, step: 150) // Avg time/img: 0.0535 s\n",
            "loss: 1.795 (epoch: 10, step: 200) // Avg time/img: 0.0537 s\n",
            "loss: 1.798 (epoch: 10, step: 250) // Avg time/img: 0.0537 s\n",
            "loss: 1.792 (epoch: 10, step: 300) // Avg time/img: 0.0541 s\n",
            "loss: 1.79 (epoch: 10, step: 350) // Avg time/img: 0.0541 s\n",
            "loss: 1.784 (epoch: 10, step: 400) // Avg time/img: 0.0541 s\n",
            "loss: 1.78 (epoch: 10, step: 450) // Avg time/img: 0.0542 s\n",
            "----- VALIDATING - EPOCH 10 -----\n",
            "VAL loss: 1.704 (epoch: 10, step: 0) // Avg time/img: 0.0446 s\n",
            "VAL loss: 1.722 (epoch: 10, step: 50) // Avg time/img: 0.0336 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m3.22\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training_void/model-010.pth (epoch: 10)\n",
            "save: ../save/enet_training_void/model_best.pth (epoch: 10)\n",
            "----- TRAINING - EPOCH 11 -----\n",
            "LEARNING RATE:  5e-06\n",
            "loss: 1.579 (epoch: 11, step: 0) // Avg time/img: 0.0497 s\n",
            "loss: 1.779 (epoch: 11, step: 50) // Avg time/img: 0.0522 s\n",
            "loss: 1.769 (epoch: 11, step: 100) // Avg time/img: 0.0530 s\n",
            "loss: 1.755 (epoch: 11, step: 150) // Avg time/img: 0.0531 s\n",
            "loss: 1.757 (epoch: 11, step: 200) // Avg time/img: 0.0532 s\n",
            "loss: 1.751 (epoch: 11, step: 250) // Avg time/img: 0.0536 s\n",
            "loss: 1.747 (epoch: 11, step: 300) // Avg time/img: 0.0535 s\n",
            "loss: 1.744 (epoch: 11, step: 350) // Avg time/img: 0.0536 s\n",
            "loss: 1.735 (epoch: 11, step: 400) // Avg time/img: 0.0537 s\n",
            "loss: 1.731 (epoch: 11, step: 450) // Avg time/img: 0.0535 s\n",
            "----- VALIDATING - EPOCH 11 -----\n",
            "VAL loss: 1.702 (epoch: 11, step: 0) // Avg time/img: 0.0638 s\n",
            "VAL loss: 1.709 (epoch: 11, step: 50) // Avg time/img: 0.0280 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m3.21\u001b[0m %\n",
            "save: ../save/enet_training_void/model-011.pth (epoch: 11)\n",
            "----- TRAINING - EPOCH 12 -----\n",
            "LEARNING RATE:  5e-06\n",
            "loss: 1.753 (epoch: 12, step: 0) // Avg time/img: 0.0716 s\n",
            "loss: 1.711 (epoch: 12, step: 50) // Avg time/img: 0.0422 s\n",
            "loss: 1.695 (epoch: 12, step: 100) // Avg time/img: 0.0417 s\n",
            "loss: 1.688 (epoch: 12, step: 150) // Avg time/img: 0.0415 s\n",
            "loss: 1.691 (epoch: 12, step: 200) // Avg time/img: 0.0417 s\n",
            "loss: 1.693 (epoch: 12, step: 250) // Avg time/img: 0.0425 s\n",
            "loss: 1.69 (epoch: 12, step: 300) // Avg time/img: 0.0429 s\n",
            "loss: 1.689 (epoch: 12, step: 350) // Avg time/img: 0.0430 s\n",
            "loss: 1.691 (epoch: 12, step: 400) // Avg time/img: 0.0428 s\n",
            "loss: 1.686 (epoch: 12, step: 450) // Avg time/img: 0.0426 s\n",
            "----- VALIDATING - EPOCH 12 -----\n",
            "VAL loss: 1.65 (epoch: 12, step: 0) // Avg time/img: 0.0700 s\n",
            "VAL loss: 1.666 (epoch: 12, step: 50) // Avg time/img: 0.0298 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m3.22\u001b[0m %\n",
            "save: ../save/enet_training_void/model-012.pth (epoch: 12)\n",
            "----- TRAINING - EPOCH 13 -----\n",
            "LEARNING RATE:  5e-06\n",
            "loss: 1.54 (epoch: 13, step: 0) // Avg time/img: 0.0888 s\n",
            "loss: 1.674 (epoch: 13, step: 50) // Avg time/img: 0.0424 s\n",
            "loss: 1.673 (epoch: 13, step: 100) // Avg time/img: 0.0420 s\n",
            "loss: 1.672 (epoch: 13, step: 150) // Avg time/img: 0.0422 s\n",
            "loss: 1.67 (epoch: 13, step: 200) // Avg time/img: 0.0427 s\n",
            "loss: 1.664 (epoch: 13, step: 250) // Avg time/img: 0.0427 s\n",
            "loss: 1.658 (epoch: 13, step: 300) // Avg time/img: 0.0426 s\n",
            "loss: 1.659 (epoch: 13, step: 350) // Avg time/img: 0.0428 s\n",
            "loss: 1.653 (epoch: 13, step: 400) // Avg time/img: 0.0428 s\n",
            "loss: 1.651 (epoch: 13, step: 450) // Avg time/img: 0.0427 s\n",
            "----- VALIDATING - EPOCH 13 -----\n",
            "VAL loss: 1.593 (epoch: 13, step: 0) // Avg time/img: 0.0643 s\n",
            "VAL loss: 1.613 (epoch: 13, step: 50) // Avg time/img: 0.0296 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m3.22\u001b[0m %\n",
            "save: ../save/enet_training_void/model-013.pth (epoch: 13)\n",
            "----- TRAINING - EPOCH 14 -----\n",
            "LEARNING RATE:  5.000000000000001e-07\n",
            "loss: 1.46 (epoch: 14, step: 0) // Avg time/img: 0.0611 s\n",
            "loss: 1.64 (epoch: 14, step: 50) // Avg time/img: 0.0530 s\n",
            "loss: 1.627 (epoch: 14, step: 100) // Avg time/img: 0.0527 s\n",
            "loss: 1.623 (epoch: 14, step: 150) // Avg time/img: 0.0534 s\n",
            "loss: 1.629 (epoch: 14, step: 200) // Avg time/img: 0.0533 s\n",
            "loss: 1.632 (epoch: 14, step: 250) // Avg time/img: 0.0533 s\n",
            "loss: 1.635 (epoch: 14, step: 300) // Avg time/img: 0.0534 s\n",
            "loss: 1.627 (epoch: 14, step: 350) // Avg time/img: 0.0534 s\n",
            "loss: 1.628 (epoch: 14, step: 400) // Avg time/img: 0.0534 s\n",
            "loss: 1.628 (epoch: 14, step: 450) // Avg time/img: 0.0535 s\n",
            "----- VALIDATING - EPOCH 14 -----\n",
            "VAL loss: 1.607 (epoch: 14, step: 0) // Avg time/img: 0.0548 s\n",
            "VAL loss: 1.616 (epoch: 14, step: 50) // Avg time/img: 0.0341 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m3.21\u001b[0m %\n",
            "save: ../save/enet_training_void/model-014.pth (epoch: 14)\n",
            "----- TRAINING - EPOCH 15 -----\n",
            "LEARNING RATE:  5.000000000000001e-07\n",
            "loss: 1.46 (epoch: 15, step: 0) // Avg time/img: 0.0495 s\n",
            "loss: 1.611 (epoch: 15, step: 50) // Avg time/img: 0.0517 s\n",
            "loss: 1.606 (epoch: 15, step: 100) // Avg time/img: 0.0533 s\n",
            "loss: 1.61 (epoch: 15, step: 150) // Avg time/img: 0.0532 s\n",
            "loss: 1.618 (epoch: 15, step: 200) // Avg time/img: 0.0529 s\n",
            "loss: 1.611 (epoch: 15, step: 250) // Avg time/img: 0.0534 s\n",
            "loss: 1.609 (epoch: 15, step: 300) // Avg time/img: 0.0536 s\n",
            "loss: 1.616 (epoch: 15, step: 350) // Avg time/img: 0.0537 s\n",
            "loss: 1.619 (epoch: 15, step: 400) // Avg time/img: 0.0538 s\n",
            "loss: 1.62 (epoch: 15, step: 450) // Avg time/img: 0.0536 s\n",
            "----- VALIDATING - EPOCH 15 -----\n",
            "VAL loss: 1.613 (epoch: 15, step: 0) // Avg time/img: 0.0535 s\n",
            "VAL loss: 1.625 (epoch: 15, step: 50) // Avg time/img: 0.0297 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m3.21\u001b[0m %\n",
            "save: ../save/enet_training_void/model-015.pth (epoch: 15)\n",
            "----- TRAINING - EPOCH 16 -----\n",
            "LEARNING RATE:  5.000000000000001e-07\n",
            "loss: 1.626 (epoch: 16, step: 0) // Avg time/img: 0.0926 s\n",
            "loss: 1.613 (epoch: 16, step: 50) // Avg time/img: 0.0547 s\n",
            "loss: 1.617 (epoch: 16, step: 100) // Avg time/img: 0.0539 s\n",
            "loss: 1.621 (epoch: 16, step: 150) // Avg time/img: 0.0546 s\n",
            "loss: 1.62 (epoch: 16, step: 200) // Avg time/img: 0.0542 s\n",
            "loss: 1.628 (epoch: 16, step: 250) // Avg time/img: 0.0543 s\n",
            "loss: 1.622 (epoch: 16, step: 300) // Avg time/img: 0.0542 s\n",
            "loss: 1.622 (epoch: 16, step: 350) // Avg time/img: 0.0541 s\n",
            "loss: 1.621 (epoch: 16, step: 400) // Avg time/img: 0.0542 s\n",
            "loss: 1.62 (epoch: 16, step: 450) // Avg time/img: 0.0541 s\n",
            "----- VALIDATING - EPOCH 16 -----\n",
            "VAL loss: 1.607 (epoch: 16, step: 0) // Avg time/img: 0.0589 s\n",
            "VAL loss: 1.622 (epoch: 16, step: 50) // Avg time/img: 0.0334 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m3.20\u001b[0m %\n",
            "save: ../save/enet_training_void/model-016.pth (epoch: 16)\n",
            "----- TRAINING - EPOCH 17 -----\n",
            "LEARNING RATE:  5.000000000000001e-07\n",
            "loss: 1.943 (epoch: 17, step: 0) // Avg time/img: 0.0541 s\n",
            "loss: 1.634 (epoch: 17, step: 50) // Avg time/img: 0.0530 s\n",
            "loss: 1.632 (epoch: 17, step: 100) // Avg time/img: 0.0532 s\n",
            "loss: 1.628 (epoch: 17, step: 150) // Avg time/img: 0.0530 s\n",
            "loss: 1.628 (epoch: 17, step: 200) // Avg time/img: 0.0534 s\n",
            "loss: 1.632 (epoch: 17, step: 250) // Avg time/img: 0.0535 s\n",
            "loss: 1.621 (epoch: 17, step: 300) // Avg time/img: 0.0537 s\n",
            "loss: 1.619 (epoch: 17, step: 350) // Avg time/img: 0.0536 s\n",
            "loss: 1.614 (epoch: 17, step: 400) // Avg time/img: 0.0537 s\n",
            "loss: 1.613 (epoch: 17, step: 450) // Avg time/img: 0.0539 s\n",
            "----- VALIDATING - EPOCH 17 -----\n",
            "VAL loss: 1.591 (epoch: 17, step: 0) // Avg time/img: 0.0596 s\n",
            "VAL loss: 1.611 (epoch: 17, step: 50) // Avg time/img: 0.0348 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m3.21\u001b[0m %\n",
            "save: ../save/enet_training_void/model-017.pth (epoch: 17)\n",
            "----- TRAINING - EPOCH 18 -----\n",
            "LEARNING RATE:  5.000000000000001e-07\n",
            "loss: 1.682 (epoch: 18, step: 0) // Avg time/img: 0.0573 s\n",
            "loss: 1.641 (epoch: 18, step: 50) // Avg time/img: 0.0547 s\n",
            "loss: 1.627 (epoch: 18, step: 100) // Avg time/img: 0.0552 s\n",
            "loss: 1.622 (epoch: 18, step: 150) // Avg time/img: 0.0545 s\n",
            "loss: 1.615 (epoch: 18, step: 200) // Avg time/img: 0.0553 s\n",
            "loss: 1.612 (epoch: 18, step: 250) // Avg time/img: 0.0551 s\n",
            "loss: 1.612 (epoch: 18, step: 300) // Avg time/img: 0.0548 s\n",
            "loss: 1.611 (epoch: 18, step: 350) // Avg time/img: 0.0549 s\n",
            "loss: 1.611 (epoch: 18, step: 400) // Avg time/img: 0.0548 s\n",
            "loss: 1.611 (epoch: 18, step: 450) // Avg time/img: 0.0548 s\n",
            "----- VALIDATING - EPOCH 18 -----\n",
            "VAL loss: 1.569 (epoch: 18, step: 0) // Avg time/img: 0.0475 s\n",
            "VAL loss: 1.587 (epoch: 18, step: 50) // Avg time/img: 0.0298 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m3.22\u001b[0m %\n",
            "save: ../save/enet_training_void/model-018.pth (epoch: 18)\n",
            "----- TRAINING - EPOCH 19 -----\n",
            "LEARNING RATE:  5.000000000000001e-07\n",
            "loss: 1.579 (epoch: 19, step: 0) // Avg time/img: 0.0534 s\n",
            "loss: 1.625 (epoch: 19, step: 50) // Avg time/img: 0.0428 s\n",
            "loss: 1.611 (epoch: 19, step: 100) // Avg time/img: 0.0434 s\n",
            "loss: 1.61 (epoch: 19, step: 150) // Avg time/img: 0.0434 s\n",
            "loss: 1.614 (epoch: 19, step: 200) // Avg time/img: 0.0433 s\n",
            "loss: 1.61 (epoch: 19, step: 250) // Avg time/img: 0.0438 s\n",
            "loss: 1.605 (epoch: 19, step: 300) // Avg time/img: 0.0438 s\n",
            "loss: 1.607 (epoch: 19, step: 350) // Avg time/img: 0.0434 s\n",
            "loss: 1.606 (epoch: 19, step: 400) // Avg time/img: 0.0438 s\n",
            "loss: 1.607 (epoch: 19, step: 450) // Avg time/img: 0.0439 s\n",
            "----- VALIDATING - EPOCH 19 -----\n",
            "VAL loss: 1.586 (epoch: 19, step: 0) // Avg time/img: 0.0464 s\n",
            "VAL loss: 1.595 (epoch: 19, step: 50) // Avg time/img: 0.0328 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m3.22\u001b[0m %\n",
            "save: ../save/enet_training_void/model-019.pth (epoch: 19)\n",
            "----- TRAINING - EPOCH 20 -----\n",
            "LEARNING RATE:  5.000000000000001e-07\n",
            "loss: 2.278 (epoch: 20, step: 0) // Avg time/img: 0.0596 s\n",
            "loss: 1.635 (epoch: 20, step: 50) // Avg time/img: 0.0421 s\n",
            "loss: 1.621 (epoch: 20, step: 100) // Avg time/img: 0.0431 s\n",
            "loss: 1.612 (epoch: 20, step: 150) // Avg time/img: 0.0427 s\n",
            "loss: 1.607 (epoch: 20, step: 200) // Avg time/img: 0.0427 s\n",
            "loss: 1.609 (epoch: 20, step: 250) // Avg time/img: 0.0429 s\n",
            "loss: 1.606 (epoch: 20, step: 300) // Avg time/img: 0.0423 s\n",
            "loss: 1.607 (epoch: 20, step: 350) // Avg time/img: 0.0423 s\n",
            "loss: 1.607 (epoch: 20, step: 400) // Avg time/img: 0.0425 s\n",
            "loss: 1.604 (epoch: 20, step: 450) // Avg time/img: 0.0426 s\n",
            "----- VALIDATING - EPOCH 20 -----\n",
            "VAL loss: 1.577 (epoch: 20, step: 0) // Avg time/img: 0.0426 s\n",
            "VAL loss: 1.591 (epoch: 20, step: 50) // Avg time/img: 0.0306 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m3.20\u001b[0m %\n",
            "save: ../save/enet_training_void/model-020.pth (epoch: 20)\n",
            "========== TRAINING FINISHED ===========\n",
            "Model saved in /content/AnomalySegmentation/save/enet_training_void\n",
            "updating: content/AnomalySegmentation/save/enet_training_void/ (stored 0%)\n",
            "updating: content/AnomalySegmentation/save/enet_training_void/opts.txt (deflated 36%)\n",
            "  adding: content/AnomalySegmentation/save/enet_training_void/model-005.pth (deflated 18%)\n",
            "  adding: content/AnomalySegmentation/save/enet_training_void/best.txt (stored 0%)\n",
            "  adding: content/AnomalySegmentation/save/enet_training_void/model.txt (deflated 96%)\n",
            "  adding: content/AnomalySegmentation/save/enet_training_void/model-015.pth (deflated 18%)\n",
            "  adding: content/AnomalySegmentation/save/enet_training_void/model-013.pth (deflated 18%)\n",
            "  adding: content/AnomalySegmentation/save/enet_training_void/model-012.pth (deflated 18%)\n",
            "  adding: content/AnomalySegmentation/save/enet_training_void/model_best.pth.tar (deflated 19%)\n",
            "  adding: content/AnomalySegmentation/save/enet_training_void/model-014.pth (deflated 18%)\n",
            "  adding: content/AnomalySegmentation/save/enet_training_void/model-004.pth (deflated 18%)\n",
            "  adding: content/AnomalySegmentation/save/enet_training_void/model-018.pth (deflated 18%)\n",
            "  adding: content/AnomalySegmentation/save/enet_training_void/model-009.pth (deflated 18%)\n",
            "  adding: content/AnomalySegmentation/save/enet_training_void/model-010.pth (deflated 18%)\n",
            "  adding: content/AnomalySegmentation/save/enet_training_void/model_best.pth (deflated 18%)\n",
            "  adding: content/AnomalySegmentation/save/enet_training_void/model-017.pth (deflated 18%)\n",
            "  adding: content/AnomalySegmentation/save/enet_training_void/model-007.pth (deflated 18%)\n",
            "  adding: content/AnomalySegmentation/save/enet_training_void/model-020.pth (deflated 18%)\n",
            "  adding: content/AnomalySegmentation/save/enet_training_void/model-016.pth (deflated 18%)\n",
            "  adding: content/AnomalySegmentation/save/enet_training_void/automated_log.txt (deflated 70%)\n",
            "  adding: content/AnomalySegmentation/save/enet_training_void/model-006.pth (deflated 18%)\n",
            "  adding: content/AnomalySegmentation/save/enet_training_void/model-001.pth (deflated 18%)\n",
            "  adding: content/AnomalySegmentation/save/enet_training_void/model-003.pth (deflated 18%)\n",
            "  adding: content/AnomalySegmentation/save/enet_training_void/checkpoint.pth.tar (deflated 19%)\n",
            "  adding: content/AnomalySegmentation/save/enet_training_void/model-008.pth (deflated 18%)\n",
            "  adding: content/AnomalySegmentation/save/enet_training_void/model-019.pth (deflated 18%)\n",
            "  adding: content/AnomalySegmentation/save/enet_training_void/model-002.pth (deflated 18%)\n",
            "  adding: content/AnomalySegmentation/save/enet_training_void/model-011.pth (deflated 18%)\n",
            "\n",
            "\n",
            "----- Fine-tuning bisenet for VOID classification -----\n",
            "odict_keys(['cp.resnet.conv1.weight', 'cp.resnet.bn1.weight', 'cp.resnet.bn1.bias', 'cp.resnet.bn1.running_mean', 'cp.resnet.bn1.running_var', 'cp.resnet.bn1.num_batches_tracked', 'cp.resnet.layer1.0.conv1.weight', 'cp.resnet.layer1.0.bn1.weight', 'cp.resnet.layer1.0.bn1.bias', 'cp.resnet.layer1.0.bn1.running_mean', 'cp.resnet.layer1.0.bn1.running_var', 'cp.resnet.layer1.0.bn1.num_batches_tracked', 'cp.resnet.layer1.0.conv2.weight', 'cp.resnet.layer1.0.bn2.weight', 'cp.resnet.layer1.0.bn2.bias', 'cp.resnet.layer1.0.bn2.running_mean', 'cp.resnet.layer1.0.bn2.running_var', 'cp.resnet.layer1.0.bn2.num_batches_tracked', 'cp.resnet.layer1.1.conv1.weight', 'cp.resnet.layer1.1.bn1.weight', 'cp.resnet.layer1.1.bn1.bias', 'cp.resnet.layer1.1.bn1.running_mean', 'cp.resnet.layer1.1.bn1.running_var', 'cp.resnet.layer1.1.bn1.num_batches_tracked', 'cp.resnet.layer1.1.conv2.weight', 'cp.resnet.layer1.1.bn2.weight', 'cp.resnet.layer1.1.bn2.bias', 'cp.resnet.layer1.1.bn2.running_mean', 'cp.resnet.layer1.1.bn2.running_var', 'cp.resnet.layer1.1.bn2.num_batches_tracked', 'cp.resnet.layer2.0.conv1.weight', 'cp.resnet.layer2.0.bn1.weight', 'cp.resnet.layer2.0.bn1.bias', 'cp.resnet.layer2.0.bn1.running_mean', 'cp.resnet.layer2.0.bn1.running_var', 'cp.resnet.layer2.0.bn1.num_batches_tracked', 'cp.resnet.layer2.0.conv2.weight', 'cp.resnet.layer2.0.bn2.weight', 'cp.resnet.layer2.0.bn2.bias', 'cp.resnet.layer2.0.bn2.running_mean', 'cp.resnet.layer2.0.bn2.running_var', 'cp.resnet.layer2.0.bn2.num_batches_tracked', 'cp.resnet.layer2.0.downsample.0.weight', 'cp.resnet.layer2.0.downsample.1.weight', 'cp.resnet.layer2.0.downsample.1.bias', 'cp.resnet.layer2.0.downsample.1.running_mean', 'cp.resnet.layer2.0.downsample.1.running_var', 'cp.resnet.layer2.0.downsample.1.num_batches_tracked', 'cp.resnet.layer2.1.conv1.weight', 'cp.resnet.layer2.1.bn1.weight', 'cp.resnet.layer2.1.bn1.bias', 'cp.resnet.layer2.1.bn1.running_mean', 'cp.resnet.layer2.1.bn1.running_var', 'cp.resnet.layer2.1.bn1.num_batches_tracked', 'cp.resnet.layer2.1.conv2.weight', 'cp.resnet.layer2.1.bn2.weight', 'cp.resnet.layer2.1.bn2.bias', 'cp.resnet.layer2.1.bn2.running_mean', 'cp.resnet.layer2.1.bn2.running_var', 'cp.resnet.layer2.1.bn2.num_batches_tracked', 'cp.resnet.layer3.0.conv1.weight', 'cp.resnet.layer3.0.bn1.weight', 'cp.resnet.layer3.0.bn1.bias', 'cp.resnet.layer3.0.bn1.running_mean', 'cp.resnet.layer3.0.bn1.running_var', 'cp.resnet.layer3.0.bn1.num_batches_tracked', 'cp.resnet.layer3.0.conv2.weight', 'cp.resnet.layer3.0.bn2.weight', 'cp.resnet.layer3.0.bn2.bias', 'cp.resnet.layer3.0.bn2.running_mean', 'cp.resnet.layer3.0.bn2.running_var', 'cp.resnet.layer3.0.bn2.num_batches_tracked', 'cp.resnet.layer3.0.downsample.0.weight', 'cp.resnet.layer3.0.downsample.1.weight', 'cp.resnet.layer3.0.downsample.1.bias', 'cp.resnet.layer3.0.downsample.1.running_mean', 'cp.resnet.layer3.0.downsample.1.running_var', 'cp.resnet.layer3.0.downsample.1.num_batches_tracked', 'cp.resnet.layer3.1.conv1.weight', 'cp.resnet.layer3.1.bn1.weight', 'cp.resnet.layer3.1.bn1.bias', 'cp.resnet.layer3.1.bn1.running_mean', 'cp.resnet.layer3.1.bn1.running_var', 'cp.resnet.layer3.1.bn1.num_batches_tracked', 'cp.resnet.layer3.1.conv2.weight', 'cp.resnet.layer3.1.bn2.weight', 'cp.resnet.layer3.1.bn2.bias', 'cp.resnet.layer3.1.bn2.running_mean', 'cp.resnet.layer3.1.bn2.running_var', 'cp.resnet.layer3.1.bn2.num_batches_tracked', 'cp.resnet.layer4.0.conv1.weight', 'cp.resnet.layer4.0.bn1.weight', 'cp.resnet.layer4.0.bn1.bias', 'cp.resnet.layer4.0.bn1.running_mean', 'cp.resnet.layer4.0.bn1.running_var', 'cp.resnet.layer4.0.bn1.num_batches_tracked', 'cp.resnet.layer4.0.conv2.weight', 'cp.resnet.layer4.0.bn2.weight', 'cp.resnet.layer4.0.bn2.bias', 'cp.resnet.layer4.0.bn2.running_mean', 'cp.resnet.layer4.0.bn2.running_var', 'cp.resnet.layer4.0.bn2.num_batches_tracked', 'cp.resnet.layer4.0.downsample.0.weight', 'cp.resnet.layer4.0.downsample.1.weight', 'cp.resnet.layer4.0.downsample.1.bias', 'cp.resnet.layer4.0.downsample.1.running_mean', 'cp.resnet.layer4.0.downsample.1.running_var', 'cp.resnet.layer4.0.downsample.1.num_batches_tracked', 'cp.resnet.layer4.1.conv1.weight', 'cp.resnet.layer4.1.bn1.weight', 'cp.resnet.layer4.1.bn1.bias', 'cp.resnet.layer4.1.bn1.running_mean', 'cp.resnet.layer4.1.bn1.running_var', 'cp.resnet.layer4.1.bn1.num_batches_tracked', 'cp.resnet.layer4.1.conv2.weight', 'cp.resnet.layer4.1.bn2.weight', 'cp.resnet.layer4.1.bn2.bias', 'cp.resnet.layer4.1.bn2.running_mean', 'cp.resnet.layer4.1.bn2.running_var', 'cp.resnet.layer4.1.bn2.num_batches_tracked', 'cp.arm16.conv.conv.weight', 'cp.arm16.conv.bn.weight', 'cp.arm16.conv.bn.bias', 'cp.arm16.conv.bn.running_mean', 'cp.arm16.conv.bn.running_var', 'cp.arm16.conv.bn.num_batches_tracked', 'cp.arm16.conv_atten.weight', 'cp.arm16.bn_atten.weight', 'cp.arm16.bn_atten.bias', 'cp.arm16.bn_atten.running_mean', 'cp.arm16.bn_atten.running_var', 'cp.arm16.bn_atten.num_batches_tracked', 'cp.arm32.conv.conv.weight', 'cp.arm32.conv.bn.weight', 'cp.arm32.conv.bn.bias', 'cp.arm32.conv.bn.running_mean', 'cp.arm32.conv.bn.running_var', 'cp.arm32.conv.bn.num_batches_tracked', 'cp.arm32.conv_atten.weight', 'cp.arm32.bn_atten.weight', 'cp.arm32.bn_atten.bias', 'cp.arm32.bn_atten.running_mean', 'cp.arm32.bn_atten.running_var', 'cp.arm32.bn_atten.num_batches_tracked', 'cp.conv_head32.conv.weight', 'cp.conv_head32.bn.weight', 'cp.conv_head32.bn.bias', 'cp.conv_head32.bn.running_mean', 'cp.conv_head32.bn.running_var', 'cp.conv_head32.bn.num_batches_tracked', 'cp.conv_head16.conv.weight', 'cp.conv_head16.bn.weight', 'cp.conv_head16.bn.bias', 'cp.conv_head16.bn.running_mean', 'cp.conv_head16.bn.running_var', 'cp.conv_head16.bn.num_batches_tracked', 'cp.conv_avg.conv.weight', 'cp.conv_avg.bn.weight', 'cp.conv_avg.bn.bias', 'cp.conv_avg.bn.running_mean', 'cp.conv_avg.bn.running_var', 'cp.conv_avg.bn.num_batches_tracked', 'sp.conv1.conv.weight', 'sp.conv1.bn.weight', 'sp.conv1.bn.bias', 'sp.conv1.bn.running_mean', 'sp.conv1.bn.running_var', 'sp.conv1.bn.num_batches_tracked', 'sp.conv2.conv.weight', 'sp.conv2.bn.weight', 'sp.conv2.bn.bias', 'sp.conv2.bn.running_mean', 'sp.conv2.bn.running_var', 'sp.conv2.bn.num_batches_tracked', 'sp.conv3.conv.weight', 'sp.conv3.bn.weight', 'sp.conv3.bn.bias', 'sp.conv3.bn.running_mean', 'sp.conv3.bn.running_var', 'sp.conv3.bn.num_batches_tracked', 'sp.conv_out.conv.weight', 'sp.conv_out.bn.weight', 'sp.conv_out.bn.bias', 'sp.conv_out.bn.running_mean', 'sp.conv_out.bn.running_var', 'sp.conv_out.bn.num_batches_tracked', 'ffm.convblk.conv.weight', 'ffm.convblk.bn.weight', 'ffm.convblk.bn.bias', 'ffm.convblk.bn.running_mean', 'ffm.convblk.bn.running_var', 'ffm.convblk.bn.num_batches_tracked', 'ffm.conv.weight', 'ffm.bn.weight', 'ffm.bn.bias', 'ffm.bn.running_mean', 'ffm.bn.running_var', 'ffm.bn.num_batches_tracked', 'conv_out.conv.conv.weight', 'conv_out.conv.bn.weight', 'conv_out.conv.bn.bias', 'conv_out.conv.bn.running_mean', 'conv_out.conv.bn.running_var', 'conv_out.conv.bn.num_batches_tracked', 'conv_out.conv_out.weight', 'conv_out.conv_out.bias', 'conv_out16.conv.conv.weight', 'conv_out16.conv.bn.weight', 'conv_out16.conv.bn.bias', 'conv_out16.conv.bn.running_mean', 'conv_out16.conv.bn.running_var', 'conv_out16.conv.bn.num_batches_tracked', 'conv_out16.conv_out.weight', 'conv_out16.conv_out.bias', 'conv_out32.conv.conv.weight', 'conv_out32.conv.bn.weight', 'conv_out32.conv.bn.bias', 'conv_out32.conv.bn.running_mean', 'conv_out32.conv.bn.running_var', 'conv_out32.conv.bn.num_batches_tracked', 'conv_out32.conv_out.weight', 'conv_out32.conv_out.bias'])\n",
            "odict_keys(['cp.resnet.conv1.weight', 'cp.resnet.bn1.weight', 'cp.resnet.bn1.bias', 'cp.resnet.bn1.running_mean', 'cp.resnet.bn1.running_var', 'cp.resnet.bn1.num_batches_tracked', 'cp.resnet.layer1.0.conv1.weight', 'cp.resnet.layer1.0.bn1.weight', 'cp.resnet.layer1.0.bn1.bias', 'cp.resnet.layer1.0.bn1.running_mean', 'cp.resnet.layer1.0.bn1.running_var', 'cp.resnet.layer1.0.bn1.num_batches_tracked', 'cp.resnet.layer1.0.conv2.weight', 'cp.resnet.layer1.0.bn2.weight', 'cp.resnet.layer1.0.bn2.bias', 'cp.resnet.layer1.0.bn2.running_mean', 'cp.resnet.layer1.0.bn2.running_var', 'cp.resnet.layer1.0.bn2.num_batches_tracked', 'cp.resnet.layer1.1.conv1.weight', 'cp.resnet.layer1.1.bn1.weight', 'cp.resnet.layer1.1.bn1.bias', 'cp.resnet.layer1.1.bn1.running_mean', 'cp.resnet.layer1.1.bn1.running_var', 'cp.resnet.layer1.1.bn1.num_batches_tracked', 'cp.resnet.layer1.1.conv2.weight', 'cp.resnet.layer1.1.bn2.weight', 'cp.resnet.layer1.1.bn2.bias', 'cp.resnet.layer1.1.bn2.running_mean', 'cp.resnet.layer1.1.bn2.running_var', 'cp.resnet.layer1.1.bn2.num_batches_tracked', 'cp.resnet.layer2.0.conv1.weight', 'cp.resnet.layer2.0.bn1.weight', 'cp.resnet.layer2.0.bn1.bias', 'cp.resnet.layer2.0.bn1.running_mean', 'cp.resnet.layer2.0.bn1.running_var', 'cp.resnet.layer2.0.bn1.num_batches_tracked', 'cp.resnet.layer2.0.conv2.weight', 'cp.resnet.layer2.0.bn2.weight', 'cp.resnet.layer2.0.bn2.bias', 'cp.resnet.layer2.0.bn2.running_mean', 'cp.resnet.layer2.0.bn2.running_var', 'cp.resnet.layer2.0.bn2.num_batches_tracked', 'cp.resnet.layer2.0.downsample.0.weight', 'cp.resnet.layer2.0.downsample.1.weight', 'cp.resnet.layer2.0.downsample.1.bias', 'cp.resnet.layer2.0.downsample.1.running_mean', 'cp.resnet.layer2.0.downsample.1.running_var', 'cp.resnet.layer2.0.downsample.1.num_batches_tracked', 'cp.resnet.layer2.1.conv1.weight', 'cp.resnet.layer2.1.bn1.weight', 'cp.resnet.layer2.1.bn1.bias', 'cp.resnet.layer2.1.bn1.running_mean', 'cp.resnet.layer2.1.bn1.running_var', 'cp.resnet.layer2.1.bn1.num_batches_tracked', 'cp.resnet.layer2.1.conv2.weight', 'cp.resnet.layer2.1.bn2.weight', 'cp.resnet.layer2.1.bn2.bias', 'cp.resnet.layer2.1.bn2.running_mean', 'cp.resnet.layer2.1.bn2.running_var', 'cp.resnet.layer2.1.bn2.num_batches_tracked', 'cp.resnet.layer3.0.conv1.weight', 'cp.resnet.layer3.0.bn1.weight', 'cp.resnet.layer3.0.bn1.bias', 'cp.resnet.layer3.0.bn1.running_mean', 'cp.resnet.layer3.0.bn1.running_var', 'cp.resnet.layer3.0.bn1.num_batches_tracked', 'cp.resnet.layer3.0.conv2.weight', 'cp.resnet.layer3.0.bn2.weight', 'cp.resnet.layer3.0.bn2.bias', 'cp.resnet.layer3.0.bn2.running_mean', 'cp.resnet.layer3.0.bn2.running_var', 'cp.resnet.layer3.0.bn2.num_batches_tracked', 'cp.resnet.layer3.0.downsample.0.weight', 'cp.resnet.layer3.0.downsample.1.weight', 'cp.resnet.layer3.0.downsample.1.bias', 'cp.resnet.layer3.0.downsample.1.running_mean', 'cp.resnet.layer3.0.downsample.1.running_var', 'cp.resnet.layer3.0.downsample.1.num_batches_tracked', 'cp.resnet.layer3.1.conv1.weight', 'cp.resnet.layer3.1.bn1.weight', 'cp.resnet.layer3.1.bn1.bias', 'cp.resnet.layer3.1.bn1.running_mean', 'cp.resnet.layer3.1.bn1.running_var', 'cp.resnet.layer3.1.bn1.num_batches_tracked', 'cp.resnet.layer3.1.conv2.weight', 'cp.resnet.layer3.1.bn2.weight', 'cp.resnet.layer3.1.bn2.bias', 'cp.resnet.layer3.1.bn2.running_mean', 'cp.resnet.layer3.1.bn2.running_var', 'cp.resnet.layer3.1.bn2.num_batches_tracked', 'cp.resnet.layer4.0.conv1.weight', 'cp.resnet.layer4.0.bn1.weight', 'cp.resnet.layer4.0.bn1.bias', 'cp.resnet.layer4.0.bn1.running_mean', 'cp.resnet.layer4.0.bn1.running_var', 'cp.resnet.layer4.0.bn1.num_batches_tracked', 'cp.resnet.layer4.0.conv2.weight', 'cp.resnet.layer4.0.bn2.weight', 'cp.resnet.layer4.0.bn2.bias', 'cp.resnet.layer4.0.bn2.running_mean', 'cp.resnet.layer4.0.bn2.running_var', 'cp.resnet.layer4.0.bn2.num_batches_tracked', 'cp.resnet.layer4.0.downsample.0.weight', 'cp.resnet.layer4.0.downsample.1.weight', 'cp.resnet.layer4.0.downsample.1.bias', 'cp.resnet.layer4.0.downsample.1.running_mean', 'cp.resnet.layer4.0.downsample.1.running_var', 'cp.resnet.layer4.0.downsample.1.num_batches_tracked', 'cp.resnet.layer4.1.conv1.weight', 'cp.resnet.layer4.1.bn1.weight', 'cp.resnet.layer4.1.bn1.bias', 'cp.resnet.layer4.1.bn1.running_mean', 'cp.resnet.layer4.1.bn1.running_var', 'cp.resnet.layer4.1.bn1.num_batches_tracked', 'cp.resnet.layer4.1.conv2.weight', 'cp.resnet.layer4.1.bn2.weight', 'cp.resnet.layer4.1.bn2.bias', 'cp.resnet.layer4.1.bn2.running_mean', 'cp.resnet.layer4.1.bn2.running_var', 'cp.resnet.layer4.1.bn2.num_batches_tracked', 'cp.arm16.conv.conv.weight', 'cp.arm16.conv.bn.weight', 'cp.arm16.conv.bn.bias', 'cp.arm16.conv.bn.running_mean', 'cp.arm16.conv.bn.running_var', 'cp.arm16.conv.bn.num_batches_tracked', 'cp.arm16.conv_atten.weight', 'cp.arm16.bn_atten.weight', 'cp.arm16.bn_atten.bias', 'cp.arm16.bn_atten.running_mean', 'cp.arm16.bn_atten.running_var', 'cp.arm16.bn_atten.num_batches_tracked', 'cp.arm32.conv.conv.weight', 'cp.arm32.conv.bn.weight', 'cp.arm32.conv.bn.bias', 'cp.arm32.conv.bn.running_mean', 'cp.arm32.conv.bn.running_var', 'cp.arm32.conv.bn.num_batches_tracked', 'cp.arm32.conv_atten.weight', 'cp.arm32.bn_atten.weight', 'cp.arm32.bn_atten.bias', 'cp.arm32.bn_atten.running_mean', 'cp.arm32.bn_atten.running_var', 'cp.arm32.bn_atten.num_batches_tracked', 'cp.conv_head32.conv.weight', 'cp.conv_head32.bn.weight', 'cp.conv_head32.bn.bias', 'cp.conv_head32.bn.running_mean', 'cp.conv_head32.bn.running_var', 'cp.conv_head32.bn.num_batches_tracked', 'cp.conv_head16.conv.weight', 'cp.conv_head16.bn.weight', 'cp.conv_head16.bn.bias', 'cp.conv_head16.bn.running_mean', 'cp.conv_head16.bn.running_var', 'cp.conv_head16.bn.num_batches_tracked', 'cp.conv_avg.conv.weight', 'cp.conv_avg.bn.weight', 'cp.conv_avg.bn.bias', 'cp.conv_avg.bn.running_mean', 'cp.conv_avg.bn.running_var', 'cp.conv_avg.bn.num_batches_tracked', 'sp.conv1.conv.weight', 'sp.conv1.bn.weight', 'sp.conv1.bn.bias', 'sp.conv1.bn.running_mean', 'sp.conv1.bn.running_var', 'sp.conv1.bn.num_batches_tracked', 'sp.conv2.conv.weight', 'sp.conv2.bn.weight', 'sp.conv2.bn.bias', 'sp.conv2.bn.running_mean', 'sp.conv2.bn.running_var', 'sp.conv2.bn.num_batches_tracked', 'sp.conv3.conv.weight', 'sp.conv3.bn.weight', 'sp.conv3.bn.bias', 'sp.conv3.bn.running_mean', 'sp.conv3.bn.running_var', 'sp.conv3.bn.num_batches_tracked', 'sp.conv_out.conv.weight', 'sp.conv_out.bn.weight', 'sp.conv_out.bn.bias', 'sp.conv_out.bn.running_mean', 'sp.conv_out.bn.running_var', 'sp.conv_out.bn.num_batches_tracked', 'ffm.convblk.conv.weight', 'ffm.convblk.bn.weight', 'ffm.convblk.bn.bias', 'ffm.convblk.bn.running_mean', 'ffm.convblk.bn.running_var', 'ffm.convblk.bn.num_batches_tracked', 'ffm.conv.weight', 'ffm.bn.weight', 'ffm.bn.bias', 'ffm.bn.running_mean', 'ffm.bn.running_var', 'ffm.bn.num_batches_tracked', 'conv_out.conv.conv.weight', 'conv_out.conv.bn.weight', 'conv_out.conv.bn.bias', 'conv_out.conv.bn.running_mean', 'conv_out.conv.bn.running_var', 'conv_out.conv.bn.num_batches_tracked', 'conv_out.conv_out.weight', 'conv_out.conv_out.bias', 'conv_out16.conv.conv.weight', 'conv_out16.conv.bn.weight', 'conv_out16.conv.bn.bias', 'conv_out16.conv.bn.running_mean', 'conv_out16.conv.bn.running_var', 'conv_out16.conv.bn.num_batches_tracked', 'conv_out16.conv_out.weight', 'conv_out16.conv_out.bias', 'conv_out32.conv.conv.weight', 'conv_out32.conv.bn.weight', 'conv_out32.conv.bn.bias', 'conv_out32.conv.bn.running_mean', 'conv_out32.conv.bn.running_var', 'conv_out32.conv.bn.num_batches_tracked', 'conv_out32.conv_out.weight', 'conv_out32.conv_out.bias'])\n",
            "conv_out.conv_out.weight in own_state, but size mismatch: torch.Size([20, 256, 1, 1]) vs torch.Size([19, 256, 1, 1])\n",
            "conv_out.conv_out.bias in own_state, but size mismatch: torch.Size([20]) vs torch.Size([19])\n",
            "conv_out16.conv_out.weight in own_state, but size mismatch: torch.Size([20, 64, 1, 1]) vs torch.Size([19, 64, 1, 1])\n",
            "conv_out16.conv_out.bias in own_state, but size mismatch: torch.Size([20]) vs torch.Size([19])\n",
            "conv_out32.conv_out.weight in own_state, but size mismatch: torch.Size([20, 64, 1, 1]) vs torch.Size([19, 64, 1, 1])\n",
            "conv_out32.conv_out.bias in own_state, but size mismatch: torch.Size([20]) vs torch.Size([19])\n",
            "Import Model bisenet with weights bisenetv1_pretrained.pth to FineTune\n",
            "========== TRAINING ===========\n",
            "/content/cityscapes/leftImg8bit/train\n",
            "/content/cityscapes/leftImg8bit/val\n",
            "----- TRAINING - EPOCH 1 -----\n",
            "LEARNING RATE:  0.0025\n",
            "loss: 21.77 (epoch: 1, step: 0) // Avg time/img: 1.0866 s\n",
            "loss: 13.47 (epoch: 1, step: 50) // Avg time/img: 0.0994 s\n",
            "loss: 13.56 (epoch: 1, step: 100) // Avg time/img: 0.0889 s\n",
            "loss: 13.65 (epoch: 1, step: 150) // Avg time/img: 0.0859 s\n",
            "loss: 13.64 (epoch: 1, step: 200) // Avg time/img: 0.0845 s\n",
            "loss: 13.66 (epoch: 1, step: 250) // Avg time/img: 0.0840 s\n",
            "loss: 13.77 (epoch: 1, step: 300) // Avg time/img: 0.0836 s\n",
            "loss: 13.76 (epoch: 1, step: 350) // Avg time/img: 0.0830 s\n",
            "loss: 13.73 (epoch: 1, step: 400) // Avg time/img: 0.0826 s\n",
            "loss: 13.8 (epoch: 1, step: 450) // Avg time/img: 0.0825 s\n",
            "----- VALIDATING - EPOCH 1 -----\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/AnomalySegmentation/train/main.py\", line 610, in <module>\n",
            "    main(parser.parse_args())\n",
            "  File \"/content/AnomalySegmentation/train/main.py\", line 576, in main\n",
            "    model = train(args, model)\n",
            "  File \"/content/AnomalySegmentation/train/main.py\", line 317, in train\n",
            "    for step, (images, labels) in enumerate(loader_val):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 701, in __next__\n",
            "    data = self._next_data()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1465, in _next_data\n",
            "    return self._process_data(data)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1491, in _process_data\n",
            "    data.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_utils.py\", line 715, in reraise\n",
            "    raise exception\n",
            "TypeError: Caught TypeError in DataLoader worker process 0.\n",
            "Original Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n",
            "    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "  File \"/content/AnomalySegmentation/train/dataset.py\", line 94, in __getitem__\n",
            "    image, label = self.co_transform(image, label)\n",
            "  File \"/content/AnomalySegmentation/train/augmentations.py\", line 73, in __call__\n",
            "    input = ToTensor()(input)\n",
            "TypeError: ToTensor.__call__() missing 1 required positional argument: 'lb'\n",
            "\n",
            "Model saved in /content/AnomalySegmentation/save/bisenet_training_void\n",
            "updating: content/AnomalySegmentation/save/bisenet_training_void/ (stored 0%)\n",
            "updating: content/AnomalySegmentation/save/bisenet_training_void/opts.txt (deflated 37%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training_void/model.txt (deflated 91%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training_void/automated_log.txt (deflated 27%)\n"
          ]
        }
      ],
      "source": [
        "# Fine-tune ERFNet, ENet and BiSeNetv1 for VOID classification using CrossEntropy\n",
        "\n",
        "models = [\"erfnet\", \"enet\", \"bisenet\"]\n",
        "savedirs = [\"erfnet_training_void\", \"enet_training_void\", \"bisenet_training_void\"]\n",
        "pretrained_weights = [\"erfnet_pretrained.pth\", \"enet_pretrained.pth\", \"bisenetv1_pretrained.pth\"]\n",
        "epochs = 20\n",
        "\n",
        "# Base directory of the project\n",
        "base_dir = \"/content/AnomalySegmentation/train\"\n",
        "# Dataset directory\n",
        "data_dir = \"/content/cityscapes\"\n",
        "\n",
        "# Loop to execute fine-tuning\n",
        "for model, savedir, pretrained_weight  in zip(models, savedirs, pretrained_weights):\n",
        "    print(f\"\\n\\n----- Fine-tuning {model} for VOID classification -----\")\n",
        "    !cd {base_dir} && python -W ignore main.py --savedir {savedir} --datadir {data_dir} --model {model} --cuda --num-epochs=20 --epochs-save=1 --FineTune --decoder --loadWeights={pretrained_weight}\n",
        "    print(f\"Model saved in /content/AnomalySegmentation/save/{savedir}\")\n",
        "    # zip folder\n",
        "    !zip -r save_{savedir}.zip /content/AnomalySegmentation/save/{savedir}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tune ERFNet, ENet and BiSeNetv1 for VOID classification using CrossEntropy\n",
        "\n",
        "models = [\"bisenet\"]\n",
        "savedirs = [\"bisenet_training_void\"]\n",
        "pretrained_weights = [\"bisenetv1_pretrained.pth\"]\n",
        "epochs = 20\n",
        "\n",
        "# Base directory of the project\n",
        "base_dir = \"/content/AnomalySegmentation/train\"\n",
        "# Dataset directory\n",
        "data_dir = \"/content/cityscapes\"\n",
        "\n",
        "# Loop to execute fine-tuning\n",
        "for model, savedir, pretrained_weight  in zip(models, savedirs, pretrained_weights):\n",
        "    print(f\"\\n\\n----- Fine-tuning {model} for VOID classification -----\")\n",
        "    !cd {base_dir} && python -W ignore main.py --savedir {savedir} --datadir {data_dir} --model {model} --cuda --num-epochs=20 --epochs-save=1 --FineTune --decoder --loadWeights={pretrained_weight}\n",
        "    print(f\"Model saved in /content/AnomalySegmentation/save/{savedir}\")\n",
        "    # zip folder\n",
        "    !zip -r save_{savedir}.zip /content/AnomalySegmentation/save/{savedir}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcsd7GFQ0CPh",
        "outputId": "6099ad4e-4529-4384-b7d8-6e531ac06ed3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "----- Fine-tuning bisenet for VOID classification -----\n",
            "odict_keys(['cp.resnet.conv1.weight', 'cp.resnet.bn1.weight', 'cp.resnet.bn1.bias', 'cp.resnet.bn1.running_mean', 'cp.resnet.bn1.running_var', 'cp.resnet.bn1.num_batches_tracked', 'cp.resnet.layer1.0.conv1.weight', 'cp.resnet.layer1.0.bn1.weight', 'cp.resnet.layer1.0.bn1.bias', 'cp.resnet.layer1.0.bn1.running_mean', 'cp.resnet.layer1.0.bn1.running_var', 'cp.resnet.layer1.0.bn1.num_batches_tracked', 'cp.resnet.layer1.0.conv2.weight', 'cp.resnet.layer1.0.bn2.weight', 'cp.resnet.layer1.0.bn2.bias', 'cp.resnet.layer1.0.bn2.running_mean', 'cp.resnet.layer1.0.bn2.running_var', 'cp.resnet.layer1.0.bn2.num_batches_tracked', 'cp.resnet.layer1.1.conv1.weight', 'cp.resnet.layer1.1.bn1.weight', 'cp.resnet.layer1.1.bn1.bias', 'cp.resnet.layer1.1.bn1.running_mean', 'cp.resnet.layer1.1.bn1.running_var', 'cp.resnet.layer1.1.bn1.num_batches_tracked', 'cp.resnet.layer1.1.conv2.weight', 'cp.resnet.layer1.1.bn2.weight', 'cp.resnet.layer1.1.bn2.bias', 'cp.resnet.layer1.1.bn2.running_mean', 'cp.resnet.layer1.1.bn2.running_var', 'cp.resnet.layer1.1.bn2.num_batches_tracked', 'cp.resnet.layer2.0.conv1.weight', 'cp.resnet.layer2.0.bn1.weight', 'cp.resnet.layer2.0.bn1.bias', 'cp.resnet.layer2.0.bn1.running_mean', 'cp.resnet.layer2.0.bn1.running_var', 'cp.resnet.layer2.0.bn1.num_batches_tracked', 'cp.resnet.layer2.0.conv2.weight', 'cp.resnet.layer2.0.bn2.weight', 'cp.resnet.layer2.0.bn2.bias', 'cp.resnet.layer2.0.bn2.running_mean', 'cp.resnet.layer2.0.bn2.running_var', 'cp.resnet.layer2.0.bn2.num_batches_tracked', 'cp.resnet.layer2.0.downsample.0.weight', 'cp.resnet.layer2.0.downsample.1.weight', 'cp.resnet.layer2.0.downsample.1.bias', 'cp.resnet.layer2.0.downsample.1.running_mean', 'cp.resnet.layer2.0.downsample.1.running_var', 'cp.resnet.layer2.0.downsample.1.num_batches_tracked', 'cp.resnet.layer2.1.conv1.weight', 'cp.resnet.layer2.1.bn1.weight', 'cp.resnet.layer2.1.bn1.bias', 'cp.resnet.layer2.1.bn1.running_mean', 'cp.resnet.layer2.1.bn1.running_var', 'cp.resnet.layer2.1.bn1.num_batches_tracked', 'cp.resnet.layer2.1.conv2.weight', 'cp.resnet.layer2.1.bn2.weight', 'cp.resnet.layer2.1.bn2.bias', 'cp.resnet.layer2.1.bn2.running_mean', 'cp.resnet.layer2.1.bn2.running_var', 'cp.resnet.layer2.1.bn2.num_batches_tracked', 'cp.resnet.layer3.0.conv1.weight', 'cp.resnet.layer3.0.bn1.weight', 'cp.resnet.layer3.0.bn1.bias', 'cp.resnet.layer3.0.bn1.running_mean', 'cp.resnet.layer3.0.bn1.running_var', 'cp.resnet.layer3.0.bn1.num_batches_tracked', 'cp.resnet.layer3.0.conv2.weight', 'cp.resnet.layer3.0.bn2.weight', 'cp.resnet.layer3.0.bn2.bias', 'cp.resnet.layer3.0.bn2.running_mean', 'cp.resnet.layer3.0.bn2.running_var', 'cp.resnet.layer3.0.bn2.num_batches_tracked', 'cp.resnet.layer3.0.downsample.0.weight', 'cp.resnet.layer3.0.downsample.1.weight', 'cp.resnet.layer3.0.downsample.1.bias', 'cp.resnet.layer3.0.downsample.1.running_mean', 'cp.resnet.layer3.0.downsample.1.running_var', 'cp.resnet.layer3.0.downsample.1.num_batches_tracked', 'cp.resnet.layer3.1.conv1.weight', 'cp.resnet.layer3.1.bn1.weight', 'cp.resnet.layer3.1.bn1.bias', 'cp.resnet.layer3.1.bn1.running_mean', 'cp.resnet.layer3.1.bn1.running_var', 'cp.resnet.layer3.1.bn1.num_batches_tracked', 'cp.resnet.layer3.1.conv2.weight', 'cp.resnet.layer3.1.bn2.weight', 'cp.resnet.layer3.1.bn2.bias', 'cp.resnet.layer3.1.bn2.running_mean', 'cp.resnet.layer3.1.bn2.running_var', 'cp.resnet.layer3.1.bn2.num_batches_tracked', 'cp.resnet.layer4.0.conv1.weight', 'cp.resnet.layer4.0.bn1.weight', 'cp.resnet.layer4.0.bn1.bias', 'cp.resnet.layer4.0.bn1.running_mean', 'cp.resnet.layer4.0.bn1.running_var', 'cp.resnet.layer4.0.bn1.num_batches_tracked', 'cp.resnet.layer4.0.conv2.weight', 'cp.resnet.layer4.0.bn2.weight', 'cp.resnet.layer4.0.bn2.bias', 'cp.resnet.layer4.0.bn2.running_mean', 'cp.resnet.layer4.0.bn2.running_var', 'cp.resnet.layer4.0.bn2.num_batches_tracked', 'cp.resnet.layer4.0.downsample.0.weight', 'cp.resnet.layer4.0.downsample.1.weight', 'cp.resnet.layer4.0.downsample.1.bias', 'cp.resnet.layer4.0.downsample.1.running_mean', 'cp.resnet.layer4.0.downsample.1.running_var', 'cp.resnet.layer4.0.downsample.1.num_batches_tracked', 'cp.resnet.layer4.1.conv1.weight', 'cp.resnet.layer4.1.bn1.weight', 'cp.resnet.layer4.1.bn1.bias', 'cp.resnet.layer4.1.bn1.running_mean', 'cp.resnet.layer4.1.bn1.running_var', 'cp.resnet.layer4.1.bn1.num_batches_tracked', 'cp.resnet.layer4.1.conv2.weight', 'cp.resnet.layer4.1.bn2.weight', 'cp.resnet.layer4.1.bn2.bias', 'cp.resnet.layer4.1.bn2.running_mean', 'cp.resnet.layer4.1.bn2.running_var', 'cp.resnet.layer4.1.bn2.num_batches_tracked', 'cp.arm16.conv.conv.weight', 'cp.arm16.conv.bn.weight', 'cp.arm16.conv.bn.bias', 'cp.arm16.conv.bn.running_mean', 'cp.arm16.conv.bn.running_var', 'cp.arm16.conv.bn.num_batches_tracked', 'cp.arm16.conv_atten.weight', 'cp.arm16.bn_atten.weight', 'cp.arm16.bn_atten.bias', 'cp.arm16.bn_atten.running_mean', 'cp.arm16.bn_atten.running_var', 'cp.arm16.bn_atten.num_batches_tracked', 'cp.arm32.conv.conv.weight', 'cp.arm32.conv.bn.weight', 'cp.arm32.conv.bn.bias', 'cp.arm32.conv.bn.running_mean', 'cp.arm32.conv.bn.running_var', 'cp.arm32.conv.bn.num_batches_tracked', 'cp.arm32.conv_atten.weight', 'cp.arm32.bn_atten.weight', 'cp.arm32.bn_atten.bias', 'cp.arm32.bn_atten.running_mean', 'cp.arm32.bn_atten.running_var', 'cp.arm32.bn_atten.num_batches_tracked', 'cp.conv_head32.conv.weight', 'cp.conv_head32.bn.weight', 'cp.conv_head32.bn.bias', 'cp.conv_head32.bn.running_mean', 'cp.conv_head32.bn.running_var', 'cp.conv_head32.bn.num_batches_tracked', 'cp.conv_head16.conv.weight', 'cp.conv_head16.bn.weight', 'cp.conv_head16.bn.bias', 'cp.conv_head16.bn.running_mean', 'cp.conv_head16.bn.running_var', 'cp.conv_head16.bn.num_batches_tracked', 'cp.conv_avg.conv.weight', 'cp.conv_avg.bn.weight', 'cp.conv_avg.bn.bias', 'cp.conv_avg.bn.running_mean', 'cp.conv_avg.bn.running_var', 'cp.conv_avg.bn.num_batches_tracked', 'sp.conv1.conv.weight', 'sp.conv1.bn.weight', 'sp.conv1.bn.bias', 'sp.conv1.bn.running_mean', 'sp.conv1.bn.running_var', 'sp.conv1.bn.num_batches_tracked', 'sp.conv2.conv.weight', 'sp.conv2.bn.weight', 'sp.conv2.bn.bias', 'sp.conv2.bn.running_mean', 'sp.conv2.bn.running_var', 'sp.conv2.bn.num_batches_tracked', 'sp.conv3.conv.weight', 'sp.conv3.bn.weight', 'sp.conv3.bn.bias', 'sp.conv3.bn.running_mean', 'sp.conv3.bn.running_var', 'sp.conv3.bn.num_batches_tracked', 'sp.conv_out.conv.weight', 'sp.conv_out.bn.weight', 'sp.conv_out.bn.bias', 'sp.conv_out.bn.running_mean', 'sp.conv_out.bn.running_var', 'sp.conv_out.bn.num_batches_tracked', 'ffm.convblk.conv.weight', 'ffm.convblk.bn.weight', 'ffm.convblk.bn.bias', 'ffm.convblk.bn.running_mean', 'ffm.convblk.bn.running_var', 'ffm.convblk.bn.num_batches_tracked', 'ffm.conv.weight', 'ffm.bn.weight', 'ffm.bn.bias', 'ffm.bn.running_mean', 'ffm.bn.running_var', 'ffm.bn.num_batches_tracked', 'conv_out.conv.conv.weight', 'conv_out.conv.bn.weight', 'conv_out.conv.bn.bias', 'conv_out.conv.bn.running_mean', 'conv_out.conv.bn.running_var', 'conv_out.conv.bn.num_batches_tracked', 'conv_out.conv_out.weight', 'conv_out.conv_out.bias', 'conv_out16.conv.conv.weight', 'conv_out16.conv.bn.weight', 'conv_out16.conv.bn.bias', 'conv_out16.conv.bn.running_mean', 'conv_out16.conv.bn.running_var', 'conv_out16.conv.bn.num_batches_tracked', 'conv_out16.conv_out.weight', 'conv_out16.conv_out.bias', 'conv_out32.conv.conv.weight', 'conv_out32.conv.bn.weight', 'conv_out32.conv.bn.bias', 'conv_out32.conv.bn.running_mean', 'conv_out32.conv.bn.running_var', 'conv_out32.conv.bn.num_batches_tracked', 'conv_out32.conv_out.weight', 'conv_out32.conv_out.bias'])\n",
            "odict_keys(['cp.resnet.conv1.weight', 'cp.resnet.bn1.weight', 'cp.resnet.bn1.bias', 'cp.resnet.bn1.running_mean', 'cp.resnet.bn1.running_var', 'cp.resnet.bn1.num_batches_tracked', 'cp.resnet.layer1.0.conv1.weight', 'cp.resnet.layer1.0.bn1.weight', 'cp.resnet.layer1.0.bn1.bias', 'cp.resnet.layer1.0.bn1.running_mean', 'cp.resnet.layer1.0.bn1.running_var', 'cp.resnet.layer1.0.bn1.num_batches_tracked', 'cp.resnet.layer1.0.conv2.weight', 'cp.resnet.layer1.0.bn2.weight', 'cp.resnet.layer1.0.bn2.bias', 'cp.resnet.layer1.0.bn2.running_mean', 'cp.resnet.layer1.0.bn2.running_var', 'cp.resnet.layer1.0.bn2.num_batches_tracked', 'cp.resnet.layer1.1.conv1.weight', 'cp.resnet.layer1.1.bn1.weight', 'cp.resnet.layer1.1.bn1.bias', 'cp.resnet.layer1.1.bn1.running_mean', 'cp.resnet.layer1.1.bn1.running_var', 'cp.resnet.layer1.1.bn1.num_batches_tracked', 'cp.resnet.layer1.1.conv2.weight', 'cp.resnet.layer1.1.bn2.weight', 'cp.resnet.layer1.1.bn2.bias', 'cp.resnet.layer1.1.bn2.running_mean', 'cp.resnet.layer1.1.bn2.running_var', 'cp.resnet.layer1.1.bn2.num_batches_tracked', 'cp.resnet.layer2.0.conv1.weight', 'cp.resnet.layer2.0.bn1.weight', 'cp.resnet.layer2.0.bn1.bias', 'cp.resnet.layer2.0.bn1.running_mean', 'cp.resnet.layer2.0.bn1.running_var', 'cp.resnet.layer2.0.bn1.num_batches_tracked', 'cp.resnet.layer2.0.conv2.weight', 'cp.resnet.layer2.0.bn2.weight', 'cp.resnet.layer2.0.bn2.bias', 'cp.resnet.layer2.0.bn2.running_mean', 'cp.resnet.layer2.0.bn2.running_var', 'cp.resnet.layer2.0.bn2.num_batches_tracked', 'cp.resnet.layer2.0.downsample.0.weight', 'cp.resnet.layer2.0.downsample.1.weight', 'cp.resnet.layer2.0.downsample.1.bias', 'cp.resnet.layer2.0.downsample.1.running_mean', 'cp.resnet.layer2.0.downsample.1.running_var', 'cp.resnet.layer2.0.downsample.1.num_batches_tracked', 'cp.resnet.layer2.1.conv1.weight', 'cp.resnet.layer2.1.bn1.weight', 'cp.resnet.layer2.1.bn1.bias', 'cp.resnet.layer2.1.bn1.running_mean', 'cp.resnet.layer2.1.bn1.running_var', 'cp.resnet.layer2.1.bn1.num_batches_tracked', 'cp.resnet.layer2.1.conv2.weight', 'cp.resnet.layer2.1.bn2.weight', 'cp.resnet.layer2.1.bn2.bias', 'cp.resnet.layer2.1.bn2.running_mean', 'cp.resnet.layer2.1.bn2.running_var', 'cp.resnet.layer2.1.bn2.num_batches_tracked', 'cp.resnet.layer3.0.conv1.weight', 'cp.resnet.layer3.0.bn1.weight', 'cp.resnet.layer3.0.bn1.bias', 'cp.resnet.layer3.0.bn1.running_mean', 'cp.resnet.layer3.0.bn1.running_var', 'cp.resnet.layer3.0.bn1.num_batches_tracked', 'cp.resnet.layer3.0.conv2.weight', 'cp.resnet.layer3.0.bn2.weight', 'cp.resnet.layer3.0.bn2.bias', 'cp.resnet.layer3.0.bn2.running_mean', 'cp.resnet.layer3.0.bn2.running_var', 'cp.resnet.layer3.0.bn2.num_batches_tracked', 'cp.resnet.layer3.0.downsample.0.weight', 'cp.resnet.layer3.0.downsample.1.weight', 'cp.resnet.layer3.0.downsample.1.bias', 'cp.resnet.layer3.0.downsample.1.running_mean', 'cp.resnet.layer3.0.downsample.1.running_var', 'cp.resnet.layer3.0.downsample.1.num_batches_tracked', 'cp.resnet.layer3.1.conv1.weight', 'cp.resnet.layer3.1.bn1.weight', 'cp.resnet.layer3.1.bn1.bias', 'cp.resnet.layer3.1.bn1.running_mean', 'cp.resnet.layer3.1.bn1.running_var', 'cp.resnet.layer3.1.bn1.num_batches_tracked', 'cp.resnet.layer3.1.conv2.weight', 'cp.resnet.layer3.1.bn2.weight', 'cp.resnet.layer3.1.bn2.bias', 'cp.resnet.layer3.1.bn2.running_mean', 'cp.resnet.layer3.1.bn2.running_var', 'cp.resnet.layer3.1.bn2.num_batches_tracked', 'cp.resnet.layer4.0.conv1.weight', 'cp.resnet.layer4.0.bn1.weight', 'cp.resnet.layer4.0.bn1.bias', 'cp.resnet.layer4.0.bn1.running_mean', 'cp.resnet.layer4.0.bn1.running_var', 'cp.resnet.layer4.0.bn1.num_batches_tracked', 'cp.resnet.layer4.0.conv2.weight', 'cp.resnet.layer4.0.bn2.weight', 'cp.resnet.layer4.0.bn2.bias', 'cp.resnet.layer4.0.bn2.running_mean', 'cp.resnet.layer4.0.bn2.running_var', 'cp.resnet.layer4.0.bn2.num_batches_tracked', 'cp.resnet.layer4.0.downsample.0.weight', 'cp.resnet.layer4.0.downsample.1.weight', 'cp.resnet.layer4.0.downsample.1.bias', 'cp.resnet.layer4.0.downsample.1.running_mean', 'cp.resnet.layer4.0.downsample.1.running_var', 'cp.resnet.layer4.0.downsample.1.num_batches_tracked', 'cp.resnet.layer4.1.conv1.weight', 'cp.resnet.layer4.1.bn1.weight', 'cp.resnet.layer4.1.bn1.bias', 'cp.resnet.layer4.1.bn1.running_mean', 'cp.resnet.layer4.1.bn1.running_var', 'cp.resnet.layer4.1.bn1.num_batches_tracked', 'cp.resnet.layer4.1.conv2.weight', 'cp.resnet.layer4.1.bn2.weight', 'cp.resnet.layer4.1.bn2.bias', 'cp.resnet.layer4.1.bn2.running_mean', 'cp.resnet.layer4.1.bn2.running_var', 'cp.resnet.layer4.1.bn2.num_batches_tracked', 'cp.arm16.conv.conv.weight', 'cp.arm16.conv.bn.weight', 'cp.arm16.conv.bn.bias', 'cp.arm16.conv.bn.running_mean', 'cp.arm16.conv.bn.running_var', 'cp.arm16.conv.bn.num_batches_tracked', 'cp.arm16.conv_atten.weight', 'cp.arm16.bn_atten.weight', 'cp.arm16.bn_atten.bias', 'cp.arm16.bn_atten.running_mean', 'cp.arm16.bn_atten.running_var', 'cp.arm16.bn_atten.num_batches_tracked', 'cp.arm32.conv.conv.weight', 'cp.arm32.conv.bn.weight', 'cp.arm32.conv.bn.bias', 'cp.arm32.conv.bn.running_mean', 'cp.arm32.conv.bn.running_var', 'cp.arm32.conv.bn.num_batches_tracked', 'cp.arm32.conv_atten.weight', 'cp.arm32.bn_atten.weight', 'cp.arm32.bn_atten.bias', 'cp.arm32.bn_atten.running_mean', 'cp.arm32.bn_atten.running_var', 'cp.arm32.bn_atten.num_batches_tracked', 'cp.conv_head32.conv.weight', 'cp.conv_head32.bn.weight', 'cp.conv_head32.bn.bias', 'cp.conv_head32.bn.running_mean', 'cp.conv_head32.bn.running_var', 'cp.conv_head32.bn.num_batches_tracked', 'cp.conv_head16.conv.weight', 'cp.conv_head16.bn.weight', 'cp.conv_head16.bn.bias', 'cp.conv_head16.bn.running_mean', 'cp.conv_head16.bn.running_var', 'cp.conv_head16.bn.num_batches_tracked', 'cp.conv_avg.conv.weight', 'cp.conv_avg.bn.weight', 'cp.conv_avg.bn.bias', 'cp.conv_avg.bn.running_mean', 'cp.conv_avg.bn.running_var', 'cp.conv_avg.bn.num_batches_tracked', 'sp.conv1.conv.weight', 'sp.conv1.bn.weight', 'sp.conv1.bn.bias', 'sp.conv1.bn.running_mean', 'sp.conv1.bn.running_var', 'sp.conv1.bn.num_batches_tracked', 'sp.conv2.conv.weight', 'sp.conv2.bn.weight', 'sp.conv2.bn.bias', 'sp.conv2.bn.running_mean', 'sp.conv2.bn.running_var', 'sp.conv2.bn.num_batches_tracked', 'sp.conv3.conv.weight', 'sp.conv3.bn.weight', 'sp.conv3.bn.bias', 'sp.conv3.bn.running_mean', 'sp.conv3.bn.running_var', 'sp.conv3.bn.num_batches_tracked', 'sp.conv_out.conv.weight', 'sp.conv_out.bn.weight', 'sp.conv_out.bn.bias', 'sp.conv_out.bn.running_mean', 'sp.conv_out.bn.running_var', 'sp.conv_out.bn.num_batches_tracked', 'ffm.convblk.conv.weight', 'ffm.convblk.bn.weight', 'ffm.convblk.bn.bias', 'ffm.convblk.bn.running_mean', 'ffm.convblk.bn.running_var', 'ffm.convblk.bn.num_batches_tracked', 'ffm.conv.weight', 'ffm.bn.weight', 'ffm.bn.bias', 'ffm.bn.running_mean', 'ffm.bn.running_var', 'ffm.bn.num_batches_tracked', 'conv_out.conv.conv.weight', 'conv_out.conv.bn.weight', 'conv_out.conv.bn.bias', 'conv_out.conv.bn.running_mean', 'conv_out.conv.bn.running_var', 'conv_out.conv.bn.num_batches_tracked', 'conv_out.conv_out.weight', 'conv_out.conv_out.bias', 'conv_out16.conv.conv.weight', 'conv_out16.conv.bn.weight', 'conv_out16.conv.bn.bias', 'conv_out16.conv.bn.running_mean', 'conv_out16.conv.bn.running_var', 'conv_out16.conv.bn.num_batches_tracked', 'conv_out16.conv_out.weight', 'conv_out16.conv_out.bias', 'conv_out32.conv.conv.weight', 'conv_out32.conv.bn.weight', 'conv_out32.conv.bn.bias', 'conv_out32.conv.bn.running_mean', 'conv_out32.conv.bn.running_var', 'conv_out32.conv.bn.num_batches_tracked', 'conv_out32.conv_out.weight', 'conv_out32.conv_out.bias'])\n",
            "conv_out.conv_out.weight in own_state, but size mismatch: torch.Size([20, 256, 1, 1]) vs torch.Size([19, 256, 1, 1])\n",
            "conv_out.conv_out.bias in own_state, but size mismatch: torch.Size([20]) vs torch.Size([19])\n",
            "conv_out16.conv_out.weight in own_state, but size mismatch: torch.Size([20, 64, 1, 1]) vs torch.Size([19, 64, 1, 1])\n",
            "conv_out16.conv_out.bias in own_state, but size mismatch: torch.Size([20]) vs torch.Size([19])\n",
            "conv_out32.conv_out.weight in own_state, but size mismatch: torch.Size([20, 64, 1, 1]) vs torch.Size([19, 64, 1, 1])\n",
            "conv_out32.conv_out.bias in own_state, but size mismatch: torch.Size([20]) vs torch.Size([19])\n",
            "Import Model bisenet with weights bisenetv1_pretrained.pth to FineTune\n",
            "========== TRAINING ===========\n",
            "/content/cityscapes/leftImg8bit/train\n",
            "/content/cityscapes/leftImg8bit/val\n",
            "----- TRAINING - EPOCH 1 -----\n",
            "LEARNING RATE:  0.0025\n",
            "loss: 11.55 (epoch: 1, step: 0) // Avg time/img: 0.8229 s\n",
            "loss: 14.26 (epoch: 1, step: 50) // Avg time/img: 0.0881 s\n",
            "loss: 13.81 (epoch: 1, step: 100) // Avg time/img: 0.0843 s\n",
            "loss: 13.94 (epoch: 1, step: 150) // Avg time/img: 0.0818 s\n",
            "loss: 13.77 (epoch: 1, step: 200) // Avg time/img: 0.0811 s\n",
            "loss: 13.86 (epoch: 1, step: 250) // Avg time/img: 0.0805 s\n",
            "loss: 13.89 (epoch: 1, step: 300) // Avg time/img: 0.0807 s\n",
            "loss: 13.9 (epoch: 1, step: 350) // Avg time/img: 0.0803 s\n",
            "loss: 13.93 (epoch: 1, step: 400) // Avg time/img: 0.0803 s\n",
            "loss: 13.89 (epoch: 1, step: 450) // Avg time/img: 0.0802 s\n",
            "----- VALIDATING - EPOCH 1 -----\n",
            "VAL loss: 17.36 (epoch: 1, step: 0) // Avg time/img: 0.1297 s\n",
            "VAL loss: 15.4 (epoch: 1, step: 50) // Avg time/img: 0.0898 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m74.16\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/bisenet_training_void/model-001.pth (epoch: 1)\n",
            "save: ../save/bisenet_training_void/model_best.pth (epoch: 1)\n",
            "----- TRAINING - EPOCH 2 -----\n",
            "LEARNING RATE:  0.0023872134540537495\n",
            "loss: 14.9 (epoch: 2, step: 0) // Avg time/img: 0.0928 s\n",
            "loss: 14.25 (epoch: 2, step: 50) // Avg time/img: 0.0806 s\n",
            "loss: 13.91 (epoch: 2, step: 100) // Avg time/img: 0.0790 s\n",
            "loss: 13.85 (epoch: 2, step: 150) // Avg time/img: 0.0785 s\n",
            "loss: 13.87 (epoch: 2, step: 200) // Avg time/img: 0.0784 s\n",
            "loss: 13.74 (epoch: 2, step: 250) // Avg time/img: 0.0784 s\n",
            "loss: 13.7 (epoch: 2, step: 300) // Avg time/img: 0.0789 s\n",
            "loss: 13.75 (epoch: 2, step: 350) // Avg time/img: 0.0791 s\n",
            "loss: 13.77 (epoch: 2, step: 400) // Avg time/img: 0.0791 s\n",
            "loss: 13.81 (epoch: 2, step: 450) // Avg time/img: 0.0792 s\n",
            "----- VALIDATING - EPOCH 2 -----\n",
            "VAL loss: 17.29 (epoch: 2, step: 0) // Avg time/img: 0.1292 s\n",
            "VAL loss: 15.31 (epoch: 2, step: 50) // Avg time/img: 0.0900 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m74.00\u001b[0m %\n",
            "save: ../save/bisenet_training_void/model-002.pth (epoch: 2)\n",
            "----- TRAINING - EPOCH 3 -----\n",
            "LEARNING RATE:  0.0022738314402074057\n",
            "loss: 10.06 (epoch: 3, step: 0) // Avg time/img: 0.0875 s\n",
            "loss: 14.04 (epoch: 3, step: 50) // Avg time/img: 0.0786 s\n",
            "loss: 14.05 (epoch: 3, step: 100) // Avg time/img: 0.0780 s\n",
            "loss: 14.01 (epoch: 3, step: 150) // Avg time/img: 0.0777 s\n",
            "loss: 13.9 (epoch: 3, step: 200) // Avg time/img: 0.0784 s\n",
            "loss: 13.9 (epoch: 3, step: 250) // Avg time/img: 0.0781 s\n",
            "loss: 13.86 (epoch: 3, step: 300) // Avg time/img: 0.0783 s\n",
            "loss: 13.84 (epoch: 3, step: 350) // Avg time/img: 0.0785 s\n",
            "loss: 13.82 (epoch: 3, step: 400) // Avg time/img: 0.0787 s\n",
            "loss: 13.78 (epoch: 3, step: 450) // Avg time/img: 0.0787 s\n",
            "----- VALIDATING - EPOCH 3 -----\n",
            "VAL loss: 17.63 (epoch: 3, step: 0) // Avg time/img: 0.1519 s\n",
            "VAL loss: 15.61 (epoch: 3, step: 50) // Avg time/img: 0.0904 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m73.67\u001b[0m %\n",
            "save: ../save/bisenet_training_void/model-003.pth (epoch: 3)\n",
            "----- TRAINING - EPOCH 4 -----\n",
            "LEARNING RATE:  0.0021598174307570477\n",
            "loss: 14.16 (epoch: 4, step: 0) // Avg time/img: 0.0843 s\n",
            "loss: 13.69 (epoch: 4, step: 50) // Avg time/img: 0.0786 s\n",
            "loss: 13.68 (epoch: 4, step: 100) // Avg time/img: 0.0771 s\n",
            "loss: 13.76 (epoch: 4, step: 150) // Avg time/img: 0.0779 s\n",
            "loss: 13.79 (epoch: 4, step: 200) // Avg time/img: 0.0781 s\n",
            "loss: 13.75 (epoch: 4, step: 250) // Avg time/img: 0.0783 s\n",
            "loss: 13.79 (epoch: 4, step: 300) // Avg time/img: 0.0783 s\n",
            "loss: 13.75 (epoch: 4, step: 350) // Avg time/img: 0.0784 s\n",
            "loss: 13.81 (epoch: 4, step: 400) // Avg time/img: 0.0784 s\n",
            "loss: 13.75 (epoch: 4, step: 450) // Avg time/img: 0.0783 s\n",
            "----- VALIDATING - EPOCH 4 -----\n",
            "VAL loss: 16.87 (epoch: 4, step: 0) // Avg time/img: 0.1499 s\n",
            "VAL loss: 15.01 (epoch: 4, step: 50) // Avg time/img: 0.0903 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m73.09\u001b[0m %\n",
            "save: ../save/bisenet_training_void/model-004.pth (epoch: 4)\n",
            "----- TRAINING - EPOCH 5 -----\n",
            "LEARNING RATE:  0.002045130365127146\n",
            "loss: 11.73 (epoch: 5, step: 0) // Avg time/img: 0.0936 s\n",
            "loss: 13.27 (epoch: 5, step: 50) // Avg time/img: 0.0799 s\n",
            "loss: 13.43 (epoch: 5, step: 100) // Avg time/img: 0.0788 s\n",
            "loss: 13.67 (epoch: 5, step: 150) // Avg time/img: 0.0792 s\n",
            "loss: 13.65 (epoch: 5, step: 200) // Avg time/img: 0.0788 s\n",
            "loss: 13.66 (epoch: 5, step: 250) // Avg time/img: 0.0789 s\n",
            "loss: 13.68 (epoch: 5, step: 300) // Avg time/img: 0.0787 s\n",
            "loss: 13.7 (epoch: 5, step: 350) // Avg time/img: 0.0785 s\n",
            "loss: 13.64 (epoch: 5, step: 400) // Avg time/img: 0.0788 s\n",
            "loss: 13.6 (epoch: 5, step: 450) // Avg time/img: 0.0786 s\n",
            "----- VALIDATING - EPOCH 5 -----\n",
            "VAL loss: 17.21 (epoch: 5, step: 0) // Avg time/img: 0.1464 s\n",
            "VAL loss: 15.29 (epoch: 5, step: 50) // Avg time/img: 0.0903 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m73.01\u001b[0m %\n",
            "save: ../save/bisenet_training_void/model-005.pth (epoch: 5)\n",
            "----- TRAINING - EPOCH 6 -----\n",
            "LEARNING RATE:  0.0019297237668089262\n",
            "loss: 11.78 (epoch: 6, step: 0) // Avg time/img: 0.0889 s\n",
            "loss: 13.52 (epoch: 6, step: 50) // Avg time/img: 0.0774 s\n",
            "loss: 13.73 (epoch: 6, step: 100) // Avg time/img: 0.0772 s\n",
            "loss: 13.53 (epoch: 6, step: 150) // Avg time/img: 0.0771 s\n",
            "loss: 13.47 (epoch: 6, step: 200) // Avg time/img: 0.0774 s\n",
            "loss: 13.6 (epoch: 6, step: 250) // Avg time/img: 0.0774 s\n",
            "loss: 13.72 (epoch: 6, step: 300) // Avg time/img: 0.0773 s\n",
            "loss: 13.7 (epoch: 6, step: 350) // Avg time/img: 0.0774 s\n",
            "loss: 13.73 (epoch: 6, step: 400) // Avg time/img: 0.0777 s\n",
            "loss: 13.69 (epoch: 6, step: 450) // Avg time/img: 0.0777 s\n",
            "----- VALIDATING - EPOCH 6 -----\n",
            "VAL loss: 17.14 (epoch: 6, step: 0) // Avg time/img: 0.1174 s\n",
            "VAL loss: 15.25 (epoch: 6, step: 50) // Avg time/img: 0.0897 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m73.87\u001b[0m %\n",
            "save: ../save/bisenet_training_void/model-006.pth (epoch: 6)\n",
            "----- TRAINING - EPOCH 7 -----\n",
            "LEARNING RATE:  0.0018135446173430498\n",
            "loss: 12.77 (epoch: 7, step: 0) // Avg time/img: 0.0978 s\n",
            "loss: 13.37 (epoch: 7, step: 50) // Avg time/img: 0.0807 s\n",
            "loss: 13.47 (epoch: 7, step: 100) // Avg time/img: 0.0793 s\n",
            "loss: 13.55 (epoch: 7, step: 150) // Avg time/img: 0.0795 s\n",
            "loss: 13.6 (epoch: 7, step: 200) // Avg time/img: 0.0791 s\n",
            "loss: 13.44 (epoch: 7, step: 250) // Avg time/img: 0.0788 s\n",
            "loss: 13.57 (epoch: 7, step: 300) // Avg time/img: 0.0789 s\n",
            "loss: 13.69 (epoch: 7, step: 350) // Avg time/img: 0.0790 s\n",
            "loss: 13.65 (epoch: 7, step: 400) // Avg time/img: 0.0790 s\n",
            "loss: 13.63 (epoch: 7, step: 450) // Avg time/img: 0.0790 s\n",
            "----- VALIDATING - EPOCH 7 -----\n",
            "VAL loss: 16.99 (epoch: 7, step: 0) // Avg time/img: 0.1131 s\n",
            "VAL loss: 15.09 (epoch: 7, step: 50) // Avg time/img: 0.0902 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m73.51\u001b[0m %\n",
            "save: ../save/bisenet_training_void/model-007.pth (epoch: 7)\n",
            "----- TRAINING - EPOCH 8 -----\n",
            "LEARNING RATE:  0.0016965318981453127\n",
            "loss: 10.43 (epoch: 8, step: 0) // Avg time/img: 0.0978 s\n",
            "loss: 13.94 (epoch: 8, step: 50) // Avg time/img: 0.0783 s\n",
            "loss: 13.64 (epoch: 8, step: 100) // Avg time/img: 0.0782 s\n",
            "loss: 13.75 (epoch: 8, step: 150) // Avg time/img: 0.0779 s\n",
            "loss: 13.72 (epoch: 8, step: 200) // Avg time/img: 0.0774 s\n",
            "loss: 13.63 (epoch: 8, step: 250) // Avg time/img: 0.0776 s\n",
            "loss: 13.62 (epoch: 8, step: 300) // Avg time/img: 0.0783 s\n",
            "loss: 13.67 (epoch: 8, step: 350) // Avg time/img: 0.0786 s\n",
            "loss: 13.64 (epoch: 8, step: 400) // Avg time/img: 0.0786 s\n",
            "loss: 13.66 (epoch: 8, step: 450) // Avg time/img: 0.0786 s\n",
            "----- VALIDATING - EPOCH 8 -----\n",
            "VAL loss: 17.1 (epoch: 8, step: 0) // Avg time/img: 0.1307 s\n",
            "VAL loss: 15.23 (epoch: 8, step: 50) // Avg time/img: 0.0905 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m74.13\u001b[0m %\n",
            "save: ../save/bisenet_training_void/model-008.pth (epoch: 8)\n",
            "----- TRAINING - EPOCH 9 -----\n",
            "LEARNING RATE:  0.0015786146687233882\n",
            "loss: 13.19 (epoch: 9, step: 0) // Avg time/img: 0.1086 s\n",
            "loss: 13.42 (epoch: 9, step: 50) // Avg time/img: 0.0798 s\n",
            "loss: 13.66 (epoch: 9, step: 100) // Avg time/img: 0.0792 s\n",
            "loss: 13.54 (epoch: 9, step: 150) // Avg time/img: 0.0791 s\n",
            "loss: 13.67 (epoch: 9, step: 200) // Avg time/img: 0.0796 s\n",
            "loss: 13.7 (epoch: 9, step: 250) // Avg time/img: 0.0791 s\n",
            "loss: 13.75 (epoch: 9, step: 300) // Avg time/img: 0.0789 s\n",
            "loss: 13.78 (epoch: 9, step: 350) // Avg time/img: 0.0791 s\n",
            "loss: 13.74 (epoch: 9, step: 400) // Avg time/img: 0.0791 s\n",
            "loss: 13.71 (epoch: 9, step: 450) // Avg time/img: 0.0791 s\n",
            "----- VALIDATING - EPOCH 9 -----\n",
            "VAL loss: 16.86 (epoch: 9, step: 0) // Avg time/img: 0.1415 s\n",
            "VAL loss: 15.1 (epoch: 9, step: 50) // Avg time/img: 0.0910 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m74.13\u001b[0m %\n",
            "save: ../save/bisenet_training_void/model-009.pth (epoch: 9)\n",
            "----- TRAINING - EPOCH 10 -----\n",
            "LEARNING RATE:  0.0014597094822999506\n",
            "loss: 16.77 (epoch: 10, step: 0) // Avg time/img: 0.0855 s\n",
            "loss: 13.64 (epoch: 10, step: 50) // Avg time/img: 0.0783 s\n",
            "loss: 13.65 (epoch: 10, step: 100) // Avg time/img: 0.0772 s\n",
            "loss: 13.65 (epoch: 10, step: 150) // Avg time/img: 0.0781 s\n",
            "loss: 13.72 (epoch: 10, step: 200) // Avg time/img: 0.0786 s\n",
            "loss: 13.79 (epoch: 10, step: 250) // Avg time/img: 0.0785 s\n",
            "loss: 13.71 (epoch: 10, step: 300) // Avg time/img: 0.0790 s\n",
            "loss: 13.8 (epoch: 10, step: 350) // Avg time/img: 0.0788 s\n",
            "loss: 13.68 (epoch: 10, step: 400) // Avg time/img: 0.0788 s\n",
            "loss: 13.72 (epoch: 10, step: 450) // Avg time/img: 0.0788 s\n",
            "----- VALIDATING - EPOCH 10 -----\n",
            "VAL loss: 17.4 (epoch: 10, step: 0) // Avg time/img: 0.1310 s\n",
            "VAL loss: 15.33 (epoch: 10, step: 50) // Avg time/img: 0.0902 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m73.26\u001b[0m %\n",
            "save: ../save/bisenet_training_void/model-010.pth (epoch: 10)\n",
            "----- TRAINING - EPOCH 11 -----\n",
            "LEARNING RATE:  0.0013397168281703664\n",
            "loss: 13.37 (epoch: 11, step: 0) // Avg time/img: 0.0832 s\n",
            "loss: 13.45 (epoch: 11, step: 50) // Avg time/img: 0.0785 s\n",
            "loss: 13.51 (epoch: 11, step: 100) // Avg time/img: 0.0778 s\n",
            "loss: 13.45 (epoch: 11, step: 150) // Avg time/img: 0.0772 s\n",
            "loss: 13.67 (epoch: 11, step: 200) // Avg time/img: 0.0770 s\n",
            "loss: 13.78 (epoch: 11, step: 250) // Avg time/img: 0.0772 s\n",
            "loss: 13.78 (epoch: 11, step: 300) // Avg time/img: 0.0775 s\n",
            "loss: 13.76 (epoch: 11, step: 350) // Avg time/img: 0.0778 s\n",
            "loss: 13.76 (epoch: 11, step: 400) // Avg time/img: 0.0779 s\n",
            "loss: 13.76 (epoch: 11, step: 450) // Avg time/img: 0.0780 s\n",
            "----- VALIDATING - EPOCH 11 -----\n",
            "VAL loss: 17.07 (epoch: 11, step: 0) // Avg time/img: 0.1516 s\n",
            "VAL loss: 15.28 (epoch: 11, step: 50) // Avg time/img: 0.0904 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m73.60\u001b[0m %\n",
            "save: ../save/bisenet_training_void/model-011.pth (epoch: 11)\n",
            "----- TRAINING - EPOCH 12 -----\n",
            "LEARNING RATE:  0.0012185160979474884\n",
            "loss: 13.12 (epoch: 12, step: 0) // Avg time/img: 0.0859 s\n",
            "loss: 13.61 (epoch: 12, step: 50) // Avg time/img: 0.0791 s\n",
            "loss: 13.59 (epoch: 12, step: 100) // Avg time/img: 0.0786 s\n",
            "loss: 13.72 (epoch: 12, step: 150) // Avg time/img: 0.0777 s\n",
            "loss: 13.61 (epoch: 12, step: 200) // Avg time/img: 0.0784 s\n",
            "loss: 13.68 (epoch: 12, step: 250) // Avg time/img: 0.0786 s\n",
            "loss: 13.66 (epoch: 12, step: 300) // Avg time/img: 0.0787 s\n",
            "loss: 13.67 (epoch: 12, step: 350) // Avg time/img: 0.0785 s\n",
            "loss: 13.71 (epoch: 12, step: 400) // Avg time/img: 0.0783 s\n",
            "loss: 13.73 (epoch: 12, step: 450) // Avg time/img: 0.0786 s\n",
            "----- VALIDATING - EPOCH 12 -----\n",
            "VAL loss: 16.68 (epoch: 12, step: 0) // Avg time/img: 0.1239 s\n",
            "VAL loss: 14.9 (epoch: 12, step: 50) // Avg time/img: 0.0896 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m73.25\u001b[0m %\n",
            "save: ../save/bisenet_training_void/model-012.pth (epoch: 12)\n",
            "----- TRAINING - EPOCH 13 -----\n",
            "LEARNING RATE:  0.0010959582263852174\n",
            "loss: 14.37 (epoch: 13, step: 0) // Avg time/img: 0.0862 s\n",
            "loss: 13.75 (epoch: 13, step: 50) // Avg time/img: 0.0797 s\n",
            "loss: 13.81 (epoch: 13, step: 100) // Avg time/img: 0.0796 s\n",
            "loss: 13.71 (epoch: 13, step: 150) // Avg time/img: 0.0785 s\n",
            "loss: 13.69 (epoch: 13, step: 200) // Avg time/img: 0.0785 s\n",
            "loss: 13.63 (epoch: 13, step: 250) // Avg time/img: 0.0784 s\n",
            "loss: 13.62 (epoch: 13, step: 300) // Avg time/img: 0.0786 s\n",
            "loss: 13.7 (epoch: 13, step: 350) // Avg time/img: 0.0785 s\n",
            "loss: 13.71 (epoch: 13, step: 400) // Avg time/img: 0.0789 s\n",
            "loss: 13.81 (epoch: 13, step: 450) // Avg time/img: 0.0789 s\n",
            "----- VALIDATING - EPOCH 13 -----\n",
            "VAL loss: 16.96 (epoch: 13, step: 0) // Avg time/img: 0.1502 s\n",
            "VAL loss: 15.09 (epoch: 13, step: 50) // Avg time/img: 0.0905 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m73.90\u001b[0m %\n",
            "save: ../save/bisenet_training_void/model-013.pth (epoch: 13)\n",
            "----- TRAINING - EPOCH 14 -----\n",
            "LEARNING RATE:  0.0009718544969969087\n",
            "loss: 14.3 (epoch: 14, step: 0) // Avg time/img: 0.0917 s\n",
            "loss: 13.43 (epoch: 14, step: 50) // Avg time/img: 0.0801 s\n",
            "loss: 13.33 (epoch: 14, step: 100) // Avg time/img: 0.0782 s\n",
            "loss: 13.36 (epoch: 14, step: 150) // Avg time/img: 0.0785 s\n",
            "loss: 13.6 (epoch: 14, step: 200) // Avg time/img: 0.0783 s\n",
            "loss: 13.63 (epoch: 14, step: 250) // Avg time/img: 0.0787 s\n",
            "loss: 13.62 (epoch: 14, step: 300) // Avg time/img: 0.0788 s\n",
            "loss: 13.63 (epoch: 14, step: 350) // Avg time/img: 0.0788 s\n",
            "loss: 13.61 (epoch: 14, step: 400) // Avg time/img: 0.0788 s\n",
            "loss: 13.58 (epoch: 14, step: 450) // Avg time/img: 0.0791 s\n",
            "----- VALIDATING - EPOCH 14 -----\n",
            "VAL loss: 16.93 (epoch: 14, step: 0) // Avg time/img: 0.1446 s\n",
            "VAL loss: 15.08 (epoch: 14, step: 50) // Avg time/img: 0.0902 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m73.75\u001b[0m %\n",
            "save: ../save/bisenet_training_void/model-014.pth (epoch: 14)\n",
            "----- TRAINING - EPOCH 15 -----\n",
            "LEARNING RATE:  0.0008459586547541247\n",
            "loss: 13.75 (epoch: 15, step: 0) // Avg time/img: 0.0865 s\n",
            "loss: 12.51 (epoch: 15, step: 50) // Avg time/img: 0.0786 s\n",
            "loss: 13.17 (epoch: 15, step: 100) // Avg time/img: 0.0786 s\n",
            "loss: 13.3 (epoch: 15, step: 150) // Avg time/img: 0.0786 s\n",
            "loss: 13.47 (epoch: 15, step: 200) // Avg time/img: 0.0787 s\n",
            "loss: 13.5 (epoch: 15, step: 250) // Avg time/img: 0.0788 s\n",
            "loss: 13.51 (epoch: 15, step: 300) // Avg time/img: 0.0791 s\n",
            "loss: 13.53 (epoch: 15, step: 350) // Avg time/img: 0.0788 s\n",
            "loss: 13.54 (epoch: 15, step: 400) // Avg time/img: 0.0788 s\n",
            "loss: 13.58 (epoch: 15, step: 450) // Avg time/img: 0.0787 s\n",
            "----- VALIDATING - EPOCH 15 -----\n",
            "VAL loss: 16.9 (epoch: 15, step: 0) // Avg time/img: 0.1254 s\n",
            "VAL loss: 15.04 (epoch: 15, step: 50) // Avg time/img: 0.0898 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m73.27\u001b[0m %\n",
            "save: ../save/bisenet_training_void/model-015.pth (epoch: 15)\n",
            "----- TRAINING - EPOCH 16 -----\n",
            "LEARNING RATE:  0.0007179364718731469\n",
            "loss: 15.3 (epoch: 16, step: 0) // Avg time/img: 0.1070 s\n",
            "loss: 14.14 (epoch: 16, step: 50) // Avg time/img: 0.0797 s\n",
            "loss: 13.81 (epoch: 16, step: 100) // Avg time/img: 0.0790 s\n",
            "loss: 13.63 (epoch: 16, step: 150) // Avg time/img: 0.0788 s\n",
            "loss: 13.62 (epoch: 16, step: 200) // Avg time/img: 0.0787 s\n",
            "loss: 13.68 (epoch: 16, step: 250) // Avg time/img: 0.0786 s\n",
            "loss: 13.64 (epoch: 16, step: 300) // Avg time/img: 0.0789 s\n",
            "loss: 13.67 (epoch: 16, step: 350) // Avg time/img: 0.0789 s\n",
            "loss: 13.68 (epoch: 16, step: 400) // Avg time/img: 0.0789 s\n",
            "loss: 13.79 (epoch: 16, step: 450) // Avg time/img: 0.0790 s\n",
            "----- VALIDATING - EPOCH 16 -----\n",
            "VAL loss: 17.26 (epoch: 16, step: 0) // Avg time/img: 0.1154 s\n",
            "VAL loss: 15.34 (epoch: 16, step: 50) // Avg time/img: 0.0907 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m73.73\u001b[0m %\n",
            "save: ../save/bisenet_training_void/model-016.pth (epoch: 16)\n",
            "----- TRAINING - EPOCH 17 -----\n",
            "LEARNING RATE:  0.0005873094715440094\n",
            "loss: 14.27 (epoch: 17, step: 0) // Avg time/img: 0.0911 s\n",
            "loss: 13.82 (epoch: 17, step: 50) // Avg time/img: 0.0802 s\n",
            "loss: 13.84 (epoch: 17, step: 100) // Avg time/img: 0.0783 s\n",
            "loss: 13.76 (epoch: 17, step: 150) // Avg time/img: 0.0785 s\n",
            "loss: 13.64 (epoch: 17, step: 200) // Avg time/img: 0.0785 s\n",
            "loss: 13.63 (epoch: 17, step: 250) // Avg time/img: 0.0785 s\n",
            "loss: 13.7 (epoch: 17, step: 300) // Avg time/img: 0.0786 s\n",
            "loss: 13.76 (epoch: 17, step: 350) // Avg time/img: 0.0785 s\n",
            "loss: 13.74 (epoch: 17, step: 400) // Avg time/img: 0.0785 s\n",
            "loss: 13.73 (epoch: 17, step: 450) // Avg time/img: 0.0786 s\n",
            "----- VALIDATING - EPOCH 17 -----\n",
            "VAL loss: 17.22 (epoch: 17, step: 0) // Avg time/img: 0.1210 s\n",
            "VAL loss: 15.19 (epoch: 17, step: 50) // Avg time/img: 0.0896 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m73.46\u001b[0m %\n",
            "save: ../save/bisenet_training_void/model-017.pth (epoch: 17)\n",
            "----- TRAINING - EPOCH 18 -----\n",
            "LEARNING RATE:  0.0004533380182841864\n",
            "loss: 10.51 (epoch: 18, step: 0) // Avg time/img: 0.0864 s\n",
            "loss: 13.57 (epoch: 18, step: 50) // Avg time/img: 0.0783 s\n",
            "loss: 13.41 (epoch: 18, step: 100) // Avg time/img: 0.0772 s\n",
            "loss: 13.43 (epoch: 18, step: 150) // Avg time/img: 0.0778 s\n",
            "loss: 13.7 (epoch: 18, step: 200) // Avg time/img: 0.0775 s\n",
            "loss: 13.73 (epoch: 18, step: 250) // Avg time/img: 0.0781 s\n",
            "loss: 13.74 (epoch: 18, step: 300) // Avg time/img: 0.0784 s\n",
            "loss: 13.65 (epoch: 18, step: 350) // Avg time/img: 0.0786 s\n",
            "loss: 13.74 (epoch: 18, step: 400) // Avg time/img: 0.0787 s\n",
            "loss: 13.72 (epoch: 18, step: 450) // Avg time/img: 0.0787 s\n",
            "----- VALIDATING - EPOCH 18 -----\n",
            "VAL loss: 17.6 (epoch: 18, step: 0) // Avg time/img: 0.1587 s\n",
            "VAL loss: 15.55 (epoch: 18, step: 50) // Avg time/img: 0.0905 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m73.35\u001b[0m %\n",
            "save: ../save/bisenet_training_void/model-018.pth (epoch: 18)\n",
            "----- TRAINING - EPOCH 19 -----\n",
            "LEARNING RATE:  0.00031473135294854176\n",
            "loss: 14.41 (epoch: 19, step: 0) // Avg time/img: 0.1037 s\n",
            "loss: 13.89 (epoch: 19, step: 50) // Avg time/img: 0.0777 s\n",
            "loss: 13.76 (epoch: 19, step: 100) // Avg time/img: 0.0780 s\n",
            "loss: 13.72 (epoch: 19, step: 150) // Avg time/img: 0.0779 s\n",
            "loss: 13.78 (epoch: 19, step: 200) // Avg time/img: 0.0781 s\n",
            "loss: 13.84 (epoch: 19, step: 250) // Avg time/img: 0.0783 s\n",
            "loss: 13.77 (epoch: 19, step: 300) // Avg time/img: 0.0784 s\n",
            "loss: 13.72 (epoch: 19, step: 350) // Avg time/img: 0.0783 s\n",
            "loss: 13.73 (epoch: 19, step: 400) // Avg time/img: 0.0783 s\n",
            "loss: 13.76 (epoch: 19, step: 450) // Avg time/img: 0.0782 s\n",
            "----- VALIDATING - EPOCH 19 -----\n",
            "VAL loss: 16.98 (epoch: 19, step: 0) // Avg time/img: 0.1226 s\n",
            "VAL loss: 15.16 (epoch: 19, step: 50) // Avg time/img: 0.0900 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m74.26\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/bisenet_training_void/model-019.pth (epoch: 19)\n",
            "save: ../save/bisenet_training_void/model_best.pth (epoch: 19)\n",
            "----- TRAINING - EPOCH 20 -----\n",
            "LEARNING RATE:  0.00016866035595919555\n",
            "loss: 15.02 (epoch: 20, step: 0) // Avg time/img: 0.0788 s\n",
            "loss: 13.32 (epoch: 20, step: 50) // Avg time/img: 0.0784 s\n",
            "loss: 13.79 (epoch: 20, step: 100) // Avg time/img: 0.0784 s\n",
            "loss: 13.73 (epoch: 20, step: 150) // Avg time/img: 0.0786 s\n",
            "loss: 13.81 (epoch: 20, step: 200) // Avg time/img: 0.0787 s\n",
            "loss: 13.83 (epoch: 20, step: 250) // Avg time/img: 0.0784 s\n",
            "loss: 13.77 (epoch: 20, step: 300) // Avg time/img: 0.0786 s\n",
            "loss: 13.77 (epoch: 20, step: 350) // Avg time/img: 0.0789 s\n",
            "loss: 13.69 (epoch: 20, step: 400) // Avg time/img: 0.0790 s\n",
            "loss: 13.82 (epoch: 20, step: 450) // Avg time/img: 0.0789 s\n",
            "----- VALIDATING - EPOCH 20 -----\n",
            "VAL loss: 17.26 (epoch: 20, step: 0) // Avg time/img: 0.1222 s\n",
            "VAL loss: 15.37 (epoch: 20, step: 50) // Avg time/img: 0.0898 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m73.58\u001b[0m %\n",
            "save: ../save/bisenet_training_void/model-020.pth (epoch: 20)\n",
            "========== TRAINING FINISHED ===========\n",
            "Model saved in /content/AnomalySegmentation/save/bisenet_training_void\n",
            "updating: content/AnomalySegmentation/save/bisenet_training_void/ (stored 0%)\n",
            "updating: content/AnomalySegmentation/save/bisenet_training_void/opts.txt (deflated 37%)\n",
            "updating: content/AnomalySegmentation/save/bisenet_training_void/model.txt (deflated 91%)\n",
            "updating: content/AnomalySegmentation/save/bisenet_training_void/automated_log.txt (deflated 63%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training_void/model-005.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training_void/best.txt (stored 0%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training_void/model-015.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training_void/model-013.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training_void/model-012.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training_void/model_best.pth.tar (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training_void/model-014.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training_void/model-004.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training_void/model-018.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training_void/model-009.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training_void/model-010.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training_void/model_best.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training_void/model-017.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training_void/model-007.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training_void/model-020.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training_void/model-016.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training_void/model-006.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training_void/model-001.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training_void/model-003.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training_void/checkpoint.pth.tar (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training_void/model-008.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training_void/model-019.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training_void/model-002.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training_void/model-011.pth (deflated 7%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMrPCZ56IShf"
      },
      "source": [
        "**Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cchB40LlIT9a",
        "outputId": "a176970e-16b1-499d-8a0c-de2a4c54d5e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------\n",
            "\n",
            "Dataset: RoadAnomaly21 net: erfnet\n",
            "100% 10/10 [00:02<00:00,  3.66it/s]\n",
            "AUPRC score: 14.498387018665449\n",
            "FPR@TPR95: 94.34378952647214\n",
            "\n",
            "Dataset: RoadObsticle21 net: erfnet\n",
            "100% 30/30 [00:06<00:00,  4.57it/s]\n",
            "AUPRC score: 0.6817572290325407\n",
            "FPR@TPR95: 94.01770484174406\n",
            "\n",
            "Dataset: FS_LostFound_full net: erfnet\n",
            "100% 100/100 [00:22<00:00,  4.43it/s]\n",
            "AUPRC score: 0.2824634642075373\n",
            "FPR@TPR95: 95.11901843952202\n",
            "\n",
            "Dataset: fs_static net: erfnet\n",
            "100% 30/30 [00:04<00:00,  7.34it/s]\n",
            "AUPRC score: 2.0646274257640505\n",
            "FPR@TPR95: 94.64748859413388\n",
            "\n",
            "Dataset: RoadAnomaly net: erfnet\n",
            "100% 60/60 [00:07<00:00,  8.49it/s]\n",
            "AUPRC score: 9.783560091665896\n",
            "FPR@TPR95: 94.56862850746488\n",
            "----------------------------\n",
            "\n",
            "Dataset: RoadAnomaly21 net: enet\n",
            "100% 10/10 [00:02<00:00,  4.24it/s]\n",
            "AUPRC score: 14.319034546889329\n",
            "FPR@TPR95: 92.91421655212373\n",
            "\n",
            "Dataset: RoadObsticle21 net: enet\n",
            "100% 30/30 [00:06<00:00,  4.31it/s]\n",
            "AUPRC score: 0.6517555616928784\n",
            "FPR@TPR95: 92.95224292518449\n",
            "\n",
            "Dataset: FS_LostFound_full net: enet\n",
            "100% 100/100 [00:19<00:00,  5.11it/s]\n",
            "AUPRC score: 0.2881349922641959\n",
            "FPR@TPR95: 95.71917367776432\n",
            "\n",
            "Dataset: fs_static net: enet\n",
            "100% 30/30 [00:03<00:00,  7.60it/s]\n",
            "AUPRC score: 2.012365209266098\n",
            "FPR@TPR95: 92.63241737725477\n",
            "\n",
            "Dataset: RoadAnomaly net: enet\n",
            "100% 60/60 [00:06<00:00,  8.85it/s]\n",
            "AUPRC score: 9.715801351804961\n",
            "FPR@TPR95: 94.10260788865837\n",
            "----------------------------\n",
            "\n",
            "Dataset: RoadAnomaly21 net: bisenet\n",
            "100% 10/10 [00:02<00:00,  3.96it/s]\n",
            "AUPRC score: 10.316554419035818\n",
            "FPR@TPR95: 98.93416972981724\n",
            "\n",
            "Dataset: RoadObsticle21 net: bisenet\n",
            "100% 30/30 [00:07<00:00,  4.19it/s]\n",
            "AUPRC score: 0.343325934241099\n",
            "FPR@TPR95: 99.97936098120007\n",
            "\n",
            "Dataset: FS_LostFound_full net: bisenet\n",
            "100% 100/100 [00:19<00:00,  5.25it/s]\n",
            "AUPRC score: 0.20174615976005844\n",
            "FPR@TPR95: 99.97758836034706\n",
            "\n",
            "Dataset: fs_static net: bisenet\n",
            "100% 30/30 [00:03<00:00,  7.66it/s]\n",
            "AUPRC score: 1.2440572546731894\n",
            "FPR@TPR95: 99.87496376645677\n",
            "\n",
            "Dataset: RoadAnomaly net: bisenet\n",
            "100% 60/60 [00:06<00:00,  9.03it/s]\n",
            "AUPRC score: 6.4262293946259375\n",
            "FPR@TPR95: 99.41657206982038\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "no_execute = False\n",
        "just_once = False\n",
        "\n",
        "for net in [\"erfnet\", \"enet\", \"bisenet\"]:\n",
        "  print(\"----------------------------\")\n",
        "  for dataset_dir in ['RoadAnomaly21', 'RoadObsticle21', 'FS_LostFound_full', 'fs_static', 'RoadAnomaly']:\n",
        "\n",
        "    if no_execute:\n",
        "      break\n",
        "\n",
        "    load_dir = f'/content/AnomalySegmentation/save/{net}_training_void'\n",
        "    weights = f'/model_best.pth'\n",
        "    format_file = os.listdir(f'/content/Validation_Dataset/{dataset_dir}/images')[0].split(\".\")[1]\n",
        "    input =f'/content/Validation_Dataset/{dataset_dir}/images/\\*.{format_file}'\n",
        "    print(f\"\\nDataset: {dataset_dir} net: {net}\")\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "      !python -W ignore /content/AnomalySegmentation/eval/evalAnomaly.py --input {input} --void --model {net} --loadDir {load_dir} --loadWeights {weights} | tail -n 2\n",
        "    else:\n",
        "      !python -W ignore /content/AnomalySegmentation/eval/evalAnomaly.py --input {input} --void --model {net} --loadDir {load_dir} --loadWeights {weights} --cpu | tail -n 2\n",
        "\n",
        "    if just_once:\n",
        "      no_execute = True\n",
        "      just_once = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xviXChktBk0"
      },
      "source": [
        "# Extensions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVugUIVbxMSq"
      },
      "source": [
        "##Mahalanobis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "S70ucntWxy4F",
        "outputId": "c7bcb158-a13b-4ba9-f3f3-f4bb98129185"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model: erfnet\n",
            "Loading weights: /content/AnomalySegmentation/trained_models/erfnet_pretrained.pth\n",
            "/content/cityscapes/leftImg8bit/train\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/content/AnomalySegmentation/eval/mahalanobis.py:90: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model = load_my_state_dict(model, torch.load(weightspath, map_location=lambda storage, loc: storage))\n",
            "odict_keys(['module.encoder.initial_block.conv.weight', 'module.encoder.initial_block.conv.bias', 'module.encoder.initial_block.bn.weight', 'module.encoder.initial_block.bn.bias', 'module.encoder.initial_block.bn.running_mean', 'module.encoder.initial_block.bn.running_var', 'module.encoder.layers.0.conv.weight', 'module.encoder.layers.0.conv.bias', 'module.encoder.layers.0.bn.weight', 'module.encoder.layers.0.bn.bias', 'module.encoder.layers.0.bn.running_mean', 'module.encoder.layers.0.bn.running_var', 'module.encoder.layers.1.conv3x1_1.weight', 'module.encoder.layers.1.conv3x1_1.bias', 'module.encoder.layers.1.conv1x3_1.weight', 'module.encoder.layers.1.conv1x3_1.bias', 'module.encoder.layers.1.conv3x1_2.weight', 'module.encoder.layers.1.conv3x1_2.bias', 'module.encoder.layers.1.conv1x3_2.weight', 'module.encoder.layers.1.conv1x3_2.bias', 'module.encoder.layers.1.bn1.weight', 'module.encoder.layers.1.bn1.bias', 'module.encoder.layers.1.bn1.running_mean', 'module.encoder.layers.1.bn1.running_var', 'module.encoder.layers.1.bn2.weight', 'module.encoder.layers.1.bn2.bias', 'module.encoder.layers.1.bn2.running_mean', 'module.encoder.layers.1.bn2.running_var', 'module.encoder.layers.2.conv3x1_1.weight', 'module.encoder.layers.2.conv3x1_1.bias', 'module.encoder.layers.2.conv1x3_1.weight', 'module.encoder.layers.2.conv1x3_1.bias', 'module.encoder.layers.2.conv3x1_2.weight', 'module.encoder.layers.2.conv3x1_2.bias', 'module.encoder.layers.2.conv1x3_2.weight', 'module.encoder.layers.2.conv1x3_2.bias', 'module.encoder.layers.2.bn1.weight', 'module.encoder.layers.2.bn1.bias', 'module.encoder.layers.2.bn1.running_mean', 'module.encoder.layers.2.bn1.running_var', 'module.encoder.layers.2.bn2.weight', 'module.encoder.layers.2.bn2.bias', 'module.encoder.layers.2.bn2.running_mean', 'module.encoder.layers.2.bn2.running_var', 'module.encoder.layers.3.conv3x1_1.weight', 'module.encoder.layers.3.conv3x1_1.bias', 'module.encoder.layers.3.conv1x3_1.weight', 'module.encoder.layers.3.conv1x3_1.bias', 'module.encoder.layers.3.conv3x1_2.weight', 'module.encoder.layers.3.conv3x1_2.bias', 'module.encoder.layers.3.conv1x3_2.weight', 'module.encoder.layers.3.conv1x3_2.bias', 'module.encoder.layers.3.bn1.weight', 'module.encoder.layers.3.bn1.bias', 'module.encoder.layers.3.bn1.running_mean', 'module.encoder.layers.3.bn1.running_var', 'module.encoder.layers.3.bn2.weight', 'module.encoder.layers.3.bn2.bias', 'module.encoder.layers.3.bn2.running_mean', 'module.encoder.layers.3.bn2.running_var', 'module.encoder.layers.4.conv3x1_1.weight', 'module.encoder.layers.4.conv3x1_1.bias', 'module.encoder.layers.4.conv1x3_1.weight', 'module.encoder.layers.4.conv1x3_1.bias', 'module.encoder.layers.4.conv3x1_2.weight', 'module.encoder.layers.4.conv3x1_2.bias', 'module.encoder.layers.4.conv1x3_2.weight', 'module.encoder.layers.4.conv1x3_2.bias', 'module.encoder.layers.4.bn1.weight', 'module.encoder.layers.4.bn1.bias', 'module.encoder.layers.4.bn1.running_mean', 'module.encoder.layers.4.bn1.running_var', 'module.encoder.layers.4.bn2.weight', 'module.encoder.layers.4.bn2.bias', 'module.encoder.layers.4.bn2.running_mean', 'module.encoder.layers.4.bn2.running_var', 'module.encoder.layers.5.conv3x1_1.weight', 'module.encoder.layers.5.conv3x1_1.bias', 'module.encoder.layers.5.conv1x3_1.weight', 'module.encoder.layers.5.conv1x3_1.bias', 'module.encoder.layers.5.conv3x1_2.weight', 'module.encoder.layers.5.conv3x1_2.bias', 'module.encoder.layers.5.conv1x3_2.weight', 'module.encoder.layers.5.conv1x3_2.bias', 'module.encoder.layers.5.bn1.weight', 'module.encoder.layers.5.bn1.bias', 'module.encoder.layers.5.bn1.running_mean', 'module.encoder.layers.5.bn1.running_var', 'module.encoder.layers.5.bn2.weight', 'module.encoder.layers.5.bn2.bias', 'module.encoder.layers.5.bn2.running_mean', 'module.encoder.layers.5.bn2.running_var', 'module.encoder.layers.6.conv.weight', 'module.encoder.layers.6.conv.bias', 'module.encoder.layers.6.bn.weight', 'module.encoder.layers.6.bn.bias', 'module.encoder.layers.6.bn.running_mean', 'module.encoder.layers.6.bn.running_var', 'module.encoder.layers.7.conv3x1_1.weight', 'module.encoder.layers.7.conv3x1_1.bias', 'module.encoder.layers.7.conv1x3_1.weight', 'module.encoder.layers.7.conv1x3_1.bias', 'module.encoder.layers.7.conv3x1_2.weight', 'module.encoder.layers.7.conv3x1_2.bias', 'module.encoder.layers.7.conv1x3_2.weight', 'module.encoder.layers.7.conv1x3_2.bias', 'module.encoder.layers.7.bn1.weight', 'module.encoder.layers.7.bn1.bias', 'module.encoder.layers.7.bn1.running_mean', 'module.encoder.layers.7.bn1.running_var', 'module.encoder.layers.7.bn2.weight', 'module.encoder.layers.7.bn2.bias', 'module.encoder.layers.7.bn2.running_mean', 'module.encoder.layers.7.bn2.running_var', 'module.encoder.layers.8.conv3x1_1.weight', 'module.encoder.layers.8.conv3x1_1.bias', 'module.encoder.layers.8.conv1x3_1.weight', 'module.encoder.layers.8.conv1x3_1.bias', 'module.encoder.layers.8.conv3x1_2.weight', 'module.encoder.layers.8.conv3x1_2.bias', 'module.encoder.layers.8.conv1x3_2.weight', 'module.encoder.layers.8.conv1x3_2.bias', 'module.encoder.layers.8.bn1.weight', 'module.encoder.layers.8.bn1.bias', 'module.encoder.layers.8.bn1.running_mean', 'module.encoder.layers.8.bn1.running_var', 'module.encoder.layers.8.bn2.weight', 'module.encoder.layers.8.bn2.bias', 'module.encoder.layers.8.bn2.running_mean', 'module.encoder.layers.8.bn2.running_var', 'module.encoder.layers.9.conv3x1_1.weight', 'module.encoder.layers.9.conv3x1_1.bias', 'module.encoder.layers.9.conv1x3_1.weight', 'module.encoder.layers.9.conv1x3_1.bias', 'module.encoder.layers.9.conv3x1_2.weight', 'module.encoder.layers.9.conv3x1_2.bias', 'module.encoder.layers.9.conv1x3_2.weight', 'module.encoder.layers.9.conv1x3_2.bias', 'module.encoder.layers.9.bn1.weight', 'module.encoder.layers.9.bn1.bias', 'module.encoder.layers.9.bn1.running_mean', 'module.encoder.layers.9.bn1.running_var', 'module.encoder.layers.9.bn2.weight', 'module.encoder.layers.9.bn2.bias', 'module.encoder.layers.9.bn2.running_mean', 'module.encoder.layers.9.bn2.running_var', 'module.encoder.layers.10.conv3x1_1.weight', 'module.encoder.layers.10.conv3x1_1.bias', 'module.encoder.layers.10.conv1x3_1.weight', 'module.encoder.layers.10.conv1x3_1.bias', 'module.encoder.layers.10.conv3x1_2.weight', 'module.encoder.layers.10.conv3x1_2.bias', 'module.encoder.layers.10.conv1x3_2.weight', 'module.encoder.layers.10.conv1x3_2.bias', 'module.encoder.layers.10.bn1.weight', 'module.encoder.layers.10.bn1.bias', 'module.encoder.layers.10.bn1.running_mean', 'module.encoder.layers.10.bn1.running_var', 'module.encoder.layers.10.bn2.weight', 'module.encoder.layers.10.bn2.bias', 'module.encoder.layers.10.bn2.running_mean', 'module.encoder.layers.10.bn2.running_var', 'module.encoder.layers.11.conv3x1_1.weight', 'module.encoder.layers.11.conv3x1_1.bias', 'module.encoder.layers.11.conv1x3_1.weight', 'module.encoder.layers.11.conv1x3_1.bias', 'module.encoder.layers.11.conv3x1_2.weight', 'module.encoder.layers.11.conv3x1_2.bias', 'module.encoder.layers.11.conv1x3_2.weight', 'module.encoder.layers.11.conv1x3_2.bias', 'module.encoder.layers.11.bn1.weight', 'module.encoder.layers.11.bn1.bias', 'module.encoder.layers.11.bn1.running_mean', 'module.encoder.layers.11.bn1.running_var', 'module.encoder.layers.11.bn2.weight', 'module.encoder.layers.11.bn2.bias', 'module.encoder.layers.11.bn2.running_mean', 'module.encoder.layers.11.bn2.running_var', 'module.encoder.layers.12.conv3x1_1.weight', 'module.encoder.layers.12.conv3x1_1.bias', 'module.encoder.layers.12.conv1x3_1.weight', 'module.encoder.layers.12.conv1x3_1.bias', 'module.encoder.layers.12.conv3x1_2.weight', 'module.encoder.layers.12.conv3x1_2.bias', 'module.encoder.layers.12.conv1x3_2.weight', 'module.encoder.layers.12.conv1x3_2.bias', 'module.encoder.layers.12.bn1.weight', 'module.encoder.layers.12.bn1.bias', 'module.encoder.layers.12.bn1.running_mean', 'module.encoder.layers.12.bn1.running_var', 'module.encoder.layers.12.bn2.weight', 'module.encoder.layers.12.bn2.bias', 'module.encoder.layers.12.bn2.running_mean', 'module.encoder.layers.12.bn2.running_var', 'module.encoder.layers.13.conv3x1_1.weight', 'module.encoder.layers.13.conv3x1_1.bias', 'module.encoder.layers.13.conv1x3_1.weight', 'module.encoder.layers.13.conv1x3_1.bias', 'module.encoder.layers.13.conv3x1_2.weight', 'module.encoder.layers.13.conv3x1_2.bias', 'module.encoder.layers.13.conv1x3_2.weight', 'module.encoder.layers.13.conv1x3_2.bias', 'module.encoder.layers.13.bn1.weight', 'module.encoder.layers.13.bn1.bias', 'module.encoder.layers.13.bn1.running_mean', 'module.encoder.layers.13.bn1.running_var', 'module.encoder.layers.13.bn2.weight', 'module.encoder.layers.13.bn2.bias', 'module.encoder.layers.13.bn2.running_mean', 'module.encoder.layers.13.bn2.running_var', 'module.encoder.layers.14.conv3x1_1.weight', 'module.encoder.layers.14.conv3x1_1.bias', 'module.encoder.layers.14.conv1x3_1.weight', 'module.encoder.layers.14.conv1x3_1.bias', 'module.encoder.layers.14.conv3x1_2.weight', 'module.encoder.layers.14.conv3x1_2.bias', 'module.encoder.layers.14.conv1x3_2.weight', 'module.encoder.layers.14.conv1x3_2.bias', 'module.encoder.layers.14.bn1.weight', 'module.encoder.layers.14.bn1.bias', 'module.encoder.layers.14.bn1.running_mean', 'module.encoder.layers.14.bn1.running_var', 'module.encoder.layers.14.bn2.weight', 'module.encoder.layers.14.bn2.bias', 'module.encoder.layers.14.bn2.running_mean', 'module.encoder.layers.14.bn2.running_var', 'module.decoder.layers.0.conv.weight', 'module.decoder.layers.0.conv.bias', 'module.decoder.layers.0.bn.weight', 'module.decoder.layers.0.bn.bias', 'module.decoder.layers.0.bn.running_mean', 'module.decoder.layers.0.bn.running_var', 'module.decoder.layers.1.conv3x1_1.weight', 'module.decoder.layers.1.conv3x1_1.bias', 'module.decoder.layers.1.conv1x3_1.weight', 'module.decoder.layers.1.conv1x3_1.bias', 'module.decoder.layers.1.bn1.weight', 'module.decoder.layers.1.bn1.bias', 'module.decoder.layers.1.bn1.running_mean', 'module.decoder.layers.1.bn1.running_var', 'module.decoder.layers.1.conv3x1_2.weight', 'module.decoder.layers.1.conv3x1_2.bias', 'module.decoder.layers.1.conv1x3_2.weight', 'module.decoder.layers.1.conv1x3_2.bias', 'module.decoder.layers.1.bn2.weight', 'module.decoder.layers.1.bn2.bias', 'module.decoder.layers.1.bn2.running_mean', 'module.decoder.layers.1.bn2.running_var', 'module.decoder.layers.2.conv3x1_1.weight', 'module.decoder.layers.2.conv3x1_1.bias', 'module.decoder.layers.2.conv1x3_1.weight', 'module.decoder.layers.2.conv1x3_1.bias', 'module.decoder.layers.2.bn1.weight', 'module.decoder.layers.2.bn1.bias', 'module.decoder.layers.2.bn1.running_mean', 'module.decoder.layers.2.bn1.running_var', 'module.decoder.layers.2.conv3x1_2.weight', 'module.decoder.layers.2.conv3x1_2.bias', 'module.decoder.layers.2.conv1x3_2.weight', 'module.decoder.layers.2.conv1x3_2.bias', 'module.decoder.layers.2.bn2.weight', 'module.decoder.layers.2.bn2.bias', 'module.decoder.layers.2.bn2.running_mean', 'module.decoder.layers.2.bn2.running_var', 'module.decoder.layers.3.conv.weight', 'module.decoder.layers.3.conv.bias', 'module.decoder.layers.3.bn.weight', 'module.decoder.layers.3.bn.bias', 'module.decoder.layers.3.bn.running_mean', 'module.decoder.layers.3.bn.running_var', 'module.decoder.layers.4.conv3x1_1.weight', 'module.decoder.layers.4.conv3x1_1.bias', 'module.decoder.layers.4.conv1x3_1.weight', 'module.decoder.layers.4.conv1x3_1.bias', 'module.decoder.layers.4.bn1.weight', 'module.decoder.layers.4.bn1.bias', 'module.decoder.layers.4.bn1.running_mean', 'module.decoder.layers.4.bn1.running_var', 'module.decoder.layers.4.conv3x1_2.weight', 'module.decoder.layers.4.conv3x1_2.bias', 'module.decoder.layers.4.conv1x3_2.weight', 'module.decoder.layers.4.conv1x3_2.bias', 'module.decoder.layers.4.bn2.weight', 'module.decoder.layers.4.bn2.bias', 'module.decoder.layers.4.bn2.running_mean', 'module.decoder.layers.4.bn2.running_var', 'module.decoder.layers.5.conv3x1_1.weight', 'module.decoder.layers.5.conv3x1_1.bias', 'module.decoder.layers.5.conv1x3_1.weight', 'module.decoder.layers.5.conv1x3_1.bias', 'module.decoder.layers.5.bn1.weight', 'module.decoder.layers.5.bn1.bias', 'module.decoder.layers.5.bn1.running_mean', 'module.decoder.layers.5.bn1.running_var', 'module.decoder.layers.5.conv3x1_2.weight', 'module.decoder.layers.5.conv3x1_2.bias', 'module.decoder.layers.5.conv1x3_2.weight', 'module.decoder.layers.5.conv1x3_2.bias', 'module.decoder.layers.5.bn2.weight', 'module.decoder.layers.5.bn2.bias', 'module.decoder.layers.5.bn2.running_mean', 'module.decoder.layers.5.bn2.running_var', 'module.decoder.output_conv.weight', 'module.decoder.output_conv.bias'])\n",
            "odict_keys(['module.encoder.initial_block.conv.weight', 'module.encoder.initial_block.conv.bias', 'module.encoder.initial_block.bn.weight', 'module.encoder.initial_block.bn.bias', 'module.encoder.initial_block.bn.running_mean', 'module.encoder.initial_block.bn.running_var', 'module.encoder.initial_block.bn.num_batches_tracked', 'module.encoder.layers.0.conv.weight', 'module.encoder.layers.0.conv.bias', 'module.encoder.layers.0.bn.weight', 'module.encoder.layers.0.bn.bias', 'module.encoder.layers.0.bn.running_mean', 'module.encoder.layers.0.bn.running_var', 'module.encoder.layers.0.bn.num_batches_tracked', 'module.encoder.layers.1.conv3x1_1.weight', 'module.encoder.layers.1.conv3x1_1.bias', 'module.encoder.layers.1.conv1x3_1.weight', 'module.encoder.layers.1.conv1x3_1.bias', 'module.encoder.layers.1.bn1.weight', 'module.encoder.layers.1.bn1.bias', 'module.encoder.layers.1.bn1.running_mean', 'module.encoder.layers.1.bn1.running_var', 'module.encoder.layers.1.bn1.num_batches_tracked', 'module.encoder.layers.1.conv3x1_2.weight', 'module.encoder.layers.1.conv3x1_2.bias', 'module.encoder.layers.1.conv1x3_2.weight', 'module.encoder.layers.1.conv1x3_2.bias', 'module.encoder.layers.1.bn2.weight', 'module.encoder.layers.1.bn2.bias', 'module.encoder.layers.1.bn2.running_mean', 'module.encoder.layers.1.bn2.running_var', 'module.encoder.layers.1.bn2.num_batches_tracked', 'module.encoder.layers.2.conv3x1_1.weight', 'module.encoder.layers.2.conv3x1_1.bias', 'module.encoder.layers.2.conv1x3_1.weight', 'module.encoder.layers.2.conv1x3_1.bias', 'module.encoder.layers.2.bn1.weight', 'module.encoder.layers.2.bn1.bias', 'module.encoder.layers.2.bn1.running_mean', 'module.encoder.layers.2.bn1.running_var', 'module.encoder.layers.2.bn1.num_batches_tracked', 'module.encoder.layers.2.conv3x1_2.weight', 'module.encoder.layers.2.conv3x1_2.bias', 'module.encoder.layers.2.conv1x3_2.weight', 'module.encoder.layers.2.conv1x3_2.bias', 'module.encoder.layers.2.bn2.weight', 'module.encoder.layers.2.bn2.bias', 'module.encoder.layers.2.bn2.running_mean', 'module.encoder.layers.2.bn2.running_var', 'module.encoder.layers.2.bn2.num_batches_tracked', 'module.encoder.layers.3.conv3x1_1.weight', 'module.encoder.layers.3.conv3x1_1.bias', 'module.encoder.layers.3.conv1x3_1.weight', 'module.encoder.layers.3.conv1x3_1.bias', 'module.encoder.layers.3.bn1.weight', 'module.encoder.layers.3.bn1.bias', 'module.encoder.layers.3.bn1.running_mean', 'module.encoder.layers.3.bn1.running_var', 'module.encoder.layers.3.bn1.num_batches_tracked', 'module.encoder.layers.3.conv3x1_2.weight', 'module.encoder.layers.3.conv3x1_2.bias', 'module.encoder.layers.3.conv1x3_2.weight', 'module.encoder.layers.3.conv1x3_2.bias', 'module.encoder.layers.3.bn2.weight', 'module.encoder.layers.3.bn2.bias', 'module.encoder.layers.3.bn2.running_mean', 'module.encoder.layers.3.bn2.running_var', 'module.encoder.layers.3.bn2.num_batches_tracked', 'module.encoder.layers.4.conv3x1_1.weight', 'module.encoder.layers.4.conv3x1_1.bias', 'module.encoder.layers.4.conv1x3_1.weight', 'module.encoder.layers.4.conv1x3_1.bias', 'module.encoder.layers.4.bn1.weight', 'module.encoder.layers.4.bn1.bias', 'module.encoder.layers.4.bn1.running_mean', 'module.encoder.layers.4.bn1.running_var', 'module.encoder.layers.4.bn1.num_batches_tracked', 'module.encoder.layers.4.conv3x1_2.weight', 'module.encoder.layers.4.conv3x1_2.bias', 'module.encoder.layers.4.conv1x3_2.weight', 'module.encoder.layers.4.conv1x3_2.bias', 'module.encoder.layers.4.bn2.weight', 'module.encoder.layers.4.bn2.bias', 'module.encoder.layers.4.bn2.running_mean', 'module.encoder.layers.4.bn2.running_var', 'module.encoder.layers.4.bn2.num_batches_tracked', 'module.encoder.layers.5.conv3x1_1.weight', 'module.encoder.layers.5.conv3x1_1.bias', 'module.encoder.layers.5.conv1x3_1.weight', 'module.encoder.layers.5.conv1x3_1.bias', 'module.encoder.layers.5.bn1.weight', 'module.encoder.layers.5.bn1.bias', 'module.encoder.layers.5.bn1.running_mean', 'module.encoder.layers.5.bn1.running_var', 'module.encoder.layers.5.bn1.num_batches_tracked', 'module.encoder.layers.5.conv3x1_2.weight', 'module.encoder.layers.5.conv3x1_2.bias', 'module.encoder.layers.5.conv1x3_2.weight', 'module.encoder.layers.5.conv1x3_2.bias', 'module.encoder.layers.5.bn2.weight', 'module.encoder.layers.5.bn2.bias', 'module.encoder.layers.5.bn2.running_mean', 'module.encoder.layers.5.bn2.running_var', 'module.encoder.layers.5.bn2.num_batches_tracked', 'module.encoder.layers.6.conv.weight', 'module.encoder.layers.6.conv.bias', 'module.encoder.layers.6.bn.weight', 'module.encoder.layers.6.bn.bias', 'module.encoder.layers.6.bn.running_mean', 'module.encoder.layers.6.bn.running_var', 'module.encoder.layers.6.bn.num_batches_tracked', 'module.encoder.layers.7.conv3x1_1.weight', 'module.encoder.layers.7.conv3x1_1.bias', 'module.encoder.layers.7.conv1x3_1.weight', 'module.encoder.layers.7.conv1x3_1.bias', 'module.encoder.layers.7.bn1.weight', 'module.encoder.layers.7.bn1.bias', 'module.encoder.layers.7.bn1.running_mean', 'module.encoder.layers.7.bn1.running_var', 'module.encoder.layers.7.bn1.num_batches_tracked', 'module.encoder.layers.7.conv3x1_2.weight', 'module.encoder.layers.7.conv3x1_2.bias', 'module.encoder.layers.7.conv1x3_2.weight', 'module.encoder.layers.7.conv1x3_2.bias', 'module.encoder.layers.7.bn2.weight', 'module.encoder.layers.7.bn2.bias', 'module.encoder.layers.7.bn2.running_mean', 'module.encoder.layers.7.bn2.running_var', 'module.encoder.layers.7.bn2.num_batches_tracked', 'module.encoder.layers.8.conv3x1_1.weight', 'module.encoder.layers.8.conv3x1_1.bias', 'module.encoder.layers.8.conv1x3_1.weight', 'module.encoder.layers.8.conv1x3_1.bias', 'module.encoder.layers.8.bn1.weight', 'module.encoder.layers.8.bn1.bias', 'module.encoder.layers.8.bn1.running_mean', 'module.encoder.layers.8.bn1.running_var', 'module.encoder.layers.8.bn1.num_batches_tracked', 'module.encoder.layers.8.conv3x1_2.weight', 'module.encoder.layers.8.conv3x1_2.bias', 'module.encoder.layers.8.conv1x3_2.weight', 'module.encoder.layers.8.conv1x3_2.bias', 'module.encoder.layers.8.bn2.weight', 'module.encoder.layers.8.bn2.bias', 'module.encoder.layers.8.bn2.running_mean', 'module.encoder.layers.8.bn2.running_var', 'module.encoder.layers.8.bn2.num_batches_tracked', 'module.encoder.layers.9.conv3x1_1.weight', 'module.encoder.layers.9.conv3x1_1.bias', 'module.encoder.layers.9.conv1x3_1.weight', 'module.encoder.layers.9.conv1x3_1.bias', 'module.encoder.layers.9.bn1.weight', 'module.encoder.layers.9.bn1.bias', 'module.encoder.layers.9.bn1.running_mean', 'module.encoder.layers.9.bn1.running_var', 'module.encoder.layers.9.bn1.num_batches_tracked', 'module.encoder.layers.9.conv3x1_2.weight', 'module.encoder.layers.9.conv3x1_2.bias', 'module.encoder.layers.9.conv1x3_2.weight', 'module.encoder.layers.9.conv1x3_2.bias', 'module.encoder.layers.9.bn2.weight', 'module.encoder.layers.9.bn2.bias', 'module.encoder.layers.9.bn2.running_mean', 'module.encoder.layers.9.bn2.running_var', 'module.encoder.layers.9.bn2.num_batches_tracked', 'module.encoder.layers.10.conv3x1_1.weight', 'module.encoder.layers.10.conv3x1_1.bias', 'module.encoder.layers.10.conv1x3_1.weight', 'module.encoder.layers.10.conv1x3_1.bias', 'module.encoder.layers.10.bn1.weight', 'module.encoder.layers.10.bn1.bias', 'module.encoder.layers.10.bn1.running_mean', 'module.encoder.layers.10.bn1.running_var', 'module.encoder.layers.10.bn1.num_batches_tracked', 'module.encoder.layers.10.conv3x1_2.weight', 'module.encoder.layers.10.conv3x1_2.bias', 'module.encoder.layers.10.conv1x3_2.weight', 'module.encoder.layers.10.conv1x3_2.bias', 'module.encoder.layers.10.bn2.weight', 'module.encoder.layers.10.bn2.bias', 'module.encoder.layers.10.bn2.running_mean', 'module.encoder.layers.10.bn2.running_var', 'module.encoder.layers.10.bn2.num_batches_tracked', 'module.encoder.layers.11.conv3x1_1.weight', 'module.encoder.layers.11.conv3x1_1.bias', 'module.encoder.layers.11.conv1x3_1.weight', 'module.encoder.layers.11.conv1x3_1.bias', 'module.encoder.layers.11.bn1.weight', 'module.encoder.layers.11.bn1.bias', 'module.encoder.layers.11.bn1.running_mean', 'module.encoder.layers.11.bn1.running_var', 'module.encoder.layers.11.bn1.num_batches_tracked', 'module.encoder.layers.11.conv3x1_2.weight', 'module.encoder.layers.11.conv3x1_2.bias', 'module.encoder.layers.11.conv1x3_2.weight', 'module.encoder.layers.11.conv1x3_2.bias', 'module.encoder.layers.11.bn2.weight', 'module.encoder.layers.11.bn2.bias', 'module.encoder.layers.11.bn2.running_mean', 'module.encoder.layers.11.bn2.running_var', 'module.encoder.layers.11.bn2.num_batches_tracked', 'module.encoder.layers.12.conv3x1_1.weight', 'module.encoder.layers.12.conv3x1_1.bias', 'module.encoder.layers.12.conv1x3_1.weight', 'module.encoder.layers.12.conv1x3_1.bias', 'module.encoder.layers.12.bn1.weight', 'module.encoder.layers.12.bn1.bias', 'module.encoder.layers.12.bn1.running_mean', 'module.encoder.layers.12.bn1.running_var', 'module.encoder.layers.12.bn1.num_batches_tracked', 'module.encoder.layers.12.conv3x1_2.weight', 'module.encoder.layers.12.conv3x1_2.bias', 'module.encoder.layers.12.conv1x3_2.weight', 'module.encoder.layers.12.conv1x3_2.bias', 'module.encoder.layers.12.bn2.weight', 'module.encoder.layers.12.bn2.bias', 'module.encoder.layers.12.bn2.running_mean', 'module.encoder.layers.12.bn2.running_var', 'module.encoder.layers.12.bn2.num_batches_tracked', 'module.encoder.layers.13.conv3x1_1.weight', 'module.encoder.layers.13.conv3x1_1.bias', 'module.encoder.layers.13.conv1x3_1.weight', 'module.encoder.layers.13.conv1x3_1.bias', 'module.encoder.layers.13.bn1.weight', 'module.encoder.layers.13.bn1.bias', 'module.encoder.layers.13.bn1.running_mean', 'module.encoder.layers.13.bn1.running_var', 'module.encoder.layers.13.bn1.num_batches_tracked', 'module.encoder.layers.13.conv3x1_2.weight', 'module.encoder.layers.13.conv3x1_2.bias', 'module.encoder.layers.13.conv1x3_2.weight', 'module.encoder.layers.13.conv1x3_2.bias', 'module.encoder.layers.13.bn2.weight', 'module.encoder.layers.13.bn2.bias', 'module.encoder.layers.13.bn2.running_mean', 'module.encoder.layers.13.bn2.running_var', 'module.encoder.layers.13.bn2.num_batches_tracked', 'module.encoder.layers.14.conv3x1_1.weight', 'module.encoder.layers.14.conv3x1_1.bias', 'module.encoder.layers.14.conv1x3_1.weight', 'module.encoder.layers.14.conv1x3_1.bias', 'module.encoder.layers.14.bn1.weight', 'module.encoder.layers.14.bn1.bias', 'module.encoder.layers.14.bn1.running_mean', 'module.encoder.layers.14.bn1.running_var', 'module.encoder.layers.14.bn1.num_batches_tracked', 'module.encoder.layers.14.conv3x1_2.weight', 'module.encoder.layers.14.conv3x1_2.bias', 'module.encoder.layers.14.conv1x3_2.weight', 'module.encoder.layers.14.conv1x3_2.bias', 'module.encoder.layers.14.bn2.weight', 'module.encoder.layers.14.bn2.bias', 'module.encoder.layers.14.bn2.running_mean', 'module.encoder.layers.14.bn2.running_var', 'module.encoder.layers.14.bn2.num_batches_tracked', 'module.encoder.output_conv.weight', 'module.encoder.output_conv.bias', 'module.decoder.layers.0.conv.weight', 'module.decoder.layers.0.conv.bias', 'module.decoder.layers.0.bn.weight', 'module.decoder.layers.0.bn.bias', 'module.decoder.layers.0.bn.running_mean', 'module.decoder.layers.0.bn.running_var', 'module.decoder.layers.0.bn.num_batches_tracked', 'module.decoder.layers.1.conv3x1_1.weight', 'module.decoder.layers.1.conv3x1_1.bias', 'module.decoder.layers.1.conv1x3_1.weight', 'module.decoder.layers.1.conv1x3_1.bias', 'module.decoder.layers.1.bn1.weight', 'module.decoder.layers.1.bn1.bias', 'module.decoder.layers.1.bn1.running_mean', 'module.decoder.layers.1.bn1.running_var', 'module.decoder.layers.1.bn1.num_batches_tracked', 'module.decoder.layers.1.conv3x1_2.weight', 'module.decoder.layers.1.conv3x1_2.bias', 'module.decoder.layers.1.conv1x3_2.weight', 'module.decoder.layers.1.conv1x3_2.bias', 'module.decoder.layers.1.bn2.weight', 'module.decoder.layers.1.bn2.bias', 'module.decoder.layers.1.bn2.running_mean', 'module.decoder.layers.1.bn2.running_var', 'module.decoder.layers.1.bn2.num_batches_tracked', 'module.decoder.layers.2.conv3x1_1.weight', 'module.decoder.layers.2.conv3x1_1.bias', 'module.decoder.layers.2.conv1x3_1.weight', 'module.decoder.layers.2.conv1x3_1.bias', 'module.decoder.layers.2.bn1.weight', 'module.decoder.layers.2.bn1.bias', 'module.decoder.layers.2.bn1.running_mean', 'module.decoder.layers.2.bn1.running_var', 'module.decoder.layers.2.bn1.num_batches_tracked', 'module.decoder.layers.2.conv3x1_2.weight', 'module.decoder.layers.2.conv3x1_2.bias', 'module.decoder.layers.2.conv1x3_2.weight', 'module.decoder.layers.2.conv1x3_2.bias', 'module.decoder.layers.2.bn2.weight', 'module.decoder.layers.2.bn2.bias', 'module.decoder.layers.2.bn2.running_mean', 'module.decoder.layers.2.bn2.running_var', 'module.decoder.layers.2.bn2.num_batches_tracked', 'module.decoder.layers.3.conv.weight', 'module.decoder.layers.3.conv.bias', 'module.decoder.layers.3.bn.weight', 'module.decoder.layers.3.bn.bias', 'module.decoder.layers.3.bn.running_mean', 'module.decoder.layers.3.bn.running_var', 'module.decoder.layers.3.bn.num_batches_tracked', 'module.decoder.layers.4.conv3x1_1.weight', 'module.decoder.layers.4.conv3x1_1.bias', 'module.decoder.layers.4.conv1x3_1.weight', 'module.decoder.layers.4.conv1x3_1.bias', 'module.decoder.layers.4.bn1.weight', 'module.decoder.layers.4.bn1.bias', 'module.decoder.layers.4.bn1.running_mean', 'module.decoder.layers.4.bn1.running_var', 'module.decoder.layers.4.bn1.num_batches_tracked', 'module.decoder.layers.4.conv3x1_2.weight', 'module.decoder.layers.4.conv3x1_2.bias', 'module.decoder.layers.4.conv1x3_2.weight', 'module.decoder.layers.4.conv1x3_2.bias', 'module.decoder.layers.4.bn2.weight', 'module.decoder.layers.4.bn2.bias', 'module.decoder.layers.4.bn2.running_mean', 'module.decoder.layers.4.bn2.running_var', 'module.decoder.layers.4.bn2.num_batches_tracked', 'module.decoder.layers.5.conv3x1_1.weight', 'module.decoder.layers.5.conv3x1_1.bias', 'module.decoder.layers.5.conv1x3_1.weight', 'module.decoder.layers.5.conv1x3_1.bias', 'module.decoder.layers.5.bn1.weight', 'module.decoder.layers.5.bn1.bias', 'module.decoder.layers.5.bn1.running_mean', 'module.decoder.layers.5.bn1.running_var', 'module.decoder.layers.5.bn1.num_batches_tracked', 'module.decoder.layers.5.conv3x1_2.weight', 'module.decoder.layers.5.conv3x1_2.bias', 'module.decoder.layers.5.conv1x3_2.weight', 'module.decoder.layers.5.conv1x3_2.bias', 'module.decoder.layers.5.bn2.weight', 'module.decoder.layers.5.bn2.bias', 'module.decoder.layers.5.bn2.running_mean', 'module.decoder.layers.5.bn2.running_var', 'module.decoder.layers.5.bn2.num_batches_tracked', 'module.decoder.output_conv.weight', 'module.decoder.output_conv.bias'])\n",
            "Model and weights LOADED successfully\n",
            "100% 2975/2975 [07:55<00:00,  6.26it/s]\n",
            "Mean per class: torch.Size([19, 19])\n",
            "Mean output saved as '/content/AnomalySegmentation/save/mean_cityscapes_erfnet.npy'\n",
            "Loading model: erfnet\n",
            "Loading weights: /content/AnomalySegmentation/trained_models/erfnet_pretrained.pth\n",
            "pre_computed_mean torch.Size([19, 19])\n",
            "/content/cityscapes/leftImg8bit/train\n",
            "/content/AnomalySegmentation/eval/mahalanobis.py:90: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model = load_my_state_dict(model, torch.load(weightspath, map_location=lambda storage, loc: storage))\n",
            "odict_keys(['module.encoder.initial_block.conv.weight', 'module.encoder.initial_block.conv.bias', 'module.encoder.initial_block.bn.weight', 'module.encoder.initial_block.bn.bias', 'module.encoder.initial_block.bn.running_mean', 'module.encoder.initial_block.bn.running_var', 'module.encoder.layers.0.conv.weight', 'module.encoder.layers.0.conv.bias', 'module.encoder.layers.0.bn.weight', 'module.encoder.layers.0.bn.bias', 'module.encoder.layers.0.bn.running_mean', 'module.encoder.layers.0.bn.running_var', 'module.encoder.layers.1.conv3x1_1.weight', 'module.encoder.layers.1.conv3x1_1.bias', 'module.encoder.layers.1.conv1x3_1.weight', 'module.encoder.layers.1.conv1x3_1.bias', 'module.encoder.layers.1.conv3x1_2.weight', 'module.encoder.layers.1.conv3x1_2.bias', 'module.encoder.layers.1.conv1x3_2.weight', 'module.encoder.layers.1.conv1x3_2.bias', 'module.encoder.layers.1.bn1.weight', 'module.encoder.layers.1.bn1.bias', 'module.encoder.layers.1.bn1.running_mean', 'module.encoder.layers.1.bn1.running_var', 'module.encoder.layers.1.bn2.weight', 'module.encoder.layers.1.bn2.bias', 'module.encoder.layers.1.bn2.running_mean', 'module.encoder.layers.1.bn2.running_var', 'module.encoder.layers.2.conv3x1_1.weight', 'module.encoder.layers.2.conv3x1_1.bias', 'module.encoder.layers.2.conv1x3_1.weight', 'module.encoder.layers.2.conv1x3_1.bias', 'module.encoder.layers.2.conv3x1_2.weight', 'module.encoder.layers.2.conv3x1_2.bias', 'module.encoder.layers.2.conv1x3_2.weight', 'module.encoder.layers.2.conv1x3_2.bias', 'module.encoder.layers.2.bn1.weight', 'module.encoder.layers.2.bn1.bias', 'module.encoder.layers.2.bn1.running_mean', 'module.encoder.layers.2.bn1.running_var', 'module.encoder.layers.2.bn2.weight', 'module.encoder.layers.2.bn2.bias', 'module.encoder.layers.2.bn2.running_mean', 'module.encoder.layers.2.bn2.running_var', 'module.encoder.layers.3.conv3x1_1.weight', 'module.encoder.layers.3.conv3x1_1.bias', 'module.encoder.layers.3.conv1x3_1.weight', 'module.encoder.layers.3.conv1x3_1.bias', 'module.encoder.layers.3.conv3x1_2.weight', 'module.encoder.layers.3.conv3x1_2.bias', 'module.encoder.layers.3.conv1x3_2.weight', 'module.encoder.layers.3.conv1x3_2.bias', 'module.encoder.layers.3.bn1.weight', 'module.encoder.layers.3.bn1.bias', 'module.encoder.layers.3.bn1.running_mean', 'module.encoder.layers.3.bn1.running_var', 'module.encoder.layers.3.bn2.weight', 'module.encoder.layers.3.bn2.bias', 'module.encoder.layers.3.bn2.running_mean', 'module.encoder.layers.3.bn2.running_var', 'module.encoder.layers.4.conv3x1_1.weight', 'module.encoder.layers.4.conv3x1_1.bias', 'module.encoder.layers.4.conv1x3_1.weight', 'module.encoder.layers.4.conv1x3_1.bias', 'module.encoder.layers.4.conv3x1_2.weight', 'module.encoder.layers.4.conv3x1_2.bias', 'module.encoder.layers.4.conv1x3_2.weight', 'module.encoder.layers.4.conv1x3_2.bias', 'module.encoder.layers.4.bn1.weight', 'module.encoder.layers.4.bn1.bias', 'module.encoder.layers.4.bn1.running_mean', 'module.encoder.layers.4.bn1.running_var', 'module.encoder.layers.4.bn2.weight', 'module.encoder.layers.4.bn2.bias', 'module.encoder.layers.4.bn2.running_mean', 'module.encoder.layers.4.bn2.running_var', 'module.encoder.layers.5.conv3x1_1.weight', 'module.encoder.layers.5.conv3x1_1.bias', 'module.encoder.layers.5.conv1x3_1.weight', 'module.encoder.layers.5.conv1x3_1.bias', 'module.encoder.layers.5.conv3x1_2.weight', 'module.encoder.layers.5.conv3x1_2.bias', 'module.encoder.layers.5.conv1x3_2.weight', 'module.encoder.layers.5.conv1x3_2.bias', 'module.encoder.layers.5.bn1.weight', 'module.encoder.layers.5.bn1.bias', 'module.encoder.layers.5.bn1.running_mean', 'module.encoder.layers.5.bn1.running_var', 'module.encoder.layers.5.bn2.weight', 'module.encoder.layers.5.bn2.bias', 'module.encoder.layers.5.bn2.running_mean', 'module.encoder.layers.5.bn2.running_var', 'module.encoder.layers.6.conv.weight', 'module.encoder.layers.6.conv.bias', 'module.encoder.layers.6.bn.weight', 'module.encoder.layers.6.bn.bias', 'module.encoder.layers.6.bn.running_mean', 'module.encoder.layers.6.bn.running_var', 'module.encoder.layers.7.conv3x1_1.weight', 'module.encoder.layers.7.conv3x1_1.bias', 'module.encoder.layers.7.conv1x3_1.weight', 'module.encoder.layers.7.conv1x3_1.bias', 'module.encoder.layers.7.conv3x1_2.weight', 'module.encoder.layers.7.conv3x1_2.bias', 'module.encoder.layers.7.conv1x3_2.weight', 'module.encoder.layers.7.conv1x3_2.bias', 'module.encoder.layers.7.bn1.weight', 'module.encoder.layers.7.bn1.bias', 'module.encoder.layers.7.bn1.running_mean', 'module.encoder.layers.7.bn1.running_var', 'module.encoder.layers.7.bn2.weight', 'module.encoder.layers.7.bn2.bias', 'module.encoder.layers.7.bn2.running_mean', 'module.encoder.layers.7.bn2.running_var', 'module.encoder.layers.8.conv3x1_1.weight', 'module.encoder.layers.8.conv3x1_1.bias', 'module.encoder.layers.8.conv1x3_1.weight', 'module.encoder.layers.8.conv1x3_1.bias', 'module.encoder.layers.8.conv3x1_2.weight', 'module.encoder.layers.8.conv3x1_2.bias', 'module.encoder.layers.8.conv1x3_2.weight', 'module.encoder.layers.8.conv1x3_2.bias', 'module.encoder.layers.8.bn1.weight', 'module.encoder.layers.8.bn1.bias', 'module.encoder.layers.8.bn1.running_mean', 'module.encoder.layers.8.bn1.running_var', 'module.encoder.layers.8.bn2.weight', 'module.encoder.layers.8.bn2.bias', 'module.encoder.layers.8.bn2.running_mean', 'module.encoder.layers.8.bn2.running_var', 'module.encoder.layers.9.conv3x1_1.weight', 'module.encoder.layers.9.conv3x1_1.bias', 'module.encoder.layers.9.conv1x3_1.weight', 'module.encoder.layers.9.conv1x3_1.bias', 'module.encoder.layers.9.conv3x1_2.weight', 'module.encoder.layers.9.conv3x1_2.bias', 'module.encoder.layers.9.conv1x3_2.weight', 'module.encoder.layers.9.conv1x3_2.bias', 'module.encoder.layers.9.bn1.weight', 'module.encoder.layers.9.bn1.bias', 'module.encoder.layers.9.bn1.running_mean', 'module.encoder.layers.9.bn1.running_var', 'module.encoder.layers.9.bn2.weight', 'module.encoder.layers.9.bn2.bias', 'module.encoder.layers.9.bn2.running_mean', 'module.encoder.layers.9.bn2.running_var', 'module.encoder.layers.10.conv3x1_1.weight', 'module.encoder.layers.10.conv3x1_1.bias', 'module.encoder.layers.10.conv1x3_1.weight', 'module.encoder.layers.10.conv1x3_1.bias', 'module.encoder.layers.10.conv3x1_2.weight', 'module.encoder.layers.10.conv3x1_2.bias', 'module.encoder.layers.10.conv1x3_2.weight', 'module.encoder.layers.10.conv1x3_2.bias', 'module.encoder.layers.10.bn1.weight', 'module.encoder.layers.10.bn1.bias', 'module.encoder.layers.10.bn1.running_mean', 'module.encoder.layers.10.bn1.running_var', 'module.encoder.layers.10.bn2.weight', 'module.encoder.layers.10.bn2.bias', 'module.encoder.layers.10.bn2.running_mean', 'module.encoder.layers.10.bn2.running_var', 'module.encoder.layers.11.conv3x1_1.weight', 'module.encoder.layers.11.conv3x1_1.bias', 'module.encoder.layers.11.conv1x3_1.weight', 'module.encoder.layers.11.conv1x3_1.bias', 'module.encoder.layers.11.conv3x1_2.weight', 'module.encoder.layers.11.conv3x1_2.bias', 'module.encoder.layers.11.conv1x3_2.weight', 'module.encoder.layers.11.conv1x3_2.bias', 'module.encoder.layers.11.bn1.weight', 'module.encoder.layers.11.bn1.bias', 'module.encoder.layers.11.bn1.running_mean', 'module.encoder.layers.11.bn1.running_var', 'module.encoder.layers.11.bn2.weight', 'module.encoder.layers.11.bn2.bias', 'module.encoder.layers.11.bn2.running_mean', 'module.encoder.layers.11.bn2.running_var', 'module.encoder.layers.12.conv3x1_1.weight', 'module.encoder.layers.12.conv3x1_1.bias', 'module.encoder.layers.12.conv1x3_1.weight', 'module.encoder.layers.12.conv1x3_1.bias', 'module.encoder.layers.12.conv3x1_2.weight', 'module.encoder.layers.12.conv3x1_2.bias', 'module.encoder.layers.12.conv1x3_2.weight', 'module.encoder.layers.12.conv1x3_2.bias', 'module.encoder.layers.12.bn1.weight', 'module.encoder.layers.12.bn1.bias', 'module.encoder.layers.12.bn1.running_mean', 'module.encoder.layers.12.bn1.running_var', 'module.encoder.layers.12.bn2.weight', 'module.encoder.layers.12.bn2.bias', 'module.encoder.layers.12.bn2.running_mean', 'module.encoder.layers.12.bn2.running_var', 'module.encoder.layers.13.conv3x1_1.weight', 'module.encoder.layers.13.conv3x1_1.bias', 'module.encoder.layers.13.conv1x3_1.weight', 'module.encoder.layers.13.conv1x3_1.bias', 'module.encoder.layers.13.conv3x1_2.weight', 'module.encoder.layers.13.conv3x1_2.bias', 'module.encoder.layers.13.conv1x3_2.weight', 'module.encoder.layers.13.conv1x3_2.bias', 'module.encoder.layers.13.bn1.weight', 'module.encoder.layers.13.bn1.bias', 'module.encoder.layers.13.bn1.running_mean', 'module.encoder.layers.13.bn1.running_var', 'module.encoder.layers.13.bn2.weight', 'module.encoder.layers.13.bn2.bias', 'module.encoder.layers.13.bn2.running_mean', 'module.encoder.layers.13.bn2.running_var', 'module.encoder.layers.14.conv3x1_1.weight', 'module.encoder.layers.14.conv3x1_1.bias', 'module.encoder.layers.14.conv1x3_1.weight', 'module.encoder.layers.14.conv1x3_1.bias', 'module.encoder.layers.14.conv3x1_2.weight', 'module.encoder.layers.14.conv3x1_2.bias', 'module.encoder.layers.14.conv1x3_2.weight', 'module.encoder.layers.14.conv1x3_2.bias', 'module.encoder.layers.14.bn1.weight', 'module.encoder.layers.14.bn1.bias', 'module.encoder.layers.14.bn1.running_mean', 'module.encoder.layers.14.bn1.running_var', 'module.encoder.layers.14.bn2.weight', 'module.encoder.layers.14.bn2.bias', 'module.encoder.layers.14.bn2.running_mean', 'module.encoder.layers.14.bn2.running_var', 'module.decoder.layers.0.conv.weight', 'module.decoder.layers.0.conv.bias', 'module.decoder.layers.0.bn.weight', 'module.decoder.layers.0.bn.bias', 'module.decoder.layers.0.bn.running_mean', 'module.decoder.layers.0.bn.running_var', 'module.decoder.layers.1.conv3x1_1.weight', 'module.decoder.layers.1.conv3x1_1.bias', 'module.decoder.layers.1.conv1x3_1.weight', 'module.decoder.layers.1.conv1x3_1.bias', 'module.decoder.layers.1.bn1.weight', 'module.decoder.layers.1.bn1.bias', 'module.decoder.layers.1.bn1.running_mean', 'module.decoder.layers.1.bn1.running_var', 'module.decoder.layers.1.conv3x1_2.weight', 'module.decoder.layers.1.conv3x1_2.bias', 'module.decoder.layers.1.conv1x3_2.weight', 'module.decoder.layers.1.conv1x3_2.bias', 'module.decoder.layers.1.bn2.weight', 'module.decoder.layers.1.bn2.bias', 'module.decoder.layers.1.bn2.running_mean', 'module.decoder.layers.1.bn2.running_var', 'module.decoder.layers.2.conv3x1_1.weight', 'module.decoder.layers.2.conv3x1_1.bias', 'module.decoder.layers.2.conv1x3_1.weight', 'module.decoder.layers.2.conv1x3_1.bias', 'module.decoder.layers.2.bn1.weight', 'module.decoder.layers.2.bn1.bias', 'module.decoder.layers.2.bn1.running_mean', 'module.decoder.layers.2.bn1.running_var', 'module.decoder.layers.2.conv3x1_2.weight', 'module.decoder.layers.2.conv3x1_2.bias', 'module.decoder.layers.2.conv1x3_2.weight', 'module.decoder.layers.2.conv1x3_2.bias', 'module.decoder.layers.2.bn2.weight', 'module.decoder.layers.2.bn2.bias', 'module.decoder.layers.2.bn2.running_mean', 'module.decoder.layers.2.bn2.running_var', 'module.decoder.layers.3.conv.weight', 'module.decoder.layers.3.conv.bias', 'module.decoder.layers.3.bn.weight', 'module.decoder.layers.3.bn.bias', 'module.decoder.layers.3.bn.running_mean', 'module.decoder.layers.3.bn.running_var', 'module.decoder.layers.4.conv3x1_1.weight', 'module.decoder.layers.4.conv3x1_1.bias', 'module.decoder.layers.4.conv1x3_1.weight', 'module.decoder.layers.4.conv1x3_1.bias', 'module.decoder.layers.4.bn1.weight', 'module.decoder.layers.4.bn1.bias', 'module.decoder.layers.4.bn1.running_mean', 'module.decoder.layers.4.bn1.running_var', 'module.decoder.layers.4.conv3x1_2.weight', 'module.decoder.layers.4.conv3x1_2.bias', 'module.decoder.layers.4.conv1x3_2.weight', 'module.decoder.layers.4.conv1x3_2.bias', 'module.decoder.layers.4.bn2.weight', 'module.decoder.layers.4.bn2.bias', 'module.decoder.layers.4.bn2.running_mean', 'module.decoder.layers.4.bn2.running_var', 'module.decoder.layers.5.conv3x1_1.weight', 'module.decoder.layers.5.conv3x1_1.bias', 'module.decoder.layers.5.conv1x3_1.weight', 'module.decoder.layers.5.conv1x3_1.bias', 'module.decoder.layers.5.bn1.weight', 'module.decoder.layers.5.bn1.bias', 'module.decoder.layers.5.bn1.running_mean', 'module.decoder.layers.5.bn1.running_var', 'module.decoder.layers.5.conv3x1_2.weight', 'module.decoder.layers.5.conv3x1_2.bias', 'module.decoder.layers.5.conv1x3_2.weight', 'module.decoder.layers.5.conv1x3_2.bias', 'module.decoder.layers.5.bn2.weight', 'module.decoder.layers.5.bn2.bias', 'module.decoder.layers.5.bn2.running_mean', 'module.decoder.layers.5.bn2.running_var', 'module.decoder.output_conv.weight', 'module.decoder.output_conv.bias'])\n",
            "odict_keys(['module.encoder.initial_block.conv.weight', 'module.encoder.initial_block.conv.bias', 'module.encoder.initial_block.bn.weight', 'module.encoder.initial_block.bn.bias', 'module.encoder.initial_block.bn.running_mean', 'module.encoder.initial_block.bn.running_var', 'module.encoder.initial_block.bn.num_batches_tracked', 'module.encoder.layers.0.conv.weight', 'module.encoder.layers.0.conv.bias', 'module.encoder.layers.0.bn.weight', 'module.encoder.layers.0.bn.bias', 'module.encoder.layers.0.bn.running_mean', 'module.encoder.layers.0.bn.running_var', 'module.encoder.layers.0.bn.num_batches_tracked', 'module.encoder.layers.1.conv3x1_1.weight', 'module.encoder.layers.1.conv3x1_1.bias', 'module.encoder.layers.1.conv1x3_1.weight', 'module.encoder.layers.1.conv1x3_1.bias', 'module.encoder.layers.1.bn1.weight', 'module.encoder.layers.1.bn1.bias', 'module.encoder.layers.1.bn1.running_mean', 'module.encoder.layers.1.bn1.running_var', 'module.encoder.layers.1.bn1.num_batches_tracked', 'module.encoder.layers.1.conv3x1_2.weight', 'module.encoder.layers.1.conv3x1_2.bias', 'module.encoder.layers.1.conv1x3_2.weight', 'module.encoder.layers.1.conv1x3_2.bias', 'module.encoder.layers.1.bn2.weight', 'module.encoder.layers.1.bn2.bias', 'module.encoder.layers.1.bn2.running_mean', 'module.encoder.layers.1.bn2.running_var', 'module.encoder.layers.1.bn2.num_batches_tracked', 'module.encoder.layers.2.conv3x1_1.weight', 'module.encoder.layers.2.conv3x1_1.bias', 'module.encoder.layers.2.conv1x3_1.weight', 'module.encoder.layers.2.conv1x3_1.bias', 'module.encoder.layers.2.bn1.weight', 'module.encoder.layers.2.bn1.bias', 'module.encoder.layers.2.bn1.running_mean', 'module.encoder.layers.2.bn1.running_var', 'module.encoder.layers.2.bn1.num_batches_tracked', 'module.encoder.layers.2.conv3x1_2.weight', 'module.encoder.layers.2.conv3x1_2.bias', 'module.encoder.layers.2.conv1x3_2.weight', 'module.encoder.layers.2.conv1x3_2.bias', 'module.encoder.layers.2.bn2.weight', 'module.encoder.layers.2.bn2.bias', 'module.encoder.layers.2.bn2.running_mean', 'module.encoder.layers.2.bn2.running_var', 'module.encoder.layers.2.bn2.num_batches_tracked', 'module.encoder.layers.3.conv3x1_1.weight', 'module.encoder.layers.3.conv3x1_1.bias', 'module.encoder.layers.3.conv1x3_1.weight', 'module.encoder.layers.3.conv1x3_1.bias', 'module.encoder.layers.3.bn1.weight', 'module.encoder.layers.3.bn1.bias', 'module.encoder.layers.3.bn1.running_mean', 'module.encoder.layers.3.bn1.running_var', 'module.encoder.layers.3.bn1.num_batches_tracked', 'module.encoder.layers.3.conv3x1_2.weight', 'module.encoder.layers.3.conv3x1_2.bias', 'module.encoder.layers.3.conv1x3_2.weight', 'module.encoder.layers.3.conv1x3_2.bias', 'module.encoder.layers.3.bn2.weight', 'module.encoder.layers.3.bn2.bias', 'module.encoder.layers.3.bn2.running_mean', 'module.encoder.layers.3.bn2.running_var', 'module.encoder.layers.3.bn2.num_batches_tracked', 'module.encoder.layers.4.conv3x1_1.weight', 'module.encoder.layers.4.conv3x1_1.bias', 'module.encoder.layers.4.conv1x3_1.weight', 'module.encoder.layers.4.conv1x3_1.bias', 'module.encoder.layers.4.bn1.weight', 'module.encoder.layers.4.bn1.bias', 'module.encoder.layers.4.bn1.running_mean', 'module.encoder.layers.4.bn1.running_var', 'module.encoder.layers.4.bn1.num_batches_tracked', 'module.encoder.layers.4.conv3x1_2.weight', 'module.encoder.layers.4.conv3x1_2.bias', 'module.encoder.layers.4.conv1x3_2.weight', 'module.encoder.layers.4.conv1x3_2.bias', 'module.encoder.layers.4.bn2.weight', 'module.encoder.layers.4.bn2.bias', 'module.encoder.layers.4.bn2.running_mean', 'module.encoder.layers.4.bn2.running_var', 'module.encoder.layers.4.bn2.num_batches_tracked', 'module.encoder.layers.5.conv3x1_1.weight', 'module.encoder.layers.5.conv3x1_1.bias', 'module.encoder.layers.5.conv1x3_1.weight', 'module.encoder.layers.5.conv1x3_1.bias', 'module.encoder.layers.5.bn1.weight', 'module.encoder.layers.5.bn1.bias', 'module.encoder.layers.5.bn1.running_mean', 'module.encoder.layers.5.bn1.running_var', 'module.encoder.layers.5.bn1.num_batches_tracked', 'module.encoder.layers.5.conv3x1_2.weight', 'module.encoder.layers.5.conv3x1_2.bias', 'module.encoder.layers.5.conv1x3_2.weight', 'module.encoder.layers.5.conv1x3_2.bias', 'module.encoder.layers.5.bn2.weight', 'module.encoder.layers.5.bn2.bias', 'module.encoder.layers.5.bn2.running_mean', 'module.encoder.layers.5.bn2.running_var', 'module.encoder.layers.5.bn2.num_batches_tracked', 'module.encoder.layers.6.conv.weight', 'module.encoder.layers.6.conv.bias', 'module.encoder.layers.6.bn.weight', 'module.encoder.layers.6.bn.bias', 'module.encoder.layers.6.bn.running_mean', 'module.encoder.layers.6.bn.running_var', 'module.encoder.layers.6.bn.num_batches_tracked', 'module.encoder.layers.7.conv3x1_1.weight', 'module.encoder.layers.7.conv3x1_1.bias', 'module.encoder.layers.7.conv1x3_1.weight', 'module.encoder.layers.7.conv1x3_1.bias', 'module.encoder.layers.7.bn1.weight', 'module.encoder.layers.7.bn1.bias', 'module.encoder.layers.7.bn1.running_mean', 'module.encoder.layers.7.bn1.running_var', 'module.encoder.layers.7.bn1.num_batches_tracked', 'module.encoder.layers.7.conv3x1_2.weight', 'module.encoder.layers.7.conv3x1_2.bias', 'module.encoder.layers.7.conv1x3_2.weight', 'module.encoder.layers.7.conv1x3_2.bias', 'module.encoder.layers.7.bn2.weight', 'module.encoder.layers.7.bn2.bias', 'module.encoder.layers.7.bn2.running_mean', 'module.encoder.layers.7.bn2.running_var', 'module.encoder.layers.7.bn2.num_batches_tracked', 'module.encoder.layers.8.conv3x1_1.weight', 'module.encoder.layers.8.conv3x1_1.bias', 'module.encoder.layers.8.conv1x3_1.weight', 'module.encoder.layers.8.conv1x3_1.bias', 'module.encoder.layers.8.bn1.weight', 'module.encoder.layers.8.bn1.bias', 'module.encoder.layers.8.bn1.running_mean', 'module.encoder.layers.8.bn1.running_var', 'module.encoder.layers.8.bn1.num_batches_tracked', 'module.encoder.layers.8.conv3x1_2.weight', 'module.encoder.layers.8.conv3x1_2.bias', 'module.encoder.layers.8.conv1x3_2.weight', 'module.encoder.layers.8.conv1x3_2.bias', 'module.encoder.layers.8.bn2.weight', 'module.encoder.layers.8.bn2.bias', 'module.encoder.layers.8.bn2.running_mean', 'module.encoder.layers.8.bn2.running_var', 'module.encoder.layers.8.bn2.num_batches_tracked', 'module.encoder.layers.9.conv3x1_1.weight', 'module.encoder.layers.9.conv3x1_1.bias', 'module.encoder.layers.9.conv1x3_1.weight', 'module.encoder.layers.9.conv1x3_1.bias', 'module.encoder.layers.9.bn1.weight', 'module.encoder.layers.9.bn1.bias', 'module.encoder.layers.9.bn1.running_mean', 'module.encoder.layers.9.bn1.running_var', 'module.encoder.layers.9.bn1.num_batches_tracked', 'module.encoder.layers.9.conv3x1_2.weight', 'module.encoder.layers.9.conv3x1_2.bias', 'module.encoder.layers.9.conv1x3_2.weight', 'module.encoder.layers.9.conv1x3_2.bias', 'module.encoder.layers.9.bn2.weight', 'module.encoder.layers.9.bn2.bias', 'module.encoder.layers.9.bn2.running_mean', 'module.encoder.layers.9.bn2.running_var', 'module.encoder.layers.9.bn2.num_batches_tracked', 'module.encoder.layers.10.conv3x1_1.weight', 'module.encoder.layers.10.conv3x1_1.bias', 'module.encoder.layers.10.conv1x3_1.weight', 'module.encoder.layers.10.conv1x3_1.bias', 'module.encoder.layers.10.bn1.weight', 'module.encoder.layers.10.bn1.bias', 'module.encoder.layers.10.bn1.running_mean', 'module.encoder.layers.10.bn1.running_var', 'module.encoder.layers.10.bn1.num_batches_tracked', 'module.encoder.layers.10.conv3x1_2.weight', 'module.encoder.layers.10.conv3x1_2.bias', 'module.encoder.layers.10.conv1x3_2.weight', 'module.encoder.layers.10.conv1x3_2.bias', 'module.encoder.layers.10.bn2.weight', 'module.encoder.layers.10.bn2.bias', 'module.encoder.layers.10.bn2.running_mean', 'module.encoder.layers.10.bn2.running_var', 'module.encoder.layers.10.bn2.num_batches_tracked', 'module.encoder.layers.11.conv3x1_1.weight', 'module.encoder.layers.11.conv3x1_1.bias', 'module.encoder.layers.11.conv1x3_1.weight', 'module.encoder.layers.11.conv1x3_1.bias', 'module.encoder.layers.11.bn1.weight', 'module.encoder.layers.11.bn1.bias', 'module.encoder.layers.11.bn1.running_mean', 'module.encoder.layers.11.bn1.running_var', 'module.encoder.layers.11.bn1.num_batches_tracked', 'module.encoder.layers.11.conv3x1_2.weight', 'module.encoder.layers.11.conv3x1_2.bias', 'module.encoder.layers.11.conv1x3_2.weight', 'module.encoder.layers.11.conv1x3_2.bias', 'module.encoder.layers.11.bn2.weight', 'module.encoder.layers.11.bn2.bias', 'module.encoder.layers.11.bn2.running_mean', 'module.encoder.layers.11.bn2.running_var', 'module.encoder.layers.11.bn2.num_batches_tracked', 'module.encoder.layers.12.conv3x1_1.weight', 'module.encoder.layers.12.conv3x1_1.bias', 'module.encoder.layers.12.conv1x3_1.weight', 'module.encoder.layers.12.conv1x3_1.bias', 'module.encoder.layers.12.bn1.weight', 'module.encoder.layers.12.bn1.bias', 'module.encoder.layers.12.bn1.running_mean', 'module.encoder.layers.12.bn1.running_var', 'module.encoder.layers.12.bn1.num_batches_tracked', 'module.encoder.layers.12.conv3x1_2.weight', 'module.encoder.layers.12.conv3x1_2.bias', 'module.encoder.layers.12.conv1x3_2.weight', 'module.encoder.layers.12.conv1x3_2.bias', 'module.encoder.layers.12.bn2.weight', 'module.encoder.layers.12.bn2.bias', 'module.encoder.layers.12.bn2.running_mean', 'module.encoder.layers.12.bn2.running_var', 'module.encoder.layers.12.bn2.num_batches_tracked', 'module.encoder.layers.13.conv3x1_1.weight', 'module.encoder.layers.13.conv3x1_1.bias', 'module.encoder.layers.13.conv1x3_1.weight', 'module.encoder.layers.13.conv1x3_1.bias', 'module.encoder.layers.13.bn1.weight', 'module.encoder.layers.13.bn1.bias', 'module.encoder.layers.13.bn1.running_mean', 'module.encoder.layers.13.bn1.running_var', 'module.encoder.layers.13.bn1.num_batches_tracked', 'module.encoder.layers.13.conv3x1_2.weight', 'module.encoder.layers.13.conv3x1_2.bias', 'module.encoder.layers.13.conv1x3_2.weight', 'module.encoder.layers.13.conv1x3_2.bias', 'module.encoder.layers.13.bn2.weight', 'module.encoder.layers.13.bn2.bias', 'module.encoder.layers.13.bn2.running_mean', 'module.encoder.layers.13.bn2.running_var', 'module.encoder.layers.13.bn2.num_batches_tracked', 'module.encoder.layers.14.conv3x1_1.weight', 'module.encoder.layers.14.conv3x1_1.bias', 'module.encoder.layers.14.conv1x3_1.weight', 'module.encoder.layers.14.conv1x3_1.bias', 'module.encoder.layers.14.bn1.weight', 'module.encoder.layers.14.bn1.bias', 'module.encoder.layers.14.bn1.running_mean', 'module.encoder.layers.14.bn1.running_var', 'module.encoder.layers.14.bn1.num_batches_tracked', 'module.encoder.layers.14.conv3x1_2.weight', 'module.encoder.layers.14.conv3x1_2.bias', 'module.encoder.layers.14.conv1x3_2.weight', 'module.encoder.layers.14.conv1x3_2.bias', 'module.encoder.layers.14.bn2.weight', 'module.encoder.layers.14.bn2.bias', 'module.encoder.layers.14.bn2.running_mean', 'module.encoder.layers.14.bn2.running_var', 'module.encoder.layers.14.bn2.num_batches_tracked', 'module.encoder.output_conv.weight', 'module.encoder.output_conv.bias', 'module.decoder.layers.0.conv.weight', 'module.decoder.layers.0.conv.bias', 'module.decoder.layers.0.bn.weight', 'module.decoder.layers.0.bn.bias', 'module.decoder.layers.0.bn.running_mean', 'module.decoder.layers.0.bn.running_var', 'module.decoder.layers.0.bn.num_batches_tracked', 'module.decoder.layers.1.conv3x1_1.weight', 'module.decoder.layers.1.conv3x1_1.bias', 'module.decoder.layers.1.conv1x3_1.weight', 'module.decoder.layers.1.conv1x3_1.bias', 'module.decoder.layers.1.bn1.weight', 'module.decoder.layers.1.bn1.bias', 'module.decoder.layers.1.bn1.running_mean', 'module.decoder.layers.1.bn1.running_var', 'module.decoder.layers.1.bn1.num_batches_tracked', 'module.decoder.layers.1.conv3x1_2.weight', 'module.decoder.layers.1.conv3x1_2.bias', 'module.decoder.layers.1.conv1x3_2.weight', 'module.decoder.layers.1.conv1x3_2.bias', 'module.decoder.layers.1.bn2.weight', 'module.decoder.layers.1.bn2.bias', 'module.decoder.layers.1.bn2.running_mean', 'module.decoder.layers.1.bn2.running_var', 'module.decoder.layers.1.bn2.num_batches_tracked', 'module.decoder.layers.2.conv3x1_1.weight', 'module.decoder.layers.2.conv3x1_1.bias', 'module.decoder.layers.2.conv1x3_1.weight', 'module.decoder.layers.2.conv1x3_1.bias', 'module.decoder.layers.2.bn1.weight', 'module.decoder.layers.2.bn1.bias', 'module.decoder.layers.2.bn1.running_mean', 'module.decoder.layers.2.bn1.running_var', 'module.decoder.layers.2.bn1.num_batches_tracked', 'module.decoder.layers.2.conv3x1_2.weight', 'module.decoder.layers.2.conv3x1_2.bias', 'module.decoder.layers.2.conv1x3_2.weight', 'module.decoder.layers.2.conv1x3_2.bias', 'module.decoder.layers.2.bn2.weight', 'module.decoder.layers.2.bn2.bias', 'module.decoder.layers.2.bn2.running_mean', 'module.decoder.layers.2.bn2.running_var', 'module.decoder.layers.2.bn2.num_batches_tracked', 'module.decoder.layers.3.conv.weight', 'module.decoder.layers.3.conv.bias', 'module.decoder.layers.3.bn.weight', 'module.decoder.layers.3.bn.bias', 'module.decoder.layers.3.bn.running_mean', 'module.decoder.layers.3.bn.running_var', 'module.decoder.layers.3.bn.num_batches_tracked', 'module.decoder.layers.4.conv3x1_1.weight', 'module.decoder.layers.4.conv3x1_1.bias', 'module.decoder.layers.4.conv1x3_1.weight', 'module.decoder.layers.4.conv1x3_1.bias', 'module.decoder.layers.4.bn1.weight', 'module.decoder.layers.4.bn1.bias', 'module.decoder.layers.4.bn1.running_mean', 'module.decoder.layers.4.bn1.running_var', 'module.decoder.layers.4.bn1.num_batches_tracked', 'module.decoder.layers.4.conv3x1_2.weight', 'module.decoder.layers.4.conv3x1_2.bias', 'module.decoder.layers.4.conv1x3_2.weight', 'module.decoder.layers.4.conv1x3_2.bias', 'module.decoder.layers.4.bn2.weight', 'module.decoder.layers.4.bn2.bias', 'module.decoder.layers.4.bn2.running_mean', 'module.decoder.layers.4.bn2.running_var', 'module.decoder.layers.4.bn2.num_batches_tracked', 'module.decoder.layers.5.conv3x1_1.weight', 'module.decoder.layers.5.conv3x1_1.bias', 'module.decoder.layers.5.conv1x3_1.weight', 'module.decoder.layers.5.conv1x3_1.bias', 'module.decoder.layers.5.bn1.weight', 'module.decoder.layers.5.bn1.bias', 'module.decoder.layers.5.bn1.running_mean', 'module.decoder.layers.5.bn1.running_var', 'module.decoder.layers.5.bn1.num_batches_tracked', 'module.decoder.layers.5.conv3x1_2.weight', 'module.decoder.layers.5.conv3x1_2.bias', 'module.decoder.layers.5.conv1x3_2.weight', 'module.decoder.layers.5.conv1x3_2.bias', 'module.decoder.layers.5.bn2.weight', 'module.decoder.layers.5.bn2.bias', 'module.decoder.layers.5.bn2.running_mean', 'module.decoder.layers.5.bn2.running_var', 'module.decoder.layers.5.bn2.num_batches_tracked', 'module.decoder.output_conv.weight', 'module.decoder.output_conv.bias'])\n",
            "Model and weights LOADED successfully\n",
            "100% 2975/2975 [07:41<00:00,  6.45it/s]\n",
            "cov_matrix tensor([[ 2.3956e+09,  5.3233e+08, -1.1117e+09,  7.9754e+07, -3.0860e+08,\n",
            "         -4.5878e+08, -9.7728e+07,  9.4537e+07,  1.2570e+08,  7.6442e+08,\n",
            "          6.7353e+08,  8.9282e+07,  2.8561e+08,  2.4825e+08,  3.8140e+08,\n",
            "          3.5417e+08,  3.4480e+08,  4.1080e+08, -5.2754e+07],\n",
            "        [ 5.3233e+08,  2.4650e+09, -9.4928e+07,  3.2850e+08, -3.8250e+08,\n",
            "          2.2306e+08, -5.7414e+08, -5.6602e+08, -5.7101e+08,  6.6016e+08,\n",
            "          4.4618e+08, -1.1564e+08, -5.4575e+08,  1.9169e+08, -3.1271e+08,\n",
            "         -1.0353e+09,  8.6958e+07, -9.8419e+07,  4.3876e+08],\n",
            "        [-1.1117e+09, -9.4928e+07,  2.9623e+09,  6.3566e+08,  3.0863e+08,\n",
            "          2.1515e+08,  5.4991e+08,  1.7934e+08, -1.1649e+08, -1.1383e+09,\n",
            "          4.6287e+08,  4.6533e+05, -2.5552e+08, -5.2459e+08,  5.5244e+08,\n",
            "         -2.8486e+08,  6.8704e+08, -9.7038e+07, -1.5765e+08],\n",
            "        [ 7.9754e+07,  3.2850e+08,  6.3566e+08,  1.7032e+09,  6.6750e+08,\n",
            "         -3.3863e+08, -3.2469e+08, -9.6637e+07,  3.9011e+08,  2.8685e+08,\n",
            "         -3.3193e+08, -2.0280e+08, -1.9761e+08,  1.3337e+08,  5.1907e+08,\n",
            "         -1.7092e+08, -5.0282e+08,  6.0503e+08, -1.8244e+08],\n",
            "        [-3.0860e+08, -3.8250e+08,  3.0863e+08,  6.6750e+08,  1.2267e+09,\n",
            "          3.1085e+07,  2.1584e+08,  2.6093e+08, -1.2286e+07,  2.6611e+07,\n",
            "         -7.7656e+08,  5.1364e+07, -6.3603e+07, -4.3606e+08, -1.9027e+08,\n",
            "          6.3518e+07, -3.7639e+08,  1.6301e+07, -1.2184e+07],\n",
            "        [-4.5878e+08,  2.2306e+08,  2.1515e+08, -3.3863e+08,  3.1085e+07,\n",
            "          1.5329e+09,  6.2660e+08,  4.7480e+08,  5.8965e+07, -2.8814e+08,\n",
            "          3.7905e+08, -5.6487e+07, -4.1709e+08, -1.3862e+08, -2.0653e+08,\n",
            "         -4.2598e+08,  1.1674e+08, -6.9226e+08,  1.2850e+08],\n",
            "        [-9.7728e+07, -5.7414e+08,  5.4991e+08, -3.2469e+08,  2.1584e+08,\n",
            "          6.2660e+08,  1.5053e+09,  8.3542e+08,  6.0996e+08, -1.2172e+08,\n",
            "          7.8723e+08,  2.7645e+08,  1.7060e+08, -4.9118e+08,  1.6640e+08,\n",
            "          5.2793e+08,  4.9058e+08, -5.4750e+08, -4.0819e+08],\n",
            "        [ 9.4537e+07, -5.6602e+08,  1.7934e+08, -9.6637e+07,  2.6093e+08,\n",
            "          4.7480e+08,  8.3542e+08,  1.4146e+09,  1.0751e+08, -3.0878e+08,\n",
            "          6.9653e+08,  8.0042e+07,  2.6429e+08,  5.1791e+07,  4.5839e+08,\n",
            "          2.0180e+08, -1.4713e+08, -6.4726e+08, -4.1251e+08],\n",
            "        [ 1.2570e+08, -5.7101e+08, -1.1649e+08,  3.9011e+08, -1.2286e+07,\n",
            "          5.8965e+07,  6.0996e+08,  1.0751e+08,  2.8543e+09,  1.0083e+09,\n",
            "          6.4216e+08, -3.7732e+08, -1.5921e+07,  2.9349e+08, -1.1691e+08,\n",
            "          4.0528e+08,  2.1552e+07, -9.7056e+07, -3.8597e+08],\n",
            "        [ 7.6442e+08,  6.6016e+08, -1.1383e+09,  2.8685e+08,  2.6611e+07,\n",
            "         -2.8814e+08, -1.2172e+08, -3.0878e+08,  1.0083e+09,  1.8508e+09,\n",
            "          4.2747e+08, -1.8856e+08, -8.3740e+07,  5.2486e+07, -6.4068e+08,\n",
            "          6.4876e+07, -4.1535e+08,  5.6166e+07,  1.3810e+08],\n",
            "        [ 6.7353e+08,  4.4618e+08,  4.6287e+08, -3.3193e+08, -7.7656e+08,\n",
            "          3.7905e+08,  7.8723e+08,  6.9653e+08,  6.4216e+08,  4.2747e+08,\n",
            "          3.0382e+09, -6.4473e+08,  3.7103e+08, -5.6801e+08,  6.2797e+08,\n",
            "          2.1460e+08,  9.0749e+08, -4.5075e+08, -2.7436e+07],\n",
            "        [ 8.9282e+07, -1.1564e+08,  4.6533e+05, -2.0280e+08,  5.1364e+07,\n",
            "         -5.6487e+07,  2.7645e+08,  8.0042e+07, -3.7732e+08, -1.8856e+08,\n",
            "         -6.4473e+08,  1.0631e+09,  2.5722e+08,  6.0621e+07, -2.6910e+08,\n",
            "         -2.4720e+07, -9.3999e+07,  1.3856e+08, -1.1033e+08],\n",
            "        [ 2.8561e+08, -5.4575e+08, -2.5552e+08, -1.9761e+08, -6.3603e+07,\n",
            "         -4.1709e+08,  1.7060e+08,  2.6429e+08, -1.5921e+07, -8.3740e+07,\n",
            "          3.7103e+08,  2.5722e+08,  8.1854e+08, -2.0374e+08,  3.7366e+08,\n",
            "          5.2344e+08,  1.9202e+08,  3.0640e+08,  1.1162e+07],\n",
            "        [ 2.4825e+08,  1.9169e+08, -5.2459e+08,  1.3337e+08, -4.3606e+08,\n",
            "         -1.3862e+08, -4.9118e+08,  5.1791e+07,  2.9349e+08,  5.2486e+07,\n",
            "         -5.6801e+08,  6.0621e+07, -2.0374e+08,  1.8755e+09,  4.1776e+08,\n",
            "          1.3729e+08, -1.5414e+08,  3.2404e+08, -9.3867e+07],\n",
            "        [ 3.8140e+08, -3.1271e+08,  5.5244e+08,  5.1907e+08, -1.9027e+08,\n",
            "         -2.0653e+08,  1.6640e+08,  4.5839e+08, -1.1691e+08, -6.4068e+08,\n",
            "          6.2797e+08, -2.6910e+08,  3.7366e+08,  4.1776e+08,  1.4679e+09,\n",
            "          6.1755e+08,  5.1360e+08,  5.0407e+08, -2.7239e+08],\n",
            "        [ 3.5417e+08, -1.0353e+09, -2.8486e+08, -1.7092e+08,  6.3518e+07,\n",
            "         -4.2598e+08,  5.2793e+08,  2.0180e+08,  4.0528e+08,  6.4876e+07,\n",
            "          2.1460e+08, -2.4720e+07,  5.2344e+08,  1.3729e+08,  6.1755e+08,\n",
            "          1.2261e+09,  5.2249e+08,  4.5047e+08, -2.0018e+08],\n",
            "        [ 3.4480e+08,  8.6958e+07,  6.8704e+08, -5.0282e+08, -3.7639e+08,\n",
            "          1.1674e+08,  4.9058e+08, -1.4713e+08,  2.1552e+07, -4.1535e+08,\n",
            "          9.0749e+08, -9.3999e+07,  1.9202e+08, -1.5414e+08,  5.1360e+08,\n",
            "          5.2249e+08,  1.4009e+09,  2.5537e+08,  2.0861e+08],\n",
            "        [ 4.1080e+08, -9.8419e+07, -9.7038e+07,  6.0503e+08,  1.6301e+07,\n",
            "         -6.9226e+08, -5.4750e+08, -6.4726e+08, -9.7056e+07,  5.6166e+07,\n",
            "         -4.5075e+08,  1.3856e+08,  3.0640e+08,  3.2404e+08,  5.0407e+08,\n",
            "          4.5047e+08,  2.5537e+08,  1.1670e+09,  1.9000e+08],\n",
            "        [-5.2754e+07,  4.3876e+08, -1.5765e+08, -1.8244e+08, -1.2184e+07,\n",
            "          1.2850e+08, -4.0819e+08, -4.1251e+08, -3.8597e+08,  1.3810e+08,\n",
            "         -2.7436e+07, -1.1033e+08,  1.1162e+07, -9.3867e+07, -2.7239e+08,\n",
            "         -2.0018e+08,  2.0861e+08,  1.9000e+08,  6.0700e+08]], device='cuda:0')\n",
            "Covariance matrix: torch.Size([19, 19])\n",
            "cov_matrix tensor([[ 1.5359e+00,  3.4129e-01, -7.1275e-01,  5.1132e-02, -1.9785e-01,\n",
            "         -2.9414e-01, -6.2656e-02,  6.0610e-02,  8.0590e-02,  4.9009e-01,\n",
            "          4.3182e-01,  5.7241e-02,  1.8311e-01,  1.5916e-01,  2.4452e-01,\n",
            "          2.2707e-01,  2.2106e-01,  2.6337e-01, -3.3822e-02],\n",
            "        [ 3.4129e-01,  1.5804e+00, -6.0861e-02,  2.1061e-01, -2.4523e-01,\n",
            "          1.4301e-01, -3.6810e-01, -3.6289e-01, -3.6609e-01,  4.2324e-01,\n",
            "          2.8606e-01, -7.4141e-02, -3.4990e-01,  1.2290e-01, -2.0049e-01,\n",
            "         -6.6376e-01,  5.5751e-02, -6.3099e-02,  2.8130e-01],\n",
            "        [-7.1275e-01, -6.0861e-02,  1.8992e+00,  4.0754e-01,  1.9787e-01,\n",
            "          1.3794e-01,  3.5256e-01,  1.1498e-01, -7.4682e-02, -7.2982e-01,\n",
            "          2.9676e-01,  2.9834e-04, -1.6382e-01, -3.3633e-01,  3.5418e-01,\n",
            "         -1.8263e-01,  4.4048e-01, -6.2213e-02, -1.0107e-01],\n",
            "        [ 5.1132e-02,  2.1061e-01,  4.0754e-01,  1.0920e+00,  4.2795e-01,\n",
            "         -2.1711e-01, -2.0817e-01, -6.1957e-02,  2.5011e-01,  1.8390e-01,\n",
            "         -2.1281e-01, -1.3002e-01, -1.2669e-01,  8.5507e-02,  3.3279e-01,\n",
            "         -1.0958e-01, -3.2237e-01,  3.8790e-01, -1.1697e-01],\n",
            "        [-1.9785e-01, -2.4523e-01,  1.9787e-01,  4.2795e-01,  7.8647e-01,\n",
            "          1.9929e-02,  1.3838e-01,  1.6729e-01, -7.8769e-03,  1.7061e-02,\n",
            "         -4.9787e-01,  3.2931e-02, -4.0777e-02, -2.7957e-01, -1.2199e-01,\n",
            "          4.0723e-02, -2.4132e-01,  1.0451e-02, -7.8113e-03],\n",
            "        [-2.9414e-01,  1.4301e-01,  1.3794e-01, -2.1711e-01,  1.9929e-02,\n",
            "          9.8279e-01,  4.0173e-01,  3.0441e-01,  3.7804e-02, -1.8473e-01,\n",
            "          2.4302e-01, -3.6215e-02, -2.6740e-01, -8.8874e-02, -1.3241e-01,\n",
            "         -2.7311e-01,  7.4848e-02, -4.4383e-01,  8.2387e-02],\n",
            "        [-6.2656e-02, -3.6810e-01,  3.5256e-01, -2.0817e-01,  1.3838e-01,\n",
            "          4.0173e-01,  9.6510e-01,  5.3561e-01,  3.9106e-01, -7.8035e-02,\n",
            "          5.0471e-01,  1.7724e-01,  1.0938e-01, -3.1491e-01,  1.0668e-01,\n",
            "          3.3847e-01,  3.1453e-01, -3.5102e-01, -2.6170e-01],\n",
            "        [ 6.0610e-02, -3.6289e-01,  1.1498e-01, -6.1957e-02,  1.6729e-01,\n",
            "          3.0441e-01,  5.3561e-01,  9.0694e-01,  6.8927e-02, -1.9797e-01,\n",
            "          4.4657e-01,  5.1317e-02,  1.6944e-01,  3.3205e-02,  2.9389e-01,\n",
            "          1.2938e-01, -9.4329e-02, -4.1498e-01, -2.6447e-01],\n",
            "        [ 8.0590e-02, -3.6609e-01, -7.4682e-02,  2.5011e-01, -7.8769e-03,\n",
            "          3.7804e-02,  3.9106e-01,  6.8927e-02,  1.8300e+00,  6.4644e-01,\n",
            "          4.1171e-01, -2.4191e-01, -1.0207e-02,  1.8816e-01, -7.4957e-02,\n",
            "          2.5984e-01,  1.3817e-02, -6.2225e-02, -2.4746e-01],\n",
            "        [ 4.9009e-01,  4.2324e-01, -7.2982e-01,  1.8390e-01,  1.7061e-02,\n",
            "         -1.8473e-01, -7.8035e-02, -1.9797e-01,  6.4644e-01,  1.1866e+00,\n",
            "          2.7407e-01, -1.2089e-01, -5.3688e-02,  3.3650e-02, -4.1076e-01,\n",
            "          4.1593e-02, -2.6629e-01,  3.6010e-02,  8.8542e-02],\n",
            "        [ 4.3182e-01,  2.8606e-01,  2.9676e-01, -2.1281e-01, -4.9787e-01,\n",
            "          2.4302e-01,  5.0471e-01,  4.4657e-01,  4.1171e-01,  2.7407e-01,\n",
            "          1.9479e+00, -4.1336e-01,  2.3788e-01, -3.6417e-01,  4.0261e-01,\n",
            "          1.3759e-01,  5.8182e-01, -2.8899e-01, -1.7590e-02],\n",
            "        [ 5.7241e-02, -7.4141e-02,  2.9834e-04, -1.3002e-01,  3.2931e-02,\n",
            "         -3.6215e-02,  1.7724e-01,  5.1317e-02, -2.4191e-01, -1.2089e-01,\n",
            "         -4.1336e-01,  6.8156e-01,  1.6491e-01,  3.8866e-02, -1.7253e-01,\n",
            "         -1.5849e-02, -6.0265e-02,  8.8833e-02, -7.0733e-02],\n",
            "        [ 1.8311e-01, -3.4990e-01, -1.6382e-01, -1.2669e-01, -4.0777e-02,\n",
            "         -2.6740e-01,  1.0938e-01,  1.6944e-01, -1.0207e-02, -5.3688e-02,\n",
            "          2.3788e-01,  1.6491e-01,  5.2479e-01, -1.3063e-01,  2.3957e-01,\n",
            "          3.3559e-01,  1.2311e-01,  1.9644e-01,  7.1560e-03],\n",
            "        [ 1.5916e-01,  1.2290e-01, -3.3633e-01,  8.5507e-02, -2.7957e-01,\n",
            "         -8.8874e-02, -3.1491e-01,  3.3205e-02,  1.8816e-01,  3.3650e-02,\n",
            "         -3.6417e-01,  3.8866e-02, -1.3063e-01,  1.2024e+00,  2.6784e-01,\n",
            "          8.8022e-02, -9.8823e-02,  2.0775e-01, -6.0181e-02],\n",
            "        [ 2.4452e-01, -2.0049e-01,  3.5418e-01,  3.3279e-01, -1.2199e-01,\n",
            "         -1.3241e-01,  1.0668e-01,  2.9389e-01, -7.4957e-02, -4.1076e-01,\n",
            "          4.0261e-01, -1.7253e-01,  2.3957e-01,  2.6784e-01,  9.4109e-01,\n",
            "          3.9593e-01,  3.2929e-01,  3.2317e-01, -1.7463e-01],\n",
            "        [ 2.2707e-01, -6.6376e-01, -1.8263e-01, -1.0958e-01,  4.0723e-02,\n",
            "         -2.7311e-01,  3.3847e-01,  1.2938e-01,  2.5984e-01,  4.1593e-02,\n",
            "          1.3759e-01, -1.5849e-02,  3.3559e-01,  8.8022e-02,  3.9593e-01,\n",
            "          7.8606e-01,  3.3498e-01,  2.8881e-01, -1.2834e-01],\n",
            "        [ 2.2106e-01,  5.5751e-02,  4.4048e-01, -3.2237e-01, -2.4132e-01,\n",
            "          7.4848e-02,  3.1453e-01, -9.4329e-02,  1.3817e-02, -2.6629e-01,\n",
            "          5.8182e-01, -6.0265e-02,  1.2311e-01, -9.8823e-02,  3.2929e-01,\n",
            "          3.3498e-01,  8.9816e-01,  1.6373e-01,  1.3374e-01],\n",
            "        [ 2.6337e-01, -6.3099e-02, -6.2213e-02,  3.8790e-01,  1.0451e-02,\n",
            "         -4.4383e-01, -3.5102e-01, -4.1498e-01, -6.2225e-02,  3.6010e-02,\n",
            "         -2.8899e-01,  8.8833e-02,  1.9644e-01,  2.0775e-01,  3.2317e-01,\n",
            "          2.8881e-01,  1.6373e-01,  7.4819e-01,  1.2181e-01],\n",
            "        [-3.3822e-02,  2.8130e-01, -1.0107e-01, -1.1697e-01, -7.8113e-03,\n",
            "          8.2387e-02, -2.6170e-01, -2.6447e-01, -2.4746e-01,  8.8542e-02,\n",
            "         -1.7590e-02, -7.0733e-02,  7.1560e-03, -6.0181e-02, -1.7463e-01,\n",
            "         -1.2834e-01,  1.3374e-01,  1.2181e-01,  3.8917e-01]], device='cuda:0')\n",
            "Covariance matrice saved as '/content/AnomalySegmentation/save/cov_matrix_erfnet.npy'\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "datadir = '/content/cityscapes'\n",
        "loadWeights = '/trained_models/erfnet_pretrained.pth'\n",
        "loadDir = '/content/AnomalySegmentation'\n",
        "\n",
        "# Compute Dataset Mean\n",
        "!python /content/AnomalySegmentation/eval/mahalanobis.py --datadir {datadir} --loadDir {loadDir} --loadWeights {loadWeights}\n",
        "\n",
        "# Compute Dataset Covariance\n",
        "mean = \"/save/mean_cityscapes_erfnet.npy\"\n",
        "!python /content/AnomalySegmentation/eval/mahalanobis.py --datadir {datadir} --loadDir {loadDir} --loadWeights {loadWeights} --mean {mean} --num-workers 2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "no_execute = False\n",
        "just_once = False\n",
        "\n",
        "for dataset_dir in ['RoadAnomaly21', 'RoadObsticle21', 'FS_LostFound_full', 'fs_static', 'RoadAnomaly']:\n",
        "  print(\"----------------------------\")\n",
        "  for method in [\"Mahalanobis\"]: # [\"Mahalanobis\", \"MSP\", \"MaxLogit\", \"MaxEntropy\"]:\n",
        "\n",
        "    if no_execute:\n",
        "      break\n",
        "\n",
        "    format_file = os.listdir(f'/content/Validation_Dataset/{dataset_dir}/images')[0].split(\".\")[1]\n",
        "    input =f'/content/Validation_Dataset/{dataset_dir}/images/\\*.{format_file}'\n",
        "    print(f\"\\nDataset: {dataset_dir} method: {method}\")\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "      !python -W ignore /content/AnomalySegmentation/eval/evalAnomaly.py --input {input} --method  {method} | tail -n 2\n",
        "    else:\n",
        "      !python -W ignore /content/AnomalySegmentation/eval/evalAnomaly.py --input {input} --method {method}  --cpu | tail -n 2\n",
        "\n",
        "    if just_once:\n",
        "      no_execute = True\n",
        "      just_once = False"
      ],
      "metadata": {
        "id": "-I1lq_KSp8E3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "c722a709-29c6-44dd-fe58-77d506d3e6f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------\n",
            "\n",
            "Dataset: RoadAnomaly21 method: Mahalanobis\n",
            "100% 10/10 [00:02<00:00,  3.75it/s]\n",
            "AUPRC score: 30.882157230765973\n",
            "FPR@TPR95: 74.4879201650216\n",
            "----------------------------\n",
            "\n",
            "Dataset: RoadObsticle21 method: Mahalanobis\n",
            "100% 30/30 [00:08<00:00,  3.63it/s]\n",
            "AUPRC score: 9.641072388098362\n",
            "FPR@TPR95: 52.43062225917142\n",
            "----------------------------\n",
            "\n",
            "Dataset: FS_LostFound_full method: Mahalanobis\n",
            "100% 100/100 [00:26<00:00,  3.78it/s]\n",
            "AUPRC score: 2.9424493821312754\n",
            "FPR@TPR95: 55.231371209083505\n",
            "----------------------------\n",
            "\n",
            "Dataset: fs_static method: Mahalanobis\n",
            "100% 30/30 [00:06<00:00,  4.96it/s]\n",
            "AUPRC score: 8.926899026567218\n",
            "FPR@TPR95: 39.33961912182676\n",
            "----------------------------\n",
            "\n",
            "Dataset: RoadAnomaly method: Mahalanobis\n",
            "100% 60/60 [00:11<00:00,  5.39it/s]\n",
            "AUPRC score: 13.528992306695075\n",
            "FPR@TPR95: 79.6254790023211\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFHIDiwduTGq"
      },
      "source": [
        "## Losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95d_Y3uG6ktP",
        "outputId": "233bd51a-c6c5-4060-b9ac-93595b7511a9",
        "collapsed": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "----- Fine-tuning with Focal loss -----\n",
            "odict_keys(['module.encoder.initial_block.conv.weight', 'module.encoder.initial_block.conv.bias', 'module.encoder.initial_block.bn.weight', 'module.encoder.initial_block.bn.bias', 'module.encoder.initial_block.bn.running_mean', 'module.encoder.initial_block.bn.running_var', 'module.encoder.layers.0.conv.weight', 'module.encoder.layers.0.conv.bias', 'module.encoder.layers.0.bn.weight', 'module.encoder.layers.0.bn.bias', 'module.encoder.layers.0.bn.running_mean', 'module.encoder.layers.0.bn.running_var', 'module.encoder.layers.1.conv3x1_1.weight', 'module.encoder.layers.1.conv3x1_1.bias', 'module.encoder.layers.1.conv1x3_1.weight', 'module.encoder.layers.1.conv1x3_1.bias', 'module.encoder.layers.1.conv3x1_2.weight', 'module.encoder.layers.1.conv3x1_2.bias', 'module.encoder.layers.1.conv1x3_2.weight', 'module.encoder.layers.1.conv1x3_2.bias', 'module.encoder.layers.1.bn1.weight', 'module.encoder.layers.1.bn1.bias', 'module.encoder.layers.1.bn1.running_mean', 'module.encoder.layers.1.bn1.running_var', 'module.encoder.layers.1.bn2.weight', 'module.encoder.layers.1.bn2.bias', 'module.encoder.layers.1.bn2.running_mean', 'module.encoder.layers.1.bn2.running_var', 'module.encoder.layers.2.conv3x1_1.weight', 'module.encoder.layers.2.conv3x1_1.bias', 'module.encoder.layers.2.conv1x3_1.weight', 'module.encoder.layers.2.conv1x3_1.bias', 'module.encoder.layers.2.conv3x1_2.weight', 'module.encoder.layers.2.conv3x1_2.bias', 'module.encoder.layers.2.conv1x3_2.weight', 'module.encoder.layers.2.conv1x3_2.bias', 'module.encoder.layers.2.bn1.weight', 'module.encoder.layers.2.bn1.bias', 'module.encoder.layers.2.bn1.running_mean', 'module.encoder.layers.2.bn1.running_var', 'module.encoder.layers.2.bn2.weight', 'module.encoder.layers.2.bn2.bias', 'module.encoder.layers.2.bn2.running_mean', 'module.encoder.layers.2.bn2.running_var', 'module.encoder.layers.3.conv3x1_1.weight', 'module.encoder.layers.3.conv3x1_1.bias', 'module.encoder.layers.3.conv1x3_1.weight', 'module.encoder.layers.3.conv1x3_1.bias', 'module.encoder.layers.3.conv3x1_2.weight', 'module.encoder.layers.3.conv3x1_2.bias', 'module.encoder.layers.3.conv1x3_2.weight', 'module.encoder.layers.3.conv1x3_2.bias', 'module.encoder.layers.3.bn1.weight', 'module.encoder.layers.3.bn1.bias', 'module.encoder.layers.3.bn1.running_mean', 'module.encoder.layers.3.bn1.running_var', 'module.encoder.layers.3.bn2.weight', 'module.encoder.layers.3.bn2.bias', 'module.encoder.layers.3.bn2.running_mean', 'module.encoder.layers.3.bn2.running_var', 'module.encoder.layers.4.conv3x1_1.weight', 'module.encoder.layers.4.conv3x1_1.bias', 'module.encoder.layers.4.conv1x3_1.weight', 'module.encoder.layers.4.conv1x3_1.bias', 'module.encoder.layers.4.conv3x1_2.weight', 'module.encoder.layers.4.conv3x1_2.bias', 'module.encoder.layers.4.conv1x3_2.weight', 'module.encoder.layers.4.conv1x3_2.bias', 'module.encoder.layers.4.bn1.weight', 'module.encoder.layers.4.bn1.bias', 'module.encoder.layers.4.bn1.running_mean', 'module.encoder.layers.4.bn1.running_var', 'module.encoder.layers.4.bn2.weight', 'module.encoder.layers.4.bn2.bias', 'module.encoder.layers.4.bn2.running_mean', 'module.encoder.layers.4.bn2.running_var', 'module.encoder.layers.5.conv3x1_1.weight', 'module.encoder.layers.5.conv3x1_1.bias', 'module.encoder.layers.5.conv1x3_1.weight', 'module.encoder.layers.5.conv1x3_1.bias', 'module.encoder.layers.5.conv3x1_2.weight', 'module.encoder.layers.5.conv3x1_2.bias', 'module.encoder.layers.5.conv1x3_2.weight', 'module.encoder.layers.5.conv1x3_2.bias', 'module.encoder.layers.5.bn1.weight', 'module.encoder.layers.5.bn1.bias', 'module.encoder.layers.5.bn1.running_mean', 'module.encoder.layers.5.bn1.running_var', 'module.encoder.layers.5.bn2.weight', 'module.encoder.layers.5.bn2.bias', 'module.encoder.layers.5.bn2.running_mean', 'module.encoder.layers.5.bn2.running_var', 'module.encoder.layers.6.conv.weight', 'module.encoder.layers.6.conv.bias', 'module.encoder.layers.6.bn.weight', 'module.encoder.layers.6.bn.bias', 'module.encoder.layers.6.bn.running_mean', 'module.encoder.layers.6.bn.running_var', 'module.encoder.layers.7.conv3x1_1.weight', 'module.encoder.layers.7.conv3x1_1.bias', 'module.encoder.layers.7.conv1x3_1.weight', 'module.encoder.layers.7.conv1x3_1.bias', 'module.encoder.layers.7.conv3x1_2.weight', 'module.encoder.layers.7.conv3x1_2.bias', 'module.encoder.layers.7.conv1x3_2.weight', 'module.encoder.layers.7.conv1x3_2.bias', 'module.encoder.layers.7.bn1.weight', 'module.encoder.layers.7.bn1.bias', 'module.encoder.layers.7.bn1.running_mean', 'module.encoder.layers.7.bn1.running_var', 'module.encoder.layers.7.bn2.weight', 'module.encoder.layers.7.bn2.bias', 'module.encoder.layers.7.bn2.running_mean', 'module.encoder.layers.7.bn2.running_var', 'module.encoder.layers.8.conv3x1_1.weight', 'module.encoder.layers.8.conv3x1_1.bias', 'module.encoder.layers.8.conv1x3_1.weight', 'module.encoder.layers.8.conv1x3_1.bias', 'module.encoder.layers.8.conv3x1_2.weight', 'module.encoder.layers.8.conv3x1_2.bias', 'module.encoder.layers.8.conv1x3_2.weight', 'module.encoder.layers.8.conv1x3_2.bias', 'module.encoder.layers.8.bn1.weight', 'module.encoder.layers.8.bn1.bias', 'module.encoder.layers.8.bn1.running_mean', 'module.encoder.layers.8.bn1.running_var', 'module.encoder.layers.8.bn2.weight', 'module.encoder.layers.8.bn2.bias', 'module.encoder.layers.8.bn2.running_mean', 'module.encoder.layers.8.bn2.running_var', 'module.encoder.layers.9.conv3x1_1.weight', 'module.encoder.layers.9.conv3x1_1.bias', 'module.encoder.layers.9.conv1x3_1.weight', 'module.encoder.layers.9.conv1x3_1.bias', 'module.encoder.layers.9.conv3x1_2.weight', 'module.encoder.layers.9.conv3x1_2.bias', 'module.encoder.layers.9.conv1x3_2.weight', 'module.encoder.layers.9.conv1x3_2.bias', 'module.encoder.layers.9.bn1.weight', 'module.encoder.layers.9.bn1.bias', 'module.encoder.layers.9.bn1.running_mean', 'module.encoder.layers.9.bn1.running_var', 'module.encoder.layers.9.bn2.weight', 'module.encoder.layers.9.bn2.bias', 'module.encoder.layers.9.bn2.running_mean', 'module.encoder.layers.9.bn2.running_var', 'module.encoder.layers.10.conv3x1_1.weight', 'module.encoder.layers.10.conv3x1_1.bias', 'module.encoder.layers.10.conv1x3_1.weight', 'module.encoder.layers.10.conv1x3_1.bias', 'module.encoder.layers.10.conv3x1_2.weight', 'module.encoder.layers.10.conv3x1_2.bias', 'module.encoder.layers.10.conv1x3_2.weight', 'module.encoder.layers.10.conv1x3_2.bias', 'module.encoder.layers.10.bn1.weight', 'module.encoder.layers.10.bn1.bias', 'module.encoder.layers.10.bn1.running_mean', 'module.encoder.layers.10.bn1.running_var', 'module.encoder.layers.10.bn2.weight', 'module.encoder.layers.10.bn2.bias', 'module.encoder.layers.10.bn2.running_mean', 'module.encoder.layers.10.bn2.running_var', 'module.encoder.layers.11.conv3x1_1.weight', 'module.encoder.layers.11.conv3x1_1.bias', 'module.encoder.layers.11.conv1x3_1.weight', 'module.encoder.layers.11.conv1x3_1.bias', 'module.encoder.layers.11.conv3x1_2.weight', 'module.encoder.layers.11.conv3x1_2.bias', 'module.encoder.layers.11.conv1x3_2.weight', 'module.encoder.layers.11.conv1x3_2.bias', 'module.encoder.layers.11.bn1.weight', 'module.encoder.layers.11.bn1.bias', 'module.encoder.layers.11.bn1.running_mean', 'module.encoder.layers.11.bn1.running_var', 'module.encoder.layers.11.bn2.weight', 'module.encoder.layers.11.bn2.bias', 'module.encoder.layers.11.bn2.running_mean', 'module.encoder.layers.11.bn2.running_var', 'module.encoder.layers.12.conv3x1_1.weight', 'module.encoder.layers.12.conv3x1_1.bias', 'module.encoder.layers.12.conv1x3_1.weight', 'module.encoder.layers.12.conv1x3_1.bias', 'module.encoder.layers.12.conv3x1_2.weight', 'module.encoder.layers.12.conv3x1_2.bias', 'module.encoder.layers.12.conv1x3_2.weight', 'module.encoder.layers.12.conv1x3_2.bias', 'module.encoder.layers.12.bn1.weight', 'module.encoder.layers.12.bn1.bias', 'module.encoder.layers.12.bn1.running_mean', 'module.encoder.layers.12.bn1.running_var', 'module.encoder.layers.12.bn2.weight', 'module.encoder.layers.12.bn2.bias', 'module.encoder.layers.12.bn2.running_mean', 'module.encoder.layers.12.bn2.running_var', 'module.encoder.layers.13.conv3x1_1.weight', 'module.encoder.layers.13.conv3x1_1.bias', 'module.encoder.layers.13.conv1x3_1.weight', 'module.encoder.layers.13.conv1x3_1.bias', 'module.encoder.layers.13.conv3x1_2.weight', 'module.encoder.layers.13.conv3x1_2.bias', 'module.encoder.layers.13.conv1x3_2.weight', 'module.encoder.layers.13.conv1x3_2.bias', 'module.encoder.layers.13.bn1.weight', 'module.encoder.layers.13.bn1.bias', 'module.encoder.layers.13.bn1.running_mean', 'module.encoder.layers.13.bn1.running_var', 'module.encoder.layers.13.bn2.weight', 'module.encoder.layers.13.bn2.bias', 'module.encoder.layers.13.bn2.running_mean', 'module.encoder.layers.13.bn2.running_var', 'module.encoder.layers.14.conv3x1_1.weight', 'module.encoder.layers.14.conv3x1_1.bias', 'module.encoder.layers.14.conv1x3_1.weight', 'module.encoder.layers.14.conv1x3_1.bias', 'module.encoder.layers.14.conv3x1_2.weight', 'module.encoder.layers.14.conv3x1_2.bias', 'module.encoder.layers.14.conv1x3_2.weight', 'module.encoder.layers.14.conv1x3_2.bias', 'module.encoder.layers.14.bn1.weight', 'module.encoder.layers.14.bn1.bias', 'module.encoder.layers.14.bn1.running_mean', 'module.encoder.layers.14.bn1.running_var', 'module.encoder.layers.14.bn2.weight', 'module.encoder.layers.14.bn2.bias', 'module.encoder.layers.14.bn2.running_mean', 'module.encoder.layers.14.bn2.running_var', 'module.decoder.layers.0.conv.weight', 'module.decoder.layers.0.conv.bias', 'module.decoder.layers.0.bn.weight', 'module.decoder.layers.0.bn.bias', 'module.decoder.layers.0.bn.running_mean', 'module.decoder.layers.0.bn.running_var', 'module.decoder.layers.1.conv3x1_1.weight', 'module.decoder.layers.1.conv3x1_1.bias', 'module.decoder.layers.1.conv1x3_1.weight', 'module.decoder.layers.1.conv1x3_1.bias', 'module.decoder.layers.1.bn1.weight', 'module.decoder.layers.1.bn1.bias', 'module.decoder.layers.1.bn1.running_mean', 'module.decoder.layers.1.bn1.running_var', 'module.decoder.layers.1.conv3x1_2.weight', 'module.decoder.layers.1.conv3x1_2.bias', 'module.decoder.layers.1.conv1x3_2.weight', 'module.decoder.layers.1.conv1x3_2.bias', 'module.decoder.layers.1.bn2.weight', 'module.decoder.layers.1.bn2.bias', 'module.decoder.layers.1.bn2.running_mean', 'module.decoder.layers.1.bn2.running_var', 'module.decoder.layers.2.conv3x1_1.weight', 'module.decoder.layers.2.conv3x1_1.bias', 'module.decoder.layers.2.conv1x3_1.weight', 'module.decoder.layers.2.conv1x3_1.bias', 'module.decoder.layers.2.bn1.weight', 'module.decoder.layers.2.bn1.bias', 'module.decoder.layers.2.bn1.running_mean', 'module.decoder.layers.2.bn1.running_var', 'module.decoder.layers.2.conv3x1_2.weight', 'module.decoder.layers.2.conv3x1_2.bias', 'module.decoder.layers.2.conv1x3_2.weight', 'module.decoder.layers.2.conv1x3_2.bias', 'module.decoder.layers.2.bn2.weight', 'module.decoder.layers.2.bn2.bias', 'module.decoder.layers.2.bn2.running_mean', 'module.decoder.layers.2.bn2.running_var', 'module.decoder.layers.3.conv.weight', 'module.decoder.layers.3.conv.bias', 'module.decoder.layers.3.bn.weight', 'module.decoder.layers.3.bn.bias', 'module.decoder.layers.3.bn.running_mean', 'module.decoder.layers.3.bn.running_var', 'module.decoder.layers.4.conv3x1_1.weight', 'module.decoder.layers.4.conv3x1_1.bias', 'module.decoder.layers.4.conv1x3_1.weight', 'module.decoder.layers.4.conv1x3_1.bias', 'module.decoder.layers.4.bn1.weight', 'module.decoder.layers.4.bn1.bias', 'module.decoder.layers.4.bn1.running_mean', 'module.decoder.layers.4.bn1.running_var', 'module.decoder.layers.4.conv3x1_2.weight', 'module.decoder.layers.4.conv3x1_2.bias', 'module.decoder.layers.4.conv1x3_2.weight', 'module.decoder.layers.4.conv1x3_2.bias', 'module.decoder.layers.4.bn2.weight', 'module.decoder.layers.4.bn2.bias', 'module.decoder.layers.4.bn2.running_mean', 'module.decoder.layers.4.bn2.running_var', 'module.decoder.layers.5.conv3x1_1.weight', 'module.decoder.layers.5.conv3x1_1.bias', 'module.decoder.layers.5.conv1x3_1.weight', 'module.decoder.layers.5.conv1x3_1.bias', 'module.decoder.layers.5.bn1.weight', 'module.decoder.layers.5.bn1.bias', 'module.decoder.layers.5.bn1.running_mean', 'module.decoder.layers.5.bn1.running_var', 'module.decoder.layers.5.conv3x1_2.weight', 'module.decoder.layers.5.conv3x1_2.bias', 'module.decoder.layers.5.conv1x3_2.weight', 'module.decoder.layers.5.conv1x3_2.bias', 'module.decoder.layers.5.bn2.weight', 'module.decoder.layers.5.bn2.bias', 'module.decoder.layers.5.bn2.running_mean', 'module.decoder.layers.5.bn2.running_var', 'module.decoder.output_conv.weight', 'module.decoder.output_conv.bias'])\n",
            "odict_keys(['encoder.initial_block.conv.weight', 'encoder.initial_block.conv.bias', 'encoder.initial_block.bn.weight', 'encoder.initial_block.bn.bias', 'encoder.initial_block.bn.running_mean', 'encoder.initial_block.bn.running_var', 'encoder.initial_block.bn.num_batches_tracked', 'encoder.layers.0.conv.weight', 'encoder.layers.0.conv.bias', 'encoder.layers.0.bn.weight', 'encoder.layers.0.bn.bias', 'encoder.layers.0.bn.running_mean', 'encoder.layers.0.bn.running_var', 'encoder.layers.0.bn.num_batches_tracked', 'encoder.layers.1.conv3x1_1.weight', 'encoder.layers.1.conv3x1_1.bias', 'encoder.layers.1.conv1x3_1.weight', 'encoder.layers.1.conv1x3_1.bias', 'encoder.layers.1.bn1.weight', 'encoder.layers.1.bn1.bias', 'encoder.layers.1.bn1.running_mean', 'encoder.layers.1.bn1.running_var', 'encoder.layers.1.bn1.num_batches_tracked', 'encoder.layers.1.conv3x1_2.weight', 'encoder.layers.1.conv3x1_2.bias', 'encoder.layers.1.conv1x3_2.weight', 'encoder.layers.1.conv1x3_2.bias', 'encoder.layers.1.bn2.weight', 'encoder.layers.1.bn2.bias', 'encoder.layers.1.bn2.running_mean', 'encoder.layers.1.bn2.running_var', 'encoder.layers.1.bn2.num_batches_tracked', 'encoder.layers.2.conv3x1_1.weight', 'encoder.layers.2.conv3x1_1.bias', 'encoder.layers.2.conv1x3_1.weight', 'encoder.layers.2.conv1x3_1.bias', 'encoder.layers.2.bn1.weight', 'encoder.layers.2.bn1.bias', 'encoder.layers.2.bn1.running_mean', 'encoder.layers.2.bn1.running_var', 'encoder.layers.2.bn1.num_batches_tracked', 'encoder.layers.2.conv3x1_2.weight', 'encoder.layers.2.conv3x1_2.bias', 'encoder.layers.2.conv1x3_2.weight', 'encoder.layers.2.conv1x3_2.bias', 'encoder.layers.2.bn2.weight', 'encoder.layers.2.bn2.bias', 'encoder.layers.2.bn2.running_mean', 'encoder.layers.2.bn2.running_var', 'encoder.layers.2.bn2.num_batches_tracked', 'encoder.layers.3.conv3x1_1.weight', 'encoder.layers.3.conv3x1_1.bias', 'encoder.layers.3.conv1x3_1.weight', 'encoder.layers.3.conv1x3_1.bias', 'encoder.layers.3.bn1.weight', 'encoder.layers.3.bn1.bias', 'encoder.layers.3.bn1.running_mean', 'encoder.layers.3.bn1.running_var', 'encoder.layers.3.bn1.num_batches_tracked', 'encoder.layers.3.conv3x1_2.weight', 'encoder.layers.3.conv3x1_2.bias', 'encoder.layers.3.conv1x3_2.weight', 'encoder.layers.3.conv1x3_2.bias', 'encoder.layers.3.bn2.weight', 'encoder.layers.3.bn2.bias', 'encoder.layers.3.bn2.running_mean', 'encoder.layers.3.bn2.running_var', 'encoder.layers.3.bn2.num_batches_tracked', 'encoder.layers.4.conv3x1_1.weight', 'encoder.layers.4.conv3x1_1.bias', 'encoder.layers.4.conv1x3_1.weight', 'encoder.layers.4.conv1x3_1.bias', 'encoder.layers.4.bn1.weight', 'encoder.layers.4.bn1.bias', 'encoder.layers.4.bn1.running_mean', 'encoder.layers.4.bn1.running_var', 'encoder.layers.4.bn1.num_batches_tracked', 'encoder.layers.4.conv3x1_2.weight', 'encoder.layers.4.conv3x1_2.bias', 'encoder.layers.4.conv1x3_2.weight', 'encoder.layers.4.conv1x3_2.bias', 'encoder.layers.4.bn2.weight', 'encoder.layers.4.bn2.bias', 'encoder.layers.4.bn2.running_mean', 'encoder.layers.4.bn2.running_var', 'encoder.layers.4.bn2.num_batches_tracked', 'encoder.layers.5.conv3x1_1.weight', 'encoder.layers.5.conv3x1_1.bias', 'encoder.layers.5.conv1x3_1.weight', 'encoder.layers.5.conv1x3_1.bias', 'encoder.layers.5.bn1.weight', 'encoder.layers.5.bn1.bias', 'encoder.layers.5.bn1.running_mean', 'encoder.layers.5.bn1.running_var', 'encoder.layers.5.bn1.num_batches_tracked', 'encoder.layers.5.conv3x1_2.weight', 'encoder.layers.5.conv3x1_2.bias', 'encoder.layers.5.conv1x3_2.weight', 'encoder.layers.5.conv1x3_2.bias', 'encoder.layers.5.bn2.weight', 'encoder.layers.5.bn2.bias', 'encoder.layers.5.bn2.running_mean', 'encoder.layers.5.bn2.running_var', 'encoder.layers.5.bn2.num_batches_tracked', 'encoder.layers.6.conv.weight', 'encoder.layers.6.conv.bias', 'encoder.layers.6.bn.weight', 'encoder.layers.6.bn.bias', 'encoder.layers.6.bn.running_mean', 'encoder.layers.6.bn.running_var', 'encoder.layers.6.bn.num_batches_tracked', 'encoder.layers.7.conv3x1_1.weight', 'encoder.layers.7.conv3x1_1.bias', 'encoder.layers.7.conv1x3_1.weight', 'encoder.layers.7.conv1x3_1.bias', 'encoder.layers.7.bn1.weight', 'encoder.layers.7.bn1.bias', 'encoder.layers.7.bn1.running_mean', 'encoder.layers.7.bn1.running_var', 'encoder.layers.7.bn1.num_batches_tracked', 'encoder.layers.7.conv3x1_2.weight', 'encoder.layers.7.conv3x1_2.bias', 'encoder.layers.7.conv1x3_2.weight', 'encoder.layers.7.conv1x3_2.bias', 'encoder.layers.7.bn2.weight', 'encoder.layers.7.bn2.bias', 'encoder.layers.7.bn2.running_mean', 'encoder.layers.7.bn2.running_var', 'encoder.layers.7.bn2.num_batches_tracked', 'encoder.layers.8.conv3x1_1.weight', 'encoder.layers.8.conv3x1_1.bias', 'encoder.layers.8.conv1x3_1.weight', 'encoder.layers.8.conv1x3_1.bias', 'encoder.layers.8.bn1.weight', 'encoder.layers.8.bn1.bias', 'encoder.layers.8.bn1.running_mean', 'encoder.layers.8.bn1.running_var', 'encoder.layers.8.bn1.num_batches_tracked', 'encoder.layers.8.conv3x1_2.weight', 'encoder.layers.8.conv3x1_2.bias', 'encoder.layers.8.conv1x3_2.weight', 'encoder.layers.8.conv1x3_2.bias', 'encoder.layers.8.bn2.weight', 'encoder.layers.8.bn2.bias', 'encoder.layers.8.bn2.running_mean', 'encoder.layers.8.bn2.running_var', 'encoder.layers.8.bn2.num_batches_tracked', 'encoder.layers.9.conv3x1_1.weight', 'encoder.layers.9.conv3x1_1.bias', 'encoder.layers.9.conv1x3_1.weight', 'encoder.layers.9.conv1x3_1.bias', 'encoder.layers.9.bn1.weight', 'encoder.layers.9.bn1.bias', 'encoder.layers.9.bn1.running_mean', 'encoder.layers.9.bn1.running_var', 'encoder.layers.9.bn1.num_batches_tracked', 'encoder.layers.9.conv3x1_2.weight', 'encoder.layers.9.conv3x1_2.bias', 'encoder.layers.9.conv1x3_2.weight', 'encoder.layers.9.conv1x3_2.bias', 'encoder.layers.9.bn2.weight', 'encoder.layers.9.bn2.bias', 'encoder.layers.9.bn2.running_mean', 'encoder.layers.9.bn2.running_var', 'encoder.layers.9.bn2.num_batches_tracked', 'encoder.layers.10.conv3x1_1.weight', 'encoder.layers.10.conv3x1_1.bias', 'encoder.layers.10.conv1x3_1.weight', 'encoder.layers.10.conv1x3_1.bias', 'encoder.layers.10.bn1.weight', 'encoder.layers.10.bn1.bias', 'encoder.layers.10.bn1.running_mean', 'encoder.layers.10.bn1.running_var', 'encoder.layers.10.bn1.num_batches_tracked', 'encoder.layers.10.conv3x1_2.weight', 'encoder.layers.10.conv3x1_2.bias', 'encoder.layers.10.conv1x3_2.weight', 'encoder.layers.10.conv1x3_2.bias', 'encoder.layers.10.bn2.weight', 'encoder.layers.10.bn2.bias', 'encoder.layers.10.bn2.running_mean', 'encoder.layers.10.bn2.running_var', 'encoder.layers.10.bn2.num_batches_tracked', 'encoder.layers.11.conv3x1_1.weight', 'encoder.layers.11.conv3x1_1.bias', 'encoder.layers.11.conv1x3_1.weight', 'encoder.layers.11.conv1x3_1.bias', 'encoder.layers.11.bn1.weight', 'encoder.layers.11.bn1.bias', 'encoder.layers.11.bn1.running_mean', 'encoder.layers.11.bn1.running_var', 'encoder.layers.11.bn1.num_batches_tracked', 'encoder.layers.11.conv3x1_2.weight', 'encoder.layers.11.conv3x1_2.bias', 'encoder.layers.11.conv1x3_2.weight', 'encoder.layers.11.conv1x3_2.bias', 'encoder.layers.11.bn2.weight', 'encoder.layers.11.bn2.bias', 'encoder.layers.11.bn2.running_mean', 'encoder.layers.11.bn2.running_var', 'encoder.layers.11.bn2.num_batches_tracked', 'encoder.layers.12.conv3x1_1.weight', 'encoder.layers.12.conv3x1_1.bias', 'encoder.layers.12.conv1x3_1.weight', 'encoder.layers.12.conv1x3_1.bias', 'encoder.layers.12.bn1.weight', 'encoder.layers.12.bn1.bias', 'encoder.layers.12.bn1.running_mean', 'encoder.layers.12.bn1.running_var', 'encoder.layers.12.bn1.num_batches_tracked', 'encoder.layers.12.conv3x1_2.weight', 'encoder.layers.12.conv3x1_2.bias', 'encoder.layers.12.conv1x3_2.weight', 'encoder.layers.12.conv1x3_2.bias', 'encoder.layers.12.bn2.weight', 'encoder.layers.12.bn2.bias', 'encoder.layers.12.bn2.running_mean', 'encoder.layers.12.bn2.running_var', 'encoder.layers.12.bn2.num_batches_tracked', 'encoder.layers.13.conv3x1_1.weight', 'encoder.layers.13.conv3x1_1.bias', 'encoder.layers.13.conv1x3_1.weight', 'encoder.layers.13.conv1x3_1.bias', 'encoder.layers.13.bn1.weight', 'encoder.layers.13.bn1.bias', 'encoder.layers.13.bn1.running_mean', 'encoder.layers.13.bn1.running_var', 'encoder.layers.13.bn1.num_batches_tracked', 'encoder.layers.13.conv3x1_2.weight', 'encoder.layers.13.conv3x1_2.bias', 'encoder.layers.13.conv1x3_2.weight', 'encoder.layers.13.conv1x3_2.bias', 'encoder.layers.13.bn2.weight', 'encoder.layers.13.bn2.bias', 'encoder.layers.13.bn2.running_mean', 'encoder.layers.13.bn2.running_var', 'encoder.layers.13.bn2.num_batches_tracked', 'encoder.layers.14.conv3x1_1.weight', 'encoder.layers.14.conv3x1_1.bias', 'encoder.layers.14.conv1x3_1.weight', 'encoder.layers.14.conv1x3_1.bias', 'encoder.layers.14.bn1.weight', 'encoder.layers.14.bn1.bias', 'encoder.layers.14.bn1.running_mean', 'encoder.layers.14.bn1.running_var', 'encoder.layers.14.bn1.num_batches_tracked', 'encoder.layers.14.conv3x1_2.weight', 'encoder.layers.14.conv3x1_2.bias', 'encoder.layers.14.conv1x3_2.weight', 'encoder.layers.14.conv1x3_2.bias', 'encoder.layers.14.bn2.weight', 'encoder.layers.14.bn2.bias', 'encoder.layers.14.bn2.running_mean', 'encoder.layers.14.bn2.running_var', 'encoder.layers.14.bn2.num_batches_tracked', 'encoder.output_conv.weight', 'encoder.output_conv.bias', 'decoder.layers.0.conv.weight', 'decoder.layers.0.conv.bias', 'decoder.layers.0.bn.weight', 'decoder.layers.0.bn.bias', 'decoder.layers.0.bn.running_mean', 'decoder.layers.0.bn.running_var', 'decoder.layers.0.bn.num_batches_tracked', 'decoder.layers.1.conv3x1_1.weight', 'decoder.layers.1.conv3x1_1.bias', 'decoder.layers.1.conv1x3_1.weight', 'decoder.layers.1.conv1x3_1.bias', 'decoder.layers.1.bn1.weight', 'decoder.layers.1.bn1.bias', 'decoder.layers.1.bn1.running_mean', 'decoder.layers.1.bn1.running_var', 'decoder.layers.1.bn1.num_batches_tracked', 'decoder.layers.1.conv3x1_2.weight', 'decoder.layers.1.conv3x1_2.bias', 'decoder.layers.1.conv1x3_2.weight', 'decoder.layers.1.conv1x3_2.bias', 'decoder.layers.1.bn2.weight', 'decoder.layers.1.bn2.bias', 'decoder.layers.1.bn2.running_mean', 'decoder.layers.1.bn2.running_var', 'decoder.layers.1.bn2.num_batches_tracked', 'decoder.layers.2.conv3x1_1.weight', 'decoder.layers.2.conv3x1_1.bias', 'decoder.layers.2.conv1x3_1.weight', 'decoder.layers.2.conv1x3_1.bias', 'decoder.layers.2.bn1.weight', 'decoder.layers.2.bn1.bias', 'decoder.layers.2.bn1.running_mean', 'decoder.layers.2.bn1.running_var', 'decoder.layers.2.bn1.num_batches_tracked', 'decoder.layers.2.conv3x1_2.weight', 'decoder.layers.2.conv3x1_2.bias', 'decoder.layers.2.conv1x3_2.weight', 'decoder.layers.2.conv1x3_2.bias', 'decoder.layers.2.bn2.weight', 'decoder.layers.2.bn2.bias', 'decoder.layers.2.bn2.running_mean', 'decoder.layers.2.bn2.running_var', 'decoder.layers.2.bn2.num_batches_tracked', 'decoder.layers.3.conv.weight', 'decoder.layers.3.conv.bias', 'decoder.layers.3.bn.weight', 'decoder.layers.3.bn.bias', 'decoder.layers.3.bn.running_mean', 'decoder.layers.3.bn.running_var', 'decoder.layers.3.bn.num_batches_tracked', 'decoder.layers.4.conv3x1_1.weight', 'decoder.layers.4.conv3x1_1.bias', 'decoder.layers.4.conv1x3_1.weight', 'decoder.layers.4.conv1x3_1.bias', 'decoder.layers.4.bn1.weight', 'decoder.layers.4.bn1.bias', 'decoder.layers.4.bn1.running_mean', 'decoder.layers.4.bn1.running_var', 'decoder.layers.4.bn1.num_batches_tracked', 'decoder.layers.4.conv3x1_2.weight', 'decoder.layers.4.conv3x1_2.bias', 'decoder.layers.4.conv1x3_2.weight', 'decoder.layers.4.conv1x3_2.bias', 'decoder.layers.4.bn2.weight', 'decoder.layers.4.bn2.bias', 'decoder.layers.4.bn2.running_mean', 'decoder.layers.4.bn2.running_var', 'decoder.layers.4.bn2.num_batches_tracked', 'decoder.layers.5.conv3x1_1.weight', 'decoder.layers.5.conv3x1_1.bias', 'decoder.layers.5.conv1x3_1.weight', 'decoder.layers.5.conv1x3_1.bias', 'decoder.layers.5.bn1.weight', 'decoder.layers.5.bn1.bias', 'decoder.layers.5.bn1.running_mean', 'decoder.layers.5.bn1.running_var', 'decoder.layers.5.bn1.num_batches_tracked', 'decoder.layers.5.conv3x1_2.weight', 'decoder.layers.5.conv3x1_2.bias', 'decoder.layers.5.conv1x3_2.weight', 'decoder.layers.5.conv1x3_2.bias', 'decoder.layers.5.bn2.weight', 'decoder.layers.5.bn2.bias', 'decoder.layers.5.bn2.running_mean', 'decoder.layers.5.bn2.running_var', 'decoder.layers.5.bn2.num_batches_tracked', 'decoder.output_conv.weight', 'decoder.output_conv.bias'])\n",
            "Import Model erfnet with weights erfnet_pretrained.pth to FineTune\n",
            "========== TRAINING ===========\n",
            "/content/cityscapes/leftImg8bit/train\n",
            "/content/cityscapes/leftImg8bit/val\n",
            "----- TRAINING - EPOCH 1 -----\n",
            "LEARNING RATE:  5e-05\n",
            "loss: 0.8898 (epoch: 1, step: 0) // Avg time/img: 0.3255 s\n",
            "loss: 1.234 (epoch: 1, step: 50) // Avg time/img: 0.0495 s\n",
            "loss: 1.208 (epoch: 1, step: 100) // Avg time/img: 0.0470 s\n",
            "loss: 1.209 (epoch: 1, step: 150) // Avg time/img: 0.0462 s\n",
            "loss: 1.19 (epoch: 1, step: 200) // Avg time/img: 0.0459 s\n",
            "loss: 1.188 (epoch: 1, step: 250) // Avg time/img: 0.0458 s\n",
            "loss: 1.185 (epoch: 1, step: 300) // Avg time/img: 0.0456 s\n",
            "loss: 1.169 (epoch: 1, step: 350) // Avg time/img: 0.0456 s\n",
            "loss: 1.169 (epoch: 1, step: 400) // Avg time/img: 0.0455 s\n",
            "loss: 1.16 (epoch: 1, step: 450) // Avg time/img: 0.0455 s\n",
            "----- VALIDATING - EPOCH 1 -----\n",
            "VAL loss: 1.152 (epoch: 1, step: 0) // Avg time/img: 0.0486 s\n",
            "VAL loss: 1.342 (epoch: 1, step: 50) // Avg time/img: 0.0338 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m72.19\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/erfnet_training_focal_loss/model-001.pth (epoch: 1)\n",
            "save: ../save/erfnet_training_focal_loss/model_best.pth (epoch: 1)\n",
            "----- TRAINING - EPOCH 2 -----\n",
            "LEARNING RATE:  4.774426908107499e-05\n",
            "loss: 1.701 (epoch: 2, step: 0) // Avg time/img: 0.0505 s\n",
            "loss: 1.016 (epoch: 2, step: 50) // Avg time/img: 0.0459 s\n",
            "loss: 1.004 (epoch: 2, step: 100) // Avg time/img: 0.0458 s\n",
            "loss: 1.011 (epoch: 2, step: 150) // Avg time/img: 0.0456 s\n",
            "loss: 1.006 (epoch: 2, step: 200) // Avg time/img: 0.0458 s\n",
            "loss: 0.9969 (epoch: 2, step: 250) // Avg time/img: 0.0459 s\n",
            "loss: 0.9928 (epoch: 2, step: 300) // Avg time/img: 0.0459 s\n",
            "loss: 0.9828 (epoch: 2, step: 350) // Avg time/img: 0.0459 s\n",
            "loss: 0.9711 (epoch: 2, step: 400) // Avg time/img: 0.0460 s\n",
            "loss: 0.9636 (epoch: 2, step: 450) // Avg time/img: 0.0461 s\n",
            "----- VALIDATING - EPOCH 2 -----\n",
            "VAL loss: 0.9424 (epoch: 2, step: 0) // Avg time/img: 0.0463 s\n",
            "VAL loss: 1.113 (epoch: 2, step: 50) // Avg time/img: 0.0341 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.81\u001b[0m %\n",
            "save: ../save/erfnet_training_focal_loss/model-002.pth (epoch: 2)\n",
            "----- TRAINING - EPOCH 3 -----\n",
            "LEARNING RATE:  4.547662880414811e-05\n",
            "loss: 0.7212 (epoch: 3, step: 0) // Avg time/img: 0.0504 s\n",
            "loss: 0.8354 (epoch: 3, step: 50) // Avg time/img: 0.0469 s\n",
            "loss: 0.8059 (epoch: 3, step: 100) // Avg time/img: 0.0468 s\n",
            "loss: 0.8033 (epoch: 3, step: 150) // Avg time/img: 0.0471 s\n",
            "loss: 0.8028 (epoch: 3, step: 200) // Avg time/img: 0.0470 s\n",
            "loss: 0.8009 (epoch: 3, step: 250) // Avg time/img: 0.0474 s\n",
            "loss: 0.7937 (epoch: 3, step: 300) // Avg time/img: 0.0477 s\n",
            "loss: 0.7847 (epoch: 3, step: 350) // Avg time/img: 0.0479 s\n",
            "loss: 0.7761 (epoch: 3, step: 400) // Avg time/img: 0.0480 s\n",
            "loss: 0.7713 (epoch: 3, step: 450) // Avg time/img: 0.0479 s\n",
            "----- VALIDATING - EPOCH 3 -----\n",
            "VAL loss: 0.774 (epoch: 3, step: 0) // Avg time/img: 0.0505 s\n",
            "VAL loss: 0.9109 (epoch: 3, step: 50) // Avg time/img: 0.0351 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m72.20\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/erfnet_training_focal_loss/model-003.pth (epoch: 3)\n",
            "save: ../save/erfnet_training_focal_loss/model_best.pth (epoch: 3)\n",
            "----- TRAINING - EPOCH 4 -----\n",
            "LEARNING RATE:  4.319634861514096e-05\n",
            "loss: 0.616 (epoch: 4, step: 0) // Avg time/img: 0.0644 s\n",
            "loss: 0.6356 (epoch: 4, step: 50) // Avg time/img: 0.0465 s\n",
            "loss: 0.6477 (epoch: 4, step: 100) // Avg time/img: 0.0469 s\n",
            "loss: 0.6538 (epoch: 4, step: 150) // Avg time/img: 0.0468 s\n",
            "loss: 0.6589 (epoch: 4, step: 200) // Avg time/img: 0.0471 s\n",
            "loss: 0.6507 (epoch: 4, step: 250) // Avg time/img: 0.0470 s\n",
            "loss: 0.6461 (epoch: 4, step: 300) // Avg time/img: 0.0471 s\n",
            "loss: 0.6281 (epoch: 4, step: 350) // Avg time/img: 0.0470 s\n",
            "loss: 0.6199 (epoch: 4, step: 400) // Avg time/img: 0.0472 s\n",
            "loss: 0.6182 (epoch: 4, step: 450) // Avg time/img: 0.0471 s\n",
            "----- VALIDATING - EPOCH 4 -----\n",
            "VAL loss: 0.6228 (epoch: 4, step: 0) // Avg time/img: 0.0465 s\n",
            "VAL loss: 0.745 (epoch: 4, step: 50) // Avg time/img: 0.0344 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.65\u001b[0m %\n",
            "save: ../save/erfnet_training_focal_loss/model-004.pth (epoch: 4)\n",
            "----- TRAINING - EPOCH 5 -----\n",
            "LEARNING RATE:  4.090260730254292e-05\n",
            "loss: 0.4657 (epoch: 5, step: 0) // Avg time/img: 0.0532 s\n",
            "loss: 0.5276 (epoch: 5, step: 50) // Avg time/img: 0.0472 s\n",
            "loss: 0.5289 (epoch: 5, step: 100) // Avg time/img: 0.0475 s\n",
            "loss: 0.5409 (epoch: 5, step: 150) // Avg time/img: 0.0473 s\n",
            "loss: 0.5328 (epoch: 5, step: 200) // Avg time/img: 0.0472 s\n",
            "loss: 0.5351 (epoch: 5, step: 250) // Avg time/img: 0.0470 s\n",
            "loss: 0.5301 (epoch: 5, step: 300) // Avg time/img: 0.0470 s\n",
            "loss: 0.5257 (epoch: 5, step: 350) // Avg time/img: 0.0469 s\n",
            "loss: 0.5196 (epoch: 5, step: 400) // Avg time/img: 0.0470 s\n",
            "loss: 0.519 (epoch: 5, step: 450) // Avg time/img: 0.0470 s\n",
            "----- VALIDATING - EPOCH 5 -----\n",
            "VAL loss: 0.5539 (epoch: 5, step: 0) // Avg time/img: 0.0493 s\n",
            "VAL loss: 0.665 (epoch: 5, step: 50) // Avg time/img: 0.0347 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m69.91\u001b[0m %\n",
            "save: ../save/erfnet_training_focal_loss/model-005.pth (epoch: 5)\n",
            "----- TRAINING - EPOCH 6 -----\n",
            "LEARNING RATE:  3.859447533617852e-05\n",
            "loss: 0.4165 (epoch: 6, step: 0) // Avg time/img: 0.0539 s\n",
            "loss: 0.5141 (epoch: 6, step: 50) // Avg time/img: 0.0467 s\n",
            "loss: 0.5019 (epoch: 6, step: 100) // Avg time/img: 0.0470 s\n",
            "loss: 0.4899 (epoch: 6, step: 150) // Avg time/img: 0.0466 s\n",
            "loss: 0.4777 (epoch: 6, step: 200) // Avg time/img: 0.0468 s\n",
            "loss: 0.4737 (epoch: 6, step: 250) // Avg time/img: 0.0467 s\n",
            "loss: 0.4698 (epoch: 6, step: 300) // Avg time/img: 0.0468 s\n",
            "loss: 0.4661 (epoch: 6, step: 350) // Avg time/img: 0.0470 s\n",
            "loss: 0.4636 (epoch: 6, step: 400) // Avg time/img: 0.0470 s\n",
            "loss: 0.4594 (epoch: 6, step: 450) // Avg time/img: 0.0473 s\n",
            "----- VALIDATING - EPOCH 6 -----\n",
            "VAL loss: 0.5128 (epoch: 6, step: 0) // Avg time/img: 0.0502 s\n",
            "VAL loss: 0.6238 (epoch: 6, step: 50) // Avg time/img: 0.0356 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m69.07\u001b[0m %\n",
            "save: ../save/erfnet_training_focal_loss/model-006.pth (epoch: 6)\n",
            "----- TRAINING - EPOCH 7 -----\n",
            "LEARNING RATE:  3.6270892346861e-05\n",
            "loss: 0.3477 (epoch: 7, step: 0) // Avg time/img: 0.0535 s\n",
            "loss: 0.4589 (epoch: 7, step: 50) // Avg time/img: 0.0467 s\n",
            "loss: 0.4558 (epoch: 7, step: 100) // Avg time/img: 0.0473 s\n",
            "loss: 0.4494 (epoch: 7, step: 150) // Avg time/img: 0.0472 s\n",
            "loss: 0.4431 (epoch: 7, step: 200) // Avg time/img: 0.0471 s\n",
            "loss: 0.4403 (epoch: 7, step: 250) // Avg time/img: 0.0471 s\n",
            "loss: 0.4344 (epoch: 7, step: 300) // Avg time/img: 0.0472 s\n",
            "loss: 0.4297 (epoch: 7, step: 350) // Avg time/img: 0.0472 s\n",
            "loss: 0.4278 (epoch: 7, step: 400) // Avg time/img: 0.0473 s\n",
            "loss: 0.4256 (epoch: 7, step: 450) // Avg time/img: 0.0472 s\n",
            "----- VALIDATING - EPOCH 7 -----\n",
            "VAL loss: 0.5099 (epoch: 7, step: 0) // Avg time/img: 0.0545 s\n",
            "VAL loss: 0.599 (epoch: 7, step: 50) // Avg time/img: 0.0344 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m68.56\u001b[0m %\n",
            "save: ../save/erfnet_training_focal_loss/model-007.pth (epoch: 7)\n",
            "----- TRAINING - EPOCH 8 -----\n",
            "LEARNING RATE:  3.393063796290625e-05\n",
            "loss: 0.2477 (epoch: 8, step: 0) // Avg time/img: 0.0552 s\n",
            "loss: 0.4132 (epoch: 8, step: 50) // Avg time/img: 0.0467 s\n",
            "loss: 0.4098 (epoch: 8, step: 100) // Avg time/img: 0.0464 s\n",
            "loss: 0.4061 (epoch: 8, step: 150) // Avg time/img: 0.0467 s\n",
            "loss: 0.4058 (epoch: 8, step: 200) // Avg time/img: 0.0467 s\n",
            "loss: 0.409 (epoch: 8, step: 250) // Avg time/img: 0.0468 s\n",
            "loss: 0.409 (epoch: 8, step: 300) // Avg time/img: 0.0467 s\n",
            "loss: 0.407 (epoch: 8, step: 350) // Avg time/img: 0.0468 s\n",
            "loss: 0.4064 (epoch: 8, step: 400) // Avg time/img: 0.0468 s\n",
            "loss: 0.4037 (epoch: 8, step: 450) // Avg time/img: 0.0469 s\n",
            "----- VALIDATING - EPOCH 8 -----\n",
            "VAL loss: 0.4708 (epoch: 8, step: 0) // Avg time/img: 0.0447 s\n",
            "VAL loss: 0.5627 (epoch: 8, step: 50) // Avg time/img: 0.0343 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m68.52\u001b[0m %\n",
            "save: ../save/erfnet_training_focal_loss/model-008.pth (epoch: 8)\n",
            "----- TRAINING - EPOCH 9 -----\n",
            "LEARNING RATE:  3.157229337446777e-05\n",
            "loss: 0.3547 (epoch: 9, step: 0) // Avg time/img: 0.0556 s\n",
            "loss: 0.3986 (epoch: 9, step: 50) // Avg time/img: 0.0463 s\n",
            "loss: 0.3984 (epoch: 9, step: 100) // Avg time/img: 0.0461 s\n",
            "loss: 0.3989 (epoch: 9, step: 150) // Avg time/img: 0.0464 s\n",
            "loss: 0.397 (epoch: 9, step: 200) // Avg time/img: 0.0464 s\n",
            "loss: 0.3944 (epoch: 9, step: 250) // Avg time/img: 0.0464 s\n",
            "loss: 0.3906 (epoch: 9, step: 300) // Avg time/img: 0.0465 s\n",
            "loss: 0.3895 (epoch: 9, step: 350) // Avg time/img: 0.0464 s\n",
            "loss: 0.3903 (epoch: 9, step: 400) // Avg time/img: 0.0464 s\n",
            "loss: 0.3894 (epoch: 9, step: 450) // Avg time/img: 0.0466 s\n",
            "----- VALIDATING - EPOCH 9 -----\n",
            "VAL loss: 0.4423 (epoch: 9, step: 0) // Avg time/img: 0.0446 s\n",
            "VAL loss: 0.5326 (epoch: 9, step: 50) // Avg time/img: 0.0356 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m68.82\u001b[0m %\n",
            "save: ../save/erfnet_training_focal_loss/model-009.pth (epoch: 9)\n",
            "----- TRAINING - EPOCH 10 -----\n",
            "LEARNING RATE:  2.9194189645999014e-05\n",
            "loss: 0.3467 (epoch: 10, step: 0) // Avg time/img: 0.0578 s\n",
            "loss: 0.3799 (epoch: 10, step: 50) // Avg time/img: 0.0469 s\n",
            "loss: 0.3782 (epoch: 10, step: 100) // Avg time/img: 0.0468 s\n",
            "loss: 0.3825 (epoch: 10, step: 150) // Avg time/img: 0.0471 s\n",
            "loss: 0.384 (epoch: 10, step: 200) // Avg time/img: 0.0470 s\n",
            "loss: 0.3815 (epoch: 10, step: 250) // Avg time/img: 0.0470 s\n",
            "loss: 0.3789 (epoch: 10, step: 300) // Avg time/img: 0.0469 s\n",
            "loss: 0.3749 (epoch: 10, step: 350) // Avg time/img: 0.0470 s\n",
            "loss: 0.3752 (epoch: 10, step: 400) // Avg time/img: 0.0469 s\n",
            "loss: 0.3765 (epoch: 10, step: 450) // Avg time/img: 0.0469 s\n",
            "----- VALIDATING - EPOCH 10 -----\n",
            "VAL loss: 0.4392 (epoch: 10, step: 0) // Avg time/img: 0.0485 s\n",
            "VAL loss: 0.5314 (epoch: 10, step: 50) // Avg time/img: 0.0344 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m69.09\u001b[0m %\n",
            "save: ../save/erfnet_training_focal_loss/model-010.pth (epoch: 10)\n",
            "----- TRAINING - EPOCH 11 -----\n",
            "LEARNING RATE:  2.679433656340733e-05\n",
            "loss: 0.3007 (epoch: 11, step: 0) // Avg time/img: 0.0611 s\n",
            "loss: 0.3743 (epoch: 11, step: 50) // Avg time/img: 0.0463 s\n",
            "loss: 0.3793 (epoch: 11, step: 100) // Avg time/img: 0.0469 s\n",
            "loss: 0.38 (epoch: 11, step: 150) // Avg time/img: 0.0467 s\n",
            "loss: 0.376 (epoch: 11, step: 200) // Avg time/img: 0.0467 s\n",
            "loss: 0.3711 (epoch: 11, step: 250) // Avg time/img: 0.0467 s\n",
            "loss: 0.3708 (epoch: 11, step: 300) // Avg time/img: 0.0467 s\n",
            "loss: 0.3703 (epoch: 11, step: 350) // Avg time/img: 0.0468 s\n",
            "loss: 0.3711 (epoch: 11, step: 400) // Avg time/img: 0.0468 s\n",
            "loss: 0.3705 (epoch: 11, step: 450) // Avg time/img: 0.0468 s\n",
            "----- VALIDATING - EPOCH 11 -----\n",
            "VAL loss: 0.4139 (epoch: 11, step: 0) // Avg time/img: 0.0459 s\n",
            "VAL loss: 0.5157 (epoch: 11, step: 50) // Avg time/img: 0.0344 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m69.41\u001b[0m %\n",
            "save: ../save/erfnet_training_focal_loss/model-011.pth (epoch: 11)\n",
            "----- TRAINING - EPOCH 12 -----\n",
            "LEARNING RATE:  2.437032195894977e-05\n",
            "loss: 0.316 (epoch: 12, step: 0) // Avg time/img: 0.0629 s\n",
            "loss: 0.3704 (epoch: 12, step: 50) // Avg time/img: 0.0471 s\n",
            "loss: 0.3638 (epoch: 12, step: 100) // Avg time/img: 0.0465 s\n",
            "loss: 0.3628 (epoch: 12, step: 150) // Avg time/img: 0.0466 s\n",
            "loss: 0.3653 (epoch: 12, step: 200) // Avg time/img: 0.0465 s\n",
            "loss: 0.3657 (epoch: 12, step: 250) // Avg time/img: 0.0466 s\n",
            "loss: 0.3631 (epoch: 12, step: 300) // Avg time/img: 0.0465 s\n",
            "loss: 0.3662 (epoch: 12, step: 350) // Avg time/img: 0.0466 s\n",
            "loss: 0.3667 (epoch: 12, step: 400) // Avg time/img: 0.0466 s\n",
            "loss: 0.3661 (epoch: 12, step: 450) // Avg time/img: 0.0467 s\n",
            "----- VALIDATING - EPOCH 12 -----\n",
            "VAL loss: 0.421 (epoch: 12, step: 0) // Avg time/img: 0.0510 s\n",
            "VAL loss: 0.5023 (epoch: 12, step: 50) // Avg time/img: 0.0363 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m69.33\u001b[0m %\n",
            "save: ../save/erfnet_training_focal_loss/model-012.pth (epoch: 12)\n",
            "----- TRAINING - EPOCH 13 -----\n",
            "LEARNING RATE:  2.191916452770435e-05\n",
            "loss: 0.3917 (epoch: 13, step: 0) // Avg time/img: 0.0540 s\n",
            "loss: 0.3655 (epoch: 13, step: 50) // Avg time/img: 0.0478 s\n",
            "loss: 0.3657 (epoch: 13, step: 100) // Avg time/img: 0.0476 s\n",
            "loss: 0.3627 (epoch: 13, step: 150) // Avg time/img: 0.0476 s\n",
            "loss: 0.3646 (epoch: 13, step: 200) // Avg time/img: 0.0477 s\n",
            "loss: 0.3629 (epoch: 13, step: 250) // Avg time/img: 0.0476 s\n",
            "loss: 0.3608 (epoch: 13, step: 300) // Avg time/img: 0.0475 s\n",
            "loss: 0.3609 (epoch: 13, step: 350) // Avg time/img: 0.0474 s\n",
            "loss: 0.3602 (epoch: 13, step: 400) // Avg time/img: 0.0473 s\n",
            "loss: 0.3603 (epoch: 13, step: 450) // Avg time/img: 0.0472 s\n",
            "----- VALIDATING - EPOCH 13 -----\n",
            "VAL loss: 0.4108 (epoch: 13, step: 0) // Avg time/img: 0.0499 s\n",
            "VAL loss: 0.4957 (epoch: 13, step: 50) // Avg time/img: 0.0346 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m69.34\u001b[0m %\n",
            "save: ../save/erfnet_training_focal_loss/model-013.pth (epoch: 13)\n",
            "----- TRAINING - EPOCH 14 -----\n",
            "LEARNING RATE:  1.9437089939938174e-05\n",
            "loss: 0.3779 (epoch: 14, step: 0) // Avg time/img: 0.0580 s\n",
            "loss: 0.3632 (epoch: 14, step: 50) // Avg time/img: 0.0470 s\n",
            "loss: 0.3619 (epoch: 14, step: 100) // Avg time/img: 0.0469 s\n",
            "loss: 0.367 (epoch: 14, step: 150) // Avg time/img: 0.0466 s\n",
            "loss: 0.3635 (epoch: 14, step: 200) // Avg time/img: 0.0467 s\n",
            "loss: 0.3611 (epoch: 14, step: 250) // Avg time/img: 0.0467 s\n",
            "loss: 0.3628 (epoch: 14, step: 300) // Avg time/img: 0.0467 s\n",
            "loss: 0.3616 (epoch: 14, step: 350) // Avg time/img: 0.0467 s\n",
            "loss: 0.3594 (epoch: 14, step: 400) // Avg time/img: 0.0468 s\n",
            "loss: 0.3573 (epoch: 14, step: 450) // Avg time/img: 0.0468 s\n",
            "----- VALIDATING - EPOCH 14 -----\n",
            "VAL loss: 0.4113 (epoch: 14, step: 0) // Avg time/img: 0.0494 s\n",
            "VAL loss: 0.4925 (epoch: 14, step: 50) // Avg time/img: 0.0352 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m70.05\u001b[0m %\n",
            "save: ../save/erfnet_training_focal_loss/model-014.pth (epoch: 14)\n",
            "----- TRAINING - EPOCH 15 -----\n",
            "LEARNING RATE:  1.6919173095082493e-05\n",
            "loss: 0.2982 (epoch: 15, step: 0) // Avg time/img: 0.0569 s\n",
            "loss: 0.359 (epoch: 15, step: 50) // Avg time/img: 0.0469 s\n",
            "loss: 0.3553 (epoch: 15, step: 100) // Avg time/img: 0.0467 s\n",
            "loss: 0.3651 (epoch: 15, step: 150) // Avg time/img: 0.0467 s\n",
            "loss: 0.361 (epoch: 15, step: 200) // Avg time/img: 0.0467 s\n",
            "loss: 0.3587 (epoch: 15, step: 250) // Avg time/img: 0.0467 s\n",
            "loss: 0.3584 (epoch: 15, step: 300) // Avg time/img: 0.0468 s\n",
            "loss: 0.3581 (epoch: 15, step: 350) // Avg time/img: 0.0467 s\n",
            "loss: 0.3572 (epoch: 15, step: 400) // Avg time/img: 0.0468 s\n",
            "loss: 0.3553 (epoch: 15, step: 450) // Avg time/img: 0.0467 s\n",
            "----- VALIDATING - EPOCH 15 -----\n",
            "VAL loss: 0.4225 (epoch: 15, step: 0) // Avg time/img: 0.0481 s\n",
            "VAL loss: 0.4985 (epoch: 15, step: 50) // Avg time/img: 0.0335 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m69.60\u001b[0m %\n",
            "save: ../save/erfnet_training_focal_loss/model-015.pth (epoch: 15)\n",
            "----- TRAINING - EPOCH 16 -----\n",
            "LEARNING RATE:  1.4358729437462937e-05\n",
            "loss: 0.3645 (epoch: 16, step: 0) // Avg time/img: 0.0453 s\n",
            "loss: 0.353 (epoch: 16, step: 50) // Avg time/img: 0.0473 s\n",
            "loss: 0.3509 (epoch: 16, step: 100) // Avg time/img: 0.0469 s\n",
            "loss: 0.3513 (epoch: 16, step: 150) // Avg time/img: 0.0472 s\n",
            "loss: 0.35 (epoch: 16, step: 200) // Avg time/img: 0.0473 s\n",
            "loss: 0.3487 (epoch: 16, step: 250) // Avg time/img: 0.0473 s\n",
            "loss: 0.3474 (epoch: 16, step: 300) // Avg time/img: 0.0474 s\n",
            "loss: 0.3482 (epoch: 16, step: 350) // Avg time/img: 0.0477 s\n",
            "loss: 0.3486 (epoch: 16, step: 400) // Avg time/img: 0.0478 s\n",
            "loss: 0.3499 (epoch: 16, step: 450) // Avg time/img: 0.0480 s\n",
            "----- VALIDATING - EPOCH 16 -----\n",
            "VAL loss: 0.4092 (epoch: 16, step: 0) // Avg time/img: 0.0463 s\n",
            "VAL loss: 0.4823 (epoch: 16, step: 50) // Avg time/img: 0.0348 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m69.83\u001b[0m %\n",
            "save: ../save/erfnet_training_focal_loss/model-016.pth (epoch: 16)\n",
            "----- TRAINING - EPOCH 17 -----\n",
            "LEARNING RATE:  1.1746189430880188e-05\n",
            "loss: 0.3241 (epoch: 17, step: 0) // Avg time/img: 0.0530 s\n",
            "loss: 0.3386 (epoch: 17, step: 50) // Avg time/img: 0.0475 s\n",
            "loss: 0.3379 (epoch: 17, step: 100) // Avg time/img: 0.0470 s\n",
            "loss: 0.3438 (epoch: 17, step: 150) // Avg time/img: 0.0471 s\n",
            "loss: 0.3383 (epoch: 17, step: 200) // Avg time/img: 0.0469 s\n",
            "loss: 0.341 (epoch: 17, step: 250) // Avg time/img: 0.0470 s\n",
            "loss: 0.345 (epoch: 17, step: 300) // Avg time/img: 0.0471 s\n",
            "loss: 0.3461 (epoch: 17, step: 350) // Avg time/img: 0.0472 s\n",
            "loss: 0.3483 (epoch: 17, step: 400) // Avg time/img: 0.0473 s\n",
            "loss: 0.348 (epoch: 17, step: 450) // Avg time/img: 0.0472 s\n",
            "----- VALIDATING - EPOCH 17 -----\n",
            "VAL loss: 0.4017 (epoch: 17, step: 0) // Avg time/img: 0.0530 s\n",
            "VAL loss: 0.4826 (epoch: 17, step: 50) // Avg time/img: 0.0342 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m70.03\u001b[0m %\n",
            "save: ../save/erfnet_training_focal_loss/model-017.pth (epoch: 17)\n",
            "----- TRAINING - EPOCH 18 -----\n",
            "LEARNING RATE:  9.066760365683729e-06\n",
            "loss: 0.3123 (epoch: 18, step: 0) // Avg time/img: 0.0519 s\n",
            "loss: 0.3497 (epoch: 18, step: 50) // Avg time/img: 0.0475 s\n",
            "loss: 0.3562 (epoch: 18, step: 100) // Avg time/img: 0.0472 s\n",
            "loss: 0.3535 (epoch: 18, step: 150) // Avg time/img: 0.0471 s\n",
            "loss: 0.3523 (epoch: 18, step: 200) // Avg time/img: 0.0470 s\n",
            "loss: 0.3519 (epoch: 18, step: 250) // Avg time/img: 0.0469 s\n",
            "loss: 0.3517 (epoch: 18, step: 300) // Avg time/img: 0.0468 s\n",
            "loss: 0.3509 (epoch: 18, step: 350) // Avg time/img: 0.0471 s\n",
            "loss: 0.3498 (epoch: 18, step: 400) // Avg time/img: 0.0473 s\n",
            "loss: 0.3499 (epoch: 18, step: 450) // Avg time/img: 0.0476 s\n",
            "----- VALIDATING - EPOCH 18 -----\n",
            "VAL loss: 0.4023 (epoch: 18, step: 0) // Avg time/img: 0.0527 s\n",
            "VAL loss: 0.483 (epoch: 18, step: 50) // Avg time/img: 0.0363 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m69.99\u001b[0m %\n",
            "save: ../save/erfnet_training_focal_loss/model-018.pth (epoch: 18)\n",
            "----- TRAINING - EPOCH 19 -----\n",
            "LEARNING RATE:  6.294627058970836e-06\n",
            "loss: 0.2718 (epoch: 19, step: 0) // Avg time/img: 0.0545 s\n",
            "loss: 0.3506 (epoch: 19, step: 50) // Avg time/img: 0.0475 s\n",
            "loss: 0.3437 (epoch: 19, step: 100) // Avg time/img: 0.0472 s\n",
            "loss: 0.3425 (epoch: 19, step: 150) // Avg time/img: 0.0474 s\n",
            "loss: 0.3446 (epoch: 19, step: 200) // Avg time/img: 0.0472 s\n",
            "loss: 0.3495 (epoch: 19, step: 250) // Avg time/img: 0.0472 s\n",
            "loss: 0.3508 (epoch: 19, step: 300) // Avg time/img: 0.0471 s\n",
            "loss: 0.3501 (epoch: 19, step: 350) // Avg time/img: 0.0471 s\n",
            "loss: 0.3497 (epoch: 19, step: 400) // Avg time/img: 0.0471 s\n",
            "loss: 0.3484 (epoch: 19, step: 450) // Avg time/img: 0.0471 s\n",
            "----- VALIDATING - EPOCH 19 -----\n",
            "VAL loss: 0.4041 (epoch: 19, step: 0) // Avg time/img: 0.0437 s\n",
            "VAL loss: 0.4803 (epoch: 19, step: 50) // Avg time/img: 0.0347 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m69.98\u001b[0m %\n",
            "save: ../save/erfnet_training_focal_loss/model-019.pth (epoch: 19)\n",
            "----- TRAINING - EPOCH 20 -----\n",
            "LEARNING RATE:  3.373207119183911e-06\n",
            "loss: 0.4666 (epoch: 20, step: 0) // Avg time/img: 0.0497 s\n",
            "loss: 0.3457 (epoch: 20, step: 50) // Avg time/img: 0.0458 s\n",
            "loss: 0.3409 (epoch: 20, step: 100) // Avg time/img: 0.0463 s\n",
            "loss: 0.3431 (epoch: 20, step: 150) // Avg time/img: 0.0465 s\n",
            "loss: 0.3445 (epoch: 20, step: 200) // Avg time/img: 0.0466 s\n",
            "loss: 0.3455 (epoch: 20, step: 250) // Avg time/img: 0.0467 s\n",
            "loss: 0.3461 (epoch: 20, step: 300) // Avg time/img: 0.0468 s\n",
            "loss: 0.3464 (epoch: 20, step: 350) // Avg time/img: 0.0468 s\n",
            "loss: 0.3469 (epoch: 20, step: 400) // Avg time/img: 0.0469 s\n",
            "loss: 0.3462 (epoch: 20, step: 450) // Avg time/img: 0.0469 s\n",
            "----- VALIDATING - EPOCH 20 -----\n",
            "VAL loss: 0.3865 (epoch: 20, step: 0) // Avg time/img: 0.0458 s\n",
            "VAL loss: 0.4719 (epoch: 20, step: 50) // Avg time/img: 0.0349 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m69.89\u001b[0m %\n",
            "save: ../save/erfnet_training_focal_loss/model-020.pth (epoch: 20)\n",
            "========== TRAINING FINISHED ===========\n",
            "Model saved in /content/AnomalySegmentation/save/erfnet_training_focal_loss\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_focal_loss/ (stored 0%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_focal_loss/model_best.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_focal_loss/model-007.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_focal_loss/model-015.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_focal_loss/model-008.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_focal_loss/model-004.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_focal_loss/model-005.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_focal_loss/model-002.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_focal_loss/opts.txt (deflated 37%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_focal_loss/model.txt (deflated 92%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_focal_loss/model-014.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_focal_loss/model-013.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_focal_loss/model-012.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_focal_loss/model-016.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_focal_loss/model-011.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_focal_loss/checkpoint.pth.tar (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_focal_loss/model-010.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_focal_loss/model-020.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_focal_loss/model_best.pth.tar (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_focal_loss/model-009.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_focal_loss/model-003.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_focal_loss/model-006.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_focal_loss/automated_log.txt (deflated 64%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_focal_loss/model-019.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_focal_loss/model-017.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_focal_loss/model-018.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_focal_loss/best.txt (stored 0%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_focal_loss/model-001.pth (deflated 10%)\n",
            "\n",
            "\n",
            "----- Fine-tuning with LogitNorm loss -----\n",
            "odict_keys(['module.encoder.initial_block.conv.weight', 'module.encoder.initial_block.conv.bias', 'module.encoder.initial_block.bn.weight', 'module.encoder.initial_block.bn.bias', 'module.encoder.initial_block.bn.running_mean', 'module.encoder.initial_block.bn.running_var', 'module.encoder.layers.0.conv.weight', 'module.encoder.layers.0.conv.bias', 'module.encoder.layers.0.bn.weight', 'module.encoder.layers.0.bn.bias', 'module.encoder.layers.0.bn.running_mean', 'module.encoder.layers.0.bn.running_var', 'module.encoder.layers.1.conv3x1_1.weight', 'module.encoder.layers.1.conv3x1_1.bias', 'module.encoder.layers.1.conv1x3_1.weight', 'module.encoder.layers.1.conv1x3_1.bias', 'module.encoder.layers.1.conv3x1_2.weight', 'module.encoder.layers.1.conv3x1_2.bias', 'module.encoder.layers.1.conv1x3_2.weight', 'module.encoder.layers.1.conv1x3_2.bias', 'module.encoder.layers.1.bn1.weight', 'module.encoder.layers.1.bn1.bias', 'module.encoder.layers.1.bn1.running_mean', 'module.encoder.layers.1.bn1.running_var', 'module.encoder.layers.1.bn2.weight', 'module.encoder.layers.1.bn2.bias', 'module.encoder.layers.1.bn2.running_mean', 'module.encoder.layers.1.bn2.running_var', 'module.encoder.layers.2.conv3x1_1.weight', 'module.encoder.layers.2.conv3x1_1.bias', 'module.encoder.layers.2.conv1x3_1.weight', 'module.encoder.layers.2.conv1x3_1.bias', 'module.encoder.layers.2.conv3x1_2.weight', 'module.encoder.layers.2.conv3x1_2.bias', 'module.encoder.layers.2.conv1x3_2.weight', 'module.encoder.layers.2.conv1x3_2.bias', 'module.encoder.layers.2.bn1.weight', 'module.encoder.layers.2.bn1.bias', 'module.encoder.layers.2.bn1.running_mean', 'module.encoder.layers.2.bn1.running_var', 'module.encoder.layers.2.bn2.weight', 'module.encoder.layers.2.bn2.bias', 'module.encoder.layers.2.bn2.running_mean', 'module.encoder.layers.2.bn2.running_var', 'module.encoder.layers.3.conv3x1_1.weight', 'module.encoder.layers.3.conv3x1_1.bias', 'module.encoder.layers.3.conv1x3_1.weight', 'module.encoder.layers.3.conv1x3_1.bias', 'module.encoder.layers.3.conv3x1_2.weight', 'module.encoder.layers.3.conv3x1_2.bias', 'module.encoder.layers.3.conv1x3_2.weight', 'module.encoder.layers.3.conv1x3_2.bias', 'module.encoder.layers.3.bn1.weight', 'module.encoder.layers.3.bn1.bias', 'module.encoder.layers.3.bn1.running_mean', 'module.encoder.layers.3.bn1.running_var', 'module.encoder.layers.3.bn2.weight', 'module.encoder.layers.3.bn2.bias', 'module.encoder.layers.3.bn2.running_mean', 'module.encoder.layers.3.bn2.running_var', 'module.encoder.layers.4.conv3x1_1.weight', 'module.encoder.layers.4.conv3x1_1.bias', 'module.encoder.layers.4.conv1x3_1.weight', 'module.encoder.layers.4.conv1x3_1.bias', 'module.encoder.layers.4.conv3x1_2.weight', 'module.encoder.layers.4.conv3x1_2.bias', 'module.encoder.layers.4.conv1x3_2.weight', 'module.encoder.layers.4.conv1x3_2.bias', 'module.encoder.layers.4.bn1.weight', 'module.encoder.layers.4.bn1.bias', 'module.encoder.layers.4.bn1.running_mean', 'module.encoder.layers.4.bn1.running_var', 'module.encoder.layers.4.bn2.weight', 'module.encoder.layers.4.bn2.bias', 'module.encoder.layers.4.bn2.running_mean', 'module.encoder.layers.4.bn2.running_var', 'module.encoder.layers.5.conv3x1_1.weight', 'module.encoder.layers.5.conv3x1_1.bias', 'module.encoder.layers.5.conv1x3_1.weight', 'module.encoder.layers.5.conv1x3_1.bias', 'module.encoder.layers.5.conv3x1_2.weight', 'module.encoder.layers.5.conv3x1_2.bias', 'module.encoder.layers.5.conv1x3_2.weight', 'module.encoder.layers.5.conv1x3_2.bias', 'module.encoder.layers.5.bn1.weight', 'module.encoder.layers.5.bn1.bias', 'module.encoder.layers.5.bn1.running_mean', 'module.encoder.layers.5.bn1.running_var', 'module.encoder.layers.5.bn2.weight', 'module.encoder.layers.5.bn2.bias', 'module.encoder.layers.5.bn2.running_mean', 'module.encoder.layers.5.bn2.running_var', 'module.encoder.layers.6.conv.weight', 'module.encoder.layers.6.conv.bias', 'module.encoder.layers.6.bn.weight', 'module.encoder.layers.6.bn.bias', 'module.encoder.layers.6.bn.running_mean', 'module.encoder.layers.6.bn.running_var', 'module.encoder.layers.7.conv3x1_1.weight', 'module.encoder.layers.7.conv3x1_1.bias', 'module.encoder.layers.7.conv1x3_1.weight', 'module.encoder.layers.7.conv1x3_1.bias', 'module.encoder.layers.7.conv3x1_2.weight', 'module.encoder.layers.7.conv3x1_2.bias', 'module.encoder.layers.7.conv1x3_2.weight', 'module.encoder.layers.7.conv1x3_2.bias', 'module.encoder.layers.7.bn1.weight', 'module.encoder.layers.7.bn1.bias', 'module.encoder.layers.7.bn1.running_mean', 'module.encoder.layers.7.bn1.running_var', 'module.encoder.layers.7.bn2.weight', 'module.encoder.layers.7.bn2.bias', 'module.encoder.layers.7.bn2.running_mean', 'module.encoder.layers.7.bn2.running_var', 'module.encoder.layers.8.conv3x1_1.weight', 'module.encoder.layers.8.conv3x1_1.bias', 'module.encoder.layers.8.conv1x3_1.weight', 'module.encoder.layers.8.conv1x3_1.bias', 'module.encoder.layers.8.conv3x1_2.weight', 'module.encoder.layers.8.conv3x1_2.bias', 'module.encoder.layers.8.conv1x3_2.weight', 'module.encoder.layers.8.conv1x3_2.bias', 'module.encoder.layers.8.bn1.weight', 'module.encoder.layers.8.bn1.bias', 'module.encoder.layers.8.bn1.running_mean', 'module.encoder.layers.8.bn1.running_var', 'module.encoder.layers.8.bn2.weight', 'module.encoder.layers.8.bn2.bias', 'module.encoder.layers.8.bn2.running_mean', 'module.encoder.layers.8.bn2.running_var', 'module.encoder.layers.9.conv3x1_1.weight', 'module.encoder.layers.9.conv3x1_1.bias', 'module.encoder.layers.9.conv1x3_1.weight', 'module.encoder.layers.9.conv1x3_1.bias', 'module.encoder.layers.9.conv3x1_2.weight', 'module.encoder.layers.9.conv3x1_2.bias', 'module.encoder.layers.9.conv1x3_2.weight', 'module.encoder.layers.9.conv1x3_2.bias', 'module.encoder.layers.9.bn1.weight', 'module.encoder.layers.9.bn1.bias', 'module.encoder.layers.9.bn1.running_mean', 'module.encoder.layers.9.bn1.running_var', 'module.encoder.layers.9.bn2.weight', 'module.encoder.layers.9.bn2.bias', 'module.encoder.layers.9.bn2.running_mean', 'module.encoder.layers.9.bn2.running_var', 'module.encoder.layers.10.conv3x1_1.weight', 'module.encoder.layers.10.conv3x1_1.bias', 'module.encoder.layers.10.conv1x3_1.weight', 'module.encoder.layers.10.conv1x3_1.bias', 'module.encoder.layers.10.conv3x1_2.weight', 'module.encoder.layers.10.conv3x1_2.bias', 'module.encoder.layers.10.conv1x3_2.weight', 'module.encoder.layers.10.conv1x3_2.bias', 'module.encoder.layers.10.bn1.weight', 'module.encoder.layers.10.bn1.bias', 'module.encoder.layers.10.bn1.running_mean', 'module.encoder.layers.10.bn1.running_var', 'module.encoder.layers.10.bn2.weight', 'module.encoder.layers.10.bn2.bias', 'module.encoder.layers.10.bn2.running_mean', 'module.encoder.layers.10.bn2.running_var', 'module.encoder.layers.11.conv3x1_1.weight', 'module.encoder.layers.11.conv3x1_1.bias', 'module.encoder.layers.11.conv1x3_1.weight', 'module.encoder.layers.11.conv1x3_1.bias', 'module.encoder.layers.11.conv3x1_2.weight', 'module.encoder.layers.11.conv3x1_2.bias', 'module.encoder.layers.11.conv1x3_2.weight', 'module.encoder.layers.11.conv1x3_2.bias', 'module.encoder.layers.11.bn1.weight', 'module.encoder.layers.11.bn1.bias', 'module.encoder.layers.11.bn1.running_mean', 'module.encoder.layers.11.bn1.running_var', 'module.encoder.layers.11.bn2.weight', 'module.encoder.layers.11.bn2.bias', 'module.encoder.layers.11.bn2.running_mean', 'module.encoder.layers.11.bn2.running_var', 'module.encoder.layers.12.conv3x1_1.weight', 'module.encoder.layers.12.conv3x1_1.bias', 'module.encoder.layers.12.conv1x3_1.weight', 'module.encoder.layers.12.conv1x3_1.bias', 'module.encoder.layers.12.conv3x1_2.weight', 'module.encoder.layers.12.conv3x1_2.bias', 'module.encoder.layers.12.conv1x3_2.weight', 'module.encoder.layers.12.conv1x3_2.bias', 'module.encoder.layers.12.bn1.weight', 'module.encoder.layers.12.bn1.bias', 'module.encoder.layers.12.bn1.running_mean', 'module.encoder.layers.12.bn1.running_var', 'module.encoder.layers.12.bn2.weight', 'module.encoder.layers.12.bn2.bias', 'module.encoder.layers.12.bn2.running_mean', 'module.encoder.layers.12.bn2.running_var', 'module.encoder.layers.13.conv3x1_1.weight', 'module.encoder.layers.13.conv3x1_1.bias', 'module.encoder.layers.13.conv1x3_1.weight', 'module.encoder.layers.13.conv1x3_1.bias', 'module.encoder.layers.13.conv3x1_2.weight', 'module.encoder.layers.13.conv3x1_2.bias', 'module.encoder.layers.13.conv1x3_2.weight', 'module.encoder.layers.13.conv1x3_2.bias', 'module.encoder.layers.13.bn1.weight', 'module.encoder.layers.13.bn1.bias', 'module.encoder.layers.13.bn1.running_mean', 'module.encoder.layers.13.bn1.running_var', 'module.encoder.layers.13.bn2.weight', 'module.encoder.layers.13.bn2.bias', 'module.encoder.layers.13.bn2.running_mean', 'module.encoder.layers.13.bn2.running_var', 'module.encoder.layers.14.conv3x1_1.weight', 'module.encoder.layers.14.conv3x1_1.bias', 'module.encoder.layers.14.conv1x3_1.weight', 'module.encoder.layers.14.conv1x3_1.bias', 'module.encoder.layers.14.conv3x1_2.weight', 'module.encoder.layers.14.conv3x1_2.bias', 'module.encoder.layers.14.conv1x3_2.weight', 'module.encoder.layers.14.conv1x3_2.bias', 'module.encoder.layers.14.bn1.weight', 'module.encoder.layers.14.bn1.bias', 'module.encoder.layers.14.bn1.running_mean', 'module.encoder.layers.14.bn1.running_var', 'module.encoder.layers.14.bn2.weight', 'module.encoder.layers.14.bn2.bias', 'module.encoder.layers.14.bn2.running_mean', 'module.encoder.layers.14.bn2.running_var', 'module.decoder.layers.0.conv.weight', 'module.decoder.layers.0.conv.bias', 'module.decoder.layers.0.bn.weight', 'module.decoder.layers.0.bn.bias', 'module.decoder.layers.0.bn.running_mean', 'module.decoder.layers.0.bn.running_var', 'module.decoder.layers.1.conv3x1_1.weight', 'module.decoder.layers.1.conv3x1_1.bias', 'module.decoder.layers.1.conv1x3_1.weight', 'module.decoder.layers.1.conv1x3_1.bias', 'module.decoder.layers.1.bn1.weight', 'module.decoder.layers.1.bn1.bias', 'module.decoder.layers.1.bn1.running_mean', 'module.decoder.layers.1.bn1.running_var', 'module.decoder.layers.1.conv3x1_2.weight', 'module.decoder.layers.1.conv3x1_2.bias', 'module.decoder.layers.1.conv1x3_2.weight', 'module.decoder.layers.1.conv1x3_2.bias', 'module.decoder.layers.1.bn2.weight', 'module.decoder.layers.1.bn2.bias', 'module.decoder.layers.1.bn2.running_mean', 'module.decoder.layers.1.bn2.running_var', 'module.decoder.layers.2.conv3x1_1.weight', 'module.decoder.layers.2.conv3x1_1.bias', 'module.decoder.layers.2.conv1x3_1.weight', 'module.decoder.layers.2.conv1x3_1.bias', 'module.decoder.layers.2.bn1.weight', 'module.decoder.layers.2.bn1.bias', 'module.decoder.layers.2.bn1.running_mean', 'module.decoder.layers.2.bn1.running_var', 'module.decoder.layers.2.conv3x1_2.weight', 'module.decoder.layers.2.conv3x1_2.bias', 'module.decoder.layers.2.conv1x3_2.weight', 'module.decoder.layers.2.conv1x3_2.bias', 'module.decoder.layers.2.bn2.weight', 'module.decoder.layers.2.bn2.bias', 'module.decoder.layers.2.bn2.running_mean', 'module.decoder.layers.2.bn2.running_var', 'module.decoder.layers.3.conv.weight', 'module.decoder.layers.3.conv.bias', 'module.decoder.layers.3.bn.weight', 'module.decoder.layers.3.bn.bias', 'module.decoder.layers.3.bn.running_mean', 'module.decoder.layers.3.bn.running_var', 'module.decoder.layers.4.conv3x1_1.weight', 'module.decoder.layers.4.conv3x1_1.bias', 'module.decoder.layers.4.conv1x3_1.weight', 'module.decoder.layers.4.conv1x3_1.bias', 'module.decoder.layers.4.bn1.weight', 'module.decoder.layers.4.bn1.bias', 'module.decoder.layers.4.bn1.running_mean', 'module.decoder.layers.4.bn1.running_var', 'module.decoder.layers.4.conv3x1_2.weight', 'module.decoder.layers.4.conv3x1_2.bias', 'module.decoder.layers.4.conv1x3_2.weight', 'module.decoder.layers.4.conv1x3_2.bias', 'module.decoder.layers.4.bn2.weight', 'module.decoder.layers.4.bn2.bias', 'module.decoder.layers.4.bn2.running_mean', 'module.decoder.layers.4.bn2.running_var', 'module.decoder.layers.5.conv3x1_1.weight', 'module.decoder.layers.5.conv3x1_1.bias', 'module.decoder.layers.5.conv1x3_1.weight', 'module.decoder.layers.5.conv1x3_1.bias', 'module.decoder.layers.5.bn1.weight', 'module.decoder.layers.5.bn1.bias', 'module.decoder.layers.5.bn1.running_mean', 'module.decoder.layers.5.bn1.running_var', 'module.decoder.layers.5.conv3x1_2.weight', 'module.decoder.layers.5.conv3x1_2.bias', 'module.decoder.layers.5.conv1x3_2.weight', 'module.decoder.layers.5.conv1x3_2.bias', 'module.decoder.layers.5.bn2.weight', 'module.decoder.layers.5.bn2.bias', 'module.decoder.layers.5.bn2.running_mean', 'module.decoder.layers.5.bn2.running_var', 'module.decoder.output_conv.weight', 'module.decoder.output_conv.bias'])\n",
            "odict_keys(['encoder.initial_block.conv.weight', 'encoder.initial_block.conv.bias', 'encoder.initial_block.bn.weight', 'encoder.initial_block.bn.bias', 'encoder.initial_block.bn.running_mean', 'encoder.initial_block.bn.running_var', 'encoder.initial_block.bn.num_batches_tracked', 'encoder.layers.0.conv.weight', 'encoder.layers.0.conv.bias', 'encoder.layers.0.bn.weight', 'encoder.layers.0.bn.bias', 'encoder.layers.0.bn.running_mean', 'encoder.layers.0.bn.running_var', 'encoder.layers.0.bn.num_batches_tracked', 'encoder.layers.1.conv3x1_1.weight', 'encoder.layers.1.conv3x1_1.bias', 'encoder.layers.1.conv1x3_1.weight', 'encoder.layers.1.conv1x3_1.bias', 'encoder.layers.1.bn1.weight', 'encoder.layers.1.bn1.bias', 'encoder.layers.1.bn1.running_mean', 'encoder.layers.1.bn1.running_var', 'encoder.layers.1.bn1.num_batches_tracked', 'encoder.layers.1.conv3x1_2.weight', 'encoder.layers.1.conv3x1_2.bias', 'encoder.layers.1.conv1x3_2.weight', 'encoder.layers.1.conv1x3_2.bias', 'encoder.layers.1.bn2.weight', 'encoder.layers.1.bn2.bias', 'encoder.layers.1.bn2.running_mean', 'encoder.layers.1.bn2.running_var', 'encoder.layers.1.bn2.num_batches_tracked', 'encoder.layers.2.conv3x1_1.weight', 'encoder.layers.2.conv3x1_1.bias', 'encoder.layers.2.conv1x3_1.weight', 'encoder.layers.2.conv1x3_1.bias', 'encoder.layers.2.bn1.weight', 'encoder.layers.2.bn1.bias', 'encoder.layers.2.bn1.running_mean', 'encoder.layers.2.bn1.running_var', 'encoder.layers.2.bn1.num_batches_tracked', 'encoder.layers.2.conv3x1_2.weight', 'encoder.layers.2.conv3x1_2.bias', 'encoder.layers.2.conv1x3_2.weight', 'encoder.layers.2.conv1x3_2.bias', 'encoder.layers.2.bn2.weight', 'encoder.layers.2.bn2.bias', 'encoder.layers.2.bn2.running_mean', 'encoder.layers.2.bn2.running_var', 'encoder.layers.2.bn2.num_batches_tracked', 'encoder.layers.3.conv3x1_1.weight', 'encoder.layers.3.conv3x1_1.bias', 'encoder.layers.3.conv1x3_1.weight', 'encoder.layers.3.conv1x3_1.bias', 'encoder.layers.3.bn1.weight', 'encoder.layers.3.bn1.bias', 'encoder.layers.3.bn1.running_mean', 'encoder.layers.3.bn1.running_var', 'encoder.layers.3.bn1.num_batches_tracked', 'encoder.layers.3.conv3x1_2.weight', 'encoder.layers.3.conv3x1_2.bias', 'encoder.layers.3.conv1x3_2.weight', 'encoder.layers.3.conv1x3_2.bias', 'encoder.layers.3.bn2.weight', 'encoder.layers.3.bn2.bias', 'encoder.layers.3.bn2.running_mean', 'encoder.layers.3.bn2.running_var', 'encoder.layers.3.bn2.num_batches_tracked', 'encoder.layers.4.conv3x1_1.weight', 'encoder.layers.4.conv3x1_1.bias', 'encoder.layers.4.conv1x3_1.weight', 'encoder.layers.4.conv1x3_1.bias', 'encoder.layers.4.bn1.weight', 'encoder.layers.4.bn1.bias', 'encoder.layers.4.bn1.running_mean', 'encoder.layers.4.bn1.running_var', 'encoder.layers.4.bn1.num_batches_tracked', 'encoder.layers.4.conv3x1_2.weight', 'encoder.layers.4.conv3x1_2.bias', 'encoder.layers.4.conv1x3_2.weight', 'encoder.layers.4.conv1x3_2.bias', 'encoder.layers.4.bn2.weight', 'encoder.layers.4.bn2.bias', 'encoder.layers.4.bn2.running_mean', 'encoder.layers.4.bn2.running_var', 'encoder.layers.4.bn2.num_batches_tracked', 'encoder.layers.5.conv3x1_1.weight', 'encoder.layers.5.conv3x1_1.bias', 'encoder.layers.5.conv1x3_1.weight', 'encoder.layers.5.conv1x3_1.bias', 'encoder.layers.5.bn1.weight', 'encoder.layers.5.bn1.bias', 'encoder.layers.5.bn1.running_mean', 'encoder.layers.5.bn1.running_var', 'encoder.layers.5.bn1.num_batches_tracked', 'encoder.layers.5.conv3x1_2.weight', 'encoder.layers.5.conv3x1_2.bias', 'encoder.layers.5.conv1x3_2.weight', 'encoder.layers.5.conv1x3_2.bias', 'encoder.layers.5.bn2.weight', 'encoder.layers.5.bn2.bias', 'encoder.layers.5.bn2.running_mean', 'encoder.layers.5.bn2.running_var', 'encoder.layers.5.bn2.num_batches_tracked', 'encoder.layers.6.conv.weight', 'encoder.layers.6.conv.bias', 'encoder.layers.6.bn.weight', 'encoder.layers.6.bn.bias', 'encoder.layers.6.bn.running_mean', 'encoder.layers.6.bn.running_var', 'encoder.layers.6.bn.num_batches_tracked', 'encoder.layers.7.conv3x1_1.weight', 'encoder.layers.7.conv3x1_1.bias', 'encoder.layers.7.conv1x3_1.weight', 'encoder.layers.7.conv1x3_1.bias', 'encoder.layers.7.bn1.weight', 'encoder.layers.7.bn1.bias', 'encoder.layers.7.bn1.running_mean', 'encoder.layers.7.bn1.running_var', 'encoder.layers.7.bn1.num_batches_tracked', 'encoder.layers.7.conv3x1_2.weight', 'encoder.layers.7.conv3x1_2.bias', 'encoder.layers.7.conv1x3_2.weight', 'encoder.layers.7.conv1x3_2.bias', 'encoder.layers.7.bn2.weight', 'encoder.layers.7.bn2.bias', 'encoder.layers.7.bn2.running_mean', 'encoder.layers.7.bn2.running_var', 'encoder.layers.7.bn2.num_batches_tracked', 'encoder.layers.8.conv3x1_1.weight', 'encoder.layers.8.conv3x1_1.bias', 'encoder.layers.8.conv1x3_1.weight', 'encoder.layers.8.conv1x3_1.bias', 'encoder.layers.8.bn1.weight', 'encoder.layers.8.bn1.bias', 'encoder.layers.8.bn1.running_mean', 'encoder.layers.8.bn1.running_var', 'encoder.layers.8.bn1.num_batches_tracked', 'encoder.layers.8.conv3x1_2.weight', 'encoder.layers.8.conv3x1_2.bias', 'encoder.layers.8.conv1x3_2.weight', 'encoder.layers.8.conv1x3_2.bias', 'encoder.layers.8.bn2.weight', 'encoder.layers.8.bn2.bias', 'encoder.layers.8.bn2.running_mean', 'encoder.layers.8.bn2.running_var', 'encoder.layers.8.bn2.num_batches_tracked', 'encoder.layers.9.conv3x1_1.weight', 'encoder.layers.9.conv3x1_1.bias', 'encoder.layers.9.conv1x3_1.weight', 'encoder.layers.9.conv1x3_1.bias', 'encoder.layers.9.bn1.weight', 'encoder.layers.9.bn1.bias', 'encoder.layers.9.bn1.running_mean', 'encoder.layers.9.bn1.running_var', 'encoder.layers.9.bn1.num_batches_tracked', 'encoder.layers.9.conv3x1_2.weight', 'encoder.layers.9.conv3x1_2.bias', 'encoder.layers.9.conv1x3_2.weight', 'encoder.layers.9.conv1x3_2.bias', 'encoder.layers.9.bn2.weight', 'encoder.layers.9.bn2.bias', 'encoder.layers.9.bn2.running_mean', 'encoder.layers.9.bn2.running_var', 'encoder.layers.9.bn2.num_batches_tracked', 'encoder.layers.10.conv3x1_1.weight', 'encoder.layers.10.conv3x1_1.bias', 'encoder.layers.10.conv1x3_1.weight', 'encoder.layers.10.conv1x3_1.bias', 'encoder.layers.10.bn1.weight', 'encoder.layers.10.bn1.bias', 'encoder.layers.10.bn1.running_mean', 'encoder.layers.10.bn1.running_var', 'encoder.layers.10.bn1.num_batches_tracked', 'encoder.layers.10.conv3x1_2.weight', 'encoder.layers.10.conv3x1_2.bias', 'encoder.layers.10.conv1x3_2.weight', 'encoder.layers.10.conv1x3_2.bias', 'encoder.layers.10.bn2.weight', 'encoder.layers.10.bn2.bias', 'encoder.layers.10.bn2.running_mean', 'encoder.layers.10.bn2.running_var', 'encoder.layers.10.bn2.num_batches_tracked', 'encoder.layers.11.conv3x1_1.weight', 'encoder.layers.11.conv3x1_1.bias', 'encoder.layers.11.conv1x3_1.weight', 'encoder.layers.11.conv1x3_1.bias', 'encoder.layers.11.bn1.weight', 'encoder.layers.11.bn1.bias', 'encoder.layers.11.bn1.running_mean', 'encoder.layers.11.bn1.running_var', 'encoder.layers.11.bn1.num_batches_tracked', 'encoder.layers.11.conv3x1_2.weight', 'encoder.layers.11.conv3x1_2.bias', 'encoder.layers.11.conv1x3_2.weight', 'encoder.layers.11.conv1x3_2.bias', 'encoder.layers.11.bn2.weight', 'encoder.layers.11.bn2.bias', 'encoder.layers.11.bn2.running_mean', 'encoder.layers.11.bn2.running_var', 'encoder.layers.11.bn2.num_batches_tracked', 'encoder.layers.12.conv3x1_1.weight', 'encoder.layers.12.conv3x1_1.bias', 'encoder.layers.12.conv1x3_1.weight', 'encoder.layers.12.conv1x3_1.bias', 'encoder.layers.12.bn1.weight', 'encoder.layers.12.bn1.bias', 'encoder.layers.12.bn1.running_mean', 'encoder.layers.12.bn1.running_var', 'encoder.layers.12.bn1.num_batches_tracked', 'encoder.layers.12.conv3x1_2.weight', 'encoder.layers.12.conv3x1_2.bias', 'encoder.layers.12.conv1x3_2.weight', 'encoder.layers.12.conv1x3_2.bias', 'encoder.layers.12.bn2.weight', 'encoder.layers.12.bn2.bias', 'encoder.layers.12.bn2.running_mean', 'encoder.layers.12.bn2.running_var', 'encoder.layers.12.bn2.num_batches_tracked', 'encoder.layers.13.conv3x1_1.weight', 'encoder.layers.13.conv3x1_1.bias', 'encoder.layers.13.conv1x3_1.weight', 'encoder.layers.13.conv1x3_1.bias', 'encoder.layers.13.bn1.weight', 'encoder.layers.13.bn1.bias', 'encoder.layers.13.bn1.running_mean', 'encoder.layers.13.bn1.running_var', 'encoder.layers.13.bn1.num_batches_tracked', 'encoder.layers.13.conv3x1_2.weight', 'encoder.layers.13.conv3x1_2.bias', 'encoder.layers.13.conv1x3_2.weight', 'encoder.layers.13.conv1x3_2.bias', 'encoder.layers.13.bn2.weight', 'encoder.layers.13.bn2.bias', 'encoder.layers.13.bn2.running_mean', 'encoder.layers.13.bn2.running_var', 'encoder.layers.13.bn2.num_batches_tracked', 'encoder.layers.14.conv3x1_1.weight', 'encoder.layers.14.conv3x1_1.bias', 'encoder.layers.14.conv1x3_1.weight', 'encoder.layers.14.conv1x3_1.bias', 'encoder.layers.14.bn1.weight', 'encoder.layers.14.bn1.bias', 'encoder.layers.14.bn1.running_mean', 'encoder.layers.14.bn1.running_var', 'encoder.layers.14.bn1.num_batches_tracked', 'encoder.layers.14.conv3x1_2.weight', 'encoder.layers.14.conv3x1_2.bias', 'encoder.layers.14.conv1x3_2.weight', 'encoder.layers.14.conv1x3_2.bias', 'encoder.layers.14.bn2.weight', 'encoder.layers.14.bn2.bias', 'encoder.layers.14.bn2.running_mean', 'encoder.layers.14.bn2.running_var', 'encoder.layers.14.bn2.num_batches_tracked', 'encoder.output_conv.weight', 'encoder.output_conv.bias', 'decoder.layers.0.conv.weight', 'decoder.layers.0.conv.bias', 'decoder.layers.0.bn.weight', 'decoder.layers.0.bn.bias', 'decoder.layers.0.bn.running_mean', 'decoder.layers.0.bn.running_var', 'decoder.layers.0.bn.num_batches_tracked', 'decoder.layers.1.conv3x1_1.weight', 'decoder.layers.1.conv3x1_1.bias', 'decoder.layers.1.conv1x3_1.weight', 'decoder.layers.1.conv1x3_1.bias', 'decoder.layers.1.bn1.weight', 'decoder.layers.1.bn1.bias', 'decoder.layers.1.bn1.running_mean', 'decoder.layers.1.bn1.running_var', 'decoder.layers.1.bn1.num_batches_tracked', 'decoder.layers.1.conv3x1_2.weight', 'decoder.layers.1.conv3x1_2.bias', 'decoder.layers.1.conv1x3_2.weight', 'decoder.layers.1.conv1x3_2.bias', 'decoder.layers.1.bn2.weight', 'decoder.layers.1.bn2.bias', 'decoder.layers.1.bn2.running_mean', 'decoder.layers.1.bn2.running_var', 'decoder.layers.1.bn2.num_batches_tracked', 'decoder.layers.2.conv3x1_1.weight', 'decoder.layers.2.conv3x1_1.bias', 'decoder.layers.2.conv1x3_1.weight', 'decoder.layers.2.conv1x3_1.bias', 'decoder.layers.2.bn1.weight', 'decoder.layers.2.bn1.bias', 'decoder.layers.2.bn1.running_mean', 'decoder.layers.2.bn1.running_var', 'decoder.layers.2.bn1.num_batches_tracked', 'decoder.layers.2.conv3x1_2.weight', 'decoder.layers.2.conv3x1_2.bias', 'decoder.layers.2.conv1x3_2.weight', 'decoder.layers.2.conv1x3_2.bias', 'decoder.layers.2.bn2.weight', 'decoder.layers.2.bn2.bias', 'decoder.layers.2.bn2.running_mean', 'decoder.layers.2.bn2.running_var', 'decoder.layers.2.bn2.num_batches_tracked', 'decoder.layers.3.conv.weight', 'decoder.layers.3.conv.bias', 'decoder.layers.3.bn.weight', 'decoder.layers.3.bn.bias', 'decoder.layers.3.bn.running_mean', 'decoder.layers.3.bn.running_var', 'decoder.layers.3.bn.num_batches_tracked', 'decoder.layers.4.conv3x1_1.weight', 'decoder.layers.4.conv3x1_1.bias', 'decoder.layers.4.conv1x3_1.weight', 'decoder.layers.4.conv1x3_1.bias', 'decoder.layers.4.bn1.weight', 'decoder.layers.4.bn1.bias', 'decoder.layers.4.bn1.running_mean', 'decoder.layers.4.bn1.running_var', 'decoder.layers.4.bn1.num_batches_tracked', 'decoder.layers.4.conv3x1_2.weight', 'decoder.layers.4.conv3x1_2.bias', 'decoder.layers.4.conv1x3_2.weight', 'decoder.layers.4.conv1x3_2.bias', 'decoder.layers.4.bn2.weight', 'decoder.layers.4.bn2.bias', 'decoder.layers.4.bn2.running_mean', 'decoder.layers.4.bn2.running_var', 'decoder.layers.4.bn2.num_batches_tracked', 'decoder.layers.5.conv3x1_1.weight', 'decoder.layers.5.conv3x1_1.bias', 'decoder.layers.5.conv1x3_1.weight', 'decoder.layers.5.conv1x3_1.bias', 'decoder.layers.5.bn1.weight', 'decoder.layers.5.bn1.bias', 'decoder.layers.5.bn1.running_mean', 'decoder.layers.5.bn1.running_var', 'decoder.layers.5.bn1.num_batches_tracked', 'decoder.layers.5.conv3x1_2.weight', 'decoder.layers.5.conv3x1_2.bias', 'decoder.layers.5.conv1x3_2.weight', 'decoder.layers.5.conv1x3_2.bias', 'decoder.layers.5.bn2.weight', 'decoder.layers.5.bn2.bias', 'decoder.layers.5.bn2.running_mean', 'decoder.layers.5.bn2.running_var', 'decoder.layers.5.bn2.num_batches_tracked', 'decoder.output_conv.weight', 'decoder.output_conv.bias'])\n",
            "Import Model erfnet with weights erfnet_pretrained.pth to FineTune\n",
            "========== TRAINING ===========\n",
            "/content/cityscapes/leftImg8bit/train\n",
            "/content/cityscapes/leftImg8bit/val\n",
            "----- TRAINING - EPOCH 1 -----\n",
            "LEARNING RATE:  5e-05\n",
            "loss: 2.943 (epoch: 1, step: 0) // Avg time/img: 0.5878 s\n",
            "loss: 2.941 (epoch: 1, step: 50) // Avg time/img: 0.0579 s\n",
            "loss: 2.941 (epoch: 1, step: 100) // Avg time/img: 0.0531 s\n",
            "loss: 2.94 (epoch: 1, step: 150) // Avg time/img: 0.0511 s\n",
            "loss: 2.94 (epoch: 1, step: 200) // Avg time/img: 0.0504 s\n",
            "loss: 2.94 (epoch: 1, step: 250) // Avg time/img: 0.0499 s\n",
            "loss: 2.94 (epoch: 1, step: 300) // Avg time/img: 0.0497 s\n",
            "loss: 2.94 (epoch: 1, step: 350) // Avg time/img: 0.0495 s\n",
            "loss: 2.94 (epoch: 1, step: 400) // Avg time/img: 0.0493 s\n",
            "loss: 2.94 (epoch: 1, step: 450) // Avg time/img: 0.0492 s\n",
            "----- VALIDATING - EPOCH 1 -----\n",
            "VAL loss: 2.942 (epoch: 1, step: 0) // Avg time/img: 0.0528 s\n",
            "VAL loss: 2.942 (epoch: 1, step: 50) // Avg time/img: 0.0350 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.89\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/erfnet_training_logitnorm_loss/model-001.pth (epoch: 1)\n",
            "save: ../save/erfnet_training_logitnorm_loss/model_best.pth (epoch: 1)\n",
            "----- TRAINING - EPOCH 2 -----\n",
            "LEARNING RATE:  4.774426908107499e-05\n",
            "loss: 2.939 (epoch: 2, step: 0) // Avg time/img: 0.0562 s\n",
            "loss: 2.939 (epoch: 2, step: 50) // Avg time/img: 0.0482 s\n",
            "loss: 2.939 (epoch: 2, step: 100) // Avg time/img: 0.0481 s\n",
            "loss: 2.939 (epoch: 2, step: 150) // Avg time/img: 0.0481 s\n",
            "loss: 2.939 (epoch: 2, step: 200) // Avg time/img: 0.0482 s\n",
            "loss: 2.939 (epoch: 2, step: 250) // Avg time/img: 0.0482 s\n",
            "loss: 2.939 (epoch: 2, step: 300) // Avg time/img: 0.0482 s\n",
            "loss: 2.939 (epoch: 2, step: 350) // Avg time/img: 0.0482 s\n",
            "loss: 2.939 (epoch: 2, step: 400) // Avg time/img: 0.0483 s\n",
            "loss: 2.939 (epoch: 2, step: 450) // Avg time/img: 0.0483 s\n",
            "----- VALIDATING - EPOCH 2 -----\n",
            "VAL loss: 2.941 (epoch: 2, step: 0) // Avg time/img: 0.0516 s\n",
            "VAL loss: 2.941 (epoch: 2, step: 50) // Avg time/img: 0.0347 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.45\u001b[0m %\n",
            "save: ../save/erfnet_training_logitnorm_loss/model-002.pth (epoch: 2)\n",
            "----- TRAINING - EPOCH 3 -----\n",
            "LEARNING RATE:  4.547662880414811e-05\n",
            "loss: 2.938 (epoch: 3, step: 0) // Avg time/img: 0.0529 s\n",
            "loss: 2.939 (epoch: 3, step: 50) // Avg time/img: 0.0479 s\n",
            "loss: 2.939 (epoch: 3, step: 100) // Avg time/img: 0.0482 s\n",
            "loss: 2.939 (epoch: 3, step: 150) // Avg time/img: 0.0483 s\n",
            "loss: 2.939 (epoch: 3, step: 200) // Avg time/img: 0.0482 s\n",
            "loss: 2.939 (epoch: 3, step: 250) // Avg time/img: 0.0482 s\n",
            "loss: 2.939 (epoch: 3, step: 300) // Avg time/img: 0.0484 s\n",
            "loss: 2.939 (epoch: 3, step: 350) // Avg time/img: 0.0484 s\n",
            "loss: 2.939 (epoch: 3, step: 400) // Avg time/img: 0.0484 s\n",
            "loss: 2.939 (epoch: 3, step: 450) // Avg time/img: 0.0484 s\n",
            "----- VALIDATING - EPOCH 3 -----\n",
            "VAL loss: 2.941 (epoch: 3, step: 0) // Avg time/img: 0.0498 s\n",
            "VAL loss: 2.941 (epoch: 3, step: 50) // Avg time/img: 0.0348 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.09\u001b[0m %\n",
            "save: ../save/erfnet_training_logitnorm_loss/model-003.pth (epoch: 3)\n",
            "----- TRAINING - EPOCH 4 -----\n",
            "LEARNING RATE:  4.319634861514096e-05\n",
            "loss: 2.937 (epoch: 4, step: 0) // Avg time/img: 0.0561 s\n",
            "loss: 2.939 (epoch: 4, step: 50) // Avg time/img: 0.0480 s\n",
            "loss: 2.939 (epoch: 4, step: 100) // Avg time/img: 0.0483 s\n",
            "loss: 2.938 (epoch: 4, step: 150) // Avg time/img: 0.0483 s\n",
            "loss: 2.938 (epoch: 4, step: 200) // Avg time/img: 0.0484 s\n",
            "loss: 2.938 (epoch: 4, step: 250) // Avg time/img: 0.0483 s\n",
            "loss: 2.938 (epoch: 4, step: 300) // Avg time/img: 0.0484 s\n",
            "loss: 2.938 (epoch: 4, step: 350) // Avg time/img: 0.0484 s\n",
            "loss: 2.938 (epoch: 4, step: 400) // Avg time/img: 0.0485 s\n",
            "loss: 2.938 (epoch: 4, step: 450) // Avg time/img: 0.0484 s\n",
            "----- VALIDATING - EPOCH 4 -----\n",
            "VAL loss: 2.94 (epoch: 4, step: 0) // Avg time/img: 0.0491 s\n",
            "VAL loss: 2.941 (epoch: 4, step: 50) // Avg time/img: 0.0353 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m70.79\u001b[0m %\n",
            "save: ../save/erfnet_training_logitnorm_loss/model-004.pth (epoch: 4)\n",
            "----- TRAINING - EPOCH 5 -----\n",
            "LEARNING RATE:  4.090260730254292e-05\n",
            "loss: 2.936 (epoch: 5, step: 0) // Avg time/img: 0.0573 s\n",
            "loss: 2.938 (epoch: 5, step: 50) // Avg time/img: 0.0487 s\n",
            "loss: 2.938 (epoch: 5, step: 100) // Avg time/img: 0.0486 s\n",
            "loss: 2.938 (epoch: 5, step: 150) // Avg time/img: 0.0483 s\n",
            "loss: 2.938 (epoch: 5, step: 200) // Avg time/img: 0.0484 s\n",
            "loss: 2.938 (epoch: 5, step: 250) // Avg time/img: 0.0483 s\n",
            "loss: 2.938 (epoch: 5, step: 300) // Avg time/img: 0.0483 s\n",
            "loss: 2.938 (epoch: 5, step: 350) // Avg time/img: 0.0482 s\n",
            "loss: 2.938 (epoch: 5, step: 400) // Avg time/img: 0.0483 s\n",
            "loss: 2.938 (epoch: 5, step: 450) // Avg time/img: 0.0483 s\n",
            "----- VALIDATING - EPOCH 5 -----\n",
            "VAL loss: 2.94 (epoch: 5, step: 0) // Avg time/img: 0.0586 s\n",
            "VAL loss: 2.94 (epoch: 5, step: 50) // Avg time/img: 0.0349 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m70.43\u001b[0m %\n",
            "save: ../save/erfnet_training_logitnorm_loss/model-005.pth (epoch: 5)\n",
            "----- TRAINING - EPOCH 6 -----\n",
            "LEARNING RATE:  3.859447533617852e-05\n",
            "loss: 2.936 (epoch: 6, step: 0) // Avg time/img: 0.0538 s\n",
            "loss: 2.937 (epoch: 6, step: 50) // Avg time/img: 0.0481 s\n",
            "loss: 2.937 (epoch: 6, step: 100) // Avg time/img: 0.0483 s\n",
            "loss: 2.937 (epoch: 6, step: 150) // Avg time/img: 0.0481 s\n",
            "loss: 2.937 (epoch: 6, step: 200) // Avg time/img: 0.0483 s\n",
            "loss: 2.937 (epoch: 6, step: 250) // Avg time/img: 0.0484 s\n",
            "loss: 2.937 (epoch: 6, step: 300) // Avg time/img: 0.0484 s\n",
            "loss: 2.937 (epoch: 6, step: 350) // Avg time/img: 0.0484 s\n",
            "loss: 2.937 (epoch: 6, step: 400) // Avg time/img: 0.0485 s\n",
            "loss: 2.937 (epoch: 6, step: 450) // Avg time/img: 0.0484 s\n",
            "----- VALIDATING - EPOCH 6 -----\n",
            "VAL loss: 2.938 (epoch: 6, step: 0) // Avg time/img: 0.0527 s\n",
            "VAL loss: 2.938 (epoch: 6, step: 50) // Avg time/img: 0.0349 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m69.99\u001b[0m %\n",
            "save: ../save/erfnet_training_logitnorm_loss/model-006.pth (epoch: 6)\n",
            "----- TRAINING - EPOCH 7 -----\n",
            "LEARNING RATE:  3.6270892346861e-05\n",
            "loss: 2.941 (epoch: 7, step: 0) // Avg time/img: 0.0559 s\n",
            "loss: 2.936 (epoch: 7, step: 50) // Avg time/img: 0.0482 s\n",
            "loss: 2.936 (epoch: 7, step: 100) // Avg time/img: 0.0483 s\n",
            "loss: 2.936 (epoch: 7, step: 150) // Avg time/img: 0.0481 s\n",
            "loss: 2.936 (epoch: 7, step: 200) // Avg time/img: 0.0482 s\n",
            "loss: 2.936 (epoch: 7, step: 250) // Avg time/img: 0.0482 s\n",
            "loss: 2.936 (epoch: 7, step: 300) // Avg time/img: 0.0482 s\n",
            "loss: 2.936 (epoch: 7, step: 350) // Avg time/img: 0.0483 s\n",
            "loss: 2.936 (epoch: 7, step: 400) // Avg time/img: 0.0483 s\n",
            "loss: 2.936 (epoch: 7, step: 450) // Avg time/img: 0.0483 s\n",
            "----- VALIDATING - EPOCH 7 -----\n",
            "VAL loss: 2.937 (epoch: 7, step: 0) // Avg time/img: 0.0477 s\n",
            "VAL loss: 2.937 (epoch: 7, step: 50) // Avg time/img: 0.0350 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m68.32\u001b[0m %\n",
            "save: ../save/erfnet_training_logitnorm_loss/model-007.pth (epoch: 7)\n",
            "----- TRAINING - EPOCH 8 -----\n",
            "LEARNING RATE:  3.393063796290625e-05\n",
            "loss: 2.934 (epoch: 8, step: 0) // Avg time/img: 0.0588 s\n",
            "loss: 2.935 (epoch: 8, step: 50) // Avg time/img: 0.0483 s\n",
            "loss: 2.935 (epoch: 8, step: 100) // Avg time/img: 0.0483 s\n",
            "loss: 2.935 (epoch: 8, step: 150) // Avg time/img: 0.0483 s\n",
            "loss: 2.935 (epoch: 8, step: 200) // Avg time/img: 0.0483 s\n",
            "loss: 2.935 (epoch: 8, step: 250) // Avg time/img: 0.0481 s\n",
            "loss: 2.935 (epoch: 8, step: 300) // Avg time/img: 0.0482 s\n",
            "loss: 2.935 (epoch: 8, step: 350) // Avg time/img: 0.0481 s\n",
            "loss: 2.935 (epoch: 8, step: 400) // Avg time/img: 0.0482 s\n",
            "loss: 2.935 (epoch: 8, step: 450) // Avg time/img: 0.0483 s\n",
            "----- VALIDATING - EPOCH 8 -----\n",
            "VAL loss: 2.936 (epoch: 8, step: 0) // Avg time/img: 0.0540 s\n",
            "VAL loss: 2.936 (epoch: 8, step: 50) // Avg time/img: 0.0348 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m66.61\u001b[0m %\n",
            "save: ../save/erfnet_training_logitnorm_loss/model-008.pth (epoch: 8)\n",
            "----- TRAINING - EPOCH 9 -----\n",
            "LEARNING RATE:  3.157229337446777e-05\n",
            "loss: 2.934 (epoch: 9, step: 0) // Avg time/img: 0.0593 s\n",
            "loss: 2.934 (epoch: 9, step: 50) // Avg time/img: 0.0481 s\n",
            "loss: 2.934 (epoch: 9, step: 100) // Avg time/img: 0.0477 s\n",
            "loss: 2.934 (epoch: 9, step: 150) // Avg time/img: 0.0479 s\n",
            "loss: 2.934 (epoch: 9, step: 200) // Avg time/img: 0.0479 s\n",
            "loss: 2.934 (epoch: 9, step: 250) // Avg time/img: 0.0480 s\n",
            "loss: 2.934 (epoch: 9, step: 300) // Avg time/img: 0.0480 s\n",
            "loss: 2.934 (epoch: 9, step: 350) // Avg time/img: 0.0481 s\n",
            "loss: 2.934 (epoch: 9, step: 400) // Avg time/img: 0.0481 s\n",
            "loss: 2.934 (epoch: 9, step: 450) // Avg time/img: 0.0482 s\n",
            "----- VALIDATING - EPOCH 9 -----\n",
            "VAL loss: 2.936 (epoch: 9, step: 0) // Avg time/img: 0.0495 s\n",
            "VAL loss: 2.935 (epoch: 9, step: 50) // Avg time/img: 0.0345 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m66.23\u001b[0m %\n",
            "save: ../save/erfnet_training_logitnorm_loss/model-009.pth (epoch: 9)\n",
            "----- TRAINING - EPOCH 10 -----\n",
            "LEARNING RATE:  2.9194189645999014e-05\n",
            "loss: 2.936 (epoch: 10, step: 0) // Avg time/img: 0.0536 s\n",
            "loss: 2.934 (epoch: 10, step: 50) // Avg time/img: 0.0483 s\n",
            "loss: 2.934 (epoch: 10, step: 100) // Avg time/img: 0.0482 s\n",
            "loss: 2.934 (epoch: 10, step: 150) // Avg time/img: 0.0481 s\n",
            "loss: 2.934 (epoch: 10, step: 200) // Avg time/img: 0.0480 s\n",
            "loss: 2.934 (epoch: 10, step: 250) // Avg time/img: 0.0480 s\n",
            "loss: 2.934 (epoch: 10, step: 300) // Avg time/img: 0.0480 s\n",
            "loss: 2.934 (epoch: 10, step: 350) // Avg time/img: 0.0480 s\n",
            "loss: 2.934 (epoch: 10, step: 400) // Avg time/img: 0.0481 s\n",
            "loss: 2.934 (epoch: 10, step: 450) // Avg time/img: 0.0481 s\n",
            "----- VALIDATING - EPOCH 10 -----\n",
            "VAL loss: 2.935 (epoch: 10, step: 0) // Avg time/img: 0.0509 s\n",
            "VAL loss: 2.935 (epoch: 10, step: 50) // Avg time/img: 0.0347 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m66.97\u001b[0m %\n",
            "save: ../save/erfnet_training_logitnorm_loss/model-010.pth (epoch: 10)\n",
            "----- TRAINING - EPOCH 11 -----\n",
            "LEARNING RATE:  2.679433656340733e-05\n",
            "loss: 2.932 (epoch: 11, step: 0) // Avg time/img: 0.0583 s\n",
            "loss: 2.934 (epoch: 11, step: 50) // Avg time/img: 0.0480 s\n",
            "loss: 2.934 (epoch: 11, step: 100) // Avg time/img: 0.0482 s\n",
            "loss: 2.934 (epoch: 11, step: 150) // Avg time/img: 0.0481 s\n",
            "loss: 2.934 (epoch: 11, step: 200) // Avg time/img: 0.0480 s\n",
            "loss: 2.934 (epoch: 11, step: 250) // Avg time/img: 0.0480 s\n",
            "loss: 2.933 (epoch: 11, step: 300) // Avg time/img: 0.0480 s\n",
            "loss: 2.933 (epoch: 11, step: 350) // Avg time/img: 0.0480 s\n",
            "loss: 2.933 (epoch: 11, step: 400) // Avg time/img: 0.0481 s\n",
            "loss: 2.933 (epoch: 11, step: 450) // Avg time/img: 0.0481 s\n",
            "----- VALIDATING - EPOCH 11 -----\n",
            "VAL loss: 2.935 (epoch: 11, step: 0) // Avg time/img: 0.0541 s\n",
            "VAL loss: 2.935 (epoch: 11, step: 50) // Avg time/img: 0.0351 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m67.43\u001b[0m %\n",
            "save: ../save/erfnet_training_logitnorm_loss/model-011.pth (epoch: 11)\n",
            "----- TRAINING - EPOCH 12 -----\n",
            "LEARNING RATE:  2.437032195894977e-05\n",
            "loss: 2.936 (epoch: 12, step: 0) // Avg time/img: 0.0631 s\n",
            "loss: 2.933 (epoch: 12, step: 50) // Avg time/img: 0.0482 s\n",
            "loss: 2.933 (epoch: 12, step: 100) // Avg time/img: 0.0478 s\n",
            "loss: 2.933 (epoch: 12, step: 150) // Avg time/img: 0.0480 s\n",
            "loss: 2.933 (epoch: 12, step: 200) // Avg time/img: 0.0479 s\n",
            "loss: 2.933 (epoch: 12, step: 250) // Avg time/img: 0.0482 s\n",
            "loss: 2.933 (epoch: 12, step: 300) // Avg time/img: 0.0482 s\n",
            "loss: 2.933 (epoch: 12, step: 350) // Avg time/img: 0.0482 s\n",
            "loss: 2.933 (epoch: 12, step: 400) // Avg time/img: 0.0482 s\n",
            "loss: 2.933 (epoch: 12, step: 450) // Avg time/img: 0.0483 s\n",
            "----- VALIDATING - EPOCH 12 -----\n",
            "VAL loss: 2.935 (epoch: 12, step: 0) // Avg time/img: 0.0455 s\n",
            "VAL loss: 2.934 (epoch: 12, step: 50) // Avg time/img: 0.0343 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m67.54\u001b[0m %\n",
            "save: ../save/erfnet_training_logitnorm_loss/model-012.pth (epoch: 12)\n",
            "----- TRAINING - EPOCH 13 -----\n",
            "LEARNING RATE:  2.191916452770435e-05\n",
            "loss: 2.934 (epoch: 13, step: 0) // Avg time/img: 0.0551 s\n",
            "loss: 2.933 (epoch: 13, step: 50) // Avg time/img: 0.0481 s\n",
            "loss: 2.933 (epoch: 13, step: 100) // Avg time/img: 0.0480 s\n",
            "loss: 2.933 (epoch: 13, step: 150) // Avg time/img: 0.0481 s\n",
            "loss: 2.933 (epoch: 13, step: 200) // Avg time/img: 0.0481 s\n",
            "loss: 2.933 (epoch: 13, step: 250) // Avg time/img: 0.0482 s\n",
            "loss: 2.933 (epoch: 13, step: 300) // Avg time/img: 0.0484 s\n",
            "loss: 2.933 (epoch: 13, step: 350) // Avg time/img: 0.0483 s\n",
            "loss: 2.933 (epoch: 13, step: 400) // Avg time/img: 0.0483 s\n",
            "loss: 2.933 (epoch: 13, step: 450) // Avg time/img: 0.0483 s\n",
            "----- VALIDATING - EPOCH 13 -----\n",
            "VAL loss: 2.935 (epoch: 13, step: 0) // Avg time/img: 0.0462 s\n",
            "VAL loss: 2.934 (epoch: 13, step: 50) // Avg time/img: 0.0349 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m67.58\u001b[0m %\n",
            "save: ../save/erfnet_training_logitnorm_loss/model-013.pth (epoch: 13)\n",
            "----- TRAINING - EPOCH 14 -----\n",
            "LEARNING RATE:  1.9437089939938174e-05\n",
            "loss: 2.934 (epoch: 14, step: 0) // Avg time/img: 0.0531 s\n",
            "loss: 2.933 (epoch: 14, step: 50) // Avg time/img: 0.0481 s\n",
            "loss: 2.933 (epoch: 14, step: 100) // Avg time/img: 0.0481 s\n",
            "loss: 2.933 (epoch: 14, step: 150) // Avg time/img: 0.0480 s\n",
            "loss: 2.933 (epoch: 14, step: 200) // Avg time/img: 0.0481 s\n",
            "loss: 2.933 (epoch: 14, step: 250) // Avg time/img: 0.0480 s\n",
            "loss: 2.933 (epoch: 14, step: 300) // Avg time/img: 0.0482 s\n",
            "loss: 2.933 (epoch: 14, step: 350) // Avg time/img: 0.0482 s\n",
            "loss: 2.933 (epoch: 14, step: 400) // Avg time/img: 0.0483 s\n",
            "loss: 2.933 (epoch: 14, step: 450) // Avg time/img: 0.0483 s\n",
            "----- VALIDATING - EPOCH 14 -----\n",
            "VAL loss: 2.935 (epoch: 14, step: 0) // Avg time/img: 0.0597 s\n",
            "VAL loss: 2.934 (epoch: 14, step: 50) // Avg time/img: 0.0353 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m67.08\u001b[0m %\n",
            "save: ../save/erfnet_training_logitnorm_loss/model-014.pth (epoch: 14)\n",
            "----- TRAINING - EPOCH 15 -----\n",
            "LEARNING RATE:  1.6919173095082493e-05\n",
            "loss: 2.932 (epoch: 15, step: 0) // Avg time/img: 0.0593 s\n",
            "loss: 2.933 (epoch: 15, step: 50) // Avg time/img: 0.0480 s\n",
            "loss: 2.933 (epoch: 15, step: 100) // Avg time/img: 0.0481 s\n",
            "loss: 2.933 (epoch: 15, step: 150) // Avg time/img: 0.0481 s\n",
            "loss: 2.933 (epoch: 15, step: 200) // Avg time/img: 0.0480 s\n",
            "loss: 2.933 (epoch: 15, step: 250) // Avg time/img: 0.0482 s\n",
            "loss: 2.933 (epoch: 15, step: 300) // Avg time/img: 0.0480 s\n",
            "loss: 2.933 (epoch: 15, step: 350) // Avg time/img: 0.0480 s\n",
            "loss: 2.933 (epoch: 15, step: 400) // Avg time/img: 0.0480 s\n",
            "loss: 2.933 (epoch: 15, step: 450) // Avg time/img: 0.0480 s\n",
            "----- VALIDATING - EPOCH 15 -----\n",
            "VAL loss: 2.935 (epoch: 15, step: 0) // Avg time/img: 0.0449 s\n",
            "VAL loss: 2.934 (epoch: 15, step: 50) // Avg time/img: 0.0348 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m67.39\u001b[0m %\n",
            "save: ../save/erfnet_training_logitnorm_loss/model-015.pth (epoch: 15)\n",
            "----- TRAINING - EPOCH 16 -----\n",
            "LEARNING RATE:  1.4358729437462937e-05\n",
            "loss: 2.936 (epoch: 16, step: 0) // Avg time/img: 0.0550 s\n",
            "loss: 2.933 (epoch: 16, step: 50) // Avg time/img: 0.0487 s\n",
            "loss: 2.933 (epoch: 16, step: 100) // Avg time/img: 0.0481 s\n",
            "loss: 2.933 (epoch: 16, step: 150) // Avg time/img: 0.0482 s\n",
            "loss: 2.933 (epoch: 16, step: 200) // Avg time/img: 0.0482 s\n",
            "loss: 2.933 (epoch: 16, step: 250) // Avg time/img: 0.0480 s\n",
            "loss: 2.933 (epoch: 16, step: 300) // Avg time/img: 0.0481 s\n",
            "loss: 2.933 (epoch: 16, step: 350) // Avg time/img: 0.0481 s\n",
            "loss: 2.933 (epoch: 16, step: 400) // Avg time/img: 0.0482 s\n",
            "loss: 2.933 (epoch: 16, step: 450) // Avg time/img: 0.0482 s\n",
            "----- VALIDATING - EPOCH 16 -----\n",
            "VAL loss: 2.935 (epoch: 16, step: 0) // Avg time/img: 0.0522 s\n",
            "VAL loss: 2.934 (epoch: 16, step: 50) // Avg time/img: 0.0349 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m67.19\u001b[0m %\n",
            "save: ../save/erfnet_training_logitnorm_loss/model-016.pth (epoch: 16)\n",
            "----- TRAINING - EPOCH 17 -----\n",
            "LEARNING RATE:  1.1746189430880188e-05\n",
            "loss: 2.933 (epoch: 17, step: 0) // Avg time/img: 0.0562 s\n",
            "loss: 2.933 (epoch: 17, step: 50) // Avg time/img: 0.0483 s\n",
            "loss: 2.933 (epoch: 17, step: 100) // Avg time/img: 0.0485 s\n",
            "loss: 2.933 (epoch: 17, step: 150) // Avg time/img: 0.0482 s\n",
            "loss: 2.933 (epoch: 17, step: 200) // Avg time/img: 0.0483 s\n",
            "loss: 2.933 (epoch: 17, step: 250) // Avg time/img: 0.0483 s\n",
            "loss: 2.933 (epoch: 17, step: 300) // Avg time/img: 0.0482 s\n",
            "loss: 2.933 (epoch: 17, step: 350) // Avg time/img: 0.0483 s\n",
            "loss: 2.933 (epoch: 17, step: 400) // Avg time/img: 0.0483 s\n",
            "loss: 2.933 (epoch: 17, step: 450) // Avg time/img: 0.0484 s\n",
            "----- VALIDATING - EPOCH 17 -----\n",
            "VAL loss: 2.935 (epoch: 17, step: 0) // Avg time/img: 0.0493 s\n",
            "VAL loss: 2.934 (epoch: 17, step: 50) // Avg time/img: 0.0351 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m67.09\u001b[0m %\n",
            "save: ../save/erfnet_training_logitnorm_loss/model-017.pth (epoch: 17)\n",
            "----- TRAINING - EPOCH 18 -----\n",
            "LEARNING RATE:  9.066760365683729e-06\n",
            "loss: 2.934 (epoch: 18, step: 0) // Avg time/img: 0.0539 s\n",
            "loss: 2.933 (epoch: 18, step: 50) // Avg time/img: 0.0484 s\n",
            "loss: 2.933 (epoch: 18, step: 100) // Avg time/img: 0.0481 s\n",
            "loss: 2.933 (epoch: 18, step: 150) // Avg time/img: 0.0480 s\n",
            "loss: 2.933 (epoch: 18, step: 200) // Avg time/img: 0.0480 s\n",
            "loss: 2.933 (epoch: 18, step: 250) // Avg time/img: 0.0479 s\n",
            "loss: 2.933 (epoch: 18, step: 300) // Avg time/img: 0.0479 s\n",
            "loss: 2.933 (epoch: 18, step: 350) // Avg time/img: 0.0479 s\n",
            "loss: 2.933 (epoch: 18, step: 400) // Avg time/img: 0.0480 s\n",
            "loss: 2.933 (epoch: 18, step: 450) // Avg time/img: 0.0480 s\n",
            "----- VALIDATING - EPOCH 18 -----\n",
            "VAL loss: 2.935 (epoch: 18, step: 0) // Avg time/img: 0.0581 s\n",
            "VAL loss: 2.934 (epoch: 18, step: 50) // Avg time/img: 0.0354 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m67.11\u001b[0m %\n",
            "save: ../save/erfnet_training_logitnorm_loss/model-018.pth (epoch: 18)\n",
            "----- TRAINING - EPOCH 19 -----\n",
            "LEARNING RATE:  6.294627058970836e-06\n",
            "loss: 2.933 (epoch: 19, step: 0) // Avg time/img: 0.0576 s\n",
            "loss: 2.933 (epoch: 19, step: 50) // Avg time/img: 0.0476 s\n",
            "loss: 2.933 (epoch: 19, step: 100) // Avg time/img: 0.0477 s\n",
            "loss: 2.933 (epoch: 19, step: 150) // Avg time/img: 0.0475 s\n",
            "loss: 2.933 (epoch: 19, step: 200) // Avg time/img: 0.0476 s\n",
            "loss: 2.933 (epoch: 19, step: 250) // Avg time/img: 0.0478 s\n",
            "loss: 2.933 (epoch: 19, step: 300) // Avg time/img: 0.0479 s\n",
            "loss: 2.933 (epoch: 19, step: 350) // Avg time/img: 0.0479 s\n",
            "loss: 2.933 (epoch: 19, step: 400) // Avg time/img: 0.0479 s\n",
            "loss: 2.933 (epoch: 19, step: 450) // Avg time/img: 0.0480 s\n",
            "----- VALIDATING - EPOCH 19 -----\n",
            "VAL loss: 2.935 (epoch: 19, step: 0) // Avg time/img: 0.0482 s\n",
            "VAL loss: 2.934 (epoch: 19, step: 50) // Avg time/img: 0.0350 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m67.07\u001b[0m %\n",
            "save: ../save/erfnet_training_logitnorm_loss/model-019.pth (epoch: 19)\n",
            "----- TRAINING - EPOCH 20 -----\n",
            "LEARNING RATE:  3.373207119183911e-06\n",
            "loss: 2.932 (epoch: 20, step: 0) // Avg time/img: 0.0556 s\n",
            "loss: 2.933 (epoch: 20, step: 50) // Avg time/img: 0.0485 s\n",
            "loss: 2.933 (epoch: 20, step: 100) // Avg time/img: 0.0483 s\n",
            "loss: 2.933 (epoch: 20, step: 150) // Avg time/img: 0.0483 s\n",
            "loss: 2.933 (epoch: 20, step: 200) // Avg time/img: 0.0483 s\n",
            "loss: 2.933 (epoch: 20, step: 250) // Avg time/img: 0.0484 s\n",
            "loss: 2.933 (epoch: 20, step: 300) // Avg time/img: 0.0485 s\n",
            "loss: 2.933 (epoch: 20, step: 350) // Avg time/img: 0.0485 s\n",
            "loss: 2.933 (epoch: 20, step: 400) // Avg time/img: 0.0485 s\n",
            "loss: 2.933 (epoch: 20, step: 450) // Avg time/img: 0.0485 s\n",
            "----- VALIDATING - EPOCH 20 -----\n",
            "VAL loss: 2.935 (epoch: 20, step: 0) // Avg time/img: 0.0530 s\n",
            "VAL loss: 2.934 (epoch: 20, step: 50) // Avg time/img: 0.0346 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m66.89\u001b[0m %\n",
            "save: ../save/erfnet_training_logitnorm_loss/model-020.pth (epoch: 20)\n",
            "========== TRAINING FINISHED ===========\n",
            "Model saved in /content/AnomalySegmentation/save/erfnet_training_logitnorm_loss\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_logitnorm_loss/ (stored 0%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_logitnorm_loss/model_best.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_logitnorm_loss/model-007.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_logitnorm_loss/model-015.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_logitnorm_loss/model-008.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_logitnorm_loss/model-004.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_logitnorm_loss/model-005.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_logitnorm_loss/model-002.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_logitnorm_loss/opts.txt (deflated 37%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_logitnorm_loss/model.txt (deflated 92%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_logitnorm_loss/model-014.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_logitnorm_loss/model-013.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_logitnorm_loss/model-012.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_logitnorm_loss/model-016.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_logitnorm_loss/model-011.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_logitnorm_loss/checkpoint.pth.tar (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_logitnorm_loss/model-010.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_logitnorm_loss/model-020.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_logitnorm_loss/model_best.pth.tar (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_logitnorm_loss/model-009.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_logitnorm_loss/model-003.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_logitnorm_loss/model-006.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_logitnorm_loss/automated_log.txt (deflated 69%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_logitnorm_loss/model-019.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_logitnorm_loss/model-017.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_logitnorm_loss/model-018.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_logitnorm_loss/best.txt (stored 0%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_logitnorm_loss/model-001.pth (deflated 10%)\n",
            "\n",
            "\n",
            "----- Fine-tuning with IsoMaxPlus loss -----\n",
            "odict_keys(['module.encoder.initial_block.conv.weight', 'module.encoder.initial_block.conv.bias', 'module.encoder.initial_block.bn.weight', 'module.encoder.initial_block.bn.bias', 'module.encoder.initial_block.bn.running_mean', 'module.encoder.initial_block.bn.running_var', 'module.encoder.layers.0.conv.weight', 'module.encoder.layers.0.conv.bias', 'module.encoder.layers.0.bn.weight', 'module.encoder.layers.0.bn.bias', 'module.encoder.layers.0.bn.running_mean', 'module.encoder.layers.0.bn.running_var', 'module.encoder.layers.1.conv3x1_1.weight', 'module.encoder.layers.1.conv3x1_1.bias', 'module.encoder.layers.1.conv1x3_1.weight', 'module.encoder.layers.1.conv1x3_1.bias', 'module.encoder.layers.1.conv3x1_2.weight', 'module.encoder.layers.1.conv3x1_2.bias', 'module.encoder.layers.1.conv1x3_2.weight', 'module.encoder.layers.1.conv1x3_2.bias', 'module.encoder.layers.1.bn1.weight', 'module.encoder.layers.1.bn1.bias', 'module.encoder.layers.1.bn1.running_mean', 'module.encoder.layers.1.bn1.running_var', 'module.encoder.layers.1.bn2.weight', 'module.encoder.layers.1.bn2.bias', 'module.encoder.layers.1.bn2.running_mean', 'module.encoder.layers.1.bn2.running_var', 'module.encoder.layers.2.conv3x1_1.weight', 'module.encoder.layers.2.conv3x1_1.bias', 'module.encoder.layers.2.conv1x3_1.weight', 'module.encoder.layers.2.conv1x3_1.bias', 'module.encoder.layers.2.conv3x1_2.weight', 'module.encoder.layers.2.conv3x1_2.bias', 'module.encoder.layers.2.conv1x3_2.weight', 'module.encoder.layers.2.conv1x3_2.bias', 'module.encoder.layers.2.bn1.weight', 'module.encoder.layers.2.bn1.bias', 'module.encoder.layers.2.bn1.running_mean', 'module.encoder.layers.2.bn1.running_var', 'module.encoder.layers.2.bn2.weight', 'module.encoder.layers.2.bn2.bias', 'module.encoder.layers.2.bn2.running_mean', 'module.encoder.layers.2.bn2.running_var', 'module.encoder.layers.3.conv3x1_1.weight', 'module.encoder.layers.3.conv3x1_1.bias', 'module.encoder.layers.3.conv1x3_1.weight', 'module.encoder.layers.3.conv1x3_1.bias', 'module.encoder.layers.3.conv3x1_2.weight', 'module.encoder.layers.3.conv3x1_2.bias', 'module.encoder.layers.3.conv1x3_2.weight', 'module.encoder.layers.3.conv1x3_2.bias', 'module.encoder.layers.3.bn1.weight', 'module.encoder.layers.3.bn1.bias', 'module.encoder.layers.3.bn1.running_mean', 'module.encoder.layers.3.bn1.running_var', 'module.encoder.layers.3.bn2.weight', 'module.encoder.layers.3.bn2.bias', 'module.encoder.layers.3.bn2.running_mean', 'module.encoder.layers.3.bn2.running_var', 'module.encoder.layers.4.conv3x1_1.weight', 'module.encoder.layers.4.conv3x1_1.bias', 'module.encoder.layers.4.conv1x3_1.weight', 'module.encoder.layers.4.conv1x3_1.bias', 'module.encoder.layers.4.conv3x1_2.weight', 'module.encoder.layers.4.conv3x1_2.bias', 'module.encoder.layers.4.conv1x3_2.weight', 'module.encoder.layers.4.conv1x3_2.bias', 'module.encoder.layers.4.bn1.weight', 'module.encoder.layers.4.bn1.bias', 'module.encoder.layers.4.bn1.running_mean', 'module.encoder.layers.4.bn1.running_var', 'module.encoder.layers.4.bn2.weight', 'module.encoder.layers.4.bn2.bias', 'module.encoder.layers.4.bn2.running_mean', 'module.encoder.layers.4.bn2.running_var', 'module.encoder.layers.5.conv3x1_1.weight', 'module.encoder.layers.5.conv3x1_1.bias', 'module.encoder.layers.5.conv1x3_1.weight', 'module.encoder.layers.5.conv1x3_1.bias', 'module.encoder.layers.5.conv3x1_2.weight', 'module.encoder.layers.5.conv3x1_2.bias', 'module.encoder.layers.5.conv1x3_2.weight', 'module.encoder.layers.5.conv1x3_2.bias', 'module.encoder.layers.5.bn1.weight', 'module.encoder.layers.5.bn1.bias', 'module.encoder.layers.5.bn1.running_mean', 'module.encoder.layers.5.bn1.running_var', 'module.encoder.layers.5.bn2.weight', 'module.encoder.layers.5.bn2.bias', 'module.encoder.layers.5.bn2.running_mean', 'module.encoder.layers.5.bn2.running_var', 'module.encoder.layers.6.conv.weight', 'module.encoder.layers.6.conv.bias', 'module.encoder.layers.6.bn.weight', 'module.encoder.layers.6.bn.bias', 'module.encoder.layers.6.bn.running_mean', 'module.encoder.layers.6.bn.running_var', 'module.encoder.layers.7.conv3x1_1.weight', 'module.encoder.layers.7.conv3x1_1.bias', 'module.encoder.layers.7.conv1x3_1.weight', 'module.encoder.layers.7.conv1x3_1.bias', 'module.encoder.layers.7.conv3x1_2.weight', 'module.encoder.layers.7.conv3x1_2.bias', 'module.encoder.layers.7.conv1x3_2.weight', 'module.encoder.layers.7.conv1x3_2.bias', 'module.encoder.layers.7.bn1.weight', 'module.encoder.layers.7.bn1.bias', 'module.encoder.layers.7.bn1.running_mean', 'module.encoder.layers.7.bn1.running_var', 'module.encoder.layers.7.bn2.weight', 'module.encoder.layers.7.bn2.bias', 'module.encoder.layers.7.bn2.running_mean', 'module.encoder.layers.7.bn2.running_var', 'module.encoder.layers.8.conv3x1_1.weight', 'module.encoder.layers.8.conv3x1_1.bias', 'module.encoder.layers.8.conv1x3_1.weight', 'module.encoder.layers.8.conv1x3_1.bias', 'module.encoder.layers.8.conv3x1_2.weight', 'module.encoder.layers.8.conv3x1_2.bias', 'module.encoder.layers.8.conv1x3_2.weight', 'module.encoder.layers.8.conv1x3_2.bias', 'module.encoder.layers.8.bn1.weight', 'module.encoder.layers.8.bn1.bias', 'module.encoder.layers.8.bn1.running_mean', 'module.encoder.layers.8.bn1.running_var', 'module.encoder.layers.8.bn2.weight', 'module.encoder.layers.8.bn2.bias', 'module.encoder.layers.8.bn2.running_mean', 'module.encoder.layers.8.bn2.running_var', 'module.encoder.layers.9.conv3x1_1.weight', 'module.encoder.layers.9.conv3x1_1.bias', 'module.encoder.layers.9.conv1x3_1.weight', 'module.encoder.layers.9.conv1x3_1.bias', 'module.encoder.layers.9.conv3x1_2.weight', 'module.encoder.layers.9.conv3x1_2.bias', 'module.encoder.layers.9.conv1x3_2.weight', 'module.encoder.layers.9.conv1x3_2.bias', 'module.encoder.layers.9.bn1.weight', 'module.encoder.layers.9.bn1.bias', 'module.encoder.layers.9.bn1.running_mean', 'module.encoder.layers.9.bn1.running_var', 'module.encoder.layers.9.bn2.weight', 'module.encoder.layers.9.bn2.bias', 'module.encoder.layers.9.bn2.running_mean', 'module.encoder.layers.9.bn2.running_var', 'module.encoder.layers.10.conv3x1_1.weight', 'module.encoder.layers.10.conv3x1_1.bias', 'module.encoder.layers.10.conv1x3_1.weight', 'module.encoder.layers.10.conv1x3_1.bias', 'module.encoder.layers.10.conv3x1_2.weight', 'module.encoder.layers.10.conv3x1_2.bias', 'module.encoder.layers.10.conv1x3_2.weight', 'module.encoder.layers.10.conv1x3_2.bias', 'module.encoder.layers.10.bn1.weight', 'module.encoder.layers.10.bn1.bias', 'module.encoder.layers.10.bn1.running_mean', 'module.encoder.layers.10.bn1.running_var', 'module.encoder.layers.10.bn2.weight', 'module.encoder.layers.10.bn2.bias', 'module.encoder.layers.10.bn2.running_mean', 'module.encoder.layers.10.bn2.running_var', 'module.encoder.layers.11.conv3x1_1.weight', 'module.encoder.layers.11.conv3x1_1.bias', 'module.encoder.layers.11.conv1x3_1.weight', 'module.encoder.layers.11.conv1x3_1.bias', 'module.encoder.layers.11.conv3x1_2.weight', 'module.encoder.layers.11.conv3x1_2.bias', 'module.encoder.layers.11.conv1x3_2.weight', 'module.encoder.layers.11.conv1x3_2.bias', 'module.encoder.layers.11.bn1.weight', 'module.encoder.layers.11.bn1.bias', 'module.encoder.layers.11.bn1.running_mean', 'module.encoder.layers.11.bn1.running_var', 'module.encoder.layers.11.bn2.weight', 'module.encoder.layers.11.bn2.bias', 'module.encoder.layers.11.bn2.running_mean', 'module.encoder.layers.11.bn2.running_var', 'module.encoder.layers.12.conv3x1_1.weight', 'module.encoder.layers.12.conv3x1_1.bias', 'module.encoder.layers.12.conv1x3_1.weight', 'module.encoder.layers.12.conv1x3_1.bias', 'module.encoder.layers.12.conv3x1_2.weight', 'module.encoder.layers.12.conv3x1_2.bias', 'module.encoder.layers.12.conv1x3_2.weight', 'module.encoder.layers.12.conv1x3_2.bias', 'module.encoder.layers.12.bn1.weight', 'module.encoder.layers.12.bn1.bias', 'module.encoder.layers.12.bn1.running_mean', 'module.encoder.layers.12.bn1.running_var', 'module.encoder.layers.12.bn2.weight', 'module.encoder.layers.12.bn2.bias', 'module.encoder.layers.12.bn2.running_mean', 'module.encoder.layers.12.bn2.running_var', 'module.encoder.layers.13.conv3x1_1.weight', 'module.encoder.layers.13.conv3x1_1.bias', 'module.encoder.layers.13.conv1x3_1.weight', 'module.encoder.layers.13.conv1x3_1.bias', 'module.encoder.layers.13.conv3x1_2.weight', 'module.encoder.layers.13.conv3x1_2.bias', 'module.encoder.layers.13.conv1x3_2.weight', 'module.encoder.layers.13.conv1x3_2.bias', 'module.encoder.layers.13.bn1.weight', 'module.encoder.layers.13.bn1.bias', 'module.encoder.layers.13.bn1.running_mean', 'module.encoder.layers.13.bn1.running_var', 'module.encoder.layers.13.bn2.weight', 'module.encoder.layers.13.bn2.bias', 'module.encoder.layers.13.bn2.running_mean', 'module.encoder.layers.13.bn2.running_var', 'module.encoder.layers.14.conv3x1_1.weight', 'module.encoder.layers.14.conv3x1_1.bias', 'module.encoder.layers.14.conv1x3_1.weight', 'module.encoder.layers.14.conv1x3_1.bias', 'module.encoder.layers.14.conv3x1_2.weight', 'module.encoder.layers.14.conv3x1_2.bias', 'module.encoder.layers.14.conv1x3_2.weight', 'module.encoder.layers.14.conv1x3_2.bias', 'module.encoder.layers.14.bn1.weight', 'module.encoder.layers.14.bn1.bias', 'module.encoder.layers.14.bn1.running_mean', 'module.encoder.layers.14.bn1.running_var', 'module.encoder.layers.14.bn2.weight', 'module.encoder.layers.14.bn2.bias', 'module.encoder.layers.14.bn2.running_mean', 'module.encoder.layers.14.bn2.running_var', 'module.decoder.layers.0.conv.weight', 'module.decoder.layers.0.conv.bias', 'module.decoder.layers.0.bn.weight', 'module.decoder.layers.0.bn.bias', 'module.decoder.layers.0.bn.running_mean', 'module.decoder.layers.0.bn.running_var', 'module.decoder.layers.1.conv3x1_1.weight', 'module.decoder.layers.1.conv3x1_1.bias', 'module.decoder.layers.1.conv1x3_1.weight', 'module.decoder.layers.1.conv1x3_1.bias', 'module.decoder.layers.1.bn1.weight', 'module.decoder.layers.1.bn1.bias', 'module.decoder.layers.1.bn1.running_mean', 'module.decoder.layers.1.bn1.running_var', 'module.decoder.layers.1.conv3x1_2.weight', 'module.decoder.layers.1.conv3x1_2.bias', 'module.decoder.layers.1.conv1x3_2.weight', 'module.decoder.layers.1.conv1x3_2.bias', 'module.decoder.layers.1.bn2.weight', 'module.decoder.layers.1.bn2.bias', 'module.decoder.layers.1.bn2.running_mean', 'module.decoder.layers.1.bn2.running_var', 'module.decoder.layers.2.conv3x1_1.weight', 'module.decoder.layers.2.conv3x1_1.bias', 'module.decoder.layers.2.conv1x3_1.weight', 'module.decoder.layers.2.conv1x3_1.bias', 'module.decoder.layers.2.bn1.weight', 'module.decoder.layers.2.bn1.bias', 'module.decoder.layers.2.bn1.running_mean', 'module.decoder.layers.2.bn1.running_var', 'module.decoder.layers.2.conv3x1_2.weight', 'module.decoder.layers.2.conv3x1_2.bias', 'module.decoder.layers.2.conv1x3_2.weight', 'module.decoder.layers.2.conv1x3_2.bias', 'module.decoder.layers.2.bn2.weight', 'module.decoder.layers.2.bn2.bias', 'module.decoder.layers.2.bn2.running_mean', 'module.decoder.layers.2.bn2.running_var', 'module.decoder.layers.3.conv.weight', 'module.decoder.layers.3.conv.bias', 'module.decoder.layers.3.bn.weight', 'module.decoder.layers.3.bn.bias', 'module.decoder.layers.3.bn.running_mean', 'module.decoder.layers.3.bn.running_var', 'module.decoder.layers.4.conv3x1_1.weight', 'module.decoder.layers.4.conv3x1_1.bias', 'module.decoder.layers.4.conv1x3_1.weight', 'module.decoder.layers.4.conv1x3_1.bias', 'module.decoder.layers.4.bn1.weight', 'module.decoder.layers.4.bn1.bias', 'module.decoder.layers.4.bn1.running_mean', 'module.decoder.layers.4.bn1.running_var', 'module.decoder.layers.4.conv3x1_2.weight', 'module.decoder.layers.4.conv3x1_2.bias', 'module.decoder.layers.4.conv1x3_2.weight', 'module.decoder.layers.4.conv1x3_2.bias', 'module.decoder.layers.4.bn2.weight', 'module.decoder.layers.4.bn2.bias', 'module.decoder.layers.4.bn2.running_mean', 'module.decoder.layers.4.bn2.running_var', 'module.decoder.layers.5.conv3x1_1.weight', 'module.decoder.layers.5.conv3x1_1.bias', 'module.decoder.layers.5.conv1x3_1.weight', 'module.decoder.layers.5.conv1x3_1.bias', 'module.decoder.layers.5.bn1.weight', 'module.decoder.layers.5.bn1.bias', 'module.decoder.layers.5.bn1.running_mean', 'module.decoder.layers.5.bn1.running_var', 'module.decoder.layers.5.conv3x1_2.weight', 'module.decoder.layers.5.conv3x1_2.bias', 'module.decoder.layers.5.conv1x3_2.weight', 'module.decoder.layers.5.conv1x3_2.bias', 'module.decoder.layers.5.bn2.weight', 'module.decoder.layers.5.bn2.bias', 'module.decoder.layers.5.bn2.running_mean', 'module.decoder.layers.5.bn2.running_var', 'module.decoder.output_conv.weight', 'module.decoder.output_conv.bias'])\n",
            "odict_keys(['encoder.initial_block.conv.weight', 'encoder.initial_block.conv.bias', 'encoder.initial_block.bn.weight', 'encoder.initial_block.bn.bias', 'encoder.initial_block.bn.running_mean', 'encoder.initial_block.bn.running_var', 'encoder.initial_block.bn.num_batches_tracked', 'encoder.layers.0.conv.weight', 'encoder.layers.0.conv.bias', 'encoder.layers.0.bn.weight', 'encoder.layers.0.bn.bias', 'encoder.layers.0.bn.running_mean', 'encoder.layers.0.bn.running_var', 'encoder.layers.0.bn.num_batches_tracked', 'encoder.layers.1.conv3x1_1.weight', 'encoder.layers.1.conv3x1_1.bias', 'encoder.layers.1.conv1x3_1.weight', 'encoder.layers.1.conv1x3_1.bias', 'encoder.layers.1.bn1.weight', 'encoder.layers.1.bn1.bias', 'encoder.layers.1.bn1.running_mean', 'encoder.layers.1.bn1.running_var', 'encoder.layers.1.bn1.num_batches_tracked', 'encoder.layers.1.conv3x1_2.weight', 'encoder.layers.1.conv3x1_2.bias', 'encoder.layers.1.conv1x3_2.weight', 'encoder.layers.1.conv1x3_2.bias', 'encoder.layers.1.bn2.weight', 'encoder.layers.1.bn2.bias', 'encoder.layers.1.bn2.running_mean', 'encoder.layers.1.bn2.running_var', 'encoder.layers.1.bn2.num_batches_tracked', 'encoder.layers.2.conv3x1_1.weight', 'encoder.layers.2.conv3x1_1.bias', 'encoder.layers.2.conv1x3_1.weight', 'encoder.layers.2.conv1x3_1.bias', 'encoder.layers.2.bn1.weight', 'encoder.layers.2.bn1.bias', 'encoder.layers.2.bn1.running_mean', 'encoder.layers.2.bn1.running_var', 'encoder.layers.2.bn1.num_batches_tracked', 'encoder.layers.2.conv3x1_2.weight', 'encoder.layers.2.conv3x1_2.bias', 'encoder.layers.2.conv1x3_2.weight', 'encoder.layers.2.conv1x3_2.bias', 'encoder.layers.2.bn2.weight', 'encoder.layers.2.bn2.bias', 'encoder.layers.2.bn2.running_mean', 'encoder.layers.2.bn2.running_var', 'encoder.layers.2.bn2.num_batches_tracked', 'encoder.layers.3.conv3x1_1.weight', 'encoder.layers.3.conv3x1_1.bias', 'encoder.layers.3.conv1x3_1.weight', 'encoder.layers.3.conv1x3_1.bias', 'encoder.layers.3.bn1.weight', 'encoder.layers.3.bn1.bias', 'encoder.layers.3.bn1.running_mean', 'encoder.layers.3.bn1.running_var', 'encoder.layers.3.bn1.num_batches_tracked', 'encoder.layers.3.conv3x1_2.weight', 'encoder.layers.3.conv3x1_2.bias', 'encoder.layers.3.conv1x3_2.weight', 'encoder.layers.3.conv1x3_2.bias', 'encoder.layers.3.bn2.weight', 'encoder.layers.3.bn2.bias', 'encoder.layers.3.bn2.running_mean', 'encoder.layers.3.bn2.running_var', 'encoder.layers.3.bn2.num_batches_tracked', 'encoder.layers.4.conv3x1_1.weight', 'encoder.layers.4.conv3x1_1.bias', 'encoder.layers.4.conv1x3_1.weight', 'encoder.layers.4.conv1x3_1.bias', 'encoder.layers.4.bn1.weight', 'encoder.layers.4.bn1.bias', 'encoder.layers.4.bn1.running_mean', 'encoder.layers.4.bn1.running_var', 'encoder.layers.4.bn1.num_batches_tracked', 'encoder.layers.4.conv3x1_2.weight', 'encoder.layers.4.conv3x1_2.bias', 'encoder.layers.4.conv1x3_2.weight', 'encoder.layers.4.conv1x3_2.bias', 'encoder.layers.4.bn2.weight', 'encoder.layers.4.bn2.bias', 'encoder.layers.4.bn2.running_mean', 'encoder.layers.4.bn2.running_var', 'encoder.layers.4.bn2.num_batches_tracked', 'encoder.layers.5.conv3x1_1.weight', 'encoder.layers.5.conv3x1_1.bias', 'encoder.layers.5.conv1x3_1.weight', 'encoder.layers.5.conv1x3_1.bias', 'encoder.layers.5.bn1.weight', 'encoder.layers.5.bn1.bias', 'encoder.layers.5.bn1.running_mean', 'encoder.layers.5.bn1.running_var', 'encoder.layers.5.bn1.num_batches_tracked', 'encoder.layers.5.conv3x1_2.weight', 'encoder.layers.5.conv3x1_2.bias', 'encoder.layers.5.conv1x3_2.weight', 'encoder.layers.5.conv1x3_2.bias', 'encoder.layers.5.bn2.weight', 'encoder.layers.5.bn2.bias', 'encoder.layers.5.bn2.running_mean', 'encoder.layers.5.bn2.running_var', 'encoder.layers.5.bn2.num_batches_tracked', 'encoder.layers.6.conv.weight', 'encoder.layers.6.conv.bias', 'encoder.layers.6.bn.weight', 'encoder.layers.6.bn.bias', 'encoder.layers.6.bn.running_mean', 'encoder.layers.6.bn.running_var', 'encoder.layers.6.bn.num_batches_tracked', 'encoder.layers.7.conv3x1_1.weight', 'encoder.layers.7.conv3x1_1.bias', 'encoder.layers.7.conv1x3_1.weight', 'encoder.layers.7.conv1x3_1.bias', 'encoder.layers.7.bn1.weight', 'encoder.layers.7.bn1.bias', 'encoder.layers.7.bn1.running_mean', 'encoder.layers.7.bn1.running_var', 'encoder.layers.7.bn1.num_batches_tracked', 'encoder.layers.7.conv3x1_2.weight', 'encoder.layers.7.conv3x1_2.bias', 'encoder.layers.7.conv1x3_2.weight', 'encoder.layers.7.conv1x3_2.bias', 'encoder.layers.7.bn2.weight', 'encoder.layers.7.bn2.bias', 'encoder.layers.7.bn2.running_mean', 'encoder.layers.7.bn2.running_var', 'encoder.layers.7.bn2.num_batches_tracked', 'encoder.layers.8.conv3x1_1.weight', 'encoder.layers.8.conv3x1_1.bias', 'encoder.layers.8.conv1x3_1.weight', 'encoder.layers.8.conv1x3_1.bias', 'encoder.layers.8.bn1.weight', 'encoder.layers.8.bn1.bias', 'encoder.layers.8.bn1.running_mean', 'encoder.layers.8.bn1.running_var', 'encoder.layers.8.bn1.num_batches_tracked', 'encoder.layers.8.conv3x1_2.weight', 'encoder.layers.8.conv3x1_2.bias', 'encoder.layers.8.conv1x3_2.weight', 'encoder.layers.8.conv1x3_2.bias', 'encoder.layers.8.bn2.weight', 'encoder.layers.8.bn2.bias', 'encoder.layers.8.bn2.running_mean', 'encoder.layers.8.bn2.running_var', 'encoder.layers.8.bn2.num_batches_tracked', 'encoder.layers.9.conv3x1_1.weight', 'encoder.layers.9.conv3x1_1.bias', 'encoder.layers.9.conv1x3_1.weight', 'encoder.layers.9.conv1x3_1.bias', 'encoder.layers.9.bn1.weight', 'encoder.layers.9.bn1.bias', 'encoder.layers.9.bn1.running_mean', 'encoder.layers.9.bn1.running_var', 'encoder.layers.9.bn1.num_batches_tracked', 'encoder.layers.9.conv3x1_2.weight', 'encoder.layers.9.conv3x1_2.bias', 'encoder.layers.9.conv1x3_2.weight', 'encoder.layers.9.conv1x3_2.bias', 'encoder.layers.9.bn2.weight', 'encoder.layers.9.bn2.bias', 'encoder.layers.9.bn2.running_mean', 'encoder.layers.9.bn2.running_var', 'encoder.layers.9.bn2.num_batches_tracked', 'encoder.layers.10.conv3x1_1.weight', 'encoder.layers.10.conv3x1_1.bias', 'encoder.layers.10.conv1x3_1.weight', 'encoder.layers.10.conv1x3_1.bias', 'encoder.layers.10.bn1.weight', 'encoder.layers.10.bn1.bias', 'encoder.layers.10.bn1.running_mean', 'encoder.layers.10.bn1.running_var', 'encoder.layers.10.bn1.num_batches_tracked', 'encoder.layers.10.conv3x1_2.weight', 'encoder.layers.10.conv3x1_2.bias', 'encoder.layers.10.conv1x3_2.weight', 'encoder.layers.10.conv1x3_2.bias', 'encoder.layers.10.bn2.weight', 'encoder.layers.10.bn2.bias', 'encoder.layers.10.bn2.running_mean', 'encoder.layers.10.bn2.running_var', 'encoder.layers.10.bn2.num_batches_tracked', 'encoder.layers.11.conv3x1_1.weight', 'encoder.layers.11.conv3x1_1.bias', 'encoder.layers.11.conv1x3_1.weight', 'encoder.layers.11.conv1x3_1.bias', 'encoder.layers.11.bn1.weight', 'encoder.layers.11.bn1.bias', 'encoder.layers.11.bn1.running_mean', 'encoder.layers.11.bn1.running_var', 'encoder.layers.11.bn1.num_batches_tracked', 'encoder.layers.11.conv3x1_2.weight', 'encoder.layers.11.conv3x1_2.bias', 'encoder.layers.11.conv1x3_2.weight', 'encoder.layers.11.conv1x3_2.bias', 'encoder.layers.11.bn2.weight', 'encoder.layers.11.bn2.bias', 'encoder.layers.11.bn2.running_mean', 'encoder.layers.11.bn2.running_var', 'encoder.layers.11.bn2.num_batches_tracked', 'encoder.layers.12.conv3x1_1.weight', 'encoder.layers.12.conv3x1_1.bias', 'encoder.layers.12.conv1x3_1.weight', 'encoder.layers.12.conv1x3_1.bias', 'encoder.layers.12.bn1.weight', 'encoder.layers.12.bn1.bias', 'encoder.layers.12.bn1.running_mean', 'encoder.layers.12.bn1.running_var', 'encoder.layers.12.bn1.num_batches_tracked', 'encoder.layers.12.conv3x1_2.weight', 'encoder.layers.12.conv3x1_2.bias', 'encoder.layers.12.conv1x3_2.weight', 'encoder.layers.12.conv1x3_2.bias', 'encoder.layers.12.bn2.weight', 'encoder.layers.12.bn2.bias', 'encoder.layers.12.bn2.running_mean', 'encoder.layers.12.bn2.running_var', 'encoder.layers.12.bn2.num_batches_tracked', 'encoder.layers.13.conv3x1_1.weight', 'encoder.layers.13.conv3x1_1.bias', 'encoder.layers.13.conv1x3_1.weight', 'encoder.layers.13.conv1x3_1.bias', 'encoder.layers.13.bn1.weight', 'encoder.layers.13.bn1.bias', 'encoder.layers.13.bn1.running_mean', 'encoder.layers.13.bn1.running_var', 'encoder.layers.13.bn1.num_batches_tracked', 'encoder.layers.13.conv3x1_2.weight', 'encoder.layers.13.conv3x1_2.bias', 'encoder.layers.13.conv1x3_2.weight', 'encoder.layers.13.conv1x3_2.bias', 'encoder.layers.13.bn2.weight', 'encoder.layers.13.bn2.bias', 'encoder.layers.13.bn2.running_mean', 'encoder.layers.13.bn2.running_var', 'encoder.layers.13.bn2.num_batches_tracked', 'encoder.layers.14.conv3x1_1.weight', 'encoder.layers.14.conv3x1_1.bias', 'encoder.layers.14.conv1x3_1.weight', 'encoder.layers.14.conv1x3_1.bias', 'encoder.layers.14.bn1.weight', 'encoder.layers.14.bn1.bias', 'encoder.layers.14.bn1.running_mean', 'encoder.layers.14.bn1.running_var', 'encoder.layers.14.bn1.num_batches_tracked', 'encoder.layers.14.conv3x1_2.weight', 'encoder.layers.14.conv3x1_2.bias', 'encoder.layers.14.conv1x3_2.weight', 'encoder.layers.14.conv1x3_2.bias', 'encoder.layers.14.bn2.weight', 'encoder.layers.14.bn2.bias', 'encoder.layers.14.bn2.running_mean', 'encoder.layers.14.bn2.running_var', 'encoder.layers.14.bn2.num_batches_tracked', 'encoder.output_conv.weight', 'encoder.output_conv.bias', 'decoder.loss_first_part.prototypes', 'decoder.loss_first_part.distance_scale', 'decoder.layers.0.conv.weight', 'decoder.layers.0.conv.bias', 'decoder.layers.0.bn.weight', 'decoder.layers.0.bn.bias', 'decoder.layers.0.bn.running_mean', 'decoder.layers.0.bn.running_var', 'decoder.layers.0.bn.num_batches_tracked', 'decoder.layers.1.conv3x1_1.weight', 'decoder.layers.1.conv3x1_1.bias', 'decoder.layers.1.conv1x3_1.weight', 'decoder.layers.1.conv1x3_1.bias', 'decoder.layers.1.bn1.weight', 'decoder.layers.1.bn1.bias', 'decoder.layers.1.bn1.running_mean', 'decoder.layers.1.bn1.running_var', 'decoder.layers.1.bn1.num_batches_tracked', 'decoder.layers.1.conv3x1_2.weight', 'decoder.layers.1.conv3x1_2.bias', 'decoder.layers.1.conv1x3_2.weight', 'decoder.layers.1.conv1x3_2.bias', 'decoder.layers.1.bn2.weight', 'decoder.layers.1.bn2.bias', 'decoder.layers.1.bn2.running_mean', 'decoder.layers.1.bn2.running_var', 'decoder.layers.1.bn2.num_batches_tracked', 'decoder.layers.2.conv3x1_1.weight', 'decoder.layers.2.conv3x1_1.bias', 'decoder.layers.2.conv1x3_1.weight', 'decoder.layers.2.conv1x3_1.bias', 'decoder.layers.2.bn1.weight', 'decoder.layers.2.bn1.bias', 'decoder.layers.2.bn1.running_mean', 'decoder.layers.2.bn1.running_var', 'decoder.layers.2.bn1.num_batches_tracked', 'decoder.layers.2.conv3x1_2.weight', 'decoder.layers.2.conv3x1_2.bias', 'decoder.layers.2.conv1x3_2.weight', 'decoder.layers.2.conv1x3_2.bias', 'decoder.layers.2.bn2.weight', 'decoder.layers.2.bn2.bias', 'decoder.layers.2.bn2.running_mean', 'decoder.layers.2.bn2.running_var', 'decoder.layers.2.bn2.num_batches_tracked', 'decoder.layers.3.conv.weight', 'decoder.layers.3.conv.bias', 'decoder.layers.3.bn.weight', 'decoder.layers.3.bn.bias', 'decoder.layers.3.bn.running_mean', 'decoder.layers.3.bn.running_var', 'decoder.layers.3.bn.num_batches_tracked', 'decoder.layers.4.conv3x1_1.weight', 'decoder.layers.4.conv3x1_1.bias', 'decoder.layers.4.conv1x3_1.weight', 'decoder.layers.4.conv1x3_1.bias', 'decoder.layers.4.bn1.weight', 'decoder.layers.4.bn1.bias', 'decoder.layers.4.bn1.running_mean', 'decoder.layers.4.bn1.running_var', 'decoder.layers.4.bn1.num_batches_tracked', 'decoder.layers.4.conv3x1_2.weight', 'decoder.layers.4.conv3x1_2.bias', 'decoder.layers.4.conv1x3_2.weight', 'decoder.layers.4.conv1x3_2.bias', 'decoder.layers.4.bn2.weight', 'decoder.layers.4.bn2.bias', 'decoder.layers.4.bn2.running_mean', 'decoder.layers.4.bn2.running_var', 'decoder.layers.4.bn2.num_batches_tracked', 'decoder.layers.5.conv3x1_1.weight', 'decoder.layers.5.conv3x1_1.bias', 'decoder.layers.5.conv1x3_1.weight', 'decoder.layers.5.conv1x3_1.bias', 'decoder.layers.5.bn1.weight', 'decoder.layers.5.bn1.bias', 'decoder.layers.5.bn1.running_mean', 'decoder.layers.5.bn1.running_var', 'decoder.layers.5.bn1.num_batches_tracked', 'decoder.layers.5.conv3x1_2.weight', 'decoder.layers.5.conv3x1_2.bias', 'decoder.layers.5.conv1x3_2.weight', 'decoder.layers.5.conv1x3_2.bias', 'decoder.layers.5.bn2.weight', 'decoder.layers.5.bn2.bias', 'decoder.layers.5.bn2.running_mean', 'decoder.layers.5.bn2.running_var', 'decoder.layers.5.bn2.num_batches_tracked', 'decoder.output_conv.weight', 'decoder.output_conv.bias'])\n",
            "Size mismatch for decoder.output_conv.weight: torch.Size([16, 16, 1, 1]) vs torch.Size([16, 20, 2, 2])\n",
            "Size mismatch for decoder.output_conv.bias: torch.Size([16]) vs torch.Size([20])\n",
            "Import Model erfnet_isomaxplus with weights erfnet_pretrained.pth to FineTune\n",
            "========== TRAINING ===========\n",
            "/content/cityscapes/leftImg8bit/train\n",
            "/content/cityscapes/leftImg8bit/val\n",
            "----- TRAINING - EPOCH 1 -----\n",
            "LEARNING RATE:  5e-05\n",
            "loss: 5.576 (epoch: 1, step: 0) // Avg time/img: 0.9179 s\n",
            "loss: 5.586 (epoch: 1, step: 50) // Avg time/img: 0.1128 s\n",
            "loss: 5.534 (epoch: 1, step: 100) // Avg time/img: 0.1064 s\n",
            "loss: 5.48 (epoch: 1, step: 150) // Avg time/img: 0.1047 s\n",
            "loss: 5.419 (epoch: 1, step: 200) // Avg time/img: 0.1034 s\n",
            "loss: 5.359 (epoch: 1, step: 250) // Avg time/img: 0.1024 s\n",
            "loss: 5.297 (epoch: 1, step: 300) // Avg time/img: 0.1016 s\n",
            "loss: 5.23 (epoch: 1, step: 350) // Avg time/img: 0.1013 s\n",
            "loss: 5.163 (epoch: 1, step: 400) // Avg time/img: 0.1010 s\n",
            "loss: 5.099 (epoch: 1, step: 450) // Avg time/img: 0.1011 s\n",
            "----- VALIDATING - EPOCH 1 -----\n",
            "VAL loss: 4.168 (epoch: 1, step: 0) // Avg time/img: 0.0568 s\n",
            "VAL loss: 4.233 (epoch: 1, step: 50) // Avg time/img: 0.0409 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m0.96\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model-001.pth (epoch: 1)\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model_best.pth (epoch: 1)\n",
            "----- TRAINING - EPOCH 2 -----\n",
            "LEARNING RATE:  4.774426908107499e-05\n",
            "loss: 4.423 (epoch: 2, step: 0) // Avg time/img: 0.1619 s\n",
            "loss: 4.21 (epoch: 2, step: 50) // Avg time/img: 0.1019 s\n",
            "loss: 4.136 (epoch: 2, step: 100) // Avg time/img: 0.1017 s\n",
            "loss: 4.054 (epoch: 2, step: 150) // Avg time/img: 0.1008 s\n",
            "loss: 3.969 (epoch: 2, step: 200) // Avg time/img: 0.1006 s\n",
            "loss: 3.89 (epoch: 2, step: 250) // Avg time/img: 0.1004 s\n",
            "loss: 3.802 (epoch: 2, step: 300) // Avg time/img: 0.1003 s\n",
            "loss: 3.711 (epoch: 2, step: 350) // Avg time/img: 0.1002 s\n",
            "loss: 3.626 (epoch: 2, step: 400) // Avg time/img: 0.1002 s\n",
            "loss: 3.542 (epoch: 2, step: 450) // Avg time/img: 0.1001 s\n",
            "----- VALIDATING - EPOCH 2 -----\n",
            "VAL loss: 2.387 (epoch: 2, step: 0) // Avg time/img: 0.0572 s\n",
            "VAL loss: 2.624 (epoch: 2, step: 50) // Avg time/img: 0.0408 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m10.39\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model-002.pth (epoch: 2)\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model_best.pth (epoch: 2)\n",
            "----- TRAINING - EPOCH 3 -----\n",
            "LEARNING RATE:  4.547662880414811e-05\n",
            "loss: 2.496 (epoch: 3, step: 0) // Avg time/img: 0.1282 s\n",
            "loss: 2.549 (epoch: 3, step: 50) // Avg time/img: 0.1002 s\n",
            "loss: 2.489 (epoch: 3, step: 100) // Avg time/img: 0.1003 s\n",
            "loss: 2.427 (epoch: 3, step: 150) // Avg time/img: 0.1003 s\n",
            "loss: 2.375 (epoch: 3, step: 200) // Avg time/img: 0.1007 s\n",
            "loss: 2.322 (epoch: 3, step: 250) // Avg time/img: 0.1005 s\n",
            "loss: 2.276 (epoch: 3, step: 300) // Avg time/img: 0.1004 s\n",
            "loss: 2.227 (epoch: 3, step: 350) // Avg time/img: 0.1004 s\n",
            "loss: 2.183 (epoch: 3, step: 400) // Avg time/img: 0.1003 s\n",
            "loss: 2.141 (epoch: 3, step: 450) // Avg time/img: 0.1006 s\n",
            "----- VALIDATING - EPOCH 3 -----\n",
            "VAL loss: 1.487 (epoch: 3, step: 0) // Avg time/img: 0.0634 s\n",
            "VAL loss: 1.772 (epoch: 3, step: 50) // Avg time/img: 0.0410 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m17.12\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model-003.pth (epoch: 3)\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model_best.pth (epoch: 3)\n",
            "----- TRAINING - EPOCH 4 -----\n",
            "LEARNING RATE:  4.319634861514096e-05\n",
            "loss: 1.764 (epoch: 4, step: 0) // Avg time/img: 0.1183 s\n",
            "loss: 1.673 (epoch: 4, step: 50) // Avg time/img: 0.1016 s\n",
            "loss: 1.66 (epoch: 4, step: 100) // Avg time/img: 0.1000 s\n",
            "loss: 1.638 (epoch: 4, step: 150) // Avg time/img: 0.1008 s\n",
            "loss: 1.616 (epoch: 4, step: 200) // Avg time/img: 0.1008 s\n",
            "loss: 1.595 (epoch: 4, step: 250) // Avg time/img: 0.1004 s\n",
            "loss: 1.576 (epoch: 4, step: 300) // Avg time/img: 0.1003 s\n",
            "loss: 1.56 (epoch: 4, step: 350) // Avg time/img: 0.1005 s\n",
            "loss: 1.544 (epoch: 4, step: 400) // Avg time/img: 0.1004 s\n",
            "loss: 1.526 (epoch: 4, step: 450) // Avg time/img: 0.1004 s\n",
            "----- VALIDATING - EPOCH 4 -----\n",
            "VAL loss: 1.168 (epoch: 4, step: 0) // Avg time/img: 0.0532 s\n",
            "VAL loss: 1.435 (epoch: 4, step: 50) // Avg time/img: 0.0418 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m21.19\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model-004.pth (epoch: 4)\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model_best.pth (epoch: 4)\n",
            "----- TRAINING - EPOCH 5 -----\n",
            "LEARNING RATE:  4.090260730254292e-05\n",
            "loss: 1.727 (epoch: 5, step: 0) // Avg time/img: 0.1210 s\n",
            "loss: 1.336 (epoch: 5, step: 50) // Avg time/img: 0.0972 s\n",
            "loss: 1.317 (epoch: 5, step: 100) // Avg time/img: 0.0992 s\n",
            "loss: 1.31 (epoch: 5, step: 150) // Avg time/img: 0.0991 s\n",
            "loss: 1.308 (epoch: 5, step: 200) // Avg time/img: 0.0986 s\n",
            "loss: 1.299 (epoch: 5, step: 250) // Avg time/img: 0.0988 s\n",
            "loss: 1.292 (epoch: 5, step: 300) // Avg time/img: 0.0992 s\n",
            "loss: 1.278 (epoch: 5, step: 350) // Avg time/img: 0.0989 s\n",
            "loss: 1.267 (epoch: 5, step: 400) // Avg time/img: 0.0990 s\n",
            "loss: 1.259 (epoch: 5, step: 450) // Avg time/img: 0.0991 s\n",
            "----- VALIDATING - EPOCH 5 -----\n",
            "VAL loss: 1.028 (epoch: 5, step: 0) // Avg time/img: 0.0566 s\n",
            "VAL loss: 1.289 (epoch: 5, step: 50) // Avg time/img: 0.0408 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m25.61\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model-005.pth (epoch: 5)\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model_best.pth (epoch: 5)\n",
            "----- TRAINING - EPOCH 6 -----\n",
            "LEARNING RATE:  3.859447533617852e-05\n",
            "loss: 1.374 (epoch: 6, step: 0) // Avg time/img: 0.1099 s\n",
            "loss: 1.158 (epoch: 6, step: 50) // Avg time/img: 0.1007 s\n",
            "loss: 1.155 (epoch: 6, step: 100) // Avg time/img: 0.1004 s\n",
            "loss: 1.14 (epoch: 6, step: 150) // Avg time/img: 0.0999 s\n",
            "loss: 1.137 (epoch: 6, step: 200) // Avg time/img: 0.1002 s\n",
            "loss: 1.129 (epoch: 6, step: 250) // Avg time/img: 0.1006 s\n",
            "loss: 1.123 (epoch: 6, step: 300) // Avg time/img: 0.1000 s\n",
            "loss: 1.118 (epoch: 6, step: 350) // Avg time/img: 0.1001 s\n",
            "loss: 1.115 (epoch: 6, step: 400) // Avg time/img: 0.0999 s\n",
            "loss: 1.116 (epoch: 6, step: 450) // Avg time/img: 0.1001 s\n",
            "----- VALIDATING - EPOCH 6 -----\n",
            "VAL loss: 0.9108 (epoch: 6, step: 0) // Avg time/img: 0.0563 s\n",
            "VAL loss: 1.166 (epoch: 6, step: 50) // Avg time/img: 0.0408 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m26.34\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model-006.pth (epoch: 6)\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model_best.pth (epoch: 6)\n",
            "----- TRAINING - EPOCH 7 -----\n",
            "LEARNING RATE:  3.6270892346861e-05\n",
            "loss: 0.9215 (epoch: 7, step: 0) // Avg time/img: 0.1302 s\n",
            "loss: 1.05 (epoch: 7, step: 50) // Avg time/img: 0.1024 s\n",
            "loss: 1.031 (epoch: 7, step: 100) // Avg time/img: 0.1012 s\n",
            "loss: 1.051 (epoch: 7, step: 150) // Avg time/img: 0.1014 s\n",
            "loss: 1.044 (epoch: 7, step: 200) // Avg time/img: 0.1004 s\n",
            "loss: 1.041 (epoch: 7, step: 250) // Avg time/img: 0.0997 s\n",
            "loss: 1.038 (epoch: 7, step: 300) // Avg time/img: 0.1000 s\n",
            "loss: 1.033 (epoch: 7, step: 350) // Avg time/img: 0.1001 s\n",
            "loss: 1.03 (epoch: 7, step: 400) // Avg time/img: 0.1001 s\n",
            "loss: 1.025 (epoch: 7, step: 450) // Avg time/img: 0.1001 s\n",
            "----- VALIDATING - EPOCH 7 -----\n",
            "VAL loss: 0.8476 (epoch: 7, step: 0) // Avg time/img: 0.0535 s\n",
            "VAL loss: 1.097 (epoch: 7, step: 50) // Avg time/img: 0.0409 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m26.80\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model-007.pth (epoch: 7)\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model_best.pth (epoch: 7)\n",
            "----- TRAINING - EPOCH 8 -----\n",
            "LEARNING RATE:  3.393063796290625e-05\n",
            "loss: 1.027 (epoch: 8, step: 0) // Avg time/img: 0.1106 s\n",
            "loss: 0.9933 (epoch: 8, step: 50) // Avg time/img: 0.0983 s\n",
            "loss: 0.9976 (epoch: 8, step: 100) // Avg time/img: 0.0988 s\n",
            "loss: 0.9948 (epoch: 8, step: 150) // Avg time/img: 0.0992 s\n",
            "loss: 0.9849 (epoch: 8, step: 200) // Avg time/img: 0.0994 s\n",
            "loss: 0.9822 (epoch: 8, step: 250) // Avg time/img: 0.0996 s\n",
            "loss: 0.9702 (epoch: 8, step: 300) // Avg time/img: 0.0998 s\n",
            "loss: 0.9649 (epoch: 8, step: 350) // Avg time/img: 0.1012 s\n",
            "loss: 0.9641 (epoch: 8, step: 400) // Avg time/img: 0.1033 s\n",
            "loss: 0.964 (epoch: 8, step: 450) // Avg time/img: 0.1041 s\n",
            "----- VALIDATING - EPOCH 8 -----\n",
            "VAL loss: 0.8218 (epoch: 8, step: 0) // Avg time/img: 0.0525 s\n",
            "VAL loss: 1.061 (epoch: 8, step: 50) // Avg time/img: 0.0407 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m27.13\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model-008.pth (epoch: 8)\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model_best.pth (epoch: 8)\n",
            "----- TRAINING - EPOCH 9 -----\n",
            "LEARNING RATE:  3.157229337446777e-05\n",
            "loss: 0.7967 (epoch: 9, step: 0) // Avg time/img: 0.1161 s\n",
            "loss: 0.9008 (epoch: 9, step: 50) // Avg time/img: 0.1029 s\n",
            "loss: 0.9177 (epoch: 9, step: 100) // Avg time/img: 0.1011 s\n",
            "loss: 0.9207 (epoch: 9, step: 150) // Avg time/img: 0.1009 s\n",
            "loss: 0.9166 (epoch: 9, step: 200) // Avg time/img: 0.1007 s\n",
            "loss: 0.9207 (epoch: 9, step: 250) // Avg time/img: 0.1000 s\n",
            "loss: 0.9152 (epoch: 9, step: 300) // Avg time/img: 0.0998 s\n",
            "loss: 0.912 (epoch: 9, step: 350) // Avg time/img: 0.0997 s\n",
            "loss: 0.9099 (epoch: 9, step: 400) // Avg time/img: 0.0994 s\n",
            "loss: 0.9102 (epoch: 9, step: 450) // Avg time/img: 0.0997 s\n",
            "----- VALIDATING - EPOCH 9 -----\n",
            "VAL loss: 0.7639 (epoch: 9, step: 0) // Avg time/img: 0.0544 s\n",
            "VAL loss: 0.9982 (epoch: 9, step: 50) // Avg time/img: 0.0410 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m27.40\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model-009.pth (epoch: 9)\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model_best.pth (epoch: 9)\n",
            "----- TRAINING - EPOCH 10 -----\n",
            "LEARNING RATE:  2.9194189645999014e-05\n",
            "loss: 1.666 (epoch: 10, step: 0) // Avg time/img: 0.1341 s\n",
            "loss: 0.9275 (epoch: 10, step: 50) // Avg time/img: 0.0993 s\n",
            "loss: 0.8985 (epoch: 10, step: 100) // Avg time/img: 0.1009 s\n",
            "loss: 0.8856 (epoch: 10, step: 150) // Avg time/img: 0.1010 s\n",
            "loss: 0.8876 (epoch: 10, step: 200) // Avg time/img: 0.1005 s\n",
            "loss: 0.8839 (epoch: 10, step: 250) // Avg time/img: 0.1010 s\n",
            "loss: 0.8842 (epoch: 10, step: 300) // Avg time/img: 0.1004 s\n",
            "loss: 0.8797 (epoch: 10, step: 350) // Avg time/img: 0.1004 s\n",
            "loss: 0.8749 (epoch: 10, step: 400) // Avg time/img: 0.1006 s\n",
            "loss: 0.871 (epoch: 10, step: 450) // Avg time/img: 0.1005 s\n",
            "----- VALIDATING - EPOCH 10 -----\n",
            "VAL loss: 0.7415 (epoch: 10, step: 0) // Avg time/img: 0.0573 s\n",
            "VAL loss: 0.974 (epoch: 10, step: 50) // Avg time/img: 0.0408 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m28.07\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model-010.pth (epoch: 10)\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model_best.pth (epoch: 10)\n",
            "----- TRAINING - EPOCH 11 -----\n",
            "LEARNING RATE:  2.679433656340733e-05\n",
            "loss: 0.7937 (epoch: 11, step: 0) // Avg time/img: 0.1178 s\n",
            "loss: 0.8408 (epoch: 11, step: 50) // Avg time/img: 0.1008 s\n",
            "loss: 0.8463 (epoch: 11, step: 100) // Avg time/img: 0.0996 s\n",
            "loss: 0.8484 (epoch: 11, step: 150) // Avg time/img: 0.0997 s\n",
            "loss: 0.8498 (epoch: 11, step: 200) // Avg time/img: 0.1001 s\n",
            "loss: 0.8522 (epoch: 11, step: 250) // Avg time/img: 0.0999 s\n",
            "loss: 0.8483 (epoch: 11, step: 300) // Avg time/img: 0.0999 s\n",
            "loss: 0.842 (epoch: 11, step: 350) // Avg time/img: 0.1000 s\n",
            "loss: 0.8376 (epoch: 11, step: 400) // Avg time/img: 0.0998 s\n",
            "loss: 0.8368 (epoch: 11, step: 450) // Avg time/img: 0.0999 s\n",
            "----- VALIDATING - EPOCH 11 -----\n",
            "VAL loss: 0.7212 (epoch: 11, step: 0) // Avg time/img: 0.0587 s\n",
            "VAL loss: 0.9473 (epoch: 11, step: 50) // Avg time/img: 0.0415 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m28.53\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model-011.pth (epoch: 11)\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model_best.pth (epoch: 11)\n",
            "----- TRAINING - EPOCH 12 -----\n",
            "LEARNING RATE:  2.437032195894977e-05\n",
            "loss: 0.5979 (epoch: 12, step: 0) // Avg time/img: 0.1156 s\n",
            "loss: 0.8109 (epoch: 12, step: 50) // Avg time/img: 0.0997 s\n",
            "loss: 0.8085 (epoch: 12, step: 100) // Avg time/img: 0.1003 s\n",
            "loss: 0.821 (epoch: 12, step: 150) // Avg time/img: 0.1005 s\n",
            "loss: 0.8137 (epoch: 12, step: 200) // Avg time/img: 0.1000 s\n",
            "loss: 0.8134 (epoch: 12, step: 250) // Avg time/img: 0.1003 s\n",
            "loss: 0.8116 (epoch: 12, step: 300) // Avg time/img: 0.1001 s\n",
            "loss: 0.8096 (epoch: 12, step: 350) // Avg time/img: 0.0999 s\n",
            "loss: 0.8098 (epoch: 12, step: 400) // Avg time/img: 0.1001 s\n",
            "loss: 0.8084 (epoch: 12, step: 450) // Avg time/img: 0.1001 s\n",
            "----- VALIDATING - EPOCH 12 -----\n",
            "VAL loss: 0.6999 (epoch: 12, step: 0) // Avg time/img: 0.0562 s\n",
            "VAL loss: 0.9212 (epoch: 12, step: 50) // Avg time/img: 0.0416 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m28.96\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model-012.pth (epoch: 12)\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model_best.pth (epoch: 12)\n",
            "----- TRAINING - EPOCH 13 -----\n",
            "LEARNING RATE:  2.191916452770435e-05\n",
            "loss: 0.6637 (epoch: 13, step: 0) // Avg time/img: 0.1687 s\n",
            "loss: 0.7712 (epoch: 13, step: 50) // Avg time/img: 0.1013 s\n",
            "loss: 0.7714 (epoch: 13, step: 100) // Avg time/img: 0.1004 s\n",
            "loss: 0.7768 (epoch: 13, step: 150) // Avg time/img: 0.0998 s\n",
            "loss: 0.7749 (epoch: 13, step: 200) // Avg time/img: 0.1004 s\n",
            "loss: 0.7807 (epoch: 13, step: 250) // Avg time/img: 0.1004 s\n",
            "loss: 0.7808 (epoch: 13, step: 300) // Avg time/img: 0.1001 s\n",
            "loss: 0.7807 (epoch: 13, step: 350) // Avg time/img: 0.1004 s\n",
            "loss: 0.7838 (epoch: 13, step: 400) // Avg time/img: 0.1007 s\n",
            "loss: 0.7835 (epoch: 13, step: 450) // Avg time/img: 0.1003 s\n",
            "----- VALIDATING - EPOCH 13 -----\n",
            "VAL loss: 0.6575 (epoch: 13, step: 0) // Avg time/img: 0.0507 s\n",
            "VAL loss: 0.8829 (epoch: 13, step: 50) // Avg time/img: 0.0405 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m29.49\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model-013.pth (epoch: 13)\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model_best.pth (epoch: 13)\n",
            "----- TRAINING - EPOCH 14 -----\n",
            "LEARNING RATE:  1.9437089939938174e-05\n",
            "loss: 0.7319 (epoch: 14, step: 0) // Avg time/img: 0.1149 s\n",
            "loss: 0.7985 (epoch: 14, step: 50) // Avg time/img: 0.1012 s\n",
            "loss: 0.7976 (epoch: 14, step: 100) // Avg time/img: 0.0995 s\n",
            "loss: 0.7778 (epoch: 14, step: 150) // Avg time/img: 0.0994 s\n",
            "loss: 0.7706 (epoch: 14, step: 200) // Avg time/img: 0.0996 s\n",
            "loss: 0.7722 (epoch: 14, step: 250) // Avg time/img: 0.0993 s\n",
            "loss: 0.7691 (epoch: 14, step: 300) // Avg time/img: 0.0997 s\n",
            "loss: 0.7662 (epoch: 14, step: 350) // Avg time/img: 0.0996 s\n",
            "loss: 0.7652 (epoch: 14, step: 400) // Avg time/img: 0.0992 s\n",
            "loss: 0.7667 (epoch: 14, step: 450) // Avg time/img: 0.0994 s\n",
            "----- VALIDATING - EPOCH 14 -----\n",
            "VAL loss: 0.6366 (epoch: 14, step: 0) // Avg time/img: 0.0607 s\n",
            "VAL loss: 0.8532 (epoch: 14, step: 50) // Avg time/img: 0.0406 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m29.97\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model-014.pth (epoch: 14)\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model_best.pth (epoch: 14)\n",
            "----- TRAINING - EPOCH 15 -----\n",
            "LEARNING RATE:  1.6919173095082493e-05\n",
            "loss: 0.7383 (epoch: 15, step: 0) // Avg time/img: 0.1103 s\n",
            "loss: 0.7497 (epoch: 15, step: 50) // Avg time/img: 0.0998 s\n",
            "loss: 0.7387 (epoch: 15, step: 100) // Avg time/img: 0.0993 s\n",
            "loss: 0.742 (epoch: 15, step: 150) // Avg time/img: 0.0994 s\n",
            "loss: 0.7542 (epoch: 15, step: 200) // Avg time/img: 0.0988 s\n",
            "loss: 0.7496 (epoch: 15, step: 250) // Avg time/img: 0.0993 s\n",
            "loss: 0.7487 (epoch: 15, step: 300) // Avg time/img: 0.0997 s\n",
            "loss: 0.7498 (epoch: 15, step: 350) // Avg time/img: 0.0995 s\n",
            "loss: 0.752 (epoch: 15, step: 400) // Avg time/img: 0.0997 s\n",
            "loss: 0.7531 (epoch: 15, step: 450) // Avg time/img: 0.0997 s\n",
            "----- VALIDATING - EPOCH 15 -----\n",
            "VAL loss: 0.6428 (epoch: 15, step: 0) // Avg time/img: 0.0533 s\n",
            "VAL loss: 0.8524 (epoch: 15, step: 50) // Avg time/img: 0.0410 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m30.35\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model-015.pth (epoch: 15)\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model_best.pth (epoch: 15)\n",
            "----- TRAINING - EPOCH 16 -----\n",
            "LEARNING RATE:  1.4358729437462937e-05\n",
            "loss: 0.7049 (epoch: 16, step: 0) // Avg time/img: 0.1315 s\n",
            "loss: 0.7369 (epoch: 16, step: 50) // Avg time/img: 0.1003 s\n",
            "loss: 0.7455 (epoch: 16, step: 100) // Avg time/img: 0.0971 s\n",
            "loss: 0.7326 (epoch: 16, step: 150) // Avg time/img: 0.0974 s\n",
            "loss: 0.7375 (epoch: 16, step: 200) // Avg time/img: 0.0983 s\n",
            "loss: 0.7427 (epoch: 16, step: 250) // Avg time/img: 0.0980 s\n",
            "loss: 0.7392 (epoch: 16, step: 300) // Avg time/img: 0.0982 s\n",
            "loss: 0.7392 (epoch: 16, step: 350) // Avg time/img: 0.0986 s\n",
            "loss: 0.7409 (epoch: 16, step: 400) // Avg time/img: 0.0987 s\n",
            "loss: 0.7408 (epoch: 16, step: 450) // Avg time/img: 0.0992 s\n",
            "----- VALIDATING - EPOCH 16 -----\n",
            "VAL loss: 0.629 (epoch: 16, step: 0) // Avg time/img: 0.0535 s\n",
            "VAL loss: 0.8345 (epoch: 16, step: 50) // Avg time/img: 0.0408 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m30.52\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model-016.pth (epoch: 16)\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model_best.pth (epoch: 16)\n",
            "----- TRAINING - EPOCH 17 -----\n",
            "LEARNING RATE:  1.1746189430880188e-05\n",
            "loss: 0.7328 (epoch: 17, step: 0) // Avg time/img: 0.1164 s\n",
            "loss: 0.7492 (epoch: 17, step: 50) // Avg time/img: 0.1020 s\n",
            "loss: 0.7384 (epoch: 17, step: 100) // Avg time/img: 0.0997 s\n",
            "loss: 0.7329 (epoch: 17, step: 150) // Avg time/img: 0.1005 s\n",
            "loss: 0.7334 (epoch: 17, step: 200) // Avg time/img: 0.1004 s\n",
            "loss: 0.737 (epoch: 17, step: 250) // Avg time/img: 0.1002 s\n",
            "loss: 0.7322 (epoch: 17, step: 300) // Avg time/img: 0.1002 s\n",
            "loss: 0.7369 (epoch: 17, step: 350) // Avg time/img: 0.1001 s\n",
            "loss: 0.7341 (epoch: 17, step: 400) // Avg time/img: 0.1003 s\n",
            "loss: 0.7301 (epoch: 17, step: 450) // Avg time/img: 0.1004 s\n",
            "----- VALIDATING - EPOCH 17 -----\n",
            "VAL loss: 0.6156 (epoch: 17, step: 0) // Avg time/img: 0.0560 s\n",
            "VAL loss: 0.8224 (epoch: 17, step: 50) // Avg time/img: 0.0417 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m30.85\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model-017.pth (epoch: 17)\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model_best.pth (epoch: 17)\n",
            "----- TRAINING - EPOCH 18 -----\n",
            "LEARNING RATE:  9.066760365683729e-06\n",
            "loss: 0.84 (epoch: 18, step: 0) // Avg time/img: 0.1167 s\n",
            "loss: 0.7256 (epoch: 18, step: 50) // Avg time/img: 0.0985 s\n",
            "loss: 0.7218 (epoch: 18, step: 100) // Avg time/img: 0.1005 s\n",
            "loss: 0.7245 (epoch: 18, step: 150) // Avg time/img: 0.1008 s\n",
            "loss: 0.7244 (epoch: 18, step: 200) // Avg time/img: 0.1011 s\n",
            "loss: 0.7235 (epoch: 18, step: 250) // Avg time/img: 0.1013 s\n",
            "loss: 0.7226 (epoch: 18, step: 300) // Avg time/img: 0.1007 s\n",
            "loss: 0.7242 (epoch: 18, step: 350) // Avg time/img: 0.1005 s\n",
            "loss: 0.7252 (epoch: 18, step: 400) // Avg time/img: 0.1005 s\n",
            "loss: 0.7204 (epoch: 18, step: 450) // Avg time/img: 0.1002 s\n",
            "----- VALIDATING - EPOCH 18 -----\n",
            "VAL loss: 0.6384 (epoch: 18, step: 0) // Avg time/img: 0.0602 s\n",
            "VAL loss: 0.8405 (epoch: 18, step: 50) // Avg time/img: 0.0417 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m31.23\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model-018.pth (epoch: 18)\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model_best.pth (epoch: 18)\n",
            "----- TRAINING - EPOCH 19 -----\n",
            "LEARNING RATE:  6.294627058970836e-06\n",
            "loss: 0.6308 (epoch: 19, step: 0) // Avg time/img: 0.1128 s\n",
            "loss: 0.7237 (epoch: 19, step: 50) // Avg time/img: 0.1001 s\n",
            "loss: 0.7241 (epoch: 19, step: 100) // Avg time/img: 0.1001 s\n",
            "loss: 0.7278 (epoch: 19, step: 150) // Avg time/img: 0.1007 s\n",
            "loss: 0.723 (epoch: 19, step: 200) // Avg time/img: 0.1005 s\n",
            "loss: 0.7205 (epoch: 19, step: 250) // Avg time/img: 0.1002 s\n",
            "loss: 0.7196 (epoch: 19, step: 300) // Avg time/img: 0.1000 s\n",
            "loss: 0.7187 (epoch: 19, step: 350) // Avg time/img: 0.0998 s\n",
            "loss: 0.7167 (epoch: 19, step: 400) // Avg time/img: 0.0995 s\n",
            "loss: 0.7187 (epoch: 19, step: 450) // Avg time/img: 0.0994 s\n",
            "----- VALIDATING - EPOCH 19 -----\n",
            "VAL loss: 0.6233 (epoch: 19, step: 0) // Avg time/img: 0.0624 s\n",
            "VAL loss: 0.8262 (epoch: 19, step: 50) // Avg time/img: 0.0413 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m31.34\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model-019.pth (epoch: 19)\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model_best.pth (epoch: 19)\n",
            "----- TRAINING - EPOCH 20 -----\n",
            "LEARNING RATE:  3.373207119183911e-06\n",
            "loss: 0.6882 (epoch: 20, step: 0) // Avg time/img: 0.1174 s\n",
            "loss: 0.7242 (epoch: 20, step: 50) // Avg time/img: 0.1009 s\n",
            "loss: 0.7215 (epoch: 20, step: 100) // Avg time/img: 0.0983 s\n",
            "loss: 0.7149 (epoch: 20, step: 150) // Avg time/img: 0.0985 s\n",
            "loss: 0.7152 (epoch: 20, step: 200) // Avg time/img: 0.0986 s\n",
            "loss: 0.7144 (epoch: 20, step: 250) // Avg time/img: 0.0988 s\n",
            "loss: 0.7153 (epoch: 20, step: 300) // Avg time/img: 0.0988 s\n",
            "loss: 0.7155 (epoch: 20, step: 350) // Avg time/img: 0.0991 s\n",
            "loss: 0.7154 (epoch: 20, step: 400) // Avg time/img: 0.0993 s\n",
            "loss: 0.7133 (epoch: 20, step: 450) // Avg time/img: 0.0993 s\n",
            "----- VALIDATING - EPOCH 20 -----\n",
            "VAL loss: 0.6154 (epoch: 20, step: 0) // Avg time/img: 0.0518 s\n",
            "VAL loss: 0.8187 (epoch: 20, step: 50) // Avg time/img: 0.0415 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m31.25\u001b[0m %\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model-020.pth (epoch: 20)\n",
            "========== TRAINING FINISHED ===========\n",
            "Model saved in /content/AnomalySegmentation/save/erfnet_training_isomaxplus_loss\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_isomaxplus_loss/ (stored 0%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_isomaxplus_loss/model_best.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_isomaxplus_loss/model-007.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_isomaxplus_loss/model-015.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_isomaxplus_loss/model-008.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_isomaxplus_loss/model-004.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_isomaxplus_loss/model-005.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_isomaxplus_loss/model-002.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_isomaxplus_loss/opts.txt (deflated 38%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_isomaxplus_loss/model.txt (deflated 92%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_isomaxplus_loss/model-014.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_isomaxplus_loss/model-013.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_isomaxplus_loss/model-012.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_isomaxplus_loss/model-016.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_isomaxplus_loss/model-011.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_isomaxplus_loss/checkpoint.pth.tar (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_isomaxplus_loss/model-010.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_isomaxplus_loss/model-020.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_isomaxplus_loss/model_best.pth.tar (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_isomaxplus_loss/model-009.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_isomaxplus_loss/model-003.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_isomaxplus_loss/model-006.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_isomaxplus_loss/automated_log.txt (deflated 63%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_isomaxplus_loss/model-019.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_isomaxplus_loss/model-017.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_isomaxplus_loss/model-018.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_isomaxplus_loss/best.txt (stored 0%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_isomaxplus_loss/model-001.pth (deflated 10%)\n"
          ]
        }
      ],
      "source": [
        "# Fine tune ERFNET with different losses\n",
        "\n",
        "losses = [\"Focal\", \"LogitNorm\", \"IsoMaxPlus\"]\n",
        "models = [\"erfnet\", \"erfnet\", \"erfnet_isomaxplus\"]\n",
        "savedirs = [\"erfnet_training_focal_loss\", \"erfnet_training_logitnorm_loss\", \"erfnet_training_isomaxplus_loss\"]\n",
        "epochs = 20\n",
        "\n",
        "# Base directory of the project\n",
        "base_dir = \"/content/AnomalySegmentation/train\"\n",
        "# Dataset directory\n",
        "data_dir = \"/content/cityscapes\"\n",
        "pretrained_weights = \"erfnet_pretrained.pth\"\n",
        "\n",
        "# Loop to execute fine-tuning\n",
        "for loss, model, savedir in zip(losses, models, savedirs):\n",
        "    print(f\"\\n\\n----- Fine-tuning with {loss} loss -----\")\n",
        "    !cd {base_dir} && python -W ignore main.py --savedir {savedir} --loss {loss} --datadir {data_dir} --model {model} --cuda --num-epochs=20 --epochs-save=1 --FineTune --decoder --loadWeights={pretrained_weights}\n",
        "    print(f\"Model saved in /content/AnomalySegmentation/save/{savedir}\")\n",
        "    # zip folder\n",
        "    !zip -r save_{savedir}.zip /content/AnomalySegmentation/save/{savedir}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluation:**"
      ],
      "metadata": {
        "id": "3-pAOTAnYE3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "no_execute = False\n",
        "just_once = False\n",
        "\n",
        "losses = [\"CrossEntropy\", \"Focal\", \"LogitNorm\", \"IsoMaxPlus\"]\n",
        "models = [\"erfnet\", \"erfnet\", \"erfnet\", \"erfnet_isomaxplus\"]\n",
        "load_dirs = [\"/content/AnomalySegmentation/trained_models/\", \"/content/AnomalySegmentation/save/erfnet_training_focal_loss/\", \"/content/AnomalySegmentation/save/erfnet_training_logitnorm_loss/\", \"/content/AnomalySegmentation/save/erfnet_training_isomaxplus_loss/\"]\n",
        "weights = [\"erfnet_pretrained.pth\", \"model_best.pth\", \"model_best.pth\", \"model_best.pth\"]\n",
        "\n",
        "for loss, model, load_dir, weight in zip(losses, models, load_dirs, weights):\n",
        "  print(f\"------ Loss function: {loss} ------\\n\")\n",
        "  for dataset_dir in ['RoadAnomaly21', 'RoadObsticle21', 'FS_LostFound_full', 'fs_static', 'RoadAnomaly']:\n",
        "    for method in [\"MSP\", \"MaxLogit\", \"MaxEntropy\"]:\n",
        "\n",
        "      if no_execute:\n",
        "        break\n",
        "\n",
        "      format_file = os.listdir(f'/content/Validation_Dataset/{dataset_dir}/images')[0].split(\".\")[1]\n",
        "      input =f'/content/Validation_Dataset/{dataset_dir}/images/\\*.{format_file}'\n",
        "\n",
        "      print(f\"\\nDataset: {dataset_dir} method: {method} loss: {loss}\")\n",
        "\n",
        "      if torch.cuda.is_available():\n",
        "        !python -W ignore /content/AnomalySegmentation/eval/evalAnomaly.py --input {input} --method  {method} --model {model} --loadDir {load_dir} --loadWeights {weight} | tail -n 2\n",
        "      else:\n",
        "        !python -W ignore /content/AnomalySegmentation/eval/evalAnomaly.py --input {input} --method {method}  --model {model} --loadDir {load_dir} --loadWeights {weight} --cpu | tail -n 2\n",
        "\n",
        "      print(\"----------------------------\")\n",
        "      if just_once:\n",
        "        no_execute = True\n",
        "        just_once = False\n",
        "    print(\"----------------------------\\n\\n\")"
      ],
      "metadata": {
        "id": "d1P6xBfcB9UO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d71f9b92-ba7b-4a38-e9ae-2ea45d011625"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------ Loss function: CrossEntropy ------\n",
            "\n",
            "\n",
            "Dataset: RoadAnomaly21 method: MSP loss: CrossEntropy\n",
            "100% 10/10 [00:02<00:00,  4.46it/s]\n",
            "AUPRC score: 29.100168300581203\n",
            "FPR@TPR95: 62.51075321069286\n",
            "----------------------------\n",
            "\n",
            "Dataset: RoadAnomaly21 method: MaxLogit loss: CrossEntropy\n",
            "100% 10/10 [00:02<00:00,  4.52it/s]\n",
            "AUPRC score: 38.31957797222208\n",
            "FPR@TPR95: 59.3370558914899\n",
            "----------------------------\n",
            "\n",
            "Dataset: RoadAnomaly21 method: MaxEntropy loss: CrossEntropy\n",
            "100% 10/10 [00:01<00:00,  5.01it/s]\n",
            "AUPRC score: 31.005102648344756\n",
            "FPR@TPR95: 62.593151130093226\n",
            "----------------------------\n",
            "----------------------------\n",
            "\n",
            "\n",
            "\n",
            "Dataset: RoadObsticle21 method: MSP loss: CrossEntropy\n",
            "100% 30/30 [00:06<00:00,  4.74it/s]\n",
            "AUPRC score: 2.7116243119338366\n",
            "FPR@TPR95: 64.9739786894368\n",
            "----------------------------\n",
            "\n",
            "Dataset: RoadObsticle21 method: MaxLogit loss: CrossEntropy\n",
            "100% 30/30 [00:06<00:00,  4.33it/s]\n",
            "AUPRC score: 4.626567617520253\n",
            "FPR@TPR95: 48.443439151949555\n",
            "----------------------------\n",
            "\n",
            "Dataset: RoadObsticle21 method: MaxEntropy loss: CrossEntropy\n",
            "100% 30/30 [00:07<00:00,  4.07it/s]\n",
            "AUPRC score: 3.051560023478638\n",
            "FPR@TPR95: 65.59968252759046\n",
            "----------------------------\n",
            "----------------------------\n",
            "\n",
            "\n",
            "\n",
            "Dataset: FS_LostFound_full method: MSP loss: CrossEntropy\n",
            "100% 100/100 [00:21<00:00,  4.74it/s]\n",
            "AUPRC score: 1.747872547607269\n",
            "FPR@TPR95: 50.76348570192957\n",
            "----------------------------\n",
            "\n",
            "Dataset: FS_LostFound_full method: MaxLogit loss: CrossEntropy\n",
            "100% 100/100 [00:21<00:00,  4.75it/s]\n",
            "AUPRC score: 3.3014401015087245\n",
            "FPR@TPR95: 45.494876929038305\n",
            "----------------------------\n",
            "\n",
            "Dataset: FS_LostFound_full method: MaxEntropy loss: CrossEntropy\n",
            "100% 100/100 [00:20<00:00,  4.92it/s]\n",
            "AUPRC score: 2.581709137723009\n",
            "FPR@TPR95: 50.368099783135676\n",
            "----------------------------\n",
            "----------------------------\n",
            "\n",
            "\n",
            "\n",
            "Dataset: fs_static method: MSP loss: CrossEntropy\n",
            "100% 30/30 [00:03<00:00,  7.70it/s]\n",
            "AUPRC score: 7.4700433549050915\n",
            "FPR@TPR95: 41.82346831776172\n",
            "----------------------------\n",
            "\n",
            "Dataset: fs_static method: MaxLogit loss: CrossEntropy\n",
            "100% 30/30 [00:03<00:00,  7.85it/s]\n",
            "AUPRC score: 9.498677970785756\n",
            "FPR@TPR95: 40.3000747567442\n",
            "----------------------------\n",
            "\n",
            "Dataset: fs_static method: MaxEntropy loss: CrossEntropy\n",
            "100% 30/30 [00:04<00:00,  6.24it/s]\n",
            "AUPRC score: 8.82636607633996\n",
            "FPR@TPR95: 41.52332673090571\n",
            "----------------------------\n",
            "----------------------------\n",
            "\n",
            "\n",
            "\n",
            "Dataset: RoadAnomaly method: MSP loss: CrossEntropy\n",
            "100% 60/60 [00:06<00:00,  9.81it/s]\n",
            "AUPRC score: 12.426265849563665\n",
            "FPR@TPR95: 82.49244029880458\n",
            "----------------------------\n",
            "\n",
            "Dataset: RoadAnomaly method: MaxLogit loss: CrossEntropy\n",
            "100% 60/60 [00:06<00:00,  9.40it/s]\n",
            "AUPRC score: 15.581983301641019\n",
            "FPR@TPR95: 73.24766535735604\n",
            "----------------------------\n",
            "\n",
            "Dataset: RoadAnomaly method: MaxEntropy loss: CrossEntropy\n",
            "100% 60/60 [00:06<00:00,  8.78it/s]\n",
            "AUPRC score: 12.678035094227063\n",
            "FPR@TPR95: 82.63192451735861\n",
            "----------------------------\n",
            "----------------------------\n",
            "\n",
            "\n",
            "------ Loss function: Focal ------\n",
            "\n",
            "\n",
            "Dataset: RoadAnomaly21 method: MSP loss: Focal\n",
            "100% 10/10 [00:02<00:00,  4.55it/s]\n",
            "AUPRC score: 27.937995777997788\n",
            "FPR@TPR95: 66.24231390243952\n",
            "----------------------------\n",
            "\n",
            "Dataset: RoadAnomaly21 method: MaxLogit loss: Focal\n",
            "100% 10/10 [00:02<00:00,  4.19it/s]\n",
            "AUPRC score: 39.12821037915182\n",
            "FPR@TPR95: 60.49840496586444\n",
            "----------------------------\n",
            "\n",
            "Dataset: RoadAnomaly21 method: MaxEntropy loss: Focal\n",
            "100% 10/10 [00:02<00:00,  4.93it/s]\n",
            "AUPRC score: 29.58065930787916\n",
            "FPR@TPR95: 66.57811422499526\n",
            "----------------------------\n",
            "----------------------------\n",
            "\n",
            "\n",
            "\n",
            "Dataset: RoadObsticle21 method: MSP loss: Focal\n",
            "100% 30/30 [00:06<00:00,  4.38it/s]\n",
            "AUPRC score: 2.9360412943475076\n",
            "FPR@TPR95: 64.90006533274155\n",
            "----------------------------\n",
            "\n",
            "Dataset: RoadObsticle21 method: MaxLogit loss: Focal\n",
            "100% 30/30 [00:07<00:00,  4.27it/s]\n",
            "AUPRC score: 6.373602388012283\n",
            "FPR@TPR95: 33.73131930735701\n",
            "----------------------------\n",
            "\n",
            "Dataset: RoadObsticle21 method: MaxEntropy loss: Focal\n",
            "100% 30/30 [00:07<00:00,  4.16it/s]\n",
            "AUPRC score: 3.232027767913617\n",
            "FPR@TPR95: 65.6167402522556\n",
            "----------------------------\n",
            "----------------------------\n",
            "\n",
            "\n",
            "\n",
            "Dataset: FS_LostFound_full method: MSP loss: Focal\n",
            "100% 100/100 [00:20<00:00,  4.97it/s]\n",
            "AUPRC score: 2.095500327723797\n",
            "FPR@TPR95: 47.65697634353836\n",
            "----------------------------\n",
            "\n",
            "Dataset: FS_LostFound_full method: MaxLogit loss: Focal\n",
            "100% 100/100 [00:21<00:00,  4.61it/s]\n",
            "AUPRC score: 3.5350621345853024\n",
            "FPR@TPR95: 47.3187485357515\n",
            "----------------------------\n",
            "\n",
            "Dataset: FS_LostFound_full method: MaxEntropy loss: Focal\n",
            "100% 100/100 [00:21<00:00,  4.61it/s]\n",
            "AUPRC score: 3.4064239080417877\n",
            "FPR@TPR95: 47.228195585652614\n",
            "----------------------------\n",
            "----------------------------\n",
            "\n",
            "\n",
            "\n",
            "Dataset: fs_static method: MSP loss: Focal\n",
            "100% 30/30 [00:03<00:00,  7.83it/s]\n",
            "AUPRC score: 7.698419438402603\n",
            "FPR@TPR95: 41.84235566962261\n",
            "----------------------------\n",
            "\n",
            "Dataset: fs_static method: MaxLogit loss: Focal\n",
            "100% 30/30 [00:04<00:00,  7.19it/s]\n",
            "AUPRC score: 8.88649571379485\n",
            "FPR@TPR95: 36.76813240404997\n",
            "----------------------------\n",
            "\n",
            "Dataset: fs_static method: MaxEntropy loss: Focal\n",
            "100% 30/30 [00:04<00:00,  6.61it/s]\n",
            "AUPRC score: 9.332818001817209\n",
            "FPR@TPR95: 41.452254359822156\n",
            "----------------------------\n",
            "----------------------------\n",
            "\n",
            "\n",
            "\n",
            "Dataset: RoadAnomaly method: MSP loss: Focal\n",
            "100% 60/60 [00:06<00:00,  9.67it/s]\n",
            "AUPRC score: 12.23426577837281\n",
            "FPR@TPR95: 82.50050371078282\n",
            "----------------------------\n",
            "\n",
            "Dataset: RoadAnomaly method: MaxLogit loss: Focal\n",
            "100% 60/60 [00:06<00:00,  9.70it/s]\n",
            "AUPRC score: 18.19446373026864\n",
            "FPR@TPR95: 70.8915649327422\n",
            "----------------------------\n",
            "\n",
            "Dataset: RoadAnomaly method: MaxEntropy loss: Focal\n",
            "100% 60/60 [00:06<00:00,  8.98it/s]\n",
            "AUPRC score: 12.574046445841569\n",
            "FPR@TPR95: 82.77112927759228\n",
            "----------------------------\n",
            "----------------------------\n",
            "\n",
            "\n",
            "------ Loss function: LogitNorm ------\n",
            "\n",
            "\n",
            "Dataset: RoadAnomaly21 method: MSP loss: LogitNorm\n",
            "100% 10/10 [00:01<00:00,  5.12it/s]\n",
            "AUPRC score: 27.332152961840183\n",
            "FPR@TPR95: 63.315658411333786\n",
            "----------------------------\n",
            "\n",
            "Dataset: RoadAnomaly21 method: MaxLogit loss: LogitNorm\n",
            "100% 10/10 [00:01<00:00,  5.11it/s]\n",
            "AUPRC score: 34.09528575789504\n",
            "FPR@TPR95: 57.82566449704293\n",
            "----------------------------\n",
            "\n",
            "Dataset: RoadAnomaly21 method: MaxEntropy loss: LogitNorm\n",
            "100% 10/10 [00:02<00:00,  4.81it/s]\n",
            "AUPRC score: 28.31452436027206\n",
            "FPR@TPR95: 63.196289286979265\n",
            "----------------------------\n",
            "----------------------------\n",
            "\n",
            "\n",
            "\n",
            "Dataset: RoadObsticle21 method: MSP loss: LogitNorm\n",
            "100% 30/30 [00:06<00:00,  4.83it/s]\n",
            "AUPRC score: 2.0877646994987082\n",
            "FPR@TPR95: 86.40865754050795\n",
            "----------------------------\n",
            "\n",
            "Dataset: RoadObsticle21 method: MaxLogit loss: LogitNorm\n",
            "100% 30/30 [00:06<00:00,  4.83it/s]\n",
            "AUPRC score: 3.257223864945248\n",
            "FPR@TPR95: 79.68902774281588\n",
            "----------------------------\n",
            "\n",
            "Dataset: RoadObsticle21 method: MaxEntropy loss: LogitNorm\n",
            "100% 30/30 [00:07<00:00,  4.22it/s]\n",
            "AUPRC score: 2.2396182295331934\n",
            "FPR@TPR95: 87.34869549808556\n",
            "----------------------------\n",
            "----------------------------\n",
            "\n",
            "\n",
            "\n",
            "Dataset: FS_LostFound_full method: MSP loss: LogitNorm\n",
            "100% 100/100 [00:21<00:00,  4.65it/s]\n",
            "AUPRC score: 1.9688425574155364\n",
            "FPR@TPR95: 46.741959474018415\n",
            "----------------------------\n",
            "\n",
            "Dataset: FS_LostFound_full method: MaxLogit loss: LogitNorm\n",
            "100% 100/100 [00:20<00:00,  4.79it/s]\n",
            "AUPRC score: 3.856449115187795\n",
            "FPR@TPR95: 40.27128069246505\n",
            "----------------------------\n",
            "\n",
            "Dataset: FS_LostFound_full method: MaxEntropy loss: LogitNorm\n",
            "100% 100/100 [00:21<00:00,  4.68it/s]\n",
            "AUPRC score: 3.010813589041711\n",
            "FPR@TPR95: 46.40480624000078\n",
            "----------------------------\n",
            "----------------------------\n",
            "\n",
            "\n",
            "\n",
            "Dataset: fs_static method: MSP loss: LogitNorm\n",
            "100% 30/30 [00:03<00:00,  7.94it/s]\n",
            "AUPRC score: 7.619169841720732\n",
            "FPR@TPR95: 37.5393652603829\n",
            "----------------------------\n",
            "\n",
            "Dataset: fs_static method: MaxLogit loss: LogitNorm\n",
            "100% 30/30 [00:04<00:00,  7.48it/s]\n",
            "AUPRC score: 9.633308541295532\n",
            "FPR@TPR95: 34.6772599561176\n",
            "----------------------------\n",
            "\n",
            "Dataset: fs_static method: MaxEntropy loss: LogitNorm\n",
            "100% 30/30 [00:04<00:00,  6.48it/s]\n",
            "AUPRC score: 9.030685037910331\n",
            "FPR@TPR95: 37.17384871475816\n",
            "----------------------------\n",
            "----------------------------\n",
            "\n",
            "\n",
            "\n",
            "Dataset: RoadAnomaly method: MSP loss: LogitNorm\n",
            "100% 60/60 [00:06<00:00,  9.67it/s]\n",
            "AUPRC score: 12.583326714049702\n",
            "FPR@TPR95: 80.56408592823482\n",
            "----------------------------\n",
            "\n",
            "Dataset: RoadAnomaly method: MaxLogit loss: LogitNorm\n",
            "100% 60/60 [00:06<00:00,  9.92it/s]\n",
            "AUPRC score: 14.745581186284248\n",
            "FPR@TPR95: 73.84179537855853\n",
            "----------------------------\n",
            "\n",
            "Dataset: RoadAnomaly method: MaxEntropy loss: LogitNorm\n",
            "100% 60/60 [00:06<00:00,  9.19it/s]\n",
            "AUPRC score: 12.651711939301363\n",
            "FPR@TPR95: 80.79110068414394\n",
            "----------------------------\n",
            "----------------------------\n",
            "\n",
            "\n",
            "------ Loss function: IsoMaxPlus ------\n",
            "\n",
            "\n",
            "Dataset: RoadAnomaly21 method: MSP loss: IsoMaxPlus\n",
            "100% 10/10 [00:02<00:00,  4.37it/s]\n",
            "AUPRC score: 37.064023438269814\n",
            "FPR@TPR95: 52.953529916197205\n",
            "----------------------------\n",
            "\n",
            "Dataset: RoadAnomaly21 method: MaxLogit loss: IsoMaxPlus\n",
            "100% 10/10 [00:02<00:00,  4.59it/s]\n",
            "AUPRC score: 41.049003841917184\n",
            "FPR@TPR95: 50.92840891140805\n",
            "----------------------------\n",
            "\n",
            "Dataset: RoadAnomaly21 method: MaxEntropy loss: IsoMaxPlus\n",
            "100% 10/10 [00:02<00:00,  3.95it/s]\n",
            "AUPRC score: 34.4714827458162\n",
            "FPR@TPR95: 49.91406400347497\n",
            "----------------------------\n",
            "----------------------------\n",
            "\n",
            "\n",
            "\n",
            "Dataset: RoadObsticle21 method: MSP loss: IsoMaxPlus\n",
            "100% 30/30 [00:07<00:00,  3.93it/s]\n",
            "AUPRC score: 3.8102045585888407\n",
            "FPR@TPR95: 26.683276557055606\n",
            "----------------------------\n",
            "\n",
            "Dataset: RoadObsticle21 method: MaxLogit loss: IsoMaxPlus\n",
            "100% 30/30 [00:06<00:00,  4.40it/s]\n",
            "AUPRC score: 6.555233349966855\n",
            "FPR@TPR95: 25.485896739490453\n",
            "----------------------------\n",
            "\n",
            "Dataset: RoadObsticle21 method: MaxEntropy loss: IsoMaxPlus\n",
            "100% 30/30 [00:06<00:00,  4.50it/s]\n",
            "AUPRC score: 20.40353598202807\n",
            "FPR@TPR95: 15.397157239099615\n",
            "----------------------------\n",
            "----------------------------\n",
            "\n",
            "\n",
            "\n",
            "Dataset: FS_LostFound_full method: MSP loss: IsoMaxPlus\n",
            "100% 100/100 [00:22<00:00,  4.42it/s]\n",
            "AUPRC score: 0.910867287577762\n",
            "FPR@TPR95: 58.682886127166555\n",
            "----------------------------\n",
            "\n",
            "Dataset: FS_LostFound_full method: MaxLogit loss: IsoMaxPlus\n",
            "100% 100/100 [00:21<00:00,  4.70it/s]\n",
            "AUPRC score: 0.797544840028386\n",
            "FPR@TPR95: 74.14919605188501\n",
            "----------------------------\n",
            "\n",
            "Dataset: FS_LostFound_full method: MaxEntropy loss: IsoMaxPlus\n",
            "100% 100/100 [00:22<00:00,  4.51it/s]\n",
            "AUPRC score: 1.0006256759715986\n",
            "FPR@TPR95: 60.019589347428045\n",
            "----------------------------\n",
            "----------------------------\n",
            "\n",
            "\n",
            "\n",
            "Dataset: fs_static method: MSP loss: IsoMaxPlus\n",
            "100% 30/30 [00:04<00:00,  6.10it/s]\n",
            "AUPRC score: 12.209707704464567\n",
            "FPR@TPR95: 36.75349370390611\n",
            "----------------------------\n",
            "\n",
            "Dataset: fs_static method: MaxLogit loss: IsoMaxPlus\n",
            "100% 30/30 [00:04<00:00,  6.31it/s]\n",
            "AUPRC score: 14.980042792907309\n",
            "FPR@TPR95: 34.02821753282753\n",
            "----------------------------\n",
            "\n",
            "Dataset: fs_static method: MaxEntropy loss: IsoMaxPlus\n",
            "100% 30/30 [00:04<00:00,  6.76it/s]\n",
            "AUPRC score: 11.730740739501897\n",
            "FPR@TPR95: 52.72669761850025\n",
            "----------------------------\n",
            "----------------------------\n",
            "\n",
            "\n",
            "\n",
            "Dataset: RoadAnomaly method: MSP loss: IsoMaxPlus\n",
            "100% 60/60 [00:07<00:00,  7.67it/s]\n",
            "AUPRC score: 19.659261196342044\n",
            "FPR@TPR95: 65.20204842606269\n",
            "----------------------------\n",
            "\n",
            "Dataset: RoadAnomaly method: MaxLogit loss: IsoMaxPlus\n",
            "100% 60/60 [00:07<00:00,  8.19it/s]\n",
            "AUPRC score: 19.348776889119453\n",
            "FPR@TPR95: 63.75743680723795\n",
            "----------------------------\n",
            "\n",
            "Dataset: RoadAnomaly method: MaxEntropy loss: IsoMaxPlus\n",
            "100% 60/60 [00:07<00:00,  7.92it/s]\n",
            "AUPRC score: 15.648008853249207\n",
            "FPR@TPR95: 75.12572233744925\n",
            "----------------------------\n",
            "----------------------------\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization"
      ],
      "metadata": {
        "id": "GbnGWNC9I1dw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Example image to color\n",
        "# Nice images: RoadAnomaly/images/28, RoadAnomaly/images/58\n",
        "input = '/content/Validation_Dataset/RoadAnomaly/images/58.jpg'\n",
        "\n",
        "### Baseline models ###\n",
        "for method in [\"MSP\", \"MaxLogit\", \"MaxEntropy\"]:\n",
        "  print(f\"Method: {method}\")\n",
        "  save_images_dir = f'/content/AnomalySegmentation/visualization/baseline_{method}'\n",
        "\n",
        "  if torch.cuda.is_available():\n",
        "    !python -W ignore /content/AnomalySegmentation/eval/evalAnomaly.py --input {input} --method  {method} --save-colored-dir {save_images_dir}  | tail -n 2\n",
        "  else:\n",
        "    !python -W ignore /content/AnomalySegmentation/eval/evalAnomaly.py --input {input} --method {method} --save-colored-dir {save_images_dir} --cpu | tail -n 2\n",
        "\n",
        "### Temperature scaling ###\n",
        "for t in [0.5, 0.75, 1.1]:\n",
        "  print(f\"Method: MSP, Temperature: {t}\")\n",
        "  save_images_dir = f'/content/AnomalySegmentation/visualization/temperature_{t}'\n",
        "\n",
        "  if torch.cuda.is_available():\n",
        "    !python -W ignore /content/AnomalySegmentation/eval/evalAnomaly.py --input {input} --method 'MSP' --temperature {t} --save-colored-dir {save_images_dir} | tail -n 2\n",
        "  else:\n",
        "    !python -W ignore /content/AnomalySegmentation/eval/evalAnomaly.py --input {input} --method 'MSP' --cpu --temperature {t} --save-colored-dir {save_images_dir} | tail -n 2\n",
        "\n",
        "### Finetuned models with void ###\n",
        "for net in [\"erfnet\", \"enet\", \"bisenet\"]:\n",
        "  save_images_dir = f'/content/AnomalySegmentation/visualization/void_{net}'\n",
        "  load_dir = f'/content/AnomalySegmentation/save/{net}_training_void'\n",
        "  weights = f'/model_best.pth'\n",
        "  print(f\"Finetuned network: {net}\")\n",
        "  if torch.cuda.is_available():\n",
        "    !python -W ignore /content/AnomalySegmentation/eval/evalAnomaly.py --input {input} --void --model {net} --loadDir {load_dir} --loadWeights {weights} --save-colored-dir {save_images_dir} | tail -n 2\n",
        "  else:\n",
        "    !python -W ignore /content/AnomalySegmentation/eval/evalAnomaly.py --input {input} --void --model {net} --loadDir {load_dir} --loadWeights {weights} --cpu --save-colored-dir {save_images_dir} | tail -n 2\n",
        "\n",
        "### Losses ###\n",
        "losses = [\"CrossEntropy\", \"Focal\", \"LogitNorm\", \"IsoMaxPlus\"]\n",
        "models = [\"erfnet\", \"erfnet\", \"erfnet\", \"erfnet_isomaxplus\"]\n",
        "load_dirs = [\"/content/AnomalySegmentation/trained_models/\", \"/content/AnomalySegmentation/save/erfnet_training_focal_loss/\", \"/content/AnomalySegmentation/save/erfnet_training_logitnorm_loss/\", \"/content/AnomalySegmentation/save/erfnet_training_isomaxplus_loss/\"]\n",
        "weights = [\"erfnet_pretrained.pth\", \"model_best.pth\", \"model_best.pth\", \"model_best.pth\"]\n",
        "for loss, model, load_dir, weight in zip(losses, models, load_dirs, weights):\n",
        "  for method in [\"MSP\", \"MaxLogit\", \"MaxEntropy\"]:\n",
        "    save_images_dir = f'/content/AnomalySegmentation/visualization/loss_{loss}_{method}'\n",
        "    print(f\"Method: {method}, loss: {loss}\")\n",
        "    if torch.cuda.is_available():\n",
        "      !python -W ignore /content/AnomalySegmentation/eval/evalAnomaly.py --input {input} --method  {method} --model {model} --loadDir {load_dir} --loadWeights {weight} --save-colored-dir {save_images_dir} | tail -n 2\n",
        "    else:\n",
        "      !python -W ignore /content/AnomalySegmentation/eval/evalAnomaly.py --input {input} --method {method}  --model {model} --loadDir {load_dir} --loadWeights {weight} --save-colored-dir {save_images_dir} --cpu | tail -n 2\n",
        "\n",
        "# Zip the images\n",
        "!zip -r colored_anomalies.zip /content/AnomalySegmentation/visualization"
      ],
      "metadata": {
        "collapsed": true,
        "id": "wg0oy_3-JB6V",
        "outputId": "f1dc77a6-54c3-4b8f-a27d-41828d6ef6b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Method: MSP\n",
            "100% 1/1 [00:01<00:00,  1.84s/it]\n",
            "AUPRC score: 45.49714388457901\n",
            "FPR@TPR95: 33.597166197756\n",
            "Method: MaxLogit\n",
            "100% 1/1 [00:01<00:00,  1.66s/it]\n",
            "AUPRC score: 70.57980993958311\n",
            "FPR@TPR95: 24.507008237008915\n",
            "Method: MaxEntropy\n",
            "100% 1/1 [00:01<00:00,  1.78s/it]\n",
            "AUPRC score: 57.50733388962147\n",
            "FPR@TPR95: 33.14569845089997\n",
            "Method: MSP, Temperature: 0.5\n",
            "100% 1/1 [00:01<00:00,  1.97s/it]\n",
            "AUPRC score: 37.3583766623776\n",
            "FPR@TPR95: 34.936574654719806\n",
            "Method: MSP, Temperature: 0.75\n",
            "100% 1/1 [00:01<00:00,  1.66s/it]\n",
            "AUPRC score: 40.98643506831958\n",
            "FPR@TPR95: 34.28464289346124\n",
            "Method: MSP, Temperature: 1.1\n",
            "100% 1/1 [00:01<00:00,  1.77s/it]\n",
            "AUPRC score: 47.32243727827538\n",
            "FPR@TPR95: 33.33954781193858\n",
            "Finetuned network: erfnet\n",
            "100% 1/1 [00:01<00:00,  1.70s/it]\n",
            "AUPRC score: 9.844441669253467\n",
            "FPR@TPR95: 94.27858377682112\n",
            "Finetuned network: enet\n",
            "100% 1/1 [00:00<00:00,  1.12it/s]\n",
            "AUPRC score: 9.802503258770894\n",
            "FPR@TPR95: 93.9001220297617\n",
            "Finetuned network: bisenet\n",
            "100% 1/1 [00:01<00:00,  1.28s/it]\n",
            "AUPRC score: 7.705231355553613\n",
            "FPR@TPR95: 94.33514965594387\n",
            "Method: MSP, loss: CrossEntropy\n",
            "100% 1/1 [00:01<00:00,  1.66s/it]\n",
            "AUPRC score: 45.49714388457901\n",
            "FPR@TPR95: 33.597166197756\n",
            "Method: MaxLogit, loss: CrossEntropy\n",
            "100% 1/1 [00:01<00:00,  1.61s/it]\n",
            "AUPRC score: 70.57980993958311\n",
            "FPR@TPR95: 24.507008237008915\n",
            "Method: MaxEntropy, loss: CrossEntropy\n",
            "100% 1/1 [00:01<00:00,  1.76s/it]\n",
            "AUPRC score: 57.50733388962147\n",
            "FPR@TPR95: 33.14569845089997\n",
            "Method: MSP, loss: Focal\n",
            "100% 1/1 [00:01<00:00,  1.73s/it]\n",
            "AUPRC score: 45.030157629641586\n",
            "FPR@TPR95: 34.3490474899156\n",
            "Method: MaxLogit, loss: Focal\n",
            "100% 1/1 [00:01<00:00,  1.46s/it]\n",
            "AUPRC score: 63.36769543009915\n",
            "FPR@TPR95: 22.864691027422797\n",
            "Method: MaxEntropy, loss: Focal\n",
            "100% 1/1 [00:01<00:00,  1.81s/it]\n",
            "AUPRC score: 56.92230407127992\n",
            "FPR@TPR95: 33.842496864513066\n",
            "Method: MSP, loss: LogitNorm\n",
            "100% 1/1 [00:01<00:00,  1.59s/it]\n",
            "AUPRC score: 39.0719558527416\n",
            "FPR@TPR95: 39.86165723195823\n",
            "Method: MaxLogit, loss: LogitNorm\n",
            "100% 1/1 [00:01<00:00,  1.76s/it]\n",
            "AUPRC score: 59.59435578369033\n",
            "FPR@TPR95: 34.17129927799058\n",
            "Method: MaxEntropy, loss: LogitNorm\n",
            "100% 1/1 [00:01<00:00,  1.78s/it]\n",
            "AUPRC score: 48.95553688869998\n",
            "FPR@TPR95: 39.7463221585709\n",
            "Method: MSP, loss: IsoMaxPlus\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/AnomalySegmentation/eval/evalAnomaly.py\", line 282, in <module>\n",
            "    main()\n",
            "  File \"/content/AnomalySegmentation/eval/evalAnomaly.py\", line 157, in main\n",
            "    model = load_my_state_dict(model, torch.load(weightspath, map_location=device))\n",
            "  File \"/content/AnomalySegmentation/eval/evalAnomaly.py\", line 137, in load_my_state_dict\n",
            "    if hasattr(model.module.decoder, 'loss_first_part'):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1931, in __getattr__\n",
            "    raise AttributeError(\n",
            "AttributeError: 'ERFNet' object has no attribute 'module'. Did you mean: 'modules'?\n",
            "dict_keys(['state_dict', 'loss_first_part_state_dict'])\n",
            "odict_keys(['encoder.initial_block.conv.weight', 'encoder.initial_block.conv.bias', 'encoder.initial_block.bn.weight', 'encoder.initial_block.bn.bias', 'encoder.initial_block.bn.running_mean', 'encoder.initial_block.bn.running_var', 'encoder.initial_block.bn.num_batches_tracked', 'encoder.layers.0.conv.weight', 'encoder.layers.0.conv.bias', 'encoder.layers.0.bn.weight', 'encoder.layers.0.bn.bias', 'encoder.layers.0.bn.running_mean', 'encoder.layers.0.bn.running_var', 'encoder.layers.0.bn.num_batches_tracked', 'encoder.layers.1.conv3x1_1.weight', 'encoder.layers.1.conv3x1_1.bias', 'encoder.layers.1.conv1x3_1.weight', 'encoder.layers.1.conv1x3_1.bias', 'encoder.layers.1.bn1.weight', 'encoder.layers.1.bn1.bias', 'encoder.layers.1.bn1.running_mean', 'encoder.layers.1.bn1.running_var', 'encoder.layers.1.bn1.num_batches_tracked', 'encoder.layers.1.conv3x1_2.weight', 'encoder.layers.1.conv3x1_2.bias', 'encoder.layers.1.conv1x3_2.weight', 'encoder.layers.1.conv1x3_2.bias', 'encoder.layers.1.bn2.weight', 'encoder.layers.1.bn2.bias', 'encoder.layers.1.bn2.running_mean', 'encoder.layers.1.bn2.running_var', 'encoder.layers.1.bn2.num_batches_tracked', 'encoder.layers.2.conv3x1_1.weight', 'encoder.layers.2.conv3x1_1.bias', 'encoder.layers.2.conv1x3_1.weight', 'encoder.layers.2.conv1x3_1.bias', 'encoder.layers.2.bn1.weight', 'encoder.layers.2.bn1.bias', 'encoder.layers.2.bn1.running_mean', 'encoder.layers.2.bn1.running_var', 'encoder.layers.2.bn1.num_batches_tracked', 'encoder.layers.2.conv3x1_2.weight', 'encoder.layers.2.conv3x1_2.bias', 'encoder.layers.2.conv1x3_2.weight', 'encoder.layers.2.conv1x3_2.bias', 'encoder.layers.2.bn2.weight', 'encoder.layers.2.bn2.bias', 'encoder.layers.2.bn2.running_mean', 'encoder.layers.2.bn2.running_var', 'encoder.layers.2.bn2.num_batches_tracked', 'encoder.layers.3.conv3x1_1.weight', 'encoder.layers.3.conv3x1_1.bias', 'encoder.layers.3.conv1x3_1.weight', 'encoder.layers.3.conv1x3_1.bias', 'encoder.layers.3.bn1.weight', 'encoder.layers.3.bn1.bias', 'encoder.layers.3.bn1.running_mean', 'encoder.layers.3.bn1.running_var', 'encoder.layers.3.bn1.num_batches_tracked', 'encoder.layers.3.conv3x1_2.weight', 'encoder.layers.3.conv3x1_2.bias', 'encoder.layers.3.conv1x3_2.weight', 'encoder.layers.3.conv1x3_2.bias', 'encoder.layers.3.bn2.weight', 'encoder.layers.3.bn2.bias', 'encoder.layers.3.bn2.running_mean', 'encoder.layers.3.bn2.running_var', 'encoder.layers.3.bn2.num_batches_tracked', 'encoder.layers.4.conv3x1_1.weight', 'encoder.layers.4.conv3x1_1.bias', 'encoder.layers.4.conv1x3_1.weight', 'encoder.layers.4.conv1x3_1.bias', 'encoder.layers.4.bn1.weight', 'encoder.layers.4.bn1.bias', 'encoder.layers.4.bn1.running_mean', 'encoder.layers.4.bn1.running_var', 'encoder.layers.4.bn1.num_batches_tracked', 'encoder.layers.4.conv3x1_2.weight', 'encoder.layers.4.conv3x1_2.bias', 'encoder.layers.4.conv1x3_2.weight', 'encoder.layers.4.conv1x3_2.bias', 'encoder.layers.4.bn2.weight', 'encoder.layers.4.bn2.bias', 'encoder.layers.4.bn2.running_mean', 'encoder.layers.4.bn2.running_var', 'encoder.layers.4.bn2.num_batches_tracked', 'encoder.layers.5.conv3x1_1.weight', 'encoder.layers.5.conv3x1_1.bias', 'encoder.layers.5.conv1x3_1.weight', 'encoder.layers.5.conv1x3_1.bias', 'encoder.layers.5.bn1.weight', 'encoder.layers.5.bn1.bias', 'encoder.layers.5.bn1.running_mean', 'encoder.layers.5.bn1.running_var', 'encoder.layers.5.bn1.num_batches_tracked', 'encoder.layers.5.conv3x1_2.weight', 'encoder.layers.5.conv3x1_2.bias', 'encoder.layers.5.conv1x3_2.weight', 'encoder.layers.5.conv1x3_2.bias', 'encoder.layers.5.bn2.weight', 'encoder.layers.5.bn2.bias', 'encoder.layers.5.bn2.running_mean', 'encoder.layers.5.bn2.running_var', 'encoder.layers.5.bn2.num_batches_tracked', 'encoder.layers.6.conv.weight', 'encoder.layers.6.conv.bias', 'encoder.layers.6.bn.weight', 'encoder.layers.6.bn.bias', 'encoder.layers.6.bn.running_mean', 'encoder.layers.6.bn.running_var', 'encoder.layers.6.bn.num_batches_tracked', 'encoder.layers.7.conv3x1_1.weight', 'encoder.layers.7.conv3x1_1.bias', 'encoder.layers.7.conv1x3_1.weight', 'encoder.layers.7.conv1x3_1.bias', 'encoder.layers.7.bn1.weight', 'encoder.layers.7.bn1.bias', 'encoder.layers.7.bn1.running_mean', 'encoder.layers.7.bn1.running_var', 'encoder.layers.7.bn1.num_batches_tracked', 'encoder.layers.7.conv3x1_2.weight', 'encoder.layers.7.conv3x1_2.bias', 'encoder.layers.7.conv1x3_2.weight', 'encoder.layers.7.conv1x3_2.bias', 'encoder.layers.7.bn2.weight', 'encoder.layers.7.bn2.bias', 'encoder.layers.7.bn2.running_mean', 'encoder.layers.7.bn2.running_var', 'encoder.layers.7.bn2.num_batches_tracked', 'encoder.layers.8.conv3x1_1.weight', 'encoder.layers.8.conv3x1_1.bias', 'encoder.layers.8.conv1x3_1.weight', 'encoder.layers.8.conv1x3_1.bias', 'encoder.layers.8.bn1.weight', 'encoder.layers.8.bn1.bias', 'encoder.layers.8.bn1.running_mean', 'encoder.layers.8.bn1.running_var', 'encoder.layers.8.bn1.num_batches_tracked', 'encoder.layers.8.conv3x1_2.weight', 'encoder.layers.8.conv3x1_2.bias', 'encoder.layers.8.conv1x3_2.weight', 'encoder.layers.8.conv1x3_2.bias', 'encoder.layers.8.bn2.weight', 'encoder.layers.8.bn2.bias', 'encoder.layers.8.bn2.running_mean', 'encoder.layers.8.bn2.running_var', 'encoder.layers.8.bn2.num_batches_tracked', 'encoder.layers.9.conv3x1_1.weight', 'encoder.layers.9.conv3x1_1.bias', 'encoder.layers.9.conv1x3_1.weight', 'encoder.layers.9.conv1x3_1.bias', 'encoder.layers.9.bn1.weight', 'encoder.layers.9.bn1.bias', 'encoder.layers.9.bn1.running_mean', 'encoder.layers.9.bn1.running_var', 'encoder.layers.9.bn1.num_batches_tracked', 'encoder.layers.9.conv3x1_2.weight', 'encoder.layers.9.conv3x1_2.bias', 'encoder.layers.9.conv1x3_2.weight', 'encoder.layers.9.conv1x3_2.bias', 'encoder.layers.9.bn2.weight', 'encoder.layers.9.bn2.bias', 'encoder.layers.9.bn2.running_mean', 'encoder.layers.9.bn2.running_var', 'encoder.layers.9.bn2.num_batches_tracked', 'encoder.layers.10.conv3x1_1.weight', 'encoder.layers.10.conv3x1_1.bias', 'encoder.layers.10.conv1x3_1.weight', 'encoder.layers.10.conv1x3_1.bias', 'encoder.layers.10.bn1.weight', 'encoder.layers.10.bn1.bias', 'encoder.layers.10.bn1.running_mean', 'encoder.layers.10.bn1.running_var', 'encoder.layers.10.bn1.num_batches_tracked', 'encoder.layers.10.conv3x1_2.weight', 'encoder.layers.10.conv3x1_2.bias', 'encoder.layers.10.conv1x3_2.weight', 'encoder.layers.10.conv1x3_2.bias', 'encoder.layers.10.bn2.weight', 'encoder.layers.10.bn2.bias', 'encoder.layers.10.bn2.running_mean', 'encoder.layers.10.bn2.running_var', 'encoder.layers.10.bn2.num_batches_tracked', 'encoder.layers.11.conv3x1_1.weight', 'encoder.layers.11.conv3x1_1.bias', 'encoder.layers.11.conv1x3_1.weight', 'encoder.layers.11.conv1x3_1.bias', 'encoder.layers.11.bn1.weight', 'encoder.layers.11.bn1.bias', 'encoder.layers.11.bn1.running_mean', 'encoder.layers.11.bn1.running_var', 'encoder.layers.11.bn1.num_batches_tracked', 'encoder.layers.11.conv3x1_2.weight', 'encoder.layers.11.conv3x1_2.bias', 'encoder.layers.11.conv1x3_2.weight', 'encoder.layers.11.conv1x3_2.bias', 'encoder.layers.11.bn2.weight', 'encoder.layers.11.bn2.bias', 'encoder.layers.11.bn2.running_mean', 'encoder.layers.11.bn2.running_var', 'encoder.layers.11.bn2.num_batches_tracked', 'encoder.layers.12.conv3x1_1.weight', 'encoder.layers.12.conv3x1_1.bias', 'encoder.layers.12.conv1x3_1.weight', 'encoder.layers.12.conv1x3_1.bias', 'encoder.layers.12.bn1.weight', 'encoder.layers.12.bn1.bias', 'encoder.layers.12.bn1.running_mean', 'encoder.layers.12.bn1.running_var', 'encoder.layers.12.bn1.num_batches_tracked', 'encoder.layers.12.conv3x1_2.weight', 'encoder.layers.12.conv3x1_2.bias', 'encoder.layers.12.conv1x3_2.weight', 'encoder.layers.12.conv1x3_2.bias', 'encoder.layers.12.bn2.weight', 'encoder.layers.12.bn2.bias', 'encoder.layers.12.bn2.running_mean', 'encoder.layers.12.bn2.running_var', 'encoder.layers.12.bn2.num_batches_tracked', 'encoder.layers.13.conv3x1_1.weight', 'encoder.layers.13.conv3x1_1.bias', 'encoder.layers.13.conv1x3_1.weight', 'encoder.layers.13.conv1x3_1.bias', 'encoder.layers.13.bn1.weight', 'encoder.layers.13.bn1.bias', 'encoder.layers.13.bn1.running_mean', 'encoder.layers.13.bn1.running_var', 'encoder.layers.13.bn1.num_batches_tracked', 'encoder.layers.13.conv3x1_2.weight', 'encoder.layers.13.conv3x1_2.bias', 'encoder.layers.13.conv1x3_2.weight', 'encoder.layers.13.conv1x3_2.bias', 'encoder.layers.13.bn2.weight', 'encoder.layers.13.bn2.bias', 'encoder.layers.13.bn2.running_mean', 'encoder.layers.13.bn2.running_var', 'encoder.layers.13.bn2.num_batches_tracked', 'encoder.layers.14.conv3x1_1.weight', 'encoder.layers.14.conv3x1_1.bias', 'encoder.layers.14.conv1x3_1.weight', 'encoder.layers.14.conv1x3_1.bias', 'encoder.layers.14.bn1.weight', 'encoder.layers.14.bn1.bias', 'encoder.layers.14.bn1.running_mean', 'encoder.layers.14.bn1.running_var', 'encoder.layers.14.bn1.num_batches_tracked', 'encoder.layers.14.conv3x1_2.weight', 'encoder.layers.14.conv3x1_2.bias', 'encoder.layers.14.conv1x3_2.weight', 'encoder.layers.14.conv1x3_2.bias', 'encoder.layers.14.bn2.weight', 'encoder.layers.14.bn2.bias', 'encoder.layers.14.bn2.running_mean', 'encoder.layers.14.bn2.running_var', 'encoder.layers.14.bn2.num_batches_tracked', 'encoder.output_conv.weight', 'encoder.output_conv.bias', 'decoder.loss_first_part.prototypes', 'decoder.loss_first_part.distance_scale', 'decoder.layers.0.conv.weight', 'decoder.layers.0.conv.bias', 'decoder.layers.0.bn.weight', 'decoder.layers.0.bn.bias', 'decoder.layers.0.bn.running_mean', 'decoder.layers.0.bn.running_var', 'decoder.layers.0.bn.num_batches_tracked', 'decoder.layers.1.conv3x1_1.weight', 'decoder.layers.1.conv3x1_1.bias', 'decoder.layers.1.conv1x3_1.weight', 'decoder.layers.1.conv1x3_1.bias', 'decoder.layers.1.bn1.weight', 'decoder.layers.1.bn1.bias', 'decoder.layers.1.bn1.running_mean', 'decoder.layers.1.bn1.running_var', 'decoder.layers.1.bn1.num_batches_tracked', 'decoder.layers.1.conv3x1_2.weight', 'decoder.layers.1.conv3x1_2.bias', 'decoder.layers.1.conv1x3_2.weight', 'decoder.layers.1.conv1x3_2.bias', 'decoder.layers.1.bn2.weight', 'decoder.layers.1.bn2.bias', 'decoder.layers.1.bn2.running_mean', 'decoder.layers.1.bn2.running_var', 'decoder.layers.1.bn2.num_batches_tracked', 'decoder.layers.2.conv3x1_1.weight', 'decoder.layers.2.conv3x1_1.bias', 'decoder.layers.2.conv1x3_1.weight', 'decoder.layers.2.conv1x3_1.bias', 'decoder.layers.2.bn1.weight', 'decoder.layers.2.bn1.bias', 'decoder.layers.2.bn1.running_mean', 'decoder.layers.2.bn1.running_var', 'decoder.layers.2.bn1.num_batches_tracked', 'decoder.layers.2.conv3x1_2.weight', 'decoder.layers.2.conv3x1_2.bias', 'decoder.layers.2.conv1x3_2.weight', 'decoder.layers.2.conv1x3_2.bias', 'decoder.layers.2.bn2.weight', 'decoder.layers.2.bn2.bias', 'decoder.layers.2.bn2.running_mean', 'decoder.layers.2.bn2.running_var', 'decoder.layers.2.bn2.num_batches_tracked', 'decoder.layers.3.conv.weight', 'decoder.layers.3.conv.bias', 'decoder.layers.3.bn.weight', 'decoder.layers.3.bn.bias', 'decoder.layers.3.bn.running_mean', 'decoder.layers.3.bn.running_var', 'decoder.layers.3.bn.num_batches_tracked', 'decoder.layers.4.conv3x1_1.weight', 'decoder.layers.4.conv3x1_1.bias', 'decoder.layers.4.conv1x3_1.weight', 'decoder.layers.4.conv1x3_1.bias', 'decoder.layers.4.bn1.weight', 'decoder.layers.4.bn1.bias', 'decoder.layers.4.bn1.running_mean', 'decoder.layers.4.bn1.running_var', 'decoder.layers.4.bn1.num_batches_tracked', 'decoder.layers.4.conv3x1_2.weight', 'decoder.layers.4.conv3x1_2.bias', 'decoder.layers.4.conv1x3_2.weight', 'decoder.layers.4.conv1x3_2.bias', 'decoder.layers.4.bn2.weight', 'decoder.layers.4.bn2.bias', 'decoder.layers.4.bn2.running_mean', 'decoder.layers.4.bn2.running_var', 'decoder.layers.4.bn2.num_batches_tracked', 'decoder.layers.5.conv3x1_1.weight', 'decoder.layers.5.conv3x1_1.bias', 'decoder.layers.5.conv1x3_1.weight', 'decoder.layers.5.conv1x3_1.bias', 'decoder.layers.5.bn1.weight', 'decoder.layers.5.bn1.bias', 'decoder.layers.5.bn1.running_mean', 'decoder.layers.5.bn1.running_var', 'decoder.layers.5.bn1.num_batches_tracked', 'decoder.layers.5.conv3x1_2.weight', 'decoder.layers.5.conv3x1_2.bias', 'decoder.layers.5.conv1x3_2.weight', 'decoder.layers.5.conv1x3_2.bias', 'decoder.layers.5.bn2.weight', 'decoder.layers.5.bn2.bias', 'decoder.layers.5.bn2.running_mean', 'decoder.layers.5.bn2.running_var', 'decoder.layers.5.bn2.num_batches_tracked', 'decoder.output_conv.weight', 'decoder.output_conv.bias'])\n",
            "Method: MaxLogit, loss: IsoMaxPlus\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/AnomalySegmentation/eval/evalAnomaly.py\", line 282, in <module>\n",
            "    main()\n",
            "  File \"/content/AnomalySegmentation/eval/evalAnomaly.py\", line 157, in main\n",
            "    model = load_my_state_dict(model, torch.load(weightspath, map_location=device))\n",
            "  File \"/content/AnomalySegmentation/eval/evalAnomaly.py\", line 137, in load_my_state_dict\n",
            "    if hasattr(model.module.decoder, 'loss_first_part'):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1931, in __getattr__\n",
            "    raise AttributeError(\n",
            "AttributeError: 'ERFNet' object has no attribute 'module'. Did you mean: 'modules'?\n",
            "dict_keys(['state_dict', 'loss_first_part_state_dict'])\n",
            "odict_keys(['encoder.initial_block.conv.weight', 'encoder.initial_block.conv.bias', 'encoder.initial_block.bn.weight', 'encoder.initial_block.bn.bias', 'encoder.initial_block.bn.running_mean', 'encoder.initial_block.bn.running_var', 'encoder.initial_block.bn.num_batches_tracked', 'encoder.layers.0.conv.weight', 'encoder.layers.0.conv.bias', 'encoder.layers.0.bn.weight', 'encoder.layers.0.bn.bias', 'encoder.layers.0.bn.running_mean', 'encoder.layers.0.bn.running_var', 'encoder.layers.0.bn.num_batches_tracked', 'encoder.layers.1.conv3x1_1.weight', 'encoder.layers.1.conv3x1_1.bias', 'encoder.layers.1.conv1x3_1.weight', 'encoder.layers.1.conv1x3_1.bias', 'encoder.layers.1.bn1.weight', 'encoder.layers.1.bn1.bias', 'encoder.layers.1.bn1.running_mean', 'encoder.layers.1.bn1.running_var', 'encoder.layers.1.bn1.num_batches_tracked', 'encoder.layers.1.conv3x1_2.weight', 'encoder.layers.1.conv3x1_2.bias', 'encoder.layers.1.conv1x3_2.weight', 'encoder.layers.1.conv1x3_2.bias', 'encoder.layers.1.bn2.weight', 'encoder.layers.1.bn2.bias', 'encoder.layers.1.bn2.running_mean', 'encoder.layers.1.bn2.running_var', 'encoder.layers.1.bn2.num_batches_tracked', 'encoder.layers.2.conv3x1_1.weight', 'encoder.layers.2.conv3x1_1.bias', 'encoder.layers.2.conv1x3_1.weight', 'encoder.layers.2.conv1x3_1.bias', 'encoder.layers.2.bn1.weight', 'encoder.layers.2.bn1.bias', 'encoder.layers.2.bn1.running_mean', 'encoder.layers.2.bn1.running_var', 'encoder.layers.2.bn1.num_batches_tracked', 'encoder.layers.2.conv3x1_2.weight', 'encoder.layers.2.conv3x1_2.bias', 'encoder.layers.2.conv1x3_2.weight', 'encoder.layers.2.conv1x3_2.bias', 'encoder.layers.2.bn2.weight', 'encoder.layers.2.bn2.bias', 'encoder.layers.2.bn2.running_mean', 'encoder.layers.2.bn2.running_var', 'encoder.layers.2.bn2.num_batches_tracked', 'encoder.layers.3.conv3x1_1.weight', 'encoder.layers.3.conv3x1_1.bias', 'encoder.layers.3.conv1x3_1.weight', 'encoder.layers.3.conv1x3_1.bias', 'encoder.layers.3.bn1.weight', 'encoder.layers.3.bn1.bias', 'encoder.layers.3.bn1.running_mean', 'encoder.layers.3.bn1.running_var', 'encoder.layers.3.bn1.num_batches_tracked', 'encoder.layers.3.conv3x1_2.weight', 'encoder.layers.3.conv3x1_2.bias', 'encoder.layers.3.conv1x3_2.weight', 'encoder.layers.3.conv1x3_2.bias', 'encoder.layers.3.bn2.weight', 'encoder.layers.3.bn2.bias', 'encoder.layers.3.bn2.running_mean', 'encoder.layers.3.bn2.running_var', 'encoder.layers.3.bn2.num_batches_tracked', 'encoder.layers.4.conv3x1_1.weight', 'encoder.layers.4.conv3x1_1.bias', 'encoder.layers.4.conv1x3_1.weight', 'encoder.layers.4.conv1x3_1.bias', 'encoder.layers.4.bn1.weight', 'encoder.layers.4.bn1.bias', 'encoder.layers.4.bn1.running_mean', 'encoder.layers.4.bn1.running_var', 'encoder.layers.4.bn1.num_batches_tracked', 'encoder.layers.4.conv3x1_2.weight', 'encoder.layers.4.conv3x1_2.bias', 'encoder.layers.4.conv1x3_2.weight', 'encoder.layers.4.conv1x3_2.bias', 'encoder.layers.4.bn2.weight', 'encoder.layers.4.bn2.bias', 'encoder.layers.4.bn2.running_mean', 'encoder.layers.4.bn2.running_var', 'encoder.layers.4.bn2.num_batches_tracked', 'encoder.layers.5.conv3x1_1.weight', 'encoder.layers.5.conv3x1_1.bias', 'encoder.layers.5.conv1x3_1.weight', 'encoder.layers.5.conv1x3_1.bias', 'encoder.layers.5.bn1.weight', 'encoder.layers.5.bn1.bias', 'encoder.layers.5.bn1.running_mean', 'encoder.layers.5.bn1.running_var', 'encoder.layers.5.bn1.num_batches_tracked', 'encoder.layers.5.conv3x1_2.weight', 'encoder.layers.5.conv3x1_2.bias', 'encoder.layers.5.conv1x3_2.weight', 'encoder.layers.5.conv1x3_2.bias', 'encoder.layers.5.bn2.weight', 'encoder.layers.5.bn2.bias', 'encoder.layers.5.bn2.running_mean', 'encoder.layers.5.bn2.running_var', 'encoder.layers.5.bn2.num_batches_tracked', 'encoder.layers.6.conv.weight', 'encoder.layers.6.conv.bias', 'encoder.layers.6.bn.weight', 'encoder.layers.6.bn.bias', 'encoder.layers.6.bn.running_mean', 'encoder.layers.6.bn.running_var', 'encoder.layers.6.bn.num_batches_tracked', 'encoder.layers.7.conv3x1_1.weight', 'encoder.layers.7.conv3x1_1.bias', 'encoder.layers.7.conv1x3_1.weight', 'encoder.layers.7.conv1x3_1.bias', 'encoder.layers.7.bn1.weight', 'encoder.layers.7.bn1.bias', 'encoder.layers.7.bn1.running_mean', 'encoder.layers.7.bn1.running_var', 'encoder.layers.7.bn1.num_batches_tracked', 'encoder.layers.7.conv3x1_2.weight', 'encoder.layers.7.conv3x1_2.bias', 'encoder.layers.7.conv1x3_2.weight', 'encoder.layers.7.conv1x3_2.bias', 'encoder.layers.7.bn2.weight', 'encoder.layers.7.bn2.bias', 'encoder.layers.7.bn2.running_mean', 'encoder.layers.7.bn2.running_var', 'encoder.layers.7.bn2.num_batches_tracked', 'encoder.layers.8.conv3x1_1.weight', 'encoder.layers.8.conv3x1_1.bias', 'encoder.layers.8.conv1x3_1.weight', 'encoder.layers.8.conv1x3_1.bias', 'encoder.layers.8.bn1.weight', 'encoder.layers.8.bn1.bias', 'encoder.layers.8.bn1.running_mean', 'encoder.layers.8.bn1.running_var', 'encoder.layers.8.bn1.num_batches_tracked', 'encoder.layers.8.conv3x1_2.weight', 'encoder.layers.8.conv3x1_2.bias', 'encoder.layers.8.conv1x3_2.weight', 'encoder.layers.8.conv1x3_2.bias', 'encoder.layers.8.bn2.weight', 'encoder.layers.8.bn2.bias', 'encoder.layers.8.bn2.running_mean', 'encoder.layers.8.bn2.running_var', 'encoder.layers.8.bn2.num_batches_tracked', 'encoder.layers.9.conv3x1_1.weight', 'encoder.layers.9.conv3x1_1.bias', 'encoder.layers.9.conv1x3_1.weight', 'encoder.layers.9.conv1x3_1.bias', 'encoder.layers.9.bn1.weight', 'encoder.layers.9.bn1.bias', 'encoder.layers.9.bn1.running_mean', 'encoder.layers.9.bn1.running_var', 'encoder.layers.9.bn1.num_batches_tracked', 'encoder.layers.9.conv3x1_2.weight', 'encoder.layers.9.conv3x1_2.bias', 'encoder.layers.9.conv1x3_2.weight', 'encoder.layers.9.conv1x3_2.bias', 'encoder.layers.9.bn2.weight', 'encoder.layers.9.bn2.bias', 'encoder.layers.9.bn2.running_mean', 'encoder.layers.9.bn2.running_var', 'encoder.layers.9.bn2.num_batches_tracked', 'encoder.layers.10.conv3x1_1.weight', 'encoder.layers.10.conv3x1_1.bias', 'encoder.layers.10.conv1x3_1.weight', 'encoder.layers.10.conv1x3_1.bias', 'encoder.layers.10.bn1.weight', 'encoder.layers.10.bn1.bias', 'encoder.layers.10.bn1.running_mean', 'encoder.layers.10.bn1.running_var', 'encoder.layers.10.bn1.num_batches_tracked', 'encoder.layers.10.conv3x1_2.weight', 'encoder.layers.10.conv3x1_2.bias', 'encoder.layers.10.conv1x3_2.weight', 'encoder.layers.10.conv1x3_2.bias', 'encoder.layers.10.bn2.weight', 'encoder.layers.10.bn2.bias', 'encoder.layers.10.bn2.running_mean', 'encoder.layers.10.bn2.running_var', 'encoder.layers.10.bn2.num_batches_tracked', 'encoder.layers.11.conv3x1_1.weight', 'encoder.layers.11.conv3x1_1.bias', 'encoder.layers.11.conv1x3_1.weight', 'encoder.layers.11.conv1x3_1.bias', 'encoder.layers.11.bn1.weight', 'encoder.layers.11.bn1.bias', 'encoder.layers.11.bn1.running_mean', 'encoder.layers.11.bn1.running_var', 'encoder.layers.11.bn1.num_batches_tracked', 'encoder.layers.11.conv3x1_2.weight', 'encoder.layers.11.conv3x1_2.bias', 'encoder.layers.11.conv1x3_2.weight', 'encoder.layers.11.conv1x3_2.bias', 'encoder.layers.11.bn2.weight', 'encoder.layers.11.bn2.bias', 'encoder.layers.11.bn2.running_mean', 'encoder.layers.11.bn2.running_var', 'encoder.layers.11.bn2.num_batches_tracked', 'encoder.layers.12.conv3x1_1.weight', 'encoder.layers.12.conv3x1_1.bias', 'encoder.layers.12.conv1x3_1.weight', 'encoder.layers.12.conv1x3_1.bias', 'encoder.layers.12.bn1.weight', 'encoder.layers.12.bn1.bias', 'encoder.layers.12.bn1.running_mean', 'encoder.layers.12.bn1.running_var', 'encoder.layers.12.bn1.num_batches_tracked', 'encoder.layers.12.conv3x1_2.weight', 'encoder.layers.12.conv3x1_2.bias', 'encoder.layers.12.conv1x3_2.weight', 'encoder.layers.12.conv1x3_2.bias', 'encoder.layers.12.bn2.weight', 'encoder.layers.12.bn2.bias', 'encoder.layers.12.bn2.running_mean', 'encoder.layers.12.bn2.running_var', 'encoder.layers.12.bn2.num_batches_tracked', 'encoder.layers.13.conv3x1_1.weight', 'encoder.layers.13.conv3x1_1.bias', 'encoder.layers.13.conv1x3_1.weight', 'encoder.layers.13.conv1x3_1.bias', 'encoder.layers.13.bn1.weight', 'encoder.layers.13.bn1.bias', 'encoder.layers.13.bn1.running_mean', 'encoder.layers.13.bn1.running_var', 'encoder.layers.13.bn1.num_batches_tracked', 'encoder.layers.13.conv3x1_2.weight', 'encoder.layers.13.conv3x1_2.bias', 'encoder.layers.13.conv1x3_2.weight', 'encoder.layers.13.conv1x3_2.bias', 'encoder.layers.13.bn2.weight', 'encoder.layers.13.bn2.bias', 'encoder.layers.13.bn2.running_mean', 'encoder.layers.13.bn2.running_var', 'encoder.layers.13.bn2.num_batches_tracked', 'encoder.layers.14.conv3x1_1.weight', 'encoder.layers.14.conv3x1_1.bias', 'encoder.layers.14.conv1x3_1.weight', 'encoder.layers.14.conv1x3_1.bias', 'encoder.layers.14.bn1.weight', 'encoder.layers.14.bn1.bias', 'encoder.layers.14.bn1.running_mean', 'encoder.layers.14.bn1.running_var', 'encoder.layers.14.bn1.num_batches_tracked', 'encoder.layers.14.conv3x1_2.weight', 'encoder.layers.14.conv3x1_2.bias', 'encoder.layers.14.conv1x3_2.weight', 'encoder.layers.14.conv1x3_2.bias', 'encoder.layers.14.bn2.weight', 'encoder.layers.14.bn2.bias', 'encoder.layers.14.bn2.running_mean', 'encoder.layers.14.bn2.running_var', 'encoder.layers.14.bn2.num_batches_tracked', 'encoder.output_conv.weight', 'encoder.output_conv.bias', 'decoder.loss_first_part.prototypes', 'decoder.loss_first_part.distance_scale', 'decoder.layers.0.conv.weight', 'decoder.layers.0.conv.bias', 'decoder.layers.0.bn.weight', 'decoder.layers.0.bn.bias', 'decoder.layers.0.bn.running_mean', 'decoder.layers.0.bn.running_var', 'decoder.layers.0.bn.num_batches_tracked', 'decoder.layers.1.conv3x1_1.weight', 'decoder.layers.1.conv3x1_1.bias', 'decoder.layers.1.conv1x3_1.weight', 'decoder.layers.1.conv1x3_1.bias', 'decoder.layers.1.bn1.weight', 'decoder.layers.1.bn1.bias', 'decoder.layers.1.bn1.running_mean', 'decoder.layers.1.bn1.running_var', 'decoder.layers.1.bn1.num_batches_tracked', 'decoder.layers.1.conv3x1_2.weight', 'decoder.layers.1.conv3x1_2.bias', 'decoder.layers.1.conv1x3_2.weight', 'decoder.layers.1.conv1x3_2.bias', 'decoder.layers.1.bn2.weight', 'decoder.layers.1.bn2.bias', 'decoder.layers.1.bn2.running_mean', 'decoder.layers.1.bn2.running_var', 'decoder.layers.1.bn2.num_batches_tracked', 'decoder.layers.2.conv3x1_1.weight', 'decoder.layers.2.conv3x1_1.bias', 'decoder.layers.2.conv1x3_1.weight', 'decoder.layers.2.conv1x3_1.bias', 'decoder.layers.2.bn1.weight', 'decoder.layers.2.bn1.bias', 'decoder.layers.2.bn1.running_mean', 'decoder.layers.2.bn1.running_var', 'decoder.layers.2.bn1.num_batches_tracked', 'decoder.layers.2.conv3x1_2.weight', 'decoder.layers.2.conv3x1_2.bias', 'decoder.layers.2.conv1x3_2.weight', 'decoder.layers.2.conv1x3_2.bias', 'decoder.layers.2.bn2.weight', 'decoder.layers.2.bn2.bias', 'decoder.layers.2.bn2.running_mean', 'decoder.layers.2.bn2.running_var', 'decoder.layers.2.bn2.num_batches_tracked', 'decoder.layers.3.conv.weight', 'decoder.layers.3.conv.bias', 'decoder.layers.3.bn.weight', 'decoder.layers.3.bn.bias', 'decoder.layers.3.bn.running_mean', 'decoder.layers.3.bn.running_var', 'decoder.layers.3.bn.num_batches_tracked', 'decoder.layers.4.conv3x1_1.weight', 'decoder.layers.4.conv3x1_1.bias', 'decoder.layers.4.conv1x3_1.weight', 'decoder.layers.4.conv1x3_1.bias', 'decoder.layers.4.bn1.weight', 'decoder.layers.4.bn1.bias', 'decoder.layers.4.bn1.running_mean', 'decoder.layers.4.bn1.running_var', 'decoder.layers.4.bn1.num_batches_tracked', 'decoder.layers.4.conv3x1_2.weight', 'decoder.layers.4.conv3x1_2.bias', 'decoder.layers.4.conv1x3_2.weight', 'decoder.layers.4.conv1x3_2.bias', 'decoder.layers.4.bn2.weight', 'decoder.layers.4.bn2.bias', 'decoder.layers.4.bn2.running_mean', 'decoder.layers.4.bn2.running_var', 'decoder.layers.4.bn2.num_batches_tracked', 'decoder.layers.5.conv3x1_1.weight', 'decoder.layers.5.conv3x1_1.bias', 'decoder.layers.5.conv1x3_1.weight', 'decoder.layers.5.conv1x3_1.bias', 'decoder.layers.5.bn1.weight', 'decoder.layers.5.bn1.bias', 'decoder.layers.5.bn1.running_mean', 'decoder.layers.5.bn1.running_var', 'decoder.layers.5.bn1.num_batches_tracked', 'decoder.layers.5.conv3x1_2.weight', 'decoder.layers.5.conv3x1_2.bias', 'decoder.layers.5.conv1x3_2.weight', 'decoder.layers.5.conv1x3_2.bias', 'decoder.layers.5.bn2.weight', 'decoder.layers.5.bn2.bias', 'decoder.layers.5.bn2.running_mean', 'decoder.layers.5.bn2.running_var', 'decoder.layers.5.bn2.num_batches_tracked', 'decoder.output_conv.weight', 'decoder.output_conv.bias'])\n",
            "Method: MaxEntropy, loss: IsoMaxPlus\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/AnomalySegmentation/eval/evalAnomaly.py\", line 282, in <module>\n",
            "    main()\n",
            "  File \"/content/AnomalySegmentation/eval/evalAnomaly.py\", line 157, in main\n",
            "    model = load_my_state_dict(model, torch.load(weightspath, map_location=device))\n",
            "  File \"/content/AnomalySegmentation/eval/evalAnomaly.py\", line 137, in load_my_state_dict\n",
            "    if hasattr(model.module.decoder, 'loss_first_part'):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1931, in __getattr__\n",
            "    raise AttributeError(\n",
            "AttributeError: 'ERFNet' object has no attribute 'module'. Did you mean: 'modules'?\n",
            "dict_keys(['state_dict', 'loss_first_part_state_dict'])\n",
            "odict_keys(['encoder.initial_block.conv.weight', 'encoder.initial_block.conv.bias', 'encoder.initial_block.bn.weight', 'encoder.initial_block.bn.bias', 'encoder.initial_block.bn.running_mean', 'encoder.initial_block.bn.running_var', 'encoder.initial_block.bn.num_batches_tracked', 'encoder.layers.0.conv.weight', 'encoder.layers.0.conv.bias', 'encoder.layers.0.bn.weight', 'encoder.layers.0.bn.bias', 'encoder.layers.0.bn.running_mean', 'encoder.layers.0.bn.running_var', 'encoder.layers.0.bn.num_batches_tracked', 'encoder.layers.1.conv3x1_1.weight', 'encoder.layers.1.conv3x1_1.bias', 'encoder.layers.1.conv1x3_1.weight', 'encoder.layers.1.conv1x3_1.bias', 'encoder.layers.1.bn1.weight', 'encoder.layers.1.bn1.bias', 'encoder.layers.1.bn1.running_mean', 'encoder.layers.1.bn1.running_var', 'encoder.layers.1.bn1.num_batches_tracked', 'encoder.layers.1.conv3x1_2.weight', 'encoder.layers.1.conv3x1_2.bias', 'encoder.layers.1.conv1x3_2.weight', 'encoder.layers.1.conv1x3_2.bias', 'encoder.layers.1.bn2.weight', 'encoder.layers.1.bn2.bias', 'encoder.layers.1.bn2.running_mean', 'encoder.layers.1.bn2.running_var', 'encoder.layers.1.bn2.num_batches_tracked', 'encoder.layers.2.conv3x1_1.weight', 'encoder.layers.2.conv3x1_1.bias', 'encoder.layers.2.conv1x3_1.weight', 'encoder.layers.2.conv1x3_1.bias', 'encoder.layers.2.bn1.weight', 'encoder.layers.2.bn1.bias', 'encoder.layers.2.bn1.running_mean', 'encoder.layers.2.bn1.running_var', 'encoder.layers.2.bn1.num_batches_tracked', 'encoder.layers.2.conv3x1_2.weight', 'encoder.layers.2.conv3x1_2.bias', 'encoder.layers.2.conv1x3_2.weight', 'encoder.layers.2.conv1x3_2.bias', 'encoder.layers.2.bn2.weight', 'encoder.layers.2.bn2.bias', 'encoder.layers.2.bn2.running_mean', 'encoder.layers.2.bn2.running_var', 'encoder.layers.2.bn2.num_batches_tracked', 'encoder.layers.3.conv3x1_1.weight', 'encoder.layers.3.conv3x1_1.bias', 'encoder.layers.3.conv1x3_1.weight', 'encoder.layers.3.conv1x3_1.bias', 'encoder.layers.3.bn1.weight', 'encoder.layers.3.bn1.bias', 'encoder.layers.3.bn1.running_mean', 'encoder.layers.3.bn1.running_var', 'encoder.layers.3.bn1.num_batches_tracked', 'encoder.layers.3.conv3x1_2.weight', 'encoder.layers.3.conv3x1_2.bias', 'encoder.layers.3.conv1x3_2.weight', 'encoder.layers.3.conv1x3_2.bias', 'encoder.layers.3.bn2.weight', 'encoder.layers.3.bn2.bias', 'encoder.layers.3.bn2.running_mean', 'encoder.layers.3.bn2.running_var', 'encoder.layers.3.bn2.num_batches_tracked', 'encoder.layers.4.conv3x1_1.weight', 'encoder.layers.4.conv3x1_1.bias', 'encoder.layers.4.conv1x3_1.weight', 'encoder.layers.4.conv1x3_1.bias', 'encoder.layers.4.bn1.weight', 'encoder.layers.4.bn1.bias', 'encoder.layers.4.bn1.running_mean', 'encoder.layers.4.bn1.running_var', 'encoder.layers.4.bn1.num_batches_tracked', 'encoder.layers.4.conv3x1_2.weight', 'encoder.layers.4.conv3x1_2.bias', 'encoder.layers.4.conv1x3_2.weight', 'encoder.layers.4.conv1x3_2.bias', 'encoder.layers.4.bn2.weight', 'encoder.layers.4.bn2.bias', 'encoder.layers.4.bn2.running_mean', 'encoder.layers.4.bn2.running_var', 'encoder.layers.4.bn2.num_batches_tracked', 'encoder.layers.5.conv3x1_1.weight', 'encoder.layers.5.conv3x1_1.bias', 'encoder.layers.5.conv1x3_1.weight', 'encoder.layers.5.conv1x3_1.bias', 'encoder.layers.5.bn1.weight', 'encoder.layers.5.bn1.bias', 'encoder.layers.5.bn1.running_mean', 'encoder.layers.5.bn1.running_var', 'encoder.layers.5.bn1.num_batches_tracked', 'encoder.layers.5.conv3x1_2.weight', 'encoder.layers.5.conv3x1_2.bias', 'encoder.layers.5.conv1x3_2.weight', 'encoder.layers.5.conv1x3_2.bias', 'encoder.layers.5.bn2.weight', 'encoder.layers.5.bn2.bias', 'encoder.layers.5.bn2.running_mean', 'encoder.layers.5.bn2.running_var', 'encoder.layers.5.bn2.num_batches_tracked', 'encoder.layers.6.conv.weight', 'encoder.layers.6.conv.bias', 'encoder.layers.6.bn.weight', 'encoder.layers.6.bn.bias', 'encoder.layers.6.bn.running_mean', 'encoder.layers.6.bn.running_var', 'encoder.layers.6.bn.num_batches_tracked', 'encoder.layers.7.conv3x1_1.weight', 'encoder.layers.7.conv3x1_1.bias', 'encoder.layers.7.conv1x3_1.weight', 'encoder.layers.7.conv1x3_1.bias', 'encoder.layers.7.bn1.weight', 'encoder.layers.7.bn1.bias', 'encoder.layers.7.bn1.running_mean', 'encoder.layers.7.bn1.running_var', 'encoder.layers.7.bn1.num_batches_tracked', 'encoder.layers.7.conv3x1_2.weight', 'encoder.layers.7.conv3x1_2.bias', 'encoder.layers.7.conv1x3_2.weight', 'encoder.layers.7.conv1x3_2.bias', 'encoder.layers.7.bn2.weight', 'encoder.layers.7.bn2.bias', 'encoder.layers.7.bn2.running_mean', 'encoder.layers.7.bn2.running_var', 'encoder.layers.7.bn2.num_batches_tracked', 'encoder.layers.8.conv3x1_1.weight', 'encoder.layers.8.conv3x1_1.bias', 'encoder.layers.8.conv1x3_1.weight', 'encoder.layers.8.conv1x3_1.bias', 'encoder.layers.8.bn1.weight', 'encoder.layers.8.bn1.bias', 'encoder.layers.8.bn1.running_mean', 'encoder.layers.8.bn1.running_var', 'encoder.layers.8.bn1.num_batches_tracked', 'encoder.layers.8.conv3x1_2.weight', 'encoder.layers.8.conv3x1_2.bias', 'encoder.layers.8.conv1x3_2.weight', 'encoder.layers.8.conv1x3_2.bias', 'encoder.layers.8.bn2.weight', 'encoder.layers.8.bn2.bias', 'encoder.layers.8.bn2.running_mean', 'encoder.layers.8.bn2.running_var', 'encoder.layers.8.bn2.num_batches_tracked', 'encoder.layers.9.conv3x1_1.weight', 'encoder.layers.9.conv3x1_1.bias', 'encoder.layers.9.conv1x3_1.weight', 'encoder.layers.9.conv1x3_1.bias', 'encoder.layers.9.bn1.weight', 'encoder.layers.9.bn1.bias', 'encoder.layers.9.bn1.running_mean', 'encoder.layers.9.bn1.running_var', 'encoder.layers.9.bn1.num_batches_tracked', 'encoder.layers.9.conv3x1_2.weight', 'encoder.layers.9.conv3x1_2.bias', 'encoder.layers.9.conv1x3_2.weight', 'encoder.layers.9.conv1x3_2.bias', 'encoder.layers.9.bn2.weight', 'encoder.layers.9.bn2.bias', 'encoder.layers.9.bn2.running_mean', 'encoder.layers.9.bn2.running_var', 'encoder.layers.9.bn2.num_batches_tracked', 'encoder.layers.10.conv3x1_1.weight', 'encoder.layers.10.conv3x1_1.bias', 'encoder.layers.10.conv1x3_1.weight', 'encoder.layers.10.conv1x3_1.bias', 'encoder.layers.10.bn1.weight', 'encoder.layers.10.bn1.bias', 'encoder.layers.10.bn1.running_mean', 'encoder.layers.10.bn1.running_var', 'encoder.layers.10.bn1.num_batches_tracked', 'encoder.layers.10.conv3x1_2.weight', 'encoder.layers.10.conv3x1_2.bias', 'encoder.layers.10.conv1x3_2.weight', 'encoder.layers.10.conv1x3_2.bias', 'encoder.layers.10.bn2.weight', 'encoder.layers.10.bn2.bias', 'encoder.layers.10.bn2.running_mean', 'encoder.layers.10.bn2.running_var', 'encoder.layers.10.bn2.num_batches_tracked', 'encoder.layers.11.conv3x1_1.weight', 'encoder.layers.11.conv3x1_1.bias', 'encoder.layers.11.conv1x3_1.weight', 'encoder.layers.11.conv1x3_1.bias', 'encoder.layers.11.bn1.weight', 'encoder.layers.11.bn1.bias', 'encoder.layers.11.bn1.running_mean', 'encoder.layers.11.bn1.running_var', 'encoder.layers.11.bn1.num_batches_tracked', 'encoder.layers.11.conv3x1_2.weight', 'encoder.layers.11.conv3x1_2.bias', 'encoder.layers.11.conv1x3_2.weight', 'encoder.layers.11.conv1x3_2.bias', 'encoder.layers.11.bn2.weight', 'encoder.layers.11.bn2.bias', 'encoder.layers.11.bn2.running_mean', 'encoder.layers.11.bn2.running_var', 'encoder.layers.11.bn2.num_batches_tracked', 'encoder.layers.12.conv3x1_1.weight', 'encoder.layers.12.conv3x1_1.bias', 'encoder.layers.12.conv1x3_1.weight', 'encoder.layers.12.conv1x3_1.bias', 'encoder.layers.12.bn1.weight', 'encoder.layers.12.bn1.bias', 'encoder.layers.12.bn1.running_mean', 'encoder.layers.12.bn1.running_var', 'encoder.layers.12.bn1.num_batches_tracked', 'encoder.layers.12.conv3x1_2.weight', 'encoder.layers.12.conv3x1_2.bias', 'encoder.layers.12.conv1x3_2.weight', 'encoder.layers.12.conv1x3_2.bias', 'encoder.layers.12.bn2.weight', 'encoder.layers.12.bn2.bias', 'encoder.layers.12.bn2.running_mean', 'encoder.layers.12.bn2.running_var', 'encoder.layers.12.bn2.num_batches_tracked', 'encoder.layers.13.conv3x1_1.weight', 'encoder.layers.13.conv3x1_1.bias', 'encoder.layers.13.conv1x3_1.weight', 'encoder.layers.13.conv1x3_1.bias', 'encoder.layers.13.bn1.weight', 'encoder.layers.13.bn1.bias', 'encoder.layers.13.bn1.running_mean', 'encoder.layers.13.bn1.running_var', 'encoder.layers.13.bn1.num_batches_tracked', 'encoder.layers.13.conv3x1_2.weight', 'encoder.layers.13.conv3x1_2.bias', 'encoder.layers.13.conv1x3_2.weight', 'encoder.layers.13.conv1x3_2.bias', 'encoder.layers.13.bn2.weight', 'encoder.layers.13.bn2.bias', 'encoder.layers.13.bn2.running_mean', 'encoder.layers.13.bn2.running_var', 'encoder.layers.13.bn2.num_batches_tracked', 'encoder.layers.14.conv3x1_1.weight', 'encoder.layers.14.conv3x1_1.bias', 'encoder.layers.14.conv1x3_1.weight', 'encoder.layers.14.conv1x3_1.bias', 'encoder.layers.14.bn1.weight', 'encoder.layers.14.bn1.bias', 'encoder.layers.14.bn1.running_mean', 'encoder.layers.14.bn1.running_var', 'encoder.layers.14.bn1.num_batches_tracked', 'encoder.layers.14.conv3x1_2.weight', 'encoder.layers.14.conv3x1_2.bias', 'encoder.layers.14.conv1x3_2.weight', 'encoder.layers.14.conv1x3_2.bias', 'encoder.layers.14.bn2.weight', 'encoder.layers.14.bn2.bias', 'encoder.layers.14.bn2.running_mean', 'encoder.layers.14.bn2.running_var', 'encoder.layers.14.bn2.num_batches_tracked', 'encoder.output_conv.weight', 'encoder.output_conv.bias', 'decoder.loss_first_part.prototypes', 'decoder.loss_first_part.distance_scale', 'decoder.layers.0.conv.weight', 'decoder.layers.0.conv.bias', 'decoder.layers.0.bn.weight', 'decoder.layers.0.bn.bias', 'decoder.layers.0.bn.running_mean', 'decoder.layers.0.bn.running_var', 'decoder.layers.0.bn.num_batches_tracked', 'decoder.layers.1.conv3x1_1.weight', 'decoder.layers.1.conv3x1_1.bias', 'decoder.layers.1.conv1x3_1.weight', 'decoder.layers.1.conv1x3_1.bias', 'decoder.layers.1.bn1.weight', 'decoder.layers.1.bn1.bias', 'decoder.layers.1.bn1.running_mean', 'decoder.layers.1.bn1.running_var', 'decoder.layers.1.bn1.num_batches_tracked', 'decoder.layers.1.conv3x1_2.weight', 'decoder.layers.1.conv3x1_2.bias', 'decoder.layers.1.conv1x3_2.weight', 'decoder.layers.1.conv1x3_2.bias', 'decoder.layers.1.bn2.weight', 'decoder.layers.1.bn2.bias', 'decoder.layers.1.bn2.running_mean', 'decoder.layers.1.bn2.running_var', 'decoder.layers.1.bn2.num_batches_tracked', 'decoder.layers.2.conv3x1_1.weight', 'decoder.layers.2.conv3x1_1.bias', 'decoder.layers.2.conv1x3_1.weight', 'decoder.layers.2.conv1x3_1.bias', 'decoder.layers.2.bn1.weight', 'decoder.layers.2.bn1.bias', 'decoder.layers.2.bn1.running_mean', 'decoder.layers.2.bn1.running_var', 'decoder.layers.2.bn1.num_batches_tracked', 'decoder.layers.2.conv3x1_2.weight', 'decoder.layers.2.conv3x1_2.bias', 'decoder.layers.2.conv1x3_2.weight', 'decoder.layers.2.conv1x3_2.bias', 'decoder.layers.2.bn2.weight', 'decoder.layers.2.bn2.bias', 'decoder.layers.2.bn2.running_mean', 'decoder.layers.2.bn2.running_var', 'decoder.layers.2.bn2.num_batches_tracked', 'decoder.layers.3.conv.weight', 'decoder.layers.3.conv.bias', 'decoder.layers.3.bn.weight', 'decoder.layers.3.bn.bias', 'decoder.layers.3.bn.running_mean', 'decoder.layers.3.bn.running_var', 'decoder.layers.3.bn.num_batches_tracked', 'decoder.layers.4.conv3x1_1.weight', 'decoder.layers.4.conv3x1_1.bias', 'decoder.layers.4.conv1x3_1.weight', 'decoder.layers.4.conv1x3_1.bias', 'decoder.layers.4.bn1.weight', 'decoder.layers.4.bn1.bias', 'decoder.layers.4.bn1.running_mean', 'decoder.layers.4.bn1.running_var', 'decoder.layers.4.bn1.num_batches_tracked', 'decoder.layers.4.conv3x1_2.weight', 'decoder.layers.4.conv3x1_2.bias', 'decoder.layers.4.conv1x3_2.weight', 'decoder.layers.4.conv1x3_2.bias', 'decoder.layers.4.bn2.weight', 'decoder.layers.4.bn2.bias', 'decoder.layers.4.bn2.running_mean', 'decoder.layers.4.bn2.running_var', 'decoder.layers.4.bn2.num_batches_tracked', 'decoder.layers.5.conv3x1_1.weight', 'decoder.layers.5.conv3x1_1.bias', 'decoder.layers.5.conv1x3_1.weight', 'decoder.layers.5.conv1x3_1.bias', 'decoder.layers.5.bn1.weight', 'decoder.layers.5.bn1.bias', 'decoder.layers.5.bn1.running_mean', 'decoder.layers.5.bn1.running_var', 'decoder.layers.5.bn1.num_batches_tracked', 'decoder.layers.5.conv3x1_2.weight', 'decoder.layers.5.conv3x1_2.bias', 'decoder.layers.5.conv1x3_2.weight', 'decoder.layers.5.conv1x3_2.bias', 'decoder.layers.5.bn2.weight', 'decoder.layers.5.bn2.bias', 'decoder.layers.5.bn2.running_mean', 'decoder.layers.5.bn2.running_var', 'decoder.layers.5.bn2.num_batches_tracked', 'decoder.output_conv.weight', 'decoder.output_conv.bias'])\n",
            "updating: content/AnomalySegmentation/visualization/ (stored 0%)\n",
            "updating: content/AnomalySegmentation/visualization/MaxEntropy/ (stored 0%)\n",
            "updating: content/AnomalySegmentation/visualization/MaxEntropy/28_colored_score_image.png (deflated 17%)\n",
            "updating: content/AnomalySegmentation/visualization/MSP/ (stored 0%)\n",
            "updating: content/AnomalySegmentation/visualization/MSP/28_colored_score_image.png (deflated 11%)\n",
            "updating: content/AnomalySegmentation/visualization/MaxLogit/ (stored 0%)\n",
            "updating: content/AnomalySegmentation/visualization/MaxLogit/28_colored_score_image.png (deflated 28%)\n",
            "updating: content/AnomalySegmentation/visualization/MaxEntropy/58_colored_score_image.png (deflated 14%)\n",
            "updating: content/AnomalySegmentation/visualization/MSP/58_colored_score_image.png (deflated 10%)\n",
            "updating: content/AnomalySegmentation/visualization/MaxLogit/58_colored_score_image.png (deflated 24%)\n",
            "updating: content/AnomalySegmentation/visualization/MSP_temperature_0.75/ (stored 0%)\n",
            "updating: content/AnomalySegmentation/visualization/MSP_temperature_0.75/58_colored_score_image.png (deflated 9%)\n",
            "updating: content/AnomalySegmentation/visualization/MSP_temperature_0.5/ (stored 0%)\n",
            "updating: content/AnomalySegmentation/visualization/MSP_temperature_0.5/58_colored_score_image.png (deflated 7%)\n",
            "updating: content/AnomalySegmentation/visualization/MSP_temperature_1.1/ (stored 0%)\n",
            "updating: content/AnomalySegmentation/visualization/MSP_temperature_1.1/58_colored_score_image.png (deflated 10%)\n",
            "updating: content/AnomalySegmentation/visualization/loss_CrossEntropy_MSP/ (stored 0%)\n",
            "updating: content/AnomalySegmentation/visualization/loss_CrossEntropy_MSP/58_colored_score_image.png (deflated 10%)\n",
            "updating: content/AnomalySegmentation/visualization/baseline_MSP/ (stored 0%)\n",
            "updating: content/AnomalySegmentation/visualization/baseline_MSP/58_colored_score_image.png (deflated 10%)\n",
            "updating: content/AnomalySegmentation/visualization/loss_CrossEntropy_MaxLogit/ (stored 0%)\n",
            "updating: content/AnomalySegmentation/visualization/loss_CrossEntropy_MaxLogit/58_colored_score_image.png (deflated 24%)\n",
            "updating: content/AnomalySegmentation/visualization/baseline_MaxLogit/ (stored 0%)\n",
            "updating: content/AnomalySegmentation/visualization/baseline_MaxLogit/58_colored_score_image.png (deflated 24%)\n",
            "updating: content/AnomalySegmentation/visualization/baseline_MaxEntropy/ (stored 0%)\n",
            "updating: content/AnomalySegmentation/visualization/baseline_MaxEntropy/58_colored_score_image.png (deflated 14%)\n",
            "updating: content/AnomalySegmentation/visualization/temperature_0.5/ (stored 0%)\n",
            "updating: content/AnomalySegmentation/visualization/temperature_0.5/58_colored_score_image.png (deflated 7%)\n",
            "updating: content/AnomalySegmentation/visualization/temperature_1.1/ (stored 0%)\n",
            "updating: content/AnomalySegmentation/visualization/temperature_1.1/58_colored_score_image.png (deflated 10%)\n",
            "updating: content/AnomalySegmentation/visualization/loss_Focal_MSP/ (stored 0%)\n",
            "updating: content/AnomalySegmentation/visualization/loss_Focal_MSP/58_colored_score_image.png (deflated 10%)\n",
            "updating: content/AnomalySegmentation/visualization/void_enet/ (stored 0%)\n",
            "updating: content/AnomalySegmentation/visualization/void_enet/58_colored_score_image.png (deflated 3%)\n",
            "updating: content/AnomalySegmentation/visualization/void_erfnet/ (stored 0%)\n",
            "updating: content/AnomalySegmentation/visualization/void_erfnet/58_colored_score_image.png (deflated 35%)\n",
            "updating: content/AnomalySegmentation/visualization/void_bisenet/ (stored 0%)\n",
            "updating: content/AnomalySegmentation/visualization/void_bisenet/58_colored_score_image.png (deflated 46%)\n",
            "updating: content/AnomalySegmentation/visualization/loss_Focal_MaxEntropy/ (stored 0%)\n",
            "updating: content/AnomalySegmentation/visualization/loss_Focal_MaxEntropy/58_colored_score_image.png (deflated 15%)\n",
            "updating: content/AnomalySegmentation/visualization/loss_LogitNorm_MaxEntropy/ (stored 0%)\n",
            "updating: content/AnomalySegmentation/visualization/loss_LogitNorm_MaxEntropy/58_colored_score_image.png (deflated 15%)\n",
            "updating: content/AnomalySegmentation/visualization/loss_LogitNorm_MaxLogit/ (stored 0%)\n",
            "updating: content/AnomalySegmentation/visualization/loss_LogitNorm_MaxLogit/58_colored_score_image.png (deflated 23%)\n",
            "updating: content/AnomalySegmentation/visualization/loss_CrossEntropy_MaxEntropy/ (stored 0%)\n",
            "updating: content/AnomalySegmentation/visualization/loss_CrossEntropy_MaxEntropy/58_colored_score_image.png (deflated 14%)\n",
            "updating: content/AnomalySegmentation/visualization/loss_LogitNorm_MSP/ (stored 0%)\n",
            "updating: content/AnomalySegmentation/visualization/loss_LogitNorm_MSP/58_colored_score_image.png (deflated 10%)\n",
            "updating: content/AnomalySegmentation/visualization/loss_Focal_MaxLogit/ (stored 0%)\n",
            "updating: content/AnomalySegmentation/visualization/loss_Focal_MaxLogit/58_colored_score_image.png (deflated 24%)\n",
            "  adding: content/AnomalySegmentation/visualization/temperature_0.75/ (stored 0%)\n",
            "  adding: content/AnomalySegmentation/visualization/temperature_0.75/58_colored_score_image.png (deflated 9%)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "vNdTJZh4IP7G"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}