{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypo8OBRZ-1p3"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/RonPlusSign/AnomalySegmentation/blob/feat%2Flosses/Project6.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUjRrYEW8-uz"
      },
      "source": [
        "# **Anomaly Segmentation Project 6**\n",
        "##*Andrea Delli, Christian Dellisanti, Giorgia Modi*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4x3MajLNeXhX"
      },
      "source": [
        "##**Dataset Preparation**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/AnomalySegmentation\n",
        "!rm -rf AnomalySegmentation"
      ],
      "metadata": {
        "id": "yorO9_xVX2bJ"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_tj8W3BeVPo",
        "outputId": "cb5eea30-c89d-44d1-8299-f32c41848db8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'AnomalySegmentation'...\n",
            "remote: Enumerating objects: 1667, done.\u001b[K\n",
            "remote: Counting objects: 100% (258/258), done.\u001b[K\n",
            "remote: Compressing objects: 100% (140/140), done.\u001b[K\n",
            "remote: Total 1667 (delta 185), reused 184 (delta 116), pack-reused 1409 (from 2)\u001b[K\n",
            "Receiving objects: 100% (1667/1667), 294.23 MiB | 34.80 MiB/s, done.\n",
            "Resolving deltas: 100% (1153/1153), done.\n",
            "Updating files: 100% (69/69), done.\n",
            "Already up to date.\n"
          ]
        }
      ],
      "source": [
        "!pip  install -q numpy matplotlib Pillow torchvision visdom ood_metrics icecream cityscapesscripts tqdm #triton\n",
        "\n",
        "import sys, os\n",
        "if not os.path.isfile('/content/Validation_Dataset.zip'):\n",
        "  !gdown 12YJq48XkCxQHjN3CmLc-zM5dThSak4Ta\n",
        "if not os.path.isdir('/content/Validation_Dataset'):\n",
        "  !unzip -q Validation_Dataset.zip\n",
        "if not os.path.isdir('/content/AnomalySegmentation'):\n",
        "  #!git clone https://github.com/shyam671/AnomalySegmentation_CourseProjectBaseCode.git\n",
        "  #token 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\n",
        "  !git clone -b mahalanobis_2 https://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@github.com/RonPlusSign/AnomalySegmentation.git\n",
        "!cd /content/AnomalySegmentation && git pull\n",
        "#!cd /content/AnomalySegmentation && git checkout main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAmA1igJhHhR"
      },
      "source": [
        "##**mIoU**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3w0jewGkphY",
        "outputId": "5b605689-c4a8-4101-e7f2-1f31ae9664f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1-fjLAk4_-GkixW1-GP_cYDBUmhnVbApL\n",
            "From (redirected): https://drive.google.com/uc?id=1-fjLAk4_-GkixW1-GP_cYDBUmhnVbApL&confirm=t&uuid=732ea457-2830-48a2-b41e-fdda9867e68f\n",
            "To: /content/cityscapes.zip\n",
            "100% 11.9G/11.9G [03:00<00:00, 65.8MB/s]\n"
          ]
        }
      ],
      "source": [
        "import  os\n",
        "# s306027@studenti.polito.it\n",
        "# %mR+g$L\\~5U03O9)IZ-_\n",
        "# Per Eseguire tutto ci mette 23 min sia CPU che GPU\n",
        "createLabel = True\n",
        "fast_download = True\n",
        "super_fast_download = True\n",
        "if super_fast_download:\n",
        "  !gdown 1-fjLAk4_-GkixW1-GP_cYDBUmhnVbApL\n",
        "  !unzip -q cityscapes.zip\n",
        "  !mv  ./content/cityscapes /content/cityscapes\n",
        "  !rm -rf ./content\n",
        "else:\n",
        "  if not os.path.isdir('/content/cityscapes'):\n",
        "    !mkdir /content/cityscapes\n",
        "\n",
        "  if not os.path.isfile('/content/cityscapes/gtFine_trainvaltest.zip'):\n",
        "    if fast_download:\n",
        "      !gdown 1J31rnVd33GBt-IYGYqC9mv73q7vc55pw -O /content/cityscapes/\n",
        "    else:\n",
        "      !csDownload gtFine_trainvaltest.zip -d /content/cityscapes/\n",
        "\n",
        "  if not os.path.isdir('/content/cityscapes/gtFine'):\n",
        "    !unzip -q /content/cityscapes/gtFine_trainvaltest.zip -d /content/cityscapes/\n",
        "    createLabel = True\n",
        "    !rm /content/cityscapes/README\n",
        "    !rm /content/cityscapes/license.txt\n",
        "\n",
        "\n",
        "  if not os.path.isfile('/content/cityscapes/leftImg8bit_trainvaltest.zip'):\n",
        "    if fast_download:\n",
        "      #https://drive.google.com/file/d/1m8Y3Zc6vG11Q9SxW7Be5EGXTDq4s4RlJ/view?usp=sharing\n",
        "      !gdown 1m8Y3Zc6vG11Q9SxW7Be5EGXTDq4s4RlJ -O /content/cityscapes/\n",
        "    else:\n",
        "      !csDownload leftImg8bit_trainvaltest.zip -d /content/cityscapes/\n",
        "\n",
        "\n",
        "  if not os.path.isdir('/content/cityscapes/leftImg8bit'):\n",
        "    !unzip -q /content/cityscapes/leftImg8bit_trainvaltest.zip -d /content/cityscapes/\n",
        "    createLabel = True\n",
        "    !rm /content/cityscapes/README\n",
        "    !rm /content/cityscapes/license.txt\n",
        "\n",
        "  if createLabel:\n",
        "    os.environ['CITYSCAPES_DATASET'] = '/content/cityscapes/'\n",
        "    !csCreateTrainIdLabelImgs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAeuewkbhM3p",
        "outputId": "91464ffe-a919-48c1-cf3e-c3a79a673dcb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "498 val/munster/munster_000172_000019_leftImg8bit.png\n",
            "499 val/munster/munster_000173_000019_leftImg8bit.png\n",
            "-------------MSP-------------------\n",
            "---------------------------------------\n",
            "Took  80.77754092216492 seconds\n",
            "=======================================\n",
            "Per-Class IoU:\n",
            "\u001b[0m97.62\u001b[0m Road\n",
            "\u001b[0m81.37\u001b[0m sidewalk\n",
            "\u001b[0m90.77\u001b[0m building\n",
            "\u001b[0m49.43\u001b[0m wall\n",
            "\u001b[0m54.93\u001b[0m fence\n",
            "\u001b[0m60.81\u001b[0m pole\n",
            "\u001b[0m62.60\u001b[0m traffic light\n",
            "\u001b[0m72.32\u001b[0m traffic sign\n",
            "\u001b[0m91.35\u001b[0m vegetation\n",
            "\u001b[0m60.97\u001b[0m terrain\n",
            "\u001b[0m93.38\u001b[0m sky\n",
            "\u001b[0m76.11\u001b[0m person\n",
            "\u001b[0m53.45\u001b[0m rider\n",
            "\u001b[0m92.91\u001b[0m car\n",
            "\u001b[0m72.78\u001b[0m truck\n",
            "\u001b[0m78.87\u001b[0m bus\n",
            "\u001b[0m63.86\u001b[0m train\n",
            "\u001b[0m46.41\u001b[0m motorcycle\n",
            "\u001b[0m71.89\u001b[0m bicycle\n",
            "=======================================\n",
            "MEAN IoU:  \u001b[0m72.20\u001b[0m %\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "# ci mette 7 min con la GPU\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  !python -W ignore /content/AnomalySegmentation/eval/eval_iou.py --loadDir /content/AnomalySegmentation/trained_models/ --datadir /content/cityscapes/  | tail -n 28\n",
        "else:\n",
        "  !python -W ignore /content/AnomalySegmentation/eval/eval_iou.py  --loadDir  /content/AnomalySegmentation/trained_models/ --datadir /content/cityscapes/  --cpu | tail -n 28"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RZTrDS4Mysu"
      },
      "source": [
        "##**Anomaly Inference**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9afwM8zdM7_l",
        "outputId": "80d0aaaa-35f3-4bb8-a9fb-b46181f31866"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------\n",
            "\n",
            "Dataset: RoadAnomaly21 method: MSP\n",
            "AUPRC score: 29.100168300581203\n",
            "FPR@TPR95: 62.51075321069286\n",
            "\n",
            "Dataset: RoadAnomaly21 method: MaxLogit\n",
            "AUPRC score: 38.31957797222208\n",
            "FPR@TPR95: 59.3370558914899\n",
            "\n",
            "Dataset: RoadAnomaly21 method: MaxEntropy\n",
            "AUPRC score: 31.005102648344756\n",
            "FPR@TPR95: 62.593151130093226\n",
            "----------------------------\n",
            "\n",
            "Dataset: RoadObsticle21 method: MSP\n",
            "AUPRC score: 2.7116243119338366\n",
            "FPR@TPR95: 64.9739786894368\n",
            "\n",
            "Dataset: RoadObsticle21 method: MaxLogit\n",
            "AUPRC score: 4.626567617520253\n",
            "FPR@TPR95: 48.443439151949555\n",
            "\n",
            "Dataset: RoadObsticle21 method: MaxEntropy\n",
            "AUPRC score: 3.051560023478638\n",
            "FPR@TPR95: 65.59968252759046\n",
            "----------------------------\n",
            "\n",
            "Dataset: FS_LostFound_full method: MSP\n",
            "AUPRC score: 1.747872547607269\n",
            "FPR@TPR95: 50.76348570192957\n",
            "\n",
            "Dataset: FS_LostFound_full method: MaxLogit\n",
            "AUPRC score: 3.3014401015087245\n",
            "FPR@TPR95: 45.494876929038305\n",
            "\n",
            "Dataset: FS_LostFound_full method: MaxEntropy\n",
            "AUPRC score: 2.581709137723009\n",
            "FPR@TPR95: 50.368099783135676\n",
            "----------------------------\n",
            "\n",
            "Dataset: fs_static method: MSP\n",
            "AUPRC score: 7.4700433549050915\n",
            "FPR@TPR95: 41.82346831776172\n",
            "\n",
            "Dataset: fs_static method: MaxLogit\n",
            "AUPRC score: 9.498677970785756\n",
            "FPR@TPR95: 40.3000747567442\n",
            "\n",
            "Dataset: fs_static method: MaxEntropy\n",
            "AUPRC score: 8.82636607633996\n",
            "FPR@TPR95: 41.52332673090571\n",
            "----------------------------\n",
            "\n",
            "Dataset: RoadAnomaly method: MSP\n",
            "AUPRC score: 12.426265849563665\n",
            "FPR@TPR95: 82.49244029880458\n",
            "\n",
            "Dataset: RoadAnomaly method: MaxLogit\n",
            "AUPRC score: 15.581983301641019\n",
            "FPR@TPR95: 73.24766535735604\n",
            "\n",
            "Dataset: RoadAnomaly method: MaxEntropy\n",
            "AUPRC score: 12.678035094227063\n",
            "FPR@TPR95: 82.63192451735861\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "no_execute = False\n",
        "just_once = False\n",
        "\n",
        "for dataset_dir in ['RoadAnomaly21', 'RoadObsticle21', 'FS_LostFound_full', 'fs_static', 'RoadAnomaly']:\n",
        "  print(\"----------------------------\")\n",
        "  for method in [\"MSP\", \"MaxLogit\", \"MaxEntropy\"]:\n",
        "\n",
        "    if no_execute:\n",
        "      break\n",
        "\n",
        "    format_file = os.listdir(f'/content/Validation_Dataset/{dataset_dir}/images')[0].split(\".\")[1]\n",
        "    input =f'/content/Validation_Dataset/{dataset_dir}/images/\\*.{format_file}'\n",
        "    print(f\"\\nDataset: {dataset_dir} method: {method}\")\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "      !python -W ignore /content/AnomalySegmentation/eval/evalAnomaly.py --input {input} --method  {method}  | tail -n 2\n",
        "    else:\n",
        "      !python -W ignore /content/AnomalySegmentation/eval/evalAnomaly.py --input {input} --method {method}  --cpu | tail -n 2\n",
        "\n",
        "    if just_once:\n",
        "      no_execute = True\n",
        "      just_once = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhQWIx8rklfO"
      },
      "source": [
        "##**Temperature Scaling**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7DsE7oO1n9G"
      },
      "source": [
        "**Anomaly Inference with temperature**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Zu-dIEeqFLq3",
        "outputId": "47825080-3d12-479f-c2c2-51ca15ecab1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------\n",
            "\n",
            "Dataset: RoadAnomaly21 method : MSP Temperature: 0.5\n",
            "AUPRC score: 27.060833635879618\n",
            "FPR@TPR95: 62.730810427606734\n",
            "\n",
            "Dataset: RoadAnomaly21 method : MSP Temperature: 0.75\n",
            "AUPRC score: 28.156063054348103\n",
            "FPR@TPR95: 62.478737323984326\n",
            "\n",
            "Dataset: RoadAnomaly21 method : MSP Temperature: 1.1\n",
            "AUPRC score: 29.40955379121979\n",
            "FPR@TPR95: 62.58986549662704\n",
            "----------------------------\n",
            "\n",
            "Dataset: RoadObsticle21 method : MSP Temperature: 0.5\n",
            "AUPRC score: 2.4195519558429823\n",
            "FPR@TPR95: 63.22544524787239\n",
            "\n",
            "Dataset: RoadObsticle21 method : MSP Temperature: 0.75\n",
            "AUPRC score: 2.5668802249367677\n",
            "FPR@TPR95: 64.05285534718263\n",
            "\n",
            "Dataset: RoadObsticle21 method : MSP Temperature: 1.1\n",
            "AUPRC score: 2.7658075767433776\n",
            "FPR@TPR95: 65.52358106228223\n",
            "----------------------------\n",
            "\n",
            "Dataset: FS_LostFound_full method : MSP Temperature: 0.5\n",
            "AUPRC score: 1.2802500246431052\n",
            "FPR@TPR95: 66.73710676943257\n",
            "\n",
            "Dataset: FS_LostFound_full method : MSP Temperature: 0.75\n",
            "AUPRC score: 1.4927065686510383\n",
            "FPR@TPR95: 51.848262648332636\n",
            "\n",
            "Dataset: FS_LostFound_full method : MSP Temperature: 1.1\n",
            "AUPRC score: 1.8596703140506141\n",
            "FPR@TPR95: 50.38650128754133\n",
            "----------------------------\n",
            "\n",
            "Dataset: fs_static method : MSP Temperature: 0.5\n",
            "AUPRC score: 6.6011970066164665\n",
            "FPR@TPR95: 43.47565874225287\n",
            "\n",
            "Dataset: fs_static method : MSP Temperature: 0.75\n",
            "AUPRC score: 6.99079114995491\n",
            "FPR@TPR95: 42.49329123307483\n",
            "\n",
            "Dataset: fs_static method : MSP Temperature: 1.1\n",
            "AUPRC score: 7.686696846804934\n",
            "FPR@TPR95: 41.586844199987\n",
            "----------------------------\n",
            "\n",
            "Dataset: RoadAnomaly method : MSP Temperature: 0.5\n",
            "AUPRC score: 12.187681345765725\n",
            "FPR@TPR95: 82.02224728951396\n",
            "\n",
            "Dataset: RoadAnomaly method : MSP Temperature: 0.75\n",
            "AUPRC score: 12.319186617225913\n",
            "FPR@TPR95: 82.28451947325927\n",
            "\n",
            "Dataset: RoadAnomaly method : MSP Temperature: 1.1\n",
            "AUPRC score: 12.465779148190585\n",
            "FPR@TPR95: 82.62125003163526\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "no_execute = False\n",
        "just_once = False\n",
        "for dataset_dir in ['RoadAnomaly21', 'RoadObsticle21', 'FS_LostFound_full', 'fs_static', 'RoadAnomaly']:\n",
        "  print(\"----------------------------\")\n",
        "  for t in [0.5, 0.75, 1.1]:\n",
        "    if no_execute:\n",
        "        break\n",
        "\n",
        "    format_file = os.listdir(f'/content/Validation_Dataset/{dataset_dir}/images')[0].split(\".\")[1]\n",
        "    input =f'/content/Validation_Dataset/{dataset_dir}/images/\\*.{format_file}'\n",
        "    print(f\"\\nDataset: {dataset_dir}, method: MSP, Temperature: {t}\")\n",
        "    if torch.cuda.is_available():\n",
        "      !python -W ignore /content/AnomalySegmentation/eval/evalAnomaly.py --input {input} --method 'MSP' --temperature {t} | tail -n 2\n",
        "    else:\n",
        "      !python -W ignore /content/AnomalySegmentation/eval/evalAnomaly.py --input {input} --method 'MSP' --cpu --temperature {t} | tail -n 2\n",
        "\n",
        "    if just_once:\n",
        "      no_execute = True\n",
        "      just_once = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nY-OAYlIjGaG"
      },
      "source": [
        "## **Void Classifier**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine-Tuning"
      ],
      "metadata": {
        "id": "vNdTJZh4IP7G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####ERFNET"
      ],
      "metadata": {
        "id": "LYpg0U39MrDL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQhmfT3zjJcG",
        "outputId": "7a42935a-0014-4f51-acca-c8057510b8a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Import Model erfnet with weights erfnet_pretrained.pth to FineTune\n",
            "========== DECODER TRAINING ===========\n",
            "/content/cityscapes/leftImg8bit/train\n",
            "/content/cityscapes/leftImg8bit/val\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/AnomalySegmentation/train/main.py\", line 604, in <module>\n",
            "    main(parser.parse_args())\n",
            "  File \"/content/AnomalySegmentation/train/main.py\", line 566, in main\n",
            "    model = train(args, model, False)   #Train decoder\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py\", line 465, in _fn\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/content/AnomalySegmentation/train/main.py\", line 150, in train\n",
            "    assert os.path.exists(args.datadir), \"Error: datadir (dataset directory) could not be loaded\"\n",
            "  File \"/content/AnomalySegmentation/train/main.py\", line 157, in torch_dynamo_resume_in_train_at_150\n",
            "    dataset_train = cityscapes(args.datadir, co_transform, 'train')\n",
            "  File \"/content/AnomalySegmentation/train/main.py\", line 158, in torch_dynamo_resume_in_train_at_157\n",
            "    dataset_val = cityscapes(args.datadir, co_transform_val, 'val')\n",
            "  File \"/content/AnomalySegmentation/train/main.py\", line 164, in torch_dynamo_resume_in_train_at_158\n",
            "    loader = DataLoader(dataset_train, num_workers=args.num_workers, batch_size=args.batch_size, shuffle=True)\n",
            "  File \"/content/AnomalySegmentation/train/main.py\", line 165, in torch_dynamo_resume_in_train_at_164\n",
            "    loader_val = DataLoader(dataset_val, num_workers=args.num_workers, batch_size=args.batch_size, shuffle=False)\n",
            "  File \"/content/AnomalySegmentation/train/main.py\", line 169, in torch_dynamo_resume_in_train_at_165\n",
            "    criterion = CrossEntropyLoss2d(weight)\n",
            "  File \"/content/AnomalySegmentation/train/main.py\", line 78, in __init__\n",
            "    self.loss = torch.nn.NLLLoss2d(weight)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/typing_extensions.py\", line 2811, in __new__\n",
            "    warnings.warn(msg, category=category, stacklevel=stacklevel + 1)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/typing_extensions.py\", line 2813, in torch_dynamo_resume_in___new___at_2811\n",
            "    return original_new(cls, *args, **kwargs)\n",
            "TypeError: object.__new__() takes exactly one argument (the type to instantiate)\n"
          ]
        }
      ],
      "source": [
        "# Fine tune ERFNet (10 epochs)\n",
        "!cd /content/AnomalySegmentation/train; python -W ignore main.py --savedir erfnet_training1 --datadir /content/cityscapes --model erfnet --cuda --num-epochs=10 --epochs-save=1 --FineTune --decoder --state=/content/AnomalySegmentation/trained_models/erfnet_pretrained.pth --loadWeights=erfnet_pretrained.pth"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r save_erfnet_training1.zip /content/AnomalySegmentation/save/erfnet_training1"
      ],
      "metadata": {
        "id": "N1UvkfWcM1ob",
        "outputId": "ba6f76b8-ba99-4a75-bfde-63692cc68dcc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/AnomalySegmentation/save/erfnet_training1/ (stored 0%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training1/model_best.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training1/model-002.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training1/automated_log.txt (deflated 63%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training1/model-001.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training1/model-003.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training1/model-006.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training1/model-010.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training1/model.txt (deflated 92%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training1/model-005.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training1/model-007.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training1/best.txt (stored 0%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training1/model_best.pth.tar (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training1/opts.txt (deflated 40%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training1/model-008.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training1/model-009.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training1/checkpoint.pth.tar (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training1/erfnet.py (deflated 78%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training1/model-004.pth (deflated 10%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine tune ERFNet (20 epochs)\n",
        "!cd /content/AnomalySegmentation/train; python -W ignore main.py --savedir erfnet_training2 --datadir /content/cityscapes --model erfnet --cuda --num-epochs=20 --epochs-save=1 --FineTune --decoder --state=/content/AnomalySegmentation/trained_models/erfnet_pretrained.pth --loadWeights=erfnet_pretrained.pth\n",
        "!zip -r save_erfnet_training2.zip /content/AnomalySegmentation/save/erfnet_training2"
      ],
      "metadata": {
        "id": "wSJMvmnGLtsU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d51917f5-33dc-4e45-fd58-6f49c9fdbbe2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Import Model erfnet with weights erfnet_pretrained.pth to FineTune\n",
            "========== DECODER TRAINING ===========\n",
            "/content/cityscapes/leftImg8bit/train\n",
            "/content/cityscapes/leftImg8bit/val\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/AnomalySegmentation/train/main.py\", line 604, in <module>\n",
            "    main(parser.parse_args())\n",
            "  File \"/content/AnomalySegmentation/train/main.py\", line 566, in main\n",
            "    model = train(args, model, False)   #Train decoder\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py\", line 465, in _fn\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/content/AnomalySegmentation/train/main.py\", line 150, in train\n",
            "    assert os.path.exists(args.datadir), \"Error: datadir (dataset directory) could not be loaded\"\n",
            "  File \"/content/AnomalySegmentation/train/main.py\", line 157, in torch_dynamo_resume_in_train_at_150\n",
            "    dataset_train = cityscapes(args.datadir, co_transform, 'train')\n",
            "  File \"/content/AnomalySegmentation/train/main.py\", line 158, in torch_dynamo_resume_in_train_at_157\n",
            "    dataset_val = cityscapes(args.datadir, co_transform_val, 'val')\n",
            "  File \"/content/AnomalySegmentation/train/main.py\", line 164, in torch_dynamo_resume_in_train_at_158\n",
            "    loader = DataLoader(dataset_train, num_workers=args.num_workers, batch_size=args.batch_size, shuffle=True)\n",
            "  File \"/content/AnomalySegmentation/train/main.py\", line 165, in torch_dynamo_resume_in_train_at_164\n",
            "    loader_val = DataLoader(dataset_val, num_workers=args.num_workers, batch_size=args.batch_size, shuffle=False)\n",
            "  File \"/content/AnomalySegmentation/train/main.py\", line 169, in torch_dynamo_resume_in_train_at_165\n",
            "    criterion = CrossEntropyLoss2d(weight)\n",
            "  File \"/content/AnomalySegmentation/train/main.py\", line 78, in __init__\n",
            "    self.loss = torch.nn.NLLLoss2d(weight)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/typing_extensions.py\", line 2811, in __new__\n",
            "    warnings.warn(msg, category=category, stacklevel=stacklevel + 1)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/typing_extensions.py\", line 2813, in torch_dynamo_resume_in___new___at_2811\n",
            "    return original_new(cls, *args, **kwargs)\n",
            "TypeError: object.__new__() takes exactly one argument (the type to instantiate)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training2/ (stored 0%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training2/opts.txt (deflated 40%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training2/erfnet.py (deflated 78%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####ENET"
      ],
      "metadata": {
        "id": "G3aC1UuCMz9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/AnomalySegmentation && git pull"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WN6v6mncLAfK",
        "outputId": "f06c3363-193b-49ef-aa58-1713b6f4f283",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "remote: Enumerating objects: 7, done.\u001b[K\n",
            "remote: Counting objects:  14% (1/7)\u001b[K\rremote: Counting objects:  28% (2/7)\u001b[K\rremote: Counting objects:  42% (3/7)\u001b[K\rremote: Counting objects:  57% (4/7)\u001b[K\rremote: Counting objects:  71% (5/7)\u001b[K\rremote: Counting objects:  85% (6/7)\u001b[K\rremote: Counting objects: 100% (7/7)\u001b[K\rremote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1/1)\u001b[K\rremote: Compressing objects: 100% (1/1), done.\u001b[K\n",
            "remote: Total 4 (delta 3), reused 4 (delta 3), pack-reused 0 (from 0)\u001b[K\n",
            "Unpacking objects:  25% (1/4)\rUnpacking objects:  50% (2/4)\rUnpacking objects:  75% (3/4)\rUnpacking objects: 100% (4/4)\rUnpacking objects: 100% (4/4), 375 bytes | 187.00 KiB/s, done.\n",
            "From https://github.com/RonPlusSign/AnomalySegmentation\n",
            "   5bbfd1c..325b655  main       -> origin/main\n",
            "Updating 5bbfd1c..325b655\n",
            "Fast-forward\n",
            " train/main.py | 3 \u001b[32m++\u001b[m\u001b[31m-\u001b[m\n",
            " 1 file changed, 2 insertions(+), 1 deletion(-)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/AnomalySegmentation/save/enet_training1"
      ],
      "metadata": {
        "id": "QqBtvzdxogPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip save_enet_training1.zip\n"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3OzWiqtlvaL",
        "outputId": "af8ced29-b8e3-46c6-9203-f92e0c9fa40e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  save_enet_training1.zip\n",
            "   creating: content/AnomalySegmentation/save/enet_training1/\n",
            "  inflating: content/AnomalySegmentation/save/enet_training1/automated_log.txt  \n",
            "  inflating: content/AnomalySegmentation/save/enet_training1/model.txt  \n",
            "  inflating: content/AnomalySegmentation/save/enet_training1/enet.py  \n",
            "  inflating: content/AnomalySegmentation/save/enet_training1/opts.txt  \n",
            "  inflating: content/AnomalySegmentation/save/enet_training1/model_best.pth  \n",
            "  inflating: content/AnomalySegmentation/save/enet_training1/model-002.pth  \n",
            "  inflating: content/AnomalySegmentation/save/enet_training1/model-001.pth  \n",
            "  inflating: content/AnomalySegmentation/save/enet_training1/model-003.pth  \n",
            "  inflating: content/AnomalySegmentation/save/enet_training1/model-006.pth  \n",
            "  inflating: content/AnomalySegmentation/save/enet_training1/model-010.pth  \n",
            "  inflating: content/AnomalySegmentation/save/enet_training1/model-005.pth  \n",
            "  inflating: content/AnomalySegmentation/save/enet_training1/model-007.pth  \n",
            " extracting: content/AnomalySegmentation/save/enet_training1/best.txt  \n",
            "  inflating: content/AnomalySegmentation/save/enet_training1/model_best.pth.tar  \n",
            "  inflating: content/AnomalySegmentation/save/enet_training1/model-008.pth  \n",
            "  inflating: content/AnomalySegmentation/save/enet_training1/model-009.pth  \n",
            "  inflating: content/AnomalySegmentation/save/enet_training1/checkpoint.pth.tar  \n",
            "  inflating: content/AnomalySegmentation/save/enet_training1/model-004.pth  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r ./content/AnomalySegmentation/save/enet_training1 /content/AnomalySegmentation/save/enet_training1"
      ],
      "metadata": {
        "id": "NTfUYQdPo64G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp  /content/AnomalySegmentation/save/enet_training1/checkpoint.pth.tar /content/AnomalySegmentation/save/checkpoint.pth.tar"
      ],
      "metadata": {
        "id": "a7Uls0tKrROn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FineTune ENet (20 epochs)\n",
        "!cd /content/AnomalySegmentation/train; python -W ignore main.py --savedir enet_training1 --datadir /content/cityscapes --model enet  --num-epochs=20 --epochs-save=1 --cuda --FineTune --loadWeights=enet_pretrained\n",
        "!zip -r save_enet_training1.zip /content/AnomalySegmentation/save/enet_training1"
      ],
      "metadata": {
        "id": "9_1Uo-kgXFGK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cda0a369-7b7c-42be-9211-0f1ee2ad9036"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "odict_keys(['initial_block.main_branch.weight', 'initial_block.batch_norm.weight', 'initial_block.batch_norm.bias', 'initial_block.batch_norm.running_mean', 'initial_block.batch_norm.running_var', 'initial_block.batch_norm.num_batches_tracked', 'initial_block.out_activation.weight', 'downsample1_0.ext_conv1.0.weight', 'downsample1_0.ext_conv1.1.weight', 'downsample1_0.ext_conv1.1.bias', 'downsample1_0.ext_conv1.1.running_mean', 'downsample1_0.ext_conv1.1.running_var', 'downsample1_0.ext_conv1.1.num_batches_tracked', 'downsample1_0.ext_conv1.2.weight', 'downsample1_0.ext_conv2.0.weight', 'downsample1_0.ext_conv2.1.weight', 'downsample1_0.ext_conv2.1.bias', 'downsample1_0.ext_conv2.1.running_mean', 'downsample1_0.ext_conv2.1.running_var', 'downsample1_0.ext_conv2.1.num_batches_tracked', 'downsample1_0.ext_conv2.2.weight', 'downsample1_0.ext_conv3.0.weight', 'downsample1_0.ext_conv3.1.weight', 'downsample1_0.ext_conv3.1.bias', 'downsample1_0.ext_conv3.1.running_mean', 'downsample1_0.ext_conv3.1.running_var', 'downsample1_0.ext_conv3.1.num_batches_tracked', 'downsample1_0.ext_conv3.2.weight', 'downsample1_0.out_activation.weight', 'regular1_1.ext_conv1.0.weight', 'regular1_1.ext_conv1.1.weight', 'regular1_1.ext_conv1.1.bias', 'regular1_1.ext_conv1.1.running_mean', 'regular1_1.ext_conv1.1.running_var', 'regular1_1.ext_conv1.1.num_batches_tracked', 'regular1_1.ext_conv1.2.weight', 'regular1_1.ext_conv2.0.weight', 'regular1_1.ext_conv2.1.weight', 'regular1_1.ext_conv2.1.bias', 'regular1_1.ext_conv2.1.running_mean', 'regular1_1.ext_conv2.1.running_var', 'regular1_1.ext_conv2.1.num_batches_tracked', 'regular1_1.ext_conv2.2.weight', 'regular1_1.ext_conv3.0.weight', 'regular1_1.ext_conv3.1.weight', 'regular1_1.ext_conv3.1.bias', 'regular1_1.ext_conv3.1.running_mean', 'regular1_1.ext_conv3.1.running_var', 'regular1_1.ext_conv3.1.num_batches_tracked', 'regular1_1.ext_conv3.2.weight', 'regular1_1.out_activation.weight', 'regular1_2.ext_conv1.0.weight', 'regular1_2.ext_conv1.1.weight', 'regular1_2.ext_conv1.1.bias', 'regular1_2.ext_conv1.1.running_mean', 'regular1_2.ext_conv1.1.running_var', 'regular1_2.ext_conv1.1.num_batches_tracked', 'regular1_2.ext_conv1.2.weight', 'regular1_2.ext_conv2.0.weight', 'regular1_2.ext_conv2.1.weight', 'regular1_2.ext_conv2.1.bias', 'regular1_2.ext_conv2.1.running_mean', 'regular1_2.ext_conv2.1.running_var', 'regular1_2.ext_conv2.1.num_batches_tracked', 'regular1_2.ext_conv2.2.weight', 'regular1_2.ext_conv3.0.weight', 'regular1_2.ext_conv3.1.weight', 'regular1_2.ext_conv3.1.bias', 'regular1_2.ext_conv3.1.running_mean', 'regular1_2.ext_conv3.1.running_var', 'regular1_2.ext_conv3.1.num_batches_tracked', 'regular1_2.ext_conv3.2.weight', 'regular1_2.out_activation.weight', 'regular1_3.ext_conv1.0.weight', 'regular1_3.ext_conv1.1.weight', 'regular1_3.ext_conv1.1.bias', 'regular1_3.ext_conv1.1.running_mean', 'regular1_3.ext_conv1.1.running_var', 'regular1_3.ext_conv1.1.num_batches_tracked', 'regular1_3.ext_conv1.2.weight', 'regular1_3.ext_conv2.0.weight', 'regular1_3.ext_conv2.1.weight', 'regular1_3.ext_conv2.1.bias', 'regular1_3.ext_conv2.1.running_mean', 'regular1_3.ext_conv2.1.running_var', 'regular1_3.ext_conv2.1.num_batches_tracked', 'regular1_3.ext_conv2.2.weight', 'regular1_3.ext_conv3.0.weight', 'regular1_3.ext_conv3.1.weight', 'regular1_3.ext_conv3.1.bias', 'regular1_3.ext_conv3.1.running_mean', 'regular1_3.ext_conv3.1.running_var', 'regular1_3.ext_conv3.1.num_batches_tracked', 'regular1_3.ext_conv3.2.weight', 'regular1_3.out_activation.weight', 'regular1_4.ext_conv1.0.weight', 'regular1_4.ext_conv1.1.weight', 'regular1_4.ext_conv1.1.bias', 'regular1_4.ext_conv1.1.running_mean', 'regular1_4.ext_conv1.1.running_var', 'regular1_4.ext_conv1.1.num_batches_tracked', 'regular1_4.ext_conv1.2.weight', 'regular1_4.ext_conv2.0.weight', 'regular1_4.ext_conv2.1.weight', 'regular1_4.ext_conv2.1.bias', 'regular1_4.ext_conv2.1.running_mean', 'regular1_4.ext_conv2.1.running_var', 'regular1_4.ext_conv2.1.num_batches_tracked', 'regular1_4.ext_conv2.2.weight', 'regular1_4.ext_conv3.0.weight', 'regular1_4.ext_conv3.1.weight', 'regular1_4.ext_conv3.1.bias', 'regular1_4.ext_conv3.1.running_mean', 'regular1_4.ext_conv3.1.running_var', 'regular1_4.ext_conv3.1.num_batches_tracked', 'regular1_4.ext_conv3.2.weight', 'regular1_4.out_activation.weight', 'downsample2_0.ext_conv1.0.weight', 'downsample2_0.ext_conv1.1.weight', 'downsample2_0.ext_conv1.1.bias', 'downsample2_0.ext_conv1.1.running_mean', 'downsample2_0.ext_conv1.1.running_var', 'downsample2_0.ext_conv1.1.num_batches_tracked', 'downsample2_0.ext_conv1.2.weight', 'downsample2_0.ext_conv2.0.weight', 'downsample2_0.ext_conv2.1.weight', 'downsample2_0.ext_conv2.1.bias', 'downsample2_0.ext_conv2.1.running_mean', 'downsample2_0.ext_conv2.1.running_var', 'downsample2_0.ext_conv2.1.num_batches_tracked', 'downsample2_0.ext_conv2.2.weight', 'downsample2_0.ext_conv3.0.weight', 'downsample2_0.ext_conv3.1.weight', 'downsample2_0.ext_conv3.1.bias', 'downsample2_0.ext_conv3.1.running_mean', 'downsample2_0.ext_conv3.1.running_var', 'downsample2_0.ext_conv3.1.num_batches_tracked', 'downsample2_0.ext_conv3.2.weight', 'downsample2_0.out_activation.weight', 'regular2_1.ext_conv1.0.weight', 'regular2_1.ext_conv1.1.weight', 'regular2_1.ext_conv1.1.bias', 'regular2_1.ext_conv1.1.running_mean', 'regular2_1.ext_conv1.1.running_var', 'regular2_1.ext_conv1.1.num_batches_tracked', 'regular2_1.ext_conv1.2.weight', 'regular2_1.ext_conv2.0.weight', 'regular2_1.ext_conv2.1.weight', 'regular2_1.ext_conv2.1.bias', 'regular2_1.ext_conv2.1.running_mean', 'regular2_1.ext_conv2.1.running_var', 'regular2_1.ext_conv2.1.num_batches_tracked', 'regular2_1.ext_conv2.2.weight', 'regular2_1.ext_conv3.0.weight', 'regular2_1.ext_conv3.1.weight', 'regular2_1.ext_conv3.1.bias', 'regular2_1.ext_conv3.1.running_mean', 'regular2_1.ext_conv3.1.running_var', 'regular2_1.ext_conv3.1.num_batches_tracked', 'regular2_1.ext_conv3.2.weight', 'regular2_1.out_activation.weight', 'dilated2_2.ext_conv1.0.weight', 'dilated2_2.ext_conv1.1.weight', 'dilated2_2.ext_conv1.1.bias', 'dilated2_2.ext_conv1.1.running_mean', 'dilated2_2.ext_conv1.1.running_var', 'dilated2_2.ext_conv1.1.num_batches_tracked', 'dilated2_2.ext_conv1.2.weight', 'dilated2_2.ext_conv2.0.weight', 'dilated2_2.ext_conv2.1.weight', 'dilated2_2.ext_conv2.1.bias', 'dilated2_2.ext_conv2.1.running_mean', 'dilated2_2.ext_conv2.1.running_var', 'dilated2_2.ext_conv2.1.num_batches_tracked', 'dilated2_2.ext_conv2.2.weight', 'dilated2_2.ext_conv3.0.weight', 'dilated2_2.ext_conv3.1.weight', 'dilated2_2.ext_conv3.1.bias', 'dilated2_2.ext_conv3.1.running_mean', 'dilated2_2.ext_conv3.1.running_var', 'dilated2_2.ext_conv3.1.num_batches_tracked', 'dilated2_2.ext_conv3.2.weight', 'dilated2_2.out_activation.weight', 'asymmetric2_3.ext_conv1.0.weight', 'asymmetric2_3.ext_conv1.1.weight', 'asymmetric2_3.ext_conv1.1.bias', 'asymmetric2_3.ext_conv1.1.running_mean', 'asymmetric2_3.ext_conv1.1.running_var', 'asymmetric2_3.ext_conv1.1.num_batches_tracked', 'asymmetric2_3.ext_conv1.2.weight', 'asymmetric2_3.ext_conv2.0.weight', 'asymmetric2_3.ext_conv2.1.weight', 'asymmetric2_3.ext_conv2.1.bias', 'asymmetric2_3.ext_conv2.1.running_mean', 'asymmetric2_3.ext_conv2.1.running_var', 'asymmetric2_3.ext_conv2.1.num_batches_tracked', 'asymmetric2_3.ext_conv2.2.weight', 'asymmetric2_3.ext_conv2.3.weight', 'asymmetric2_3.ext_conv2.4.weight', 'asymmetric2_3.ext_conv2.4.bias', 'asymmetric2_3.ext_conv2.4.running_mean', 'asymmetric2_3.ext_conv2.4.running_var', 'asymmetric2_3.ext_conv2.4.num_batches_tracked', 'asymmetric2_3.ext_conv2.5.weight', 'asymmetric2_3.ext_conv3.0.weight', 'asymmetric2_3.ext_conv3.1.weight', 'asymmetric2_3.ext_conv3.1.bias', 'asymmetric2_3.ext_conv3.1.running_mean', 'asymmetric2_3.ext_conv3.1.running_var', 'asymmetric2_3.ext_conv3.1.num_batches_tracked', 'asymmetric2_3.ext_conv3.2.weight', 'asymmetric2_3.out_activation.weight', 'dilated2_4.ext_conv1.0.weight', 'dilated2_4.ext_conv1.1.weight', 'dilated2_4.ext_conv1.1.bias', 'dilated2_4.ext_conv1.1.running_mean', 'dilated2_4.ext_conv1.1.running_var', 'dilated2_4.ext_conv1.1.num_batches_tracked', 'dilated2_4.ext_conv1.2.weight', 'dilated2_4.ext_conv2.0.weight', 'dilated2_4.ext_conv2.1.weight', 'dilated2_4.ext_conv2.1.bias', 'dilated2_4.ext_conv2.1.running_mean', 'dilated2_4.ext_conv2.1.running_var', 'dilated2_4.ext_conv2.1.num_batches_tracked', 'dilated2_4.ext_conv2.2.weight', 'dilated2_4.ext_conv3.0.weight', 'dilated2_4.ext_conv3.1.weight', 'dilated2_4.ext_conv3.1.bias', 'dilated2_4.ext_conv3.1.running_mean', 'dilated2_4.ext_conv3.1.running_var', 'dilated2_4.ext_conv3.1.num_batches_tracked', 'dilated2_4.ext_conv3.2.weight', 'dilated2_4.out_activation.weight', 'regular2_5.ext_conv1.0.weight', 'regular2_5.ext_conv1.1.weight', 'regular2_5.ext_conv1.1.bias', 'regular2_5.ext_conv1.1.running_mean', 'regular2_5.ext_conv1.1.running_var', 'regular2_5.ext_conv1.1.num_batches_tracked', 'regular2_5.ext_conv1.2.weight', 'regular2_5.ext_conv2.0.weight', 'regular2_5.ext_conv2.1.weight', 'regular2_5.ext_conv2.1.bias', 'regular2_5.ext_conv2.1.running_mean', 'regular2_5.ext_conv2.1.running_var', 'regular2_5.ext_conv2.1.num_batches_tracked', 'regular2_5.ext_conv2.2.weight', 'regular2_5.ext_conv3.0.weight', 'regular2_5.ext_conv3.1.weight', 'regular2_5.ext_conv3.1.bias', 'regular2_5.ext_conv3.1.running_mean', 'regular2_5.ext_conv3.1.running_var', 'regular2_5.ext_conv3.1.num_batches_tracked', 'regular2_5.ext_conv3.2.weight', 'regular2_5.out_activation.weight', 'dilated2_6.ext_conv1.0.weight', 'dilated2_6.ext_conv1.1.weight', 'dilated2_6.ext_conv1.1.bias', 'dilated2_6.ext_conv1.1.running_mean', 'dilated2_6.ext_conv1.1.running_var', 'dilated2_6.ext_conv1.1.num_batches_tracked', 'dilated2_6.ext_conv1.2.weight', 'dilated2_6.ext_conv2.0.weight', 'dilated2_6.ext_conv2.1.weight', 'dilated2_6.ext_conv2.1.bias', 'dilated2_6.ext_conv2.1.running_mean', 'dilated2_6.ext_conv2.1.running_var', 'dilated2_6.ext_conv2.1.num_batches_tracked', 'dilated2_6.ext_conv2.2.weight', 'dilated2_6.ext_conv3.0.weight', 'dilated2_6.ext_conv3.1.weight', 'dilated2_6.ext_conv3.1.bias', 'dilated2_6.ext_conv3.1.running_mean', 'dilated2_6.ext_conv3.1.running_var', 'dilated2_6.ext_conv3.1.num_batches_tracked', 'dilated2_6.ext_conv3.2.weight', 'dilated2_6.out_activation.weight', 'asymmetric2_7.ext_conv1.0.weight', 'asymmetric2_7.ext_conv1.1.weight', 'asymmetric2_7.ext_conv1.1.bias', 'asymmetric2_7.ext_conv1.1.running_mean', 'asymmetric2_7.ext_conv1.1.running_var', 'asymmetric2_7.ext_conv1.1.num_batches_tracked', 'asymmetric2_7.ext_conv1.2.weight', 'asymmetric2_7.ext_conv2.0.weight', 'asymmetric2_7.ext_conv2.1.weight', 'asymmetric2_7.ext_conv2.1.bias', 'asymmetric2_7.ext_conv2.1.running_mean', 'asymmetric2_7.ext_conv2.1.running_var', 'asymmetric2_7.ext_conv2.1.num_batches_tracked', 'asymmetric2_7.ext_conv2.2.weight', 'asymmetric2_7.ext_conv2.3.weight', 'asymmetric2_7.ext_conv2.4.weight', 'asymmetric2_7.ext_conv2.4.bias', 'asymmetric2_7.ext_conv2.4.running_mean', 'asymmetric2_7.ext_conv2.4.running_var', 'asymmetric2_7.ext_conv2.4.num_batches_tracked', 'asymmetric2_7.ext_conv2.5.weight', 'asymmetric2_7.ext_conv3.0.weight', 'asymmetric2_7.ext_conv3.1.weight', 'asymmetric2_7.ext_conv3.1.bias', 'asymmetric2_7.ext_conv3.1.running_mean', 'asymmetric2_7.ext_conv3.1.running_var', 'asymmetric2_7.ext_conv3.1.num_batches_tracked', 'asymmetric2_7.ext_conv3.2.weight', 'asymmetric2_7.out_activation.weight', 'dilated2_8.ext_conv1.0.weight', 'dilated2_8.ext_conv1.1.weight', 'dilated2_8.ext_conv1.1.bias', 'dilated2_8.ext_conv1.1.running_mean', 'dilated2_8.ext_conv1.1.running_var', 'dilated2_8.ext_conv1.1.num_batches_tracked', 'dilated2_8.ext_conv1.2.weight', 'dilated2_8.ext_conv2.0.weight', 'dilated2_8.ext_conv2.1.weight', 'dilated2_8.ext_conv2.1.bias', 'dilated2_8.ext_conv2.1.running_mean', 'dilated2_8.ext_conv2.1.running_var', 'dilated2_8.ext_conv2.1.num_batches_tracked', 'dilated2_8.ext_conv2.2.weight', 'dilated2_8.ext_conv3.0.weight', 'dilated2_8.ext_conv3.1.weight', 'dilated2_8.ext_conv3.1.bias', 'dilated2_8.ext_conv3.1.running_mean', 'dilated2_8.ext_conv3.1.running_var', 'dilated2_8.ext_conv3.1.num_batches_tracked', 'dilated2_8.ext_conv3.2.weight', 'dilated2_8.out_activation.weight', 'regular3_0.ext_conv1.0.weight', 'regular3_0.ext_conv1.1.weight', 'regular3_0.ext_conv1.1.bias', 'regular3_0.ext_conv1.1.running_mean', 'regular3_0.ext_conv1.1.running_var', 'regular3_0.ext_conv1.1.num_batches_tracked', 'regular3_0.ext_conv1.2.weight', 'regular3_0.ext_conv2.0.weight', 'regular3_0.ext_conv2.1.weight', 'regular3_0.ext_conv2.1.bias', 'regular3_0.ext_conv2.1.running_mean', 'regular3_0.ext_conv2.1.running_var', 'regular3_0.ext_conv2.1.num_batches_tracked', 'regular3_0.ext_conv2.2.weight', 'regular3_0.ext_conv3.0.weight', 'regular3_0.ext_conv3.1.weight', 'regular3_0.ext_conv3.1.bias', 'regular3_0.ext_conv3.1.running_mean', 'regular3_0.ext_conv3.1.running_var', 'regular3_0.ext_conv3.1.num_batches_tracked', 'regular3_0.ext_conv3.2.weight', 'regular3_0.out_activation.weight', 'dilated3_1.ext_conv1.0.weight', 'dilated3_1.ext_conv1.1.weight', 'dilated3_1.ext_conv1.1.bias', 'dilated3_1.ext_conv1.1.running_mean', 'dilated3_1.ext_conv1.1.running_var', 'dilated3_1.ext_conv1.1.num_batches_tracked', 'dilated3_1.ext_conv1.2.weight', 'dilated3_1.ext_conv2.0.weight', 'dilated3_1.ext_conv2.1.weight', 'dilated3_1.ext_conv2.1.bias', 'dilated3_1.ext_conv2.1.running_mean', 'dilated3_1.ext_conv2.1.running_var', 'dilated3_1.ext_conv2.1.num_batches_tracked', 'dilated3_1.ext_conv2.2.weight', 'dilated3_1.ext_conv3.0.weight', 'dilated3_1.ext_conv3.1.weight', 'dilated3_1.ext_conv3.1.bias', 'dilated3_1.ext_conv3.1.running_mean', 'dilated3_1.ext_conv3.1.running_var', 'dilated3_1.ext_conv3.1.num_batches_tracked', 'dilated3_1.ext_conv3.2.weight', 'dilated3_1.out_activation.weight', 'asymmetric3_2.ext_conv1.0.weight', 'asymmetric3_2.ext_conv1.1.weight', 'asymmetric3_2.ext_conv1.1.bias', 'asymmetric3_2.ext_conv1.1.running_mean', 'asymmetric3_2.ext_conv1.1.running_var', 'asymmetric3_2.ext_conv1.1.num_batches_tracked', 'asymmetric3_2.ext_conv1.2.weight', 'asymmetric3_2.ext_conv2.0.weight', 'asymmetric3_2.ext_conv2.1.weight', 'asymmetric3_2.ext_conv2.1.bias', 'asymmetric3_2.ext_conv2.1.running_mean', 'asymmetric3_2.ext_conv2.1.running_var', 'asymmetric3_2.ext_conv2.1.num_batches_tracked', 'asymmetric3_2.ext_conv2.2.weight', 'asymmetric3_2.ext_conv2.3.weight', 'asymmetric3_2.ext_conv2.4.weight', 'asymmetric3_2.ext_conv2.4.bias', 'asymmetric3_2.ext_conv2.4.running_mean', 'asymmetric3_2.ext_conv2.4.running_var', 'asymmetric3_2.ext_conv2.4.num_batches_tracked', 'asymmetric3_2.ext_conv2.5.weight', 'asymmetric3_2.ext_conv3.0.weight', 'asymmetric3_2.ext_conv3.1.weight', 'asymmetric3_2.ext_conv3.1.bias', 'asymmetric3_2.ext_conv3.1.running_mean', 'asymmetric3_2.ext_conv3.1.running_var', 'asymmetric3_2.ext_conv3.1.num_batches_tracked', 'asymmetric3_2.ext_conv3.2.weight', 'asymmetric3_2.out_activation.weight', 'dilated3_3.ext_conv1.0.weight', 'dilated3_3.ext_conv1.1.weight', 'dilated3_3.ext_conv1.1.bias', 'dilated3_3.ext_conv1.1.running_mean', 'dilated3_3.ext_conv1.1.running_var', 'dilated3_3.ext_conv1.1.num_batches_tracked', 'dilated3_3.ext_conv1.2.weight', 'dilated3_3.ext_conv2.0.weight', 'dilated3_3.ext_conv2.1.weight', 'dilated3_3.ext_conv2.1.bias', 'dilated3_3.ext_conv2.1.running_mean', 'dilated3_3.ext_conv2.1.running_var', 'dilated3_3.ext_conv2.1.num_batches_tracked', 'dilated3_3.ext_conv2.2.weight', 'dilated3_3.ext_conv3.0.weight', 'dilated3_3.ext_conv3.1.weight', 'dilated3_3.ext_conv3.1.bias', 'dilated3_3.ext_conv3.1.running_mean', 'dilated3_3.ext_conv3.1.running_var', 'dilated3_3.ext_conv3.1.num_batches_tracked', 'dilated3_3.ext_conv3.2.weight', 'dilated3_3.out_activation.weight', 'regular3_4.ext_conv1.0.weight', 'regular3_4.ext_conv1.1.weight', 'regular3_4.ext_conv1.1.bias', 'regular3_4.ext_conv1.1.running_mean', 'regular3_4.ext_conv1.1.running_var', 'regular3_4.ext_conv1.1.num_batches_tracked', 'regular3_4.ext_conv1.2.weight', 'regular3_4.ext_conv2.0.weight', 'regular3_4.ext_conv2.1.weight', 'regular3_4.ext_conv2.1.bias', 'regular3_4.ext_conv2.1.running_mean', 'regular3_4.ext_conv2.1.running_var', 'regular3_4.ext_conv2.1.num_batches_tracked', 'regular3_4.ext_conv2.2.weight', 'regular3_4.ext_conv3.0.weight', 'regular3_4.ext_conv3.1.weight', 'regular3_4.ext_conv3.1.bias', 'regular3_4.ext_conv3.1.running_mean', 'regular3_4.ext_conv3.1.running_var', 'regular3_4.ext_conv3.1.num_batches_tracked', 'regular3_4.ext_conv3.2.weight', 'regular3_4.out_activation.weight', 'dilated3_5.ext_conv1.0.weight', 'dilated3_5.ext_conv1.1.weight', 'dilated3_5.ext_conv1.1.bias', 'dilated3_5.ext_conv1.1.running_mean', 'dilated3_5.ext_conv1.1.running_var', 'dilated3_5.ext_conv1.1.num_batches_tracked', 'dilated3_5.ext_conv1.2.weight', 'dilated3_5.ext_conv2.0.weight', 'dilated3_5.ext_conv2.1.weight', 'dilated3_5.ext_conv2.1.bias', 'dilated3_5.ext_conv2.1.running_mean', 'dilated3_5.ext_conv2.1.running_var', 'dilated3_5.ext_conv2.1.num_batches_tracked', 'dilated3_5.ext_conv2.2.weight', 'dilated3_5.ext_conv3.0.weight', 'dilated3_5.ext_conv3.1.weight', 'dilated3_5.ext_conv3.1.bias', 'dilated3_5.ext_conv3.1.running_mean', 'dilated3_5.ext_conv3.1.running_var', 'dilated3_5.ext_conv3.1.num_batches_tracked', 'dilated3_5.ext_conv3.2.weight', 'dilated3_5.out_activation.weight', 'asymmetric3_6.ext_conv1.0.weight', 'asymmetric3_6.ext_conv1.1.weight', 'asymmetric3_6.ext_conv1.1.bias', 'asymmetric3_6.ext_conv1.1.running_mean', 'asymmetric3_6.ext_conv1.1.running_var', 'asymmetric3_6.ext_conv1.1.num_batches_tracked', 'asymmetric3_6.ext_conv1.2.weight', 'asymmetric3_6.ext_conv2.0.weight', 'asymmetric3_6.ext_conv2.1.weight', 'asymmetric3_6.ext_conv2.1.bias', 'asymmetric3_6.ext_conv2.1.running_mean', 'asymmetric3_6.ext_conv2.1.running_var', 'asymmetric3_6.ext_conv2.1.num_batches_tracked', 'asymmetric3_6.ext_conv2.2.weight', 'asymmetric3_6.ext_conv2.3.weight', 'asymmetric3_6.ext_conv2.4.weight', 'asymmetric3_6.ext_conv2.4.bias', 'asymmetric3_6.ext_conv2.4.running_mean', 'asymmetric3_6.ext_conv2.4.running_var', 'asymmetric3_6.ext_conv2.4.num_batches_tracked', 'asymmetric3_6.ext_conv2.5.weight', 'asymmetric3_6.ext_conv3.0.weight', 'asymmetric3_6.ext_conv3.1.weight', 'asymmetric3_6.ext_conv3.1.bias', 'asymmetric3_6.ext_conv3.1.running_mean', 'asymmetric3_6.ext_conv3.1.running_var', 'asymmetric3_6.ext_conv3.1.num_batches_tracked', 'asymmetric3_6.ext_conv3.2.weight', 'asymmetric3_6.out_activation.weight', 'dilated3_7.ext_conv1.0.weight', 'dilated3_7.ext_conv1.1.weight', 'dilated3_7.ext_conv1.1.bias', 'dilated3_7.ext_conv1.1.running_mean', 'dilated3_7.ext_conv1.1.running_var', 'dilated3_7.ext_conv1.1.num_batches_tracked', 'dilated3_7.ext_conv1.2.weight', 'dilated3_7.ext_conv2.0.weight', 'dilated3_7.ext_conv2.1.weight', 'dilated3_7.ext_conv2.1.bias', 'dilated3_7.ext_conv2.1.running_mean', 'dilated3_7.ext_conv2.1.running_var', 'dilated3_7.ext_conv2.1.num_batches_tracked', 'dilated3_7.ext_conv2.2.weight', 'dilated3_7.ext_conv3.0.weight', 'dilated3_7.ext_conv3.1.weight', 'dilated3_7.ext_conv3.1.bias', 'dilated3_7.ext_conv3.1.running_mean', 'dilated3_7.ext_conv3.1.running_var', 'dilated3_7.ext_conv3.1.num_batches_tracked', 'dilated3_7.ext_conv3.2.weight', 'dilated3_7.out_activation.weight', 'upsample4_0.main_conv1.0.weight', 'upsample4_0.main_conv1.1.weight', 'upsample4_0.main_conv1.1.bias', 'upsample4_0.main_conv1.1.running_mean', 'upsample4_0.main_conv1.1.running_var', 'upsample4_0.main_conv1.1.num_batches_tracked', 'upsample4_0.ext_conv1.0.weight', 'upsample4_0.ext_conv1.1.weight', 'upsample4_0.ext_conv1.1.bias', 'upsample4_0.ext_conv1.1.running_mean', 'upsample4_0.ext_conv1.1.running_var', 'upsample4_0.ext_conv1.1.num_batches_tracked', 'upsample4_0.ext_tconv1.weight', 'upsample4_0.ext_tconv1_bnorm.weight', 'upsample4_0.ext_tconv1_bnorm.bias', 'upsample4_0.ext_tconv1_bnorm.running_mean', 'upsample4_0.ext_tconv1_bnorm.running_var', 'upsample4_0.ext_tconv1_bnorm.num_batches_tracked', 'upsample4_0.ext_conv2.0.weight', 'upsample4_0.ext_conv2.1.weight', 'upsample4_0.ext_conv2.1.bias', 'upsample4_0.ext_conv2.1.running_mean', 'upsample4_0.ext_conv2.1.running_var', 'upsample4_0.ext_conv2.1.num_batches_tracked', 'regular4_1.ext_conv1.0.weight', 'regular4_1.ext_conv1.1.weight', 'regular4_1.ext_conv1.1.bias', 'regular4_1.ext_conv1.1.running_mean', 'regular4_1.ext_conv1.1.running_var', 'regular4_1.ext_conv1.1.num_batches_tracked', 'regular4_1.ext_conv2.0.weight', 'regular4_1.ext_conv2.1.weight', 'regular4_1.ext_conv2.1.bias', 'regular4_1.ext_conv2.1.running_mean', 'regular4_1.ext_conv2.1.running_var', 'regular4_1.ext_conv2.1.num_batches_tracked', 'regular4_1.ext_conv3.0.weight', 'regular4_1.ext_conv3.1.weight', 'regular4_1.ext_conv3.1.bias', 'regular4_1.ext_conv3.1.running_mean', 'regular4_1.ext_conv3.1.running_var', 'regular4_1.ext_conv3.1.num_batches_tracked', 'regular4_2.ext_conv1.0.weight', 'regular4_2.ext_conv1.1.weight', 'regular4_2.ext_conv1.1.bias', 'regular4_2.ext_conv1.1.running_mean', 'regular4_2.ext_conv1.1.running_var', 'regular4_2.ext_conv1.1.num_batches_tracked', 'regular4_2.ext_conv2.0.weight', 'regular4_2.ext_conv2.1.weight', 'regular4_2.ext_conv2.1.bias', 'regular4_2.ext_conv2.1.running_mean', 'regular4_2.ext_conv2.1.running_var', 'regular4_2.ext_conv2.1.num_batches_tracked', 'regular4_2.ext_conv3.0.weight', 'regular4_2.ext_conv3.1.weight', 'regular4_2.ext_conv3.1.bias', 'regular4_2.ext_conv3.1.running_mean', 'regular4_2.ext_conv3.1.running_var', 'regular4_2.ext_conv3.1.num_batches_tracked', 'upsample5_0.main_conv1.0.weight', 'upsample5_0.main_conv1.1.weight', 'upsample5_0.main_conv1.1.bias', 'upsample5_0.main_conv1.1.running_mean', 'upsample5_0.main_conv1.1.running_var', 'upsample5_0.main_conv1.1.num_batches_tracked', 'upsample5_0.ext_conv1.0.weight', 'upsample5_0.ext_conv1.1.weight', 'upsample5_0.ext_conv1.1.bias', 'upsample5_0.ext_conv1.1.running_mean', 'upsample5_0.ext_conv1.1.running_var', 'upsample5_0.ext_conv1.1.num_batches_tracked', 'upsample5_0.ext_tconv1.weight', 'upsample5_0.ext_tconv1_bnorm.weight', 'upsample5_0.ext_tconv1_bnorm.bias', 'upsample5_0.ext_tconv1_bnorm.running_mean', 'upsample5_0.ext_tconv1_bnorm.running_var', 'upsample5_0.ext_tconv1_bnorm.num_batches_tracked', 'upsample5_0.ext_conv2.0.weight', 'upsample5_0.ext_conv2.1.weight', 'upsample5_0.ext_conv2.1.bias', 'upsample5_0.ext_conv2.1.running_mean', 'upsample5_0.ext_conv2.1.running_var', 'upsample5_0.ext_conv2.1.num_batches_tracked', 'regular5_1.ext_conv1.0.weight', 'regular5_1.ext_conv1.1.weight', 'regular5_1.ext_conv1.1.bias', 'regular5_1.ext_conv1.1.running_mean', 'regular5_1.ext_conv1.1.running_var', 'regular5_1.ext_conv1.1.num_batches_tracked', 'regular5_1.ext_conv2.0.weight', 'regular5_1.ext_conv2.1.weight', 'regular5_1.ext_conv2.1.bias', 'regular5_1.ext_conv2.1.running_mean', 'regular5_1.ext_conv2.1.running_var', 'regular5_1.ext_conv2.1.num_batches_tracked', 'regular5_1.ext_conv3.0.weight', 'regular5_1.ext_conv3.1.weight', 'regular5_1.ext_conv3.1.bias', 'regular5_1.ext_conv3.1.running_mean', 'regular5_1.ext_conv3.1.running_var', 'regular5_1.ext_conv3.1.num_batches_tracked', 'transposed_conv.weight'])\n",
            "odict_keys(['initial_block.main_branch.weight', 'initial_block.batch_norm.weight', 'initial_block.batch_norm.bias', 'initial_block.batch_norm.running_mean', 'initial_block.batch_norm.running_var', 'initial_block.batch_norm.num_batches_tracked', 'initial_block.out_activation.weight', 'downsample1_0.ext_conv1.0.weight', 'downsample1_0.ext_conv1.1.weight', 'downsample1_0.ext_conv1.1.bias', 'downsample1_0.ext_conv1.1.running_mean', 'downsample1_0.ext_conv1.1.running_var', 'downsample1_0.ext_conv1.1.num_batches_tracked', 'downsample1_0.ext_conv1.2.weight', 'downsample1_0.ext_conv2.0.weight', 'downsample1_0.ext_conv2.1.weight', 'downsample1_0.ext_conv2.1.bias', 'downsample1_0.ext_conv2.1.running_mean', 'downsample1_0.ext_conv2.1.running_var', 'downsample1_0.ext_conv2.1.num_batches_tracked', 'downsample1_0.ext_conv2.2.weight', 'downsample1_0.ext_conv3.0.weight', 'downsample1_0.ext_conv3.1.weight', 'downsample1_0.ext_conv3.1.bias', 'downsample1_0.ext_conv3.1.running_mean', 'downsample1_0.ext_conv3.1.running_var', 'downsample1_0.ext_conv3.1.num_batches_tracked', 'downsample1_0.ext_conv3.2.weight', 'downsample1_0.out_activation.weight', 'regular1_1.ext_conv1.0.weight', 'regular1_1.ext_conv1.1.weight', 'regular1_1.ext_conv1.1.bias', 'regular1_1.ext_conv1.1.running_mean', 'regular1_1.ext_conv1.1.running_var', 'regular1_1.ext_conv1.1.num_batches_tracked', 'regular1_1.ext_conv1.2.weight', 'regular1_1.ext_conv2.0.weight', 'regular1_1.ext_conv2.1.weight', 'regular1_1.ext_conv2.1.bias', 'regular1_1.ext_conv2.1.running_mean', 'regular1_1.ext_conv2.1.running_var', 'regular1_1.ext_conv2.1.num_batches_tracked', 'regular1_1.ext_conv2.2.weight', 'regular1_1.ext_conv3.0.weight', 'regular1_1.ext_conv3.1.weight', 'regular1_1.ext_conv3.1.bias', 'regular1_1.ext_conv3.1.running_mean', 'regular1_1.ext_conv3.1.running_var', 'regular1_1.ext_conv3.1.num_batches_tracked', 'regular1_1.ext_conv3.2.weight', 'regular1_1.out_activation.weight', 'regular1_2.ext_conv1.0.weight', 'regular1_2.ext_conv1.1.weight', 'regular1_2.ext_conv1.1.bias', 'regular1_2.ext_conv1.1.running_mean', 'regular1_2.ext_conv1.1.running_var', 'regular1_2.ext_conv1.1.num_batches_tracked', 'regular1_2.ext_conv1.2.weight', 'regular1_2.ext_conv2.0.weight', 'regular1_2.ext_conv2.1.weight', 'regular1_2.ext_conv2.1.bias', 'regular1_2.ext_conv2.1.running_mean', 'regular1_2.ext_conv2.1.running_var', 'regular1_2.ext_conv2.1.num_batches_tracked', 'regular1_2.ext_conv2.2.weight', 'regular1_2.ext_conv3.0.weight', 'regular1_2.ext_conv3.1.weight', 'regular1_2.ext_conv3.1.bias', 'regular1_2.ext_conv3.1.running_mean', 'regular1_2.ext_conv3.1.running_var', 'regular1_2.ext_conv3.1.num_batches_tracked', 'regular1_2.ext_conv3.2.weight', 'regular1_2.out_activation.weight', 'regular1_3.ext_conv1.0.weight', 'regular1_3.ext_conv1.1.weight', 'regular1_3.ext_conv1.1.bias', 'regular1_3.ext_conv1.1.running_mean', 'regular1_3.ext_conv1.1.running_var', 'regular1_3.ext_conv1.1.num_batches_tracked', 'regular1_3.ext_conv1.2.weight', 'regular1_3.ext_conv2.0.weight', 'regular1_3.ext_conv2.1.weight', 'regular1_3.ext_conv2.1.bias', 'regular1_3.ext_conv2.1.running_mean', 'regular1_3.ext_conv2.1.running_var', 'regular1_3.ext_conv2.1.num_batches_tracked', 'regular1_3.ext_conv2.2.weight', 'regular1_3.ext_conv3.0.weight', 'regular1_3.ext_conv3.1.weight', 'regular1_3.ext_conv3.1.bias', 'regular1_3.ext_conv3.1.running_mean', 'regular1_3.ext_conv3.1.running_var', 'regular1_3.ext_conv3.1.num_batches_tracked', 'regular1_3.ext_conv3.2.weight', 'regular1_3.out_activation.weight', 'regular1_4.ext_conv1.0.weight', 'regular1_4.ext_conv1.1.weight', 'regular1_4.ext_conv1.1.bias', 'regular1_4.ext_conv1.1.running_mean', 'regular1_4.ext_conv1.1.running_var', 'regular1_4.ext_conv1.1.num_batches_tracked', 'regular1_4.ext_conv1.2.weight', 'regular1_4.ext_conv2.0.weight', 'regular1_4.ext_conv2.1.weight', 'regular1_4.ext_conv2.1.bias', 'regular1_4.ext_conv2.1.running_mean', 'regular1_4.ext_conv2.1.running_var', 'regular1_4.ext_conv2.1.num_batches_tracked', 'regular1_4.ext_conv2.2.weight', 'regular1_4.ext_conv3.0.weight', 'regular1_4.ext_conv3.1.weight', 'regular1_4.ext_conv3.1.bias', 'regular1_4.ext_conv3.1.running_mean', 'regular1_4.ext_conv3.1.running_var', 'regular1_4.ext_conv3.1.num_batches_tracked', 'regular1_4.ext_conv3.2.weight', 'regular1_4.out_activation.weight', 'downsample2_0.ext_conv1.0.weight', 'downsample2_0.ext_conv1.1.weight', 'downsample2_0.ext_conv1.1.bias', 'downsample2_0.ext_conv1.1.running_mean', 'downsample2_0.ext_conv1.1.running_var', 'downsample2_0.ext_conv1.1.num_batches_tracked', 'downsample2_0.ext_conv1.2.weight', 'downsample2_0.ext_conv2.0.weight', 'downsample2_0.ext_conv2.1.weight', 'downsample2_0.ext_conv2.1.bias', 'downsample2_0.ext_conv2.1.running_mean', 'downsample2_0.ext_conv2.1.running_var', 'downsample2_0.ext_conv2.1.num_batches_tracked', 'downsample2_0.ext_conv2.2.weight', 'downsample2_0.ext_conv3.0.weight', 'downsample2_0.ext_conv3.1.weight', 'downsample2_0.ext_conv3.1.bias', 'downsample2_0.ext_conv3.1.running_mean', 'downsample2_0.ext_conv3.1.running_var', 'downsample2_0.ext_conv3.1.num_batches_tracked', 'downsample2_0.ext_conv3.2.weight', 'downsample2_0.out_activation.weight', 'regular2_1.ext_conv1.0.weight', 'regular2_1.ext_conv1.1.weight', 'regular2_1.ext_conv1.1.bias', 'regular2_1.ext_conv1.1.running_mean', 'regular2_1.ext_conv1.1.running_var', 'regular2_1.ext_conv1.1.num_batches_tracked', 'regular2_1.ext_conv1.2.weight', 'regular2_1.ext_conv2.0.weight', 'regular2_1.ext_conv2.1.weight', 'regular2_1.ext_conv2.1.bias', 'regular2_1.ext_conv2.1.running_mean', 'regular2_1.ext_conv2.1.running_var', 'regular2_1.ext_conv2.1.num_batches_tracked', 'regular2_1.ext_conv2.2.weight', 'regular2_1.ext_conv3.0.weight', 'regular2_1.ext_conv3.1.weight', 'regular2_1.ext_conv3.1.bias', 'regular2_1.ext_conv3.1.running_mean', 'regular2_1.ext_conv3.1.running_var', 'regular2_1.ext_conv3.1.num_batches_tracked', 'regular2_1.ext_conv3.2.weight', 'regular2_1.out_activation.weight', 'dilated2_2.ext_conv1.0.weight', 'dilated2_2.ext_conv1.1.weight', 'dilated2_2.ext_conv1.1.bias', 'dilated2_2.ext_conv1.1.running_mean', 'dilated2_2.ext_conv1.1.running_var', 'dilated2_2.ext_conv1.1.num_batches_tracked', 'dilated2_2.ext_conv1.2.weight', 'dilated2_2.ext_conv2.0.weight', 'dilated2_2.ext_conv2.1.weight', 'dilated2_2.ext_conv2.1.bias', 'dilated2_2.ext_conv2.1.running_mean', 'dilated2_2.ext_conv2.1.running_var', 'dilated2_2.ext_conv2.1.num_batches_tracked', 'dilated2_2.ext_conv2.2.weight', 'dilated2_2.ext_conv3.0.weight', 'dilated2_2.ext_conv3.1.weight', 'dilated2_2.ext_conv3.1.bias', 'dilated2_2.ext_conv3.1.running_mean', 'dilated2_2.ext_conv3.1.running_var', 'dilated2_2.ext_conv3.1.num_batches_tracked', 'dilated2_2.ext_conv3.2.weight', 'dilated2_2.out_activation.weight', 'asymmetric2_3.ext_conv1.0.weight', 'asymmetric2_3.ext_conv1.1.weight', 'asymmetric2_3.ext_conv1.1.bias', 'asymmetric2_3.ext_conv1.1.running_mean', 'asymmetric2_3.ext_conv1.1.running_var', 'asymmetric2_3.ext_conv1.1.num_batches_tracked', 'asymmetric2_3.ext_conv1.2.weight', 'asymmetric2_3.ext_conv2.0.weight', 'asymmetric2_3.ext_conv2.1.weight', 'asymmetric2_3.ext_conv2.1.bias', 'asymmetric2_3.ext_conv2.1.running_mean', 'asymmetric2_3.ext_conv2.1.running_var', 'asymmetric2_3.ext_conv2.1.num_batches_tracked', 'asymmetric2_3.ext_conv2.2.weight', 'asymmetric2_3.ext_conv2.3.weight', 'asymmetric2_3.ext_conv2.4.weight', 'asymmetric2_3.ext_conv2.4.bias', 'asymmetric2_3.ext_conv2.4.running_mean', 'asymmetric2_3.ext_conv2.4.running_var', 'asymmetric2_3.ext_conv2.4.num_batches_tracked', 'asymmetric2_3.ext_conv2.5.weight', 'asymmetric2_3.ext_conv3.0.weight', 'asymmetric2_3.ext_conv3.1.weight', 'asymmetric2_3.ext_conv3.1.bias', 'asymmetric2_3.ext_conv3.1.running_mean', 'asymmetric2_3.ext_conv3.1.running_var', 'asymmetric2_3.ext_conv3.1.num_batches_tracked', 'asymmetric2_3.ext_conv3.2.weight', 'asymmetric2_3.out_activation.weight', 'dilated2_4.ext_conv1.0.weight', 'dilated2_4.ext_conv1.1.weight', 'dilated2_4.ext_conv1.1.bias', 'dilated2_4.ext_conv1.1.running_mean', 'dilated2_4.ext_conv1.1.running_var', 'dilated2_4.ext_conv1.1.num_batches_tracked', 'dilated2_4.ext_conv1.2.weight', 'dilated2_4.ext_conv2.0.weight', 'dilated2_4.ext_conv2.1.weight', 'dilated2_4.ext_conv2.1.bias', 'dilated2_4.ext_conv2.1.running_mean', 'dilated2_4.ext_conv2.1.running_var', 'dilated2_4.ext_conv2.1.num_batches_tracked', 'dilated2_4.ext_conv2.2.weight', 'dilated2_4.ext_conv3.0.weight', 'dilated2_4.ext_conv3.1.weight', 'dilated2_4.ext_conv3.1.bias', 'dilated2_4.ext_conv3.1.running_mean', 'dilated2_4.ext_conv3.1.running_var', 'dilated2_4.ext_conv3.1.num_batches_tracked', 'dilated2_4.ext_conv3.2.weight', 'dilated2_4.out_activation.weight', 'regular2_5.ext_conv1.0.weight', 'regular2_5.ext_conv1.1.weight', 'regular2_5.ext_conv1.1.bias', 'regular2_5.ext_conv1.1.running_mean', 'regular2_5.ext_conv1.1.running_var', 'regular2_5.ext_conv1.1.num_batches_tracked', 'regular2_5.ext_conv1.2.weight', 'regular2_5.ext_conv2.0.weight', 'regular2_5.ext_conv2.1.weight', 'regular2_5.ext_conv2.1.bias', 'regular2_5.ext_conv2.1.running_mean', 'regular2_5.ext_conv2.1.running_var', 'regular2_5.ext_conv2.1.num_batches_tracked', 'regular2_5.ext_conv2.2.weight', 'regular2_5.ext_conv3.0.weight', 'regular2_5.ext_conv3.1.weight', 'regular2_5.ext_conv3.1.bias', 'regular2_5.ext_conv3.1.running_mean', 'regular2_5.ext_conv3.1.running_var', 'regular2_5.ext_conv3.1.num_batches_tracked', 'regular2_5.ext_conv3.2.weight', 'regular2_5.out_activation.weight', 'dilated2_6.ext_conv1.0.weight', 'dilated2_6.ext_conv1.1.weight', 'dilated2_6.ext_conv1.1.bias', 'dilated2_6.ext_conv1.1.running_mean', 'dilated2_6.ext_conv1.1.running_var', 'dilated2_6.ext_conv1.1.num_batches_tracked', 'dilated2_6.ext_conv1.2.weight', 'dilated2_6.ext_conv2.0.weight', 'dilated2_6.ext_conv2.1.weight', 'dilated2_6.ext_conv2.1.bias', 'dilated2_6.ext_conv2.1.running_mean', 'dilated2_6.ext_conv2.1.running_var', 'dilated2_6.ext_conv2.1.num_batches_tracked', 'dilated2_6.ext_conv2.2.weight', 'dilated2_6.ext_conv3.0.weight', 'dilated2_6.ext_conv3.1.weight', 'dilated2_6.ext_conv3.1.bias', 'dilated2_6.ext_conv3.1.running_mean', 'dilated2_6.ext_conv3.1.running_var', 'dilated2_6.ext_conv3.1.num_batches_tracked', 'dilated2_6.ext_conv3.2.weight', 'dilated2_6.out_activation.weight', 'asymmetric2_7.ext_conv1.0.weight', 'asymmetric2_7.ext_conv1.1.weight', 'asymmetric2_7.ext_conv1.1.bias', 'asymmetric2_7.ext_conv1.1.running_mean', 'asymmetric2_7.ext_conv1.1.running_var', 'asymmetric2_7.ext_conv1.1.num_batches_tracked', 'asymmetric2_7.ext_conv1.2.weight', 'asymmetric2_7.ext_conv2.0.weight', 'asymmetric2_7.ext_conv2.1.weight', 'asymmetric2_7.ext_conv2.1.bias', 'asymmetric2_7.ext_conv2.1.running_mean', 'asymmetric2_7.ext_conv2.1.running_var', 'asymmetric2_7.ext_conv2.1.num_batches_tracked', 'asymmetric2_7.ext_conv2.2.weight', 'asymmetric2_7.ext_conv2.3.weight', 'asymmetric2_7.ext_conv2.4.weight', 'asymmetric2_7.ext_conv2.4.bias', 'asymmetric2_7.ext_conv2.4.running_mean', 'asymmetric2_7.ext_conv2.4.running_var', 'asymmetric2_7.ext_conv2.4.num_batches_tracked', 'asymmetric2_7.ext_conv2.5.weight', 'asymmetric2_7.ext_conv3.0.weight', 'asymmetric2_7.ext_conv3.1.weight', 'asymmetric2_7.ext_conv3.1.bias', 'asymmetric2_7.ext_conv3.1.running_mean', 'asymmetric2_7.ext_conv3.1.running_var', 'asymmetric2_7.ext_conv3.1.num_batches_tracked', 'asymmetric2_7.ext_conv3.2.weight', 'asymmetric2_7.out_activation.weight', 'dilated2_8.ext_conv1.0.weight', 'dilated2_8.ext_conv1.1.weight', 'dilated2_8.ext_conv1.1.bias', 'dilated2_8.ext_conv1.1.running_mean', 'dilated2_8.ext_conv1.1.running_var', 'dilated2_8.ext_conv1.1.num_batches_tracked', 'dilated2_8.ext_conv1.2.weight', 'dilated2_8.ext_conv2.0.weight', 'dilated2_8.ext_conv2.1.weight', 'dilated2_8.ext_conv2.1.bias', 'dilated2_8.ext_conv2.1.running_mean', 'dilated2_8.ext_conv2.1.running_var', 'dilated2_8.ext_conv2.1.num_batches_tracked', 'dilated2_8.ext_conv2.2.weight', 'dilated2_8.ext_conv3.0.weight', 'dilated2_8.ext_conv3.1.weight', 'dilated2_8.ext_conv3.1.bias', 'dilated2_8.ext_conv3.1.running_mean', 'dilated2_8.ext_conv3.1.running_var', 'dilated2_8.ext_conv3.1.num_batches_tracked', 'dilated2_8.ext_conv3.2.weight', 'dilated2_8.out_activation.weight', 'regular3_0.ext_conv1.0.weight', 'regular3_0.ext_conv1.1.weight', 'regular3_0.ext_conv1.1.bias', 'regular3_0.ext_conv1.1.running_mean', 'regular3_0.ext_conv1.1.running_var', 'regular3_0.ext_conv1.1.num_batches_tracked', 'regular3_0.ext_conv1.2.weight', 'regular3_0.ext_conv2.0.weight', 'regular3_0.ext_conv2.1.weight', 'regular3_0.ext_conv2.1.bias', 'regular3_0.ext_conv2.1.running_mean', 'regular3_0.ext_conv2.1.running_var', 'regular3_0.ext_conv2.1.num_batches_tracked', 'regular3_0.ext_conv2.2.weight', 'regular3_0.ext_conv3.0.weight', 'regular3_0.ext_conv3.1.weight', 'regular3_0.ext_conv3.1.bias', 'regular3_0.ext_conv3.1.running_mean', 'regular3_0.ext_conv3.1.running_var', 'regular3_0.ext_conv3.1.num_batches_tracked', 'regular3_0.ext_conv3.2.weight', 'regular3_0.out_activation.weight', 'dilated3_1.ext_conv1.0.weight', 'dilated3_1.ext_conv1.1.weight', 'dilated3_1.ext_conv1.1.bias', 'dilated3_1.ext_conv1.1.running_mean', 'dilated3_1.ext_conv1.1.running_var', 'dilated3_1.ext_conv1.1.num_batches_tracked', 'dilated3_1.ext_conv1.2.weight', 'dilated3_1.ext_conv2.0.weight', 'dilated3_1.ext_conv2.1.weight', 'dilated3_1.ext_conv2.1.bias', 'dilated3_1.ext_conv2.1.running_mean', 'dilated3_1.ext_conv2.1.running_var', 'dilated3_1.ext_conv2.1.num_batches_tracked', 'dilated3_1.ext_conv2.2.weight', 'dilated3_1.ext_conv3.0.weight', 'dilated3_1.ext_conv3.1.weight', 'dilated3_1.ext_conv3.1.bias', 'dilated3_1.ext_conv3.1.running_mean', 'dilated3_1.ext_conv3.1.running_var', 'dilated3_1.ext_conv3.1.num_batches_tracked', 'dilated3_1.ext_conv3.2.weight', 'dilated3_1.out_activation.weight', 'asymmetric3_2.ext_conv1.0.weight', 'asymmetric3_2.ext_conv1.1.weight', 'asymmetric3_2.ext_conv1.1.bias', 'asymmetric3_2.ext_conv1.1.running_mean', 'asymmetric3_2.ext_conv1.1.running_var', 'asymmetric3_2.ext_conv1.1.num_batches_tracked', 'asymmetric3_2.ext_conv1.2.weight', 'asymmetric3_2.ext_conv2.0.weight', 'asymmetric3_2.ext_conv2.1.weight', 'asymmetric3_2.ext_conv2.1.bias', 'asymmetric3_2.ext_conv2.1.running_mean', 'asymmetric3_2.ext_conv2.1.running_var', 'asymmetric3_2.ext_conv2.1.num_batches_tracked', 'asymmetric3_2.ext_conv2.2.weight', 'asymmetric3_2.ext_conv2.3.weight', 'asymmetric3_2.ext_conv2.4.weight', 'asymmetric3_2.ext_conv2.4.bias', 'asymmetric3_2.ext_conv2.4.running_mean', 'asymmetric3_2.ext_conv2.4.running_var', 'asymmetric3_2.ext_conv2.4.num_batches_tracked', 'asymmetric3_2.ext_conv2.5.weight', 'asymmetric3_2.ext_conv3.0.weight', 'asymmetric3_2.ext_conv3.1.weight', 'asymmetric3_2.ext_conv3.1.bias', 'asymmetric3_2.ext_conv3.1.running_mean', 'asymmetric3_2.ext_conv3.1.running_var', 'asymmetric3_2.ext_conv3.1.num_batches_tracked', 'asymmetric3_2.ext_conv3.2.weight', 'asymmetric3_2.out_activation.weight', 'dilated3_3.ext_conv1.0.weight', 'dilated3_3.ext_conv1.1.weight', 'dilated3_3.ext_conv1.1.bias', 'dilated3_3.ext_conv1.1.running_mean', 'dilated3_3.ext_conv1.1.running_var', 'dilated3_3.ext_conv1.1.num_batches_tracked', 'dilated3_3.ext_conv1.2.weight', 'dilated3_3.ext_conv2.0.weight', 'dilated3_3.ext_conv2.1.weight', 'dilated3_3.ext_conv2.1.bias', 'dilated3_3.ext_conv2.1.running_mean', 'dilated3_3.ext_conv2.1.running_var', 'dilated3_3.ext_conv2.1.num_batches_tracked', 'dilated3_3.ext_conv2.2.weight', 'dilated3_3.ext_conv3.0.weight', 'dilated3_3.ext_conv3.1.weight', 'dilated3_3.ext_conv3.1.bias', 'dilated3_3.ext_conv3.1.running_mean', 'dilated3_3.ext_conv3.1.running_var', 'dilated3_3.ext_conv3.1.num_batches_tracked', 'dilated3_3.ext_conv3.2.weight', 'dilated3_3.out_activation.weight', 'regular3_4.ext_conv1.0.weight', 'regular3_4.ext_conv1.1.weight', 'regular3_4.ext_conv1.1.bias', 'regular3_4.ext_conv1.1.running_mean', 'regular3_4.ext_conv1.1.running_var', 'regular3_4.ext_conv1.1.num_batches_tracked', 'regular3_4.ext_conv1.2.weight', 'regular3_4.ext_conv2.0.weight', 'regular3_4.ext_conv2.1.weight', 'regular3_4.ext_conv2.1.bias', 'regular3_4.ext_conv2.1.running_mean', 'regular3_4.ext_conv2.1.running_var', 'regular3_4.ext_conv2.1.num_batches_tracked', 'regular3_4.ext_conv2.2.weight', 'regular3_4.ext_conv3.0.weight', 'regular3_4.ext_conv3.1.weight', 'regular3_4.ext_conv3.1.bias', 'regular3_4.ext_conv3.1.running_mean', 'regular3_4.ext_conv3.1.running_var', 'regular3_4.ext_conv3.1.num_batches_tracked', 'regular3_4.ext_conv3.2.weight', 'regular3_4.out_activation.weight', 'dilated3_5.ext_conv1.0.weight', 'dilated3_5.ext_conv1.1.weight', 'dilated3_5.ext_conv1.1.bias', 'dilated3_5.ext_conv1.1.running_mean', 'dilated3_5.ext_conv1.1.running_var', 'dilated3_5.ext_conv1.1.num_batches_tracked', 'dilated3_5.ext_conv1.2.weight', 'dilated3_5.ext_conv2.0.weight', 'dilated3_5.ext_conv2.1.weight', 'dilated3_5.ext_conv2.1.bias', 'dilated3_5.ext_conv2.1.running_mean', 'dilated3_5.ext_conv2.1.running_var', 'dilated3_5.ext_conv2.1.num_batches_tracked', 'dilated3_5.ext_conv2.2.weight', 'dilated3_5.ext_conv3.0.weight', 'dilated3_5.ext_conv3.1.weight', 'dilated3_5.ext_conv3.1.bias', 'dilated3_5.ext_conv3.1.running_mean', 'dilated3_5.ext_conv3.1.running_var', 'dilated3_5.ext_conv3.1.num_batches_tracked', 'dilated3_5.ext_conv3.2.weight', 'dilated3_5.out_activation.weight', 'asymmetric3_6.ext_conv1.0.weight', 'asymmetric3_6.ext_conv1.1.weight', 'asymmetric3_6.ext_conv1.1.bias', 'asymmetric3_6.ext_conv1.1.running_mean', 'asymmetric3_6.ext_conv1.1.running_var', 'asymmetric3_6.ext_conv1.1.num_batches_tracked', 'asymmetric3_6.ext_conv1.2.weight', 'asymmetric3_6.ext_conv2.0.weight', 'asymmetric3_6.ext_conv2.1.weight', 'asymmetric3_6.ext_conv2.1.bias', 'asymmetric3_6.ext_conv2.1.running_mean', 'asymmetric3_6.ext_conv2.1.running_var', 'asymmetric3_6.ext_conv2.1.num_batches_tracked', 'asymmetric3_6.ext_conv2.2.weight', 'asymmetric3_6.ext_conv2.3.weight', 'asymmetric3_6.ext_conv2.4.weight', 'asymmetric3_6.ext_conv2.4.bias', 'asymmetric3_6.ext_conv2.4.running_mean', 'asymmetric3_6.ext_conv2.4.running_var', 'asymmetric3_6.ext_conv2.4.num_batches_tracked', 'asymmetric3_6.ext_conv2.5.weight', 'asymmetric3_6.ext_conv3.0.weight', 'asymmetric3_6.ext_conv3.1.weight', 'asymmetric3_6.ext_conv3.1.bias', 'asymmetric3_6.ext_conv3.1.running_mean', 'asymmetric3_6.ext_conv3.1.running_var', 'asymmetric3_6.ext_conv3.1.num_batches_tracked', 'asymmetric3_6.ext_conv3.2.weight', 'asymmetric3_6.out_activation.weight', 'dilated3_7.ext_conv1.0.weight', 'dilated3_7.ext_conv1.1.weight', 'dilated3_7.ext_conv1.1.bias', 'dilated3_7.ext_conv1.1.running_mean', 'dilated3_7.ext_conv1.1.running_var', 'dilated3_7.ext_conv1.1.num_batches_tracked', 'dilated3_7.ext_conv1.2.weight', 'dilated3_7.ext_conv2.0.weight', 'dilated3_7.ext_conv2.1.weight', 'dilated3_7.ext_conv2.1.bias', 'dilated3_7.ext_conv2.1.running_mean', 'dilated3_7.ext_conv2.1.running_var', 'dilated3_7.ext_conv2.1.num_batches_tracked', 'dilated3_7.ext_conv2.2.weight', 'dilated3_7.ext_conv3.0.weight', 'dilated3_7.ext_conv3.1.weight', 'dilated3_7.ext_conv3.1.bias', 'dilated3_7.ext_conv3.1.running_mean', 'dilated3_7.ext_conv3.1.running_var', 'dilated3_7.ext_conv3.1.num_batches_tracked', 'dilated3_7.ext_conv3.2.weight', 'dilated3_7.out_activation.weight', 'upsample4_0.main_conv1.0.weight', 'upsample4_0.main_conv1.1.weight', 'upsample4_0.main_conv1.1.bias', 'upsample4_0.main_conv1.1.running_mean', 'upsample4_0.main_conv1.1.running_var', 'upsample4_0.main_conv1.1.num_batches_tracked', 'upsample4_0.ext_conv1.0.weight', 'upsample4_0.ext_conv1.1.weight', 'upsample4_0.ext_conv1.1.bias', 'upsample4_0.ext_conv1.1.running_mean', 'upsample4_0.ext_conv1.1.running_var', 'upsample4_0.ext_conv1.1.num_batches_tracked', 'upsample4_0.ext_tconv1.weight', 'upsample4_0.ext_tconv1_bnorm.weight', 'upsample4_0.ext_tconv1_bnorm.bias', 'upsample4_0.ext_tconv1_bnorm.running_mean', 'upsample4_0.ext_tconv1_bnorm.running_var', 'upsample4_0.ext_tconv1_bnorm.num_batches_tracked', 'upsample4_0.ext_conv2.0.weight', 'upsample4_0.ext_conv2.1.weight', 'upsample4_0.ext_conv2.1.bias', 'upsample4_0.ext_conv2.1.running_mean', 'upsample4_0.ext_conv2.1.running_var', 'upsample4_0.ext_conv2.1.num_batches_tracked', 'regular4_1.ext_conv1.0.weight', 'regular4_1.ext_conv1.1.weight', 'regular4_1.ext_conv1.1.bias', 'regular4_1.ext_conv1.1.running_mean', 'regular4_1.ext_conv1.1.running_var', 'regular4_1.ext_conv1.1.num_batches_tracked', 'regular4_1.ext_conv2.0.weight', 'regular4_1.ext_conv2.1.weight', 'regular4_1.ext_conv2.1.bias', 'regular4_1.ext_conv2.1.running_mean', 'regular4_1.ext_conv2.1.running_var', 'regular4_1.ext_conv2.1.num_batches_tracked', 'regular4_1.ext_conv3.0.weight', 'regular4_1.ext_conv3.1.weight', 'regular4_1.ext_conv3.1.bias', 'regular4_1.ext_conv3.1.running_mean', 'regular4_1.ext_conv3.1.running_var', 'regular4_1.ext_conv3.1.num_batches_tracked', 'regular4_2.ext_conv1.0.weight', 'regular4_2.ext_conv1.1.weight', 'regular4_2.ext_conv1.1.bias', 'regular4_2.ext_conv1.1.running_mean', 'regular4_2.ext_conv1.1.running_var', 'regular4_2.ext_conv1.1.num_batches_tracked', 'regular4_2.ext_conv2.0.weight', 'regular4_2.ext_conv2.1.weight', 'regular4_2.ext_conv2.1.bias', 'regular4_2.ext_conv2.1.running_mean', 'regular4_2.ext_conv2.1.running_var', 'regular4_2.ext_conv2.1.num_batches_tracked', 'regular4_2.ext_conv3.0.weight', 'regular4_2.ext_conv3.1.weight', 'regular4_2.ext_conv3.1.bias', 'regular4_2.ext_conv3.1.running_mean', 'regular4_2.ext_conv3.1.running_var', 'regular4_2.ext_conv3.1.num_batches_tracked', 'upsample5_0.main_conv1.0.weight', 'upsample5_0.main_conv1.1.weight', 'upsample5_0.main_conv1.1.bias', 'upsample5_0.main_conv1.1.running_mean', 'upsample5_0.main_conv1.1.running_var', 'upsample5_0.main_conv1.1.num_batches_tracked', 'upsample5_0.ext_conv1.0.weight', 'upsample5_0.ext_conv1.1.weight', 'upsample5_0.ext_conv1.1.bias', 'upsample5_0.ext_conv1.1.running_mean', 'upsample5_0.ext_conv1.1.running_var', 'upsample5_0.ext_conv1.1.num_batches_tracked', 'upsample5_0.ext_tconv1.weight', 'upsample5_0.ext_tconv1_bnorm.weight', 'upsample5_0.ext_tconv1_bnorm.bias', 'upsample5_0.ext_tconv1_bnorm.running_mean', 'upsample5_0.ext_tconv1_bnorm.running_var', 'upsample5_0.ext_tconv1_bnorm.num_batches_tracked', 'upsample5_0.ext_conv2.0.weight', 'upsample5_0.ext_conv2.1.weight', 'upsample5_0.ext_conv2.1.bias', 'upsample5_0.ext_conv2.1.running_mean', 'upsample5_0.ext_conv2.1.running_var', 'upsample5_0.ext_conv2.1.num_batches_tracked', 'regular5_1.ext_conv1.0.weight', 'regular5_1.ext_conv1.1.weight', 'regular5_1.ext_conv1.1.bias', 'regular5_1.ext_conv1.1.running_mean', 'regular5_1.ext_conv1.1.running_var', 'regular5_1.ext_conv1.1.num_batches_tracked', 'regular5_1.ext_conv2.0.weight', 'regular5_1.ext_conv2.1.weight', 'regular5_1.ext_conv2.1.bias', 'regular5_1.ext_conv2.1.running_mean', 'regular5_1.ext_conv2.1.running_var', 'regular5_1.ext_conv2.1.num_batches_tracked', 'regular5_1.ext_conv3.0.weight', 'regular5_1.ext_conv3.1.weight', 'regular5_1.ext_conv3.1.bias', 'regular5_1.ext_conv3.1.running_mean', 'regular5_1.ext_conv3.1.running_var', 'regular5_1.ext_conv3.1.num_batches_tracked', 'transposed_conv.weight'])\n",
            "Import Model enet with weights enet_pretrained to FineTune\n",
            "========== TRAINING ===========\n",
            "/content/cityscapes/leftImg8bit/train\n",
            "/content/cityscapes/leftImg8bit/val\n",
            "<class '__main__.CrossEntropyLoss2d'>\n",
            "----- TRAINING - EPOCH 1 -----\n",
            "LEARNING RATE:  5e-05\n",
            "loss: 10.41 (epoch: 1, step: 0) // Avg time/img: 0.6229 s\n",
            "loss: 10.08 (epoch: 1, step: 50) // Avg time/img: 0.0636 s\n",
            "loss: 9.986 (epoch: 1, step: 100) // Avg time/img: 0.0579 s\n",
            "loss: 9.887 (epoch: 1, step: 150) // Avg time/img: 0.0568 s\n",
            "loss: 9.761 (epoch: 1, step: 200) // Avg time/img: 0.0562 s\n",
            "loss: 9.651 (epoch: 1, step: 250) // Avg time/img: 0.0558 s\n",
            "loss: 9.534 (epoch: 1, step: 300) // Avg time/img: 0.0552 s\n",
            "loss: 9.426 (epoch: 1, step: 350) // Avg time/img: 0.0550 s\n",
            "loss: 9.327 (epoch: 1, step: 400) // Avg time/img: 0.0548 s\n",
            "loss: 9.227 (epoch: 1, step: 450) // Avg time/img: 0.0548 s\n",
            "----- VALIDATING - EPOCH 1 -----\n",
            "VAL loss: 8.321 (epoch: 1, step: 0) // Avg time/img: 0.0417 s\n",
            "VAL loss: 7.994 (epoch: 1, step: 50) // Avg time/img: 0.0318 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m1.24\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training1/model-001.pth (epoch: 1)\n",
            "save: ../save/enet_training1/model_best.pth (epoch: 1)\n",
            "----- TRAINING - EPOCH 2 -----\n",
            "LEARNING RATE:  4.774426908107499e-05\n",
            "loss: 7.93 (epoch: 2, step: 0) // Avg time/img: 0.0601 s\n",
            "loss: 7.96 (epoch: 2, step: 50) // Avg time/img: 0.0531 s\n",
            "loss: 7.928 (epoch: 2, step: 100) // Avg time/img: 0.0541 s\n",
            "loss: 7.81 (epoch: 2, step: 150) // Avg time/img: 0.0538 s\n",
            "loss: 7.702 (epoch: 2, step: 200) // Avg time/img: 0.0540 s\n",
            "loss: 7.6 (epoch: 2, step: 250) // Avg time/img: 0.0535 s\n",
            "loss: 7.512 (epoch: 2, step: 300) // Avg time/img: 0.0537 s\n",
            "loss: 7.419 (epoch: 2, step: 350) // Avg time/img: 0.0535 s\n",
            "loss: 7.327 (epoch: 2, step: 400) // Avg time/img: 0.0537 s\n",
            "loss: 7.242 (epoch: 2, step: 450) // Avg time/img: 0.0537 s\n",
            "----- VALIDATING - EPOCH 2 -----\n",
            "VAL loss: 6.321 (epoch: 2, step: 0) // Avg time/img: 0.0525 s\n",
            "VAL loss: 6.296 (epoch: 2, step: 50) // Avg time/img: 0.0338 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m2.38\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training1/model-002.pth (epoch: 2)\n",
            "save: ../save/enet_training1/model_best.pth (epoch: 2)\n",
            "----- TRAINING - EPOCH 3 -----\n",
            "LEARNING RATE:  4.547662880414811e-05\n",
            "loss: 6.217 (epoch: 3, step: 0) // Avg time/img: 0.0586 s\n",
            "loss: 6.255 (epoch: 3, step: 50) // Avg time/img: 0.0535 s\n",
            "loss: 6.177 (epoch: 3, step: 100) // Avg time/img: 0.0529 s\n",
            "loss: 6.106 (epoch: 3, step: 150) // Avg time/img: 0.0526 s\n",
            "loss: 6.04 (epoch: 3, step: 200) // Avg time/img: 0.0529 s\n",
            "loss: 5.954 (epoch: 3, step: 250) // Avg time/img: 0.0528 s\n",
            "loss: 5.891 (epoch: 3, step: 300) // Avg time/img: 0.0533 s\n",
            "loss: 5.815 (epoch: 3, step: 350) // Avg time/img: 0.0533 s\n",
            "loss: 5.741 (epoch: 3, step: 400) // Avg time/img: 0.0532 s\n",
            "loss: 5.675 (epoch: 3, step: 450) // Avg time/img: 0.0534 s\n",
            "----- VALIDATING - EPOCH 3 -----\n",
            "VAL loss: 4.804 (epoch: 3, step: 0) // Avg time/img: 0.0549 s\n",
            "VAL loss: 4.963 (epoch: 3, step: 50) // Avg time/img: 0.0338 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m5.06\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training1/model-003.pth (epoch: 3)\n",
            "save: ../save/enet_training1/model_best.pth (epoch: 3)\n",
            "----- TRAINING - EPOCH 4 -----\n",
            "LEARNING RATE:  4.319634861514096e-05\n",
            "loss: 5.254 (epoch: 4, step: 0) // Avg time/img: 0.0636 s\n",
            "loss: 4.952 (epoch: 4, step: 50) // Avg time/img: 0.0542 s\n",
            "loss: 4.894 (epoch: 4, step: 100) // Avg time/img: 0.0541 s\n",
            "loss: 4.842 (epoch: 4, step: 150) // Avg time/img: 0.0537 s\n",
            "loss: 4.747 (epoch: 4, step: 200) // Avg time/img: 0.0537 s\n",
            "loss: 4.703 (epoch: 4, step: 250) // Avg time/img: 0.0537 s\n",
            "loss: 4.639 (epoch: 4, step: 300) // Avg time/img: 0.0536 s\n",
            "loss: 4.591 (epoch: 4, step: 350) // Avg time/img: 0.0539 s\n",
            "loss: 4.537 (epoch: 4, step: 400) // Avg time/img: 0.0540 s\n",
            "loss: 4.491 (epoch: 4, step: 450) // Avg time/img: 0.0541 s\n",
            "----- VALIDATING - EPOCH 4 -----\n",
            "VAL loss: 3.702 (epoch: 4, step: 0) // Avg time/img: 0.0559 s\n",
            "VAL loss: 3.96 (epoch: 4, step: 50) // Avg time/img: 0.0340 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m8.24\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training1/model-004.pth (epoch: 4)\n",
            "save: ../save/enet_training1/model_best.pth (epoch: 4)\n",
            "----- TRAINING - EPOCH 5 -----\n",
            "LEARNING RATE:  4.090260730254292e-05\n",
            "loss: 4.192 (epoch: 5, step: 0) // Avg time/img: 0.0414 s\n",
            "loss: 4.0 (epoch: 5, step: 50) // Avg time/img: 0.0526 s\n",
            "loss: 3.942 (epoch: 5, step: 100) // Avg time/img: 0.0534 s\n",
            "loss: 3.891 (epoch: 5, step: 150) // Avg time/img: 0.0529 s\n",
            "loss: 3.831 (epoch: 5, step: 200) // Avg time/img: 0.0532 s\n",
            "loss: 3.792 (epoch: 5, step: 250) // Avg time/img: 0.0530 s\n",
            "loss: 3.762 (epoch: 5, step: 300) // Avg time/img: 0.0531 s\n",
            "loss: 3.716 (epoch: 5, step: 350) // Avg time/img: 0.0533 s\n",
            "loss: 3.684 (epoch: 5, step: 400) // Avg time/img: 0.0533 s\n",
            "loss: 3.657 (epoch: 5, step: 450) // Avg time/img: 0.0534 s\n",
            "----- VALIDATING - EPOCH 5 -----\n",
            "VAL loss: 3.008 (epoch: 5, step: 0) // Avg time/img: 0.0495 s\n",
            "VAL loss: 3.35 (epoch: 5, step: 50) // Avg time/img: 0.0348 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m12.11\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training1/model-005.pth (epoch: 5)\n",
            "save: ../save/enet_training1/model_best.pth (epoch: 5)\n",
            "----- TRAINING - EPOCH 6 -----\n",
            "LEARNING RATE:  3.859447533617852e-05\n",
            "loss: 2.847 (epoch: 6, step: 0) // Avg time/img: 0.0525 s\n",
            "loss: 3.266 (epoch: 6, step: 50) // Avg time/img: 0.0549 s\n",
            "loss: 3.236 (epoch: 6, step: 100) // Avg time/img: 0.0541 s\n",
            "loss: 3.186 (epoch: 6, step: 150) // Avg time/img: 0.0544 s\n",
            "loss: 3.159 (epoch: 6, step: 200) // Avg time/img: 0.0542 s\n",
            "loss: 3.147 (epoch: 6, step: 250) // Avg time/img: 0.0537 s\n",
            "loss: 3.121 (epoch: 6, step: 300) // Avg time/img: 0.0541 s\n",
            "loss: 3.082 (epoch: 6, step: 350) // Avg time/img: 0.0539 s\n",
            "loss: 3.056 (epoch: 6, step: 400) // Avg time/img: 0.0541 s\n",
            "loss: 3.027 (epoch: 6, step: 450) // Avg time/img: 0.0540 s\n",
            "----- VALIDATING - EPOCH 6 -----\n",
            "VAL loss: 2.454 (epoch: 6, step: 0) // Avg time/img: 0.0584 s\n",
            "VAL loss: 2.802 (epoch: 6, step: 50) // Avg time/img: 0.0339 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m14.85\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training1/model-006.pth (epoch: 6)\n",
            "save: ../save/enet_training1/model_best.pth (epoch: 6)\n",
            "----- TRAINING - EPOCH 7 -----\n",
            "LEARNING RATE:  3.6270892346861e-05\n",
            "loss: 2.981 (epoch: 7, step: 0) // Avg time/img: 0.0772 s\n",
            "loss: 2.752 (epoch: 7, step: 50) // Avg time/img: 0.0566 s\n",
            "loss: 2.704 (epoch: 7, step: 100) // Avg time/img: 0.0551 s\n",
            "loss: 2.685 (epoch: 7, step: 150) // Avg time/img: 0.0554 s\n",
            "loss: 2.645 (epoch: 7, step: 200) // Avg time/img: 0.0550 s\n",
            "loss: 2.626 (epoch: 7, step: 250) // Avg time/img: 0.0552 s\n",
            "loss: 2.619 (epoch: 7, step: 300) // Avg time/img: 0.0549 s\n",
            "loss: 2.582 (epoch: 7, step: 350) // Avg time/img: 0.0548 s\n",
            "loss: 2.56 (epoch: 7, step: 400) // Avg time/img: 0.0547 s\n",
            "loss: 2.537 (epoch: 7, step: 450) // Avg time/img: 0.0545 s\n",
            "----- VALIDATING - EPOCH 7 -----\n",
            "VAL loss: 2.024 (epoch: 7, step: 0) // Avg time/img: 0.0571 s\n",
            "VAL loss: 2.394 (epoch: 7, step: 50) // Avg time/img: 0.0331 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m17.25\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training1/model-007.pth (epoch: 7)\n",
            "save: ../save/enet_training1/model_best.pth (epoch: 7)\n",
            "----- TRAINING - EPOCH 8 -----\n",
            "LEARNING RATE:  3.393063796290625e-05\n",
            "loss: 2.452 (epoch: 8, step: 0) // Avg time/img: 0.0613 s\n",
            "loss: 2.323 (epoch: 8, step: 50) // Avg time/img: 0.0529 s\n",
            "loss: 2.302 (epoch: 8, step: 100) // Avg time/img: 0.0542 s\n",
            "loss: 2.277 (epoch: 8, step: 150) // Avg time/img: 0.0543 s\n",
            "loss: 2.265 (epoch: 8, step: 200) // Avg time/img: 0.0544 s\n",
            "loss: 2.252 (epoch: 8, step: 250) // Avg time/img: 0.0544 s\n",
            "loss: 2.225 (epoch: 8, step: 300) // Avg time/img: 0.0539 s\n",
            "loss: 2.208 (epoch: 8, step: 350) // Avg time/img: 0.0541 s\n",
            "loss: 2.203 (epoch: 8, step: 400) // Avg time/img: 0.0540 s\n",
            "loss: 2.172 (epoch: 8, step: 450) // Avg time/img: 0.0541 s\n",
            "----- VALIDATING - EPOCH 8 -----\n",
            "VAL loss: 1.754 (epoch: 8, step: 0) // Avg time/img: 0.0584 s\n",
            "VAL loss: 2.102 (epoch: 8, step: 50) // Avg time/img: 0.0345 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m18.76\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training1/model-008.pth (epoch: 8)\n",
            "save: ../save/enet_training1/model_best.pth (epoch: 8)\n",
            "----- TRAINING - EPOCH 9 -----\n",
            "LEARNING RATE:  3.157229337446777e-05\n",
            "loss: 1.984 (epoch: 9, step: 0) // Avg time/img: 0.0516 s\n",
            "loss: 2.026 (epoch: 9, step: 50) // Avg time/img: 0.0556 s\n",
            "loss: 2.006 (epoch: 9, step: 100) // Avg time/img: 0.0545 s\n",
            "loss: 1.997 (epoch: 9, step: 150) // Avg time/img: 0.0543 s\n",
            "loss: 1.965 (epoch: 9, step: 200) // Avg time/img: 0.0542 s\n",
            "loss: 1.955 (epoch: 9, step: 250) // Avg time/img: 0.0540 s\n",
            "loss: 1.933 (epoch: 9, step: 300) // Avg time/img: 0.0545 s\n",
            "loss: 1.912 (epoch: 9, step: 350) // Avg time/img: 0.0544 s\n",
            "loss: 1.903 (epoch: 9, step: 400) // Avg time/img: 0.0545 s\n",
            "loss: 1.881 (epoch: 9, step: 450) // Avg time/img: 0.0544 s\n",
            "----- VALIDATING - EPOCH 9 -----\n",
            "VAL loss: 1.539 (epoch: 9, step: 0) // Avg time/img: 0.0760 s\n",
            "VAL loss: 1.862 (epoch: 9, step: 50) // Avg time/img: 0.0329 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m22.78\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training1/model-009.pth (epoch: 9)\n",
            "save: ../save/enet_training1/model_best.pth (epoch: 9)\n",
            "----- TRAINING - EPOCH 10 -----\n",
            "LEARNING RATE:  2.9194189645999014e-05\n",
            "loss: 1.598 (epoch: 10, step: 0) // Avg time/img: 0.0831 s\n",
            "loss: 1.736 (epoch: 10, step: 50) // Avg time/img: 0.0527 s\n",
            "loss: 1.701 (epoch: 10, step: 100) // Avg time/img: 0.0534 s\n",
            "loss: 1.726 (epoch: 10, step: 150) // Avg time/img: 0.0538 s\n",
            "loss: 1.726 (epoch: 10, step: 200) // Avg time/img: 0.0539 s\n",
            "loss: 1.721 (epoch: 10, step: 250) // Avg time/img: 0.0539 s\n",
            "loss: 1.712 (epoch: 10, step: 300) // Avg time/img: 0.0539 s\n",
            "loss: 1.7 (epoch: 10, step: 350) // Avg time/img: 0.0542 s\n",
            "loss: 1.694 (epoch: 10, step: 400) // Avg time/img: 0.0540 s\n",
            "loss: 1.685 (epoch: 10, step: 450) // Avg time/img: 0.0540 s\n",
            "----- VALIDATING - EPOCH 10 -----\n",
            "VAL loss: 1.355 (epoch: 10, step: 0) // Avg time/img: 0.0579 s\n",
            "VAL loss: 1.684 (epoch: 10, step: 50) // Avg time/img: 0.0350 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m25.10\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training1/model-010.pth (epoch: 10)\n",
            "save: ../save/enet_training1/model_best.pth (epoch: 10)\n",
            "----- TRAINING - EPOCH 11 -----\n",
            "LEARNING RATE:  2.679433656340733e-05\n",
            "loss: 1.355 (epoch: 11, step: 0) // Avg time/img: 0.0565 s\n",
            "loss: 1.636 (epoch: 11, step: 50) // Avg time/img: 0.0520 s\n",
            "loss: 1.587 (epoch: 11, step: 100) // Avg time/img: 0.0540 s\n",
            "loss: 1.562 (epoch: 11, step: 150) // Avg time/img: 0.0538 s\n",
            "loss: 1.558 (epoch: 11, step: 200) // Avg time/img: 0.0541 s\n",
            "loss: 1.541 (epoch: 11, step: 250) // Avg time/img: 0.0539 s\n",
            "loss: 1.538 (epoch: 11, step: 300) // Avg time/img: 0.0539 s\n",
            "loss: 1.527 (epoch: 11, step: 350) // Avg time/img: 0.0542 s\n",
            "loss: 1.52 (epoch: 11, step: 400) // Avg time/img: 0.0540 s\n",
            "loss: 1.512 (epoch: 11, step: 450) // Avg time/img: 0.0540 s\n",
            "----- VALIDATING - EPOCH 11 -----\n",
            "VAL loss: 1.227 (epoch: 11, step: 0) // Avg time/img: 0.0578 s\n",
            "VAL loss: 1.548 (epoch: 11, step: 50) // Avg time/img: 0.0354 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m26.59\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training1/model-011.pth (epoch: 11)\n",
            "save: ../save/enet_training1/model_best.pth (epoch: 11)\n",
            "----- TRAINING - EPOCH 12 -----\n",
            "LEARNING RATE:  2.437032195894977e-05\n",
            "loss: 1.219 (epoch: 12, step: 0) // Avg time/img: 0.0555 s\n",
            "loss: 1.434 (epoch: 12, step: 50) // Avg time/img: 0.0517 s\n",
            "loss: 1.442 (epoch: 12, step: 100) // Avg time/img: 0.0524 s\n",
            "loss: 1.45 (epoch: 12, step: 150) // Avg time/img: 0.0526 s\n",
            "loss: 1.437 (epoch: 12, step: 200) // Avg time/img: 0.0532 s\n",
            "loss: 1.415 (epoch: 12, step: 250) // Avg time/img: 0.0531 s\n",
            "loss: 1.416 (epoch: 12, step: 300) // Avg time/img: 0.0534 s\n",
            "loss: 1.4 (epoch: 12, step: 350) // Avg time/img: 0.0533 s\n",
            "loss: 1.389 (epoch: 12, step: 400) // Avg time/img: 0.0532 s\n",
            "loss: 1.387 (epoch: 12, step: 450) // Avg time/img: 0.0532 s\n",
            "----- VALIDATING - EPOCH 12 -----\n",
            "VAL loss: 1.138 (epoch: 12, step: 0) // Avg time/img: 0.0574 s\n",
            "VAL loss: 1.443 (epoch: 12, step: 50) // Avg time/img: 0.0327 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m28.11\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training1/model-012.pth (epoch: 12)\n",
            "save: ../save/enet_training1/model_best.pth (epoch: 12)\n",
            "----- TRAINING - EPOCH 13 -----\n",
            "LEARNING RATE:  2.191916452770435e-05\n",
            "loss: 1.304 (epoch: 13, step: 0) // Avg time/img: 0.0546 s\n",
            "loss: 1.334 (epoch: 13, step: 50) // Avg time/img: 0.0547 s\n",
            "loss: 1.306 (epoch: 13, step: 100) // Avg time/img: 0.0540 s\n",
            "loss: 1.319 (epoch: 13, step: 150) // Avg time/img: 0.0541 s\n",
            "loss: 1.319 (epoch: 13, step: 200) // Avg time/img: 0.0540 s\n",
            "loss: 1.299 (epoch: 13, step: 250) // Avg time/img: 0.0541 s\n",
            "loss: 1.295 (epoch: 13, step: 300) // Avg time/img: 0.0541 s\n",
            "loss: 1.287 (epoch: 13, step: 350) // Avg time/img: 0.0538 s\n",
            "loss: 1.279 (epoch: 13, step: 400) // Avg time/img: 0.0541 s\n",
            "loss: 1.278 (epoch: 13, step: 450) // Avg time/img: 0.0541 s\n",
            "----- VALIDATING - EPOCH 13 -----\n",
            "VAL loss: 1.03 (epoch: 13, step: 0) // Avg time/img: 0.0693 s\n",
            "VAL loss: 1.342 (epoch: 13, step: 50) // Avg time/img: 0.0359 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m29.28\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training1/model-013.pth (epoch: 13)\n",
            "save: ../save/enet_training1/model_best.pth (epoch: 13)\n",
            "----- TRAINING - EPOCH 14 -----\n",
            "LEARNING RATE:  1.9437089939938174e-05\n",
            "loss: 1.462 (epoch: 14, step: 0) // Avg time/img: 0.0675 s\n",
            "loss: 1.215 (epoch: 14, step: 50) // Avg time/img: 0.0552 s\n",
            "loss: 1.192 (epoch: 14, step: 100) // Avg time/img: 0.0544 s\n",
            "loss: 1.201 (epoch: 14, step: 150) // Avg time/img: 0.0545 s\n",
            "loss: 1.193 (epoch: 14, step: 200) // Avg time/img: 0.0546 s\n",
            "loss: 1.196 (epoch: 14, step: 250) // Avg time/img: 0.0551 s\n",
            "loss: 1.206 (epoch: 14, step: 300) // Avg time/img: 0.0545 s\n",
            "loss: 1.2 (epoch: 14, step: 350) // Avg time/img: 0.0548 s\n",
            "loss: 1.199 (epoch: 14, step: 400) // Avg time/img: 0.0546 s\n",
            "loss: 1.201 (epoch: 14, step: 450) // Avg time/img: 0.0546 s\n",
            "----- VALIDATING - EPOCH 14 -----\n",
            "VAL loss: 0.9944 (epoch: 14, step: 0) // Avg time/img: 0.0537 s\n",
            "VAL loss: 1.267 (epoch: 14, step: 50) // Avg time/img: 0.0343 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m30.69\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training1/model-014.pth (epoch: 14)\n",
            "save: ../save/enet_training1/model_best.pth (epoch: 14)\n",
            "----- TRAINING - EPOCH 15 -----\n",
            "LEARNING RATE:  1.6919173095082493e-05\n",
            "loss: 1.076 (epoch: 15, step: 0) // Avg time/img: 0.0529 s\n",
            "loss: 1.125 (epoch: 15, step: 50) // Avg time/img: 0.0529 s\n",
            "loss: 1.137 (epoch: 15, step: 100) // Avg time/img: 0.0548 s\n",
            "loss: 1.152 (epoch: 15, step: 150) // Avg time/img: 0.0543 s\n",
            "loss: 1.156 (epoch: 15, step: 200) // Avg time/img: 0.0548 s\n",
            "loss: 1.148 (epoch: 15, step: 250) // Avg time/img: 0.0547 s\n",
            "loss: 1.145 (epoch: 15, step: 300) // Avg time/img: 0.0549 s\n",
            "loss: 1.136 (epoch: 15, step: 350) // Avg time/img: 0.0547 s\n",
            "loss: 1.133 (epoch: 15, step: 400) // Avg time/img: 0.0547 s\n",
            "loss: 1.127 (epoch: 15, step: 450) // Avg time/img: 0.0548 s\n",
            "----- VALIDATING - EPOCH 15 -----\n",
            "VAL loss: 0.9062 (epoch: 15, step: 0) // Avg time/img: 0.0501 s\n",
            "VAL loss: 1.202 (epoch: 15, step: 50) // Avg time/img: 0.0352 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m31.87\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training1/model-015.pth (epoch: 15)\n",
            "save: ../save/enet_training1/model_best.pth (epoch: 15)\n",
            "----- TRAINING - EPOCH 16 -----\n",
            "LEARNING RATE:  1.4358729437462937e-05\n",
            "loss: 1.037 (epoch: 16, step: 0) // Avg time/img: 0.0593 s\n",
            "loss: 1.129 (epoch: 16, step: 50) // Avg time/img: 0.0555 s\n",
            "loss: 1.119 (epoch: 16, step: 100) // Avg time/img: 0.0546 s\n",
            "loss: 1.089 (epoch: 16, step: 150) // Avg time/img: 0.0549 s\n",
            "loss: 1.086 (epoch: 16, step: 200) // Avg time/img: 0.0547 s\n",
            "loss: 1.072 (epoch: 16, step: 250) // Avg time/img: 0.0548 s\n",
            "loss: 1.063 (epoch: 16, step: 300) // Avg time/img: 0.0547 s\n",
            "loss: 1.069 (epoch: 16, step: 350) // Avg time/img: 0.0549 s\n",
            "loss: 1.069 (epoch: 16, step: 400) // Avg time/img: 0.0547 s\n",
            "loss: 1.067 (epoch: 16, step: 450) // Avg time/img: 0.0550 s\n",
            "----- VALIDATING - EPOCH 16 -----\n",
            "VAL loss: 0.8613 (epoch: 16, step: 0) // Avg time/img: 0.0559 s\n",
            "VAL loss: 1.158 (epoch: 16, step: 50) // Avg time/img: 0.0348 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m32.83\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training1/model-016.pth (epoch: 16)\n",
            "save: ../save/enet_training1/model_best.pth (epoch: 16)\n",
            "----- TRAINING - EPOCH 17 -----\n",
            "LEARNING RATE:  1.1746189430880188e-05\n",
            "loss: 0.8075 (epoch: 17, step: 0) // Avg time/img: 0.0529 s\n",
            "loss: 1.077 (epoch: 17, step: 50) // Avg time/img: 0.0544 s\n",
            "loss: 1.07 (epoch: 17, step: 100) // Avg time/img: 0.0550 s\n",
            "loss: 1.053 (epoch: 17, step: 150) // Avg time/img: 0.0553 s\n",
            "loss: 1.039 (epoch: 17, step: 200) // Avg time/img: 0.0560 s\n",
            "loss: 1.025 (epoch: 17, step: 250) // Avg time/img: 0.0555 s\n",
            "loss: 1.023 (epoch: 17, step: 300) // Avg time/img: 0.0559 s\n",
            "loss: 1.023 (epoch: 17, step: 350) // Avg time/img: 0.0556 s\n",
            "loss: 1.025 (epoch: 17, step: 400) // Avg time/img: 0.0555 s\n",
            "loss: 1.019 (epoch: 17, step: 450) // Avg time/img: 0.0552 s\n",
            "----- VALIDATING - EPOCH 17 -----\n",
            "VAL loss: 0.8445 (epoch: 17, step: 0) // Avg time/img: 0.0481 s\n",
            "VAL loss: 1.118 (epoch: 17, step: 50) // Avg time/img: 0.0333 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m33.44\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training1/model-017.pth (epoch: 17)\n",
            "save: ../save/enet_training1/model_best.pth (epoch: 17)\n",
            "----- TRAINING - EPOCH 18 -----\n",
            "LEARNING RATE:  9.066760365683729e-06\n",
            "loss: 1.003 (epoch: 18, step: 0) // Avg time/img: 0.0772 s\n",
            "loss: 0.9728 (epoch: 18, step: 50) // Avg time/img: 0.0537 s\n",
            "loss: 0.9814 (epoch: 18, step: 100) // Avg time/img: 0.0543 s\n",
            "loss: 0.9928 (epoch: 18, step: 150) // Avg time/img: 0.0548 s\n",
            "loss: 0.9875 (epoch: 18, step: 200) // Avg time/img: 0.0546 s\n",
            "loss: 0.9903 (epoch: 18, step: 250) // Avg time/img: 0.0547 s\n",
            "loss: 0.987 (epoch: 18, step: 300) // Avg time/img: 0.0546 s\n",
            "loss: 0.9849 (epoch: 18, step: 350) // Avg time/img: 0.0546 s\n",
            "loss: 0.9821 (epoch: 18, step: 400) // Avg time/img: 0.0545 s\n",
            "loss: 0.9828 (epoch: 18, step: 450) // Avg time/img: 0.0548 s\n",
            "----- VALIDATING - EPOCH 18 -----\n",
            "VAL loss: 0.8267 (epoch: 18, step: 0) // Avg time/img: 0.0498 s\n",
            "VAL loss: 1.095 (epoch: 18, step: 50) // Avg time/img: 0.0350 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m33.71\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training1/model-018.pth (epoch: 18)\n",
            "save: ../save/enet_training1/model_best.pth (epoch: 18)\n",
            "----- TRAINING - EPOCH 19 -----\n",
            "LEARNING RATE:  6.294627058970836e-06\n",
            "loss: 0.8197 (epoch: 19, step: 0) // Avg time/img: 0.0633 s\n",
            "loss: 0.9671 (epoch: 19, step: 50) // Avg time/img: 0.0529 s\n",
            "loss: 0.984 (epoch: 19, step: 100) // Avg time/img: 0.0538 s\n",
            "loss: 0.9722 (epoch: 19, step: 150) // Avg time/img: 0.0541 s\n",
            "loss: 0.9649 (epoch: 19, step: 200) // Avg time/img: 0.0544 s\n",
            "loss: 0.9679 (epoch: 19, step: 250) // Avg time/img: 0.0540 s\n",
            "loss: 0.9711 (epoch: 19, step: 300) // Avg time/img: 0.0540 s\n",
            "loss: 0.9675 (epoch: 19, step: 350) // Avg time/img: 0.0540 s\n",
            "loss: 0.9665 (epoch: 19, step: 400) // Avg time/img: 0.0544 s\n",
            "loss: 0.9619 (epoch: 19, step: 450) // Avg time/img: 0.0546 s\n",
            "----- VALIDATING - EPOCH 19 -----\n",
            "VAL loss: 0.792 (epoch: 19, step: 0) // Avg time/img: 0.0560 s\n",
            "VAL loss: 1.067 (epoch: 19, step: 50) // Avg time/img: 0.0350 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m34.07\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training1/model-019.pth (epoch: 19)\n",
            "save: ../save/enet_training1/model_best.pth (epoch: 19)\n",
            "----- TRAINING - EPOCH 20 -----\n",
            "LEARNING RATE:  3.373207119183911e-06\n",
            "loss: 0.8169 (epoch: 20, step: 0) // Avg time/img: 0.0620 s\n",
            "loss: 0.9404 (epoch: 20, step: 50) // Avg time/img: 0.0537 s\n",
            "loss: 0.9419 (epoch: 20, step: 100) // Avg time/img: 0.0538 s\n",
            "loss: 0.9288 (epoch: 20, step: 150) // Avg time/img: 0.0547 s\n",
            "loss: 0.9415 (epoch: 20, step: 200) // Avg time/img: 0.0550 s\n",
            "loss: 0.952 (epoch: 20, step: 250) // Avg time/img: 0.0551 s\n",
            "loss: 0.9574 (epoch: 20, step: 300) // Avg time/img: 0.0550 s\n",
            "loss: 0.9569 (epoch: 20, step: 350) // Avg time/img: 0.0552 s\n",
            "loss: 0.9585 (epoch: 20, step: 400) // Avg time/img: 0.0552 s\n",
            "loss: 0.9473 (epoch: 20, step: 450) // Avg time/img: 0.0552 s\n",
            "----- VALIDATING - EPOCH 20 -----\n",
            "VAL loss: 0.7734 (epoch: 20, step: 0) // Avg time/img: 0.0648 s\n",
            "VAL loss: 1.056 (epoch: 20, step: 50) // Avg time/img: 0.0374 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m34.19\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training1/model-020.pth (epoch: 20)\n",
            "save: ../save/enet_training1/model_best.pth (epoch: 20)\n",
            "========== TRAINING FINISHED ===========\n",
            "updating: content/AnomalySegmentation/save/enet_training1/ (stored 0%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/automated_log.txt (deflated 66%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model.txt (deflated 96%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/enet.py (deflated 84%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/opts.txt (deflated 37%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model_best.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-002.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-001.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-003.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-006.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-010.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-005.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-007.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/best.txt (stored 0%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model_best.pth.tar (deflated 19%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-008.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-009.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/checkpoint.pth.tar (deflated 19%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-004.pth (deflated 18%)\n",
            "  adding: content/AnomalySegmentation/save/enet_training1/model-014.pth (deflated 18%)\n",
            "  adding: content/AnomalySegmentation/save/enet_training1/model-011.pth (deflated 18%)\n",
            "  adding: content/AnomalySegmentation/save/enet_training1/model-020.pth (deflated 18%)\n",
            "  adding: content/AnomalySegmentation/save/enet_training1/model-015.pth (deflated 18%)\n",
            "  adding: content/AnomalySegmentation/save/enet_training1/model-012.pth (deflated 18%)\n",
            "  adding: content/AnomalySegmentation/save/enet_training1/model-018.pth (deflated 18%)\n",
            "  adding: content/AnomalySegmentation/save/enet_training1/model-016.pth (deflated 18%)\n",
            "  adding: content/AnomalySegmentation/save/enet_training1/model-019.pth (deflated 18%)\n",
            "  adding: content/AnomalySegmentation/save/enet_training1/model-013.pth (deflated 18%)\n",
            "  adding: content/AnomalySegmentation/save/enet_training1/model-017.pth (deflated 18%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r save_enet_training1.zip /content/AnomalySegmentation/save/enet_training1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYLM7axZRkRx",
        "outputId": "5ffbe005-ee11-4208-aad9-d94a56bcf9d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updating: content/AnomalySegmentation/save/enet_training1/ (stored 0%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/automated_log.txt (deflated 66%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model.txt (deflated 96%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/enet.py (deflated 84%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/opts.txt (deflated 37%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model_best.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-002.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-001.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-003.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-006.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-010.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-005.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-007.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/best.txt (stored 0%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model_best.pth.tar (deflated 19%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-008.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-009.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/checkpoint.pth.tar (deflated 19%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-004.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-014.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-011.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-020.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-015.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-012.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-018.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-016.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-019.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-013.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-017.pth (deflated 18%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9hzrOzuQRjuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####BISENET"
      ],
      "metadata": {
        "id": "Cm1ITQbYM9n7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/AnomalySegmentation && git pull"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f80b9df-737a-492a-f51b-2daa5e46cb65",
        "collapsed": true,
        "id": "wyZLcBENT_Hy"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "remote: Enumerating objects: 7, done.\u001b[K\n",
            "remote: Counting objects:  14% (1/7)\u001b[K\rremote: Counting objects:  28% (2/7)\u001b[K\rremote: Counting objects:  42% (3/7)\u001b[K\rremote: Counting objects:  57% (4/7)\u001b[K\rremote: Counting objects:  71% (5/7)\u001b[K\rremote: Counting objects:  85% (6/7)\u001b[K\rremote: Counting objects: 100% (7/7)\u001b[K\rremote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1/1)\u001b[K\rremote: Compressing objects: 100% (1/1), done.\u001b[K\n",
            "remote: Total 4 (delta 3), reused 4 (delta 3), pack-reused 0 (from 0)\u001b[K\n",
            "Unpacking objects:  25% (1/4)\rUnpacking objects:  50% (2/4)\rUnpacking objects:  75% (3/4)\rUnpacking objects: 100% (4/4)\rUnpacking objects: 100% (4/4), 358 bytes | 179.00 KiB/s, done.\n",
            "From https://github.com/RonPlusSign/AnomalySegmentation\n",
            "   07c6057..bf241fd  main       -> origin/main\n",
            "Updating 07c6057..bf241fd\n",
            "Fast-forward\n",
            " train/main.py | 2 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " 1 file changed, 1 insertion(+), 1 deletion(-)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Finetune BiSeNet (20 epochs)\n",
        "!cd /content/AnomalySegmentation/train; python -W ignore main.py --savedir bisenet_training1 --datadir /content/cityscapes --model bisenet --cuda --num-epochs=20 --epochs-save=1 --FineTune --loadWeights=bisenetv1_pretrained.pth\n",
        "!zip -r save_bisenet_training1.zip /content/AnomalySegmentation/save/bisenet_training1"
      ],
      "metadata": {
        "id": "h-ftZ-p1Yn9s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f13ccc48-93d2-4527-e695-a693a2af691d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "odict_keys(['cp.resnet.conv1.weight', 'cp.resnet.bn1.weight', 'cp.resnet.bn1.bias', 'cp.resnet.bn1.running_mean', 'cp.resnet.bn1.running_var', 'cp.resnet.bn1.num_batches_tracked', 'cp.resnet.layer1.0.conv1.weight', 'cp.resnet.layer1.0.bn1.weight', 'cp.resnet.layer1.0.bn1.bias', 'cp.resnet.layer1.0.bn1.running_mean', 'cp.resnet.layer1.0.bn1.running_var', 'cp.resnet.layer1.0.bn1.num_batches_tracked', 'cp.resnet.layer1.0.conv2.weight', 'cp.resnet.layer1.0.bn2.weight', 'cp.resnet.layer1.0.bn2.bias', 'cp.resnet.layer1.0.bn2.running_mean', 'cp.resnet.layer1.0.bn2.running_var', 'cp.resnet.layer1.0.bn2.num_batches_tracked', 'cp.resnet.layer1.1.conv1.weight', 'cp.resnet.layer1.1.bn1.weight', 'cp.resnet.layer1.1.bn1.bias', 'cp.resnet.layer1.1.bn1.running_mean', 'cp.resnet.layer1.1.bn1.running_var', 'cp.resnet.layer1.1.bn1.num_batches_tracked', 'cp.resnet.layer1.1.conv2.weight', 'cp.resnet.layer1.1.bn2.weight', 'cp.resnet.layer1.1.bn2.bias', 'cp.resnet.layer1.1.bn2.running_mean', 'cp.resnet.layer1.1.bn2.running_var', 'cp.resnet.layer1.1.bn2.num_batches_tracked', 'cp.resnet.layer2.0.conv1.weight', 'cp.resnet.layer2.0.bn1.weight', 'cp.resnet.layer2.0.bn1.bias', 'cp.resnet.layer2.0.bn1.running_mean', 'cp.resnet.layer2.0.bn1.running_var', 'cp.resnet.layer2.0.bn1.num_batches_tracked', 'cp.resnet.layer2.0.conv2.weight', 'cp.resnet.layer2.0.bn2.weight', 'cp.resnet.layer2.0.bn2.bias', 'cp.resnet.layer2.0.bn2.running_mean', 'cp.resnet.layer2.0.bn2.running_var', 'cp.resnet.layer2.0.bn2.num_batches_tracked', 'cp.resnet.layer2.0.downsample.0.weight', 'cp.resnet.layer2.0.downsample.1.weight', 'cp.resnet.layer2.0.downsample.1.bias', 'cp.resnet.layer2.0.downsample.1.running_mean', 'cp.resnet.layer2.0.downsample.1.running_var', 'cp.resnet.layer2.0.downsample.1.num_batches_tracked', 'cp.resnet.layer2.1.conv1.weight', 'cp.resnet.layer2.1.bn1.weight', 'cp.resnet.layer2.1.bn1.bias', 'cp.resnet.layer2.1.bn1.running_mean', 'cp.resnet.layer2.1.bn1.running_var', 'cp.resnet.layer2.1.bn1.num_batches_tracked', 'cp.resnet.layer2.1.conv2.weight', 'cp.resnet.layer2.1.bn2.weight', 'cp.resnet.layer2.1.bn2.bias', 'cp.resnet.layer2.1.bn2.running_mean', 'cp.resnet.layer2.1.bn2.running_var', 'cp.resnet.layer2.1.bn2.num_batches_tracked', 'cp.resnet.layer3.0.conv1.weight', 'cp.resnet.layer3.0.bn1.weight', 'cp.resnet.layer3.0.bn1.bias', 'cp.resnet.layer3.0.bn1.running_mean', 'cp.resnet.layer3.0.bn1.running_var', 'cp.resnet.layer3.0.bn1.num_batches_tracked', 'cp.resnet.layer3.0.conv2.weight', 'cp.resnet.layer3.0.bn2.weight', 'cp.resnet.layer3.0.bn2.bias', 'cp.resnet.layer3.0.bn2.running_mean', 'cp.resnet.layer3.0.bn2.running_var', 'cp.resnet.layer3.0.bn2.num_batches_tracked', 'cp.resnet.layer3.0.downsample.0.weight', 'cp.resnet.layer3.0.downsample.1.weight', 'cp.resnet.layer3.0.downsample.1.bias', 'cp.resnet.layer3.0.downsample.1.running_mean', 'cp.resnet.layer3.0.downsample.1.running_var', 'cp.resnet.layer3.0.downsample.1.num_batches_tracked', 'cp.resnet.layer3.1.conv1.weight', 'cp.resnet.layer3.1.bn1.weight', 'cp.resnet.layer3.1.bn1.bias', 'cp.resnet.layer3.1.bn1.running_mean', 'cp.resnet.layer3.1.bn1.running_var', 'cp.resnet.layer3.1.bn1.num_batches_tracked', 'cp.resnet.layer3.1.conv2.weight', 'cp.resnet.layer3.1.bn2.weight', 'cp.resnet.layer3.1.bn2.bias', 'cp.resnet.layer3.1.bn2.running_mean', 'cp.resnet.layer3.1.bn2.running_var', 'cp.resnet.layer3.1.bn2.num_batches_tracked', 'cp.resnet.layer4.0.conv1.weight', 'cp.resnet.layer4.0.bn1.weight', 'cp.resnet.layer4.0.bn1.bias', 'cp.resnet.layer4.0.bn1.running_mean', 'cp.resnet.layer4.0.bn1.running_var', 'cp.resnet.layer4.0.bn1.num_batches_tracked', 'cp.resnet.layer4.0.conv2.weight', 'cp.resnet.layer4.0.bn2.weight', 'cp.resnet.layer4.0.bn2.bias', 'cp.resnet.layer4.0.bn2.running_mean', 'cp.resnet.layer4.0.bn2.running_var', 'cp.resnet.layer4.0.bn2.num_batches_tracked', 'cp.resnet.layer4.0.downsample.0.weight', 'cp.resnet.layer4.0.downsample.1.weight', 'cp.resnet.layer4.0.downsample.1.bias', 'cp.resnet.layer4.0.downsample.1.running_mean', 'cp.resnet.layer4.0.downsample.1.running_var', 'cp.resnet.layer4.0.downsample.1.num_batches_tracked', 'cp.resnet.layer4.1.conv1.weight', 'cp.resnet.layer4.1.bn1.weight', 'cp.resnet.layer4.1.bn1.bias', 'cp.resnet.layer4.1.bn1.running_mean', 'cp.resnet.layer4.1.bn1.running_var', 'cp.resnet.layer4.1.bn1.num_batches_tracked', 'cp.resnet.layer4.1.conv2.weight', 'cp.resnet.layer4.1.bn2.weight', 'cp.resnet.layer4.1.bn2.bias', 'cp.resnet.layer4.1.bn2.running_mean', 'cp.resnet.layer4.1.bn2.running_var', 'cp.resnet.layer4.1.bn2.num_batches_tracked', 'cp.arm16.conv.conv.weight', 'cp.arm16.conv.bn.weight', 'cp.arm16.conv.bn.bias', 'cp.arm16.conv.bn.running_mean', 'cp.arm16.conv.bn.running_var', 'cp.arm16.conv.bn.num_batches_tracked', 'cp.arm16.conv_atten.weight', 'cp.arm16.bn_atten.weight', 'cp.arm16.bn_atten.bias', 'cp.arm16.bn_atten.running_mean', 'cp.arm16.bn_atten.running_var', 'cp.arm16.bn_atten.num_batches_tracked', 'cp.arm32.conv.conv.weight', 'cp.arm32.conv.bn.weight', 'cp.arm32.conv.bn.bias', 'cp.arm32.conv.bn.running_mean', 'cp.arm32.conv.bn.running_var', 'cp.arm32.conv.bn.num_batches_tracked', 'cp.arm32.conv_atten.weight', 'cp.arm32.bn_atten.weight', 'cp.arm32.bn_atten.bias', 'cp.arm32.bn_atten.running_mean', 'cp.arm32.bn_atten.running_var', 'cp.arm32.bn_atten.num_batches_tracked', 'cp.conv_head32.conv.weight', 'cp.conv_head32.bn.weight', 'cp.conv_head32.bn.bias', 'cp.conv_head32.bn.running_mean', 'cp.conv_head32.bn.running_var', 'cp.conv_head32.bn.num_batches_tracked', 'cp.conv_head16.conv.weight', 'cp.conv_head16.bn.weight', 'cp.conv_head16.bn.bias', 'cp.conv_head16.bn.running_mean', 'cp.conv_head16.bn.running_var', 'cp.conv_head16.bn.num_batches_tracked', 'cp.conv_avg.conv.weight', 'cp.conv_avg.bn.weight', 'cp.conv_avg.bn.bias', 'cp.conv_avg.bn.running_mean', 'cp.conv_avg.bn.running_var', 'cp.conv_avg.bn.num_batches_tracked', 'sp.conv1.conv.weight', 'sp.conv1.bn.weight', 'sp.conv1.bn.bias', 'sp.conv1.bn.running_mean', 'sp.conv1.bn.running_var', 'sp.conv1.bn.num_batches_tracked', 'sp.conv2.conv.weight', 'sp.conv2.bn.weight', 'sp.conv2.bn.bias', 'sp.conv2.bn.running_mean', 'sp.conv2.bn.running_var', 'sp.conv2.bn.num_batches_tracked', 'sp.conv3.conv.weight', 'sp.conv3.bn.weight', 'sp.conv3.bn.bias', 'sp.conv3.bn.running_mean', 'sp.conv3.bn.running_var', 'sp.conv3.bn.num_batches_tracked', 'sp.conv_out.conv.weight', 'sp.conv_out.bn.weight', 'sp.conv_out.bn.bias', 'sp.conv_out.bn.running_mean', 'sp.conv_out.bn.running_var', 'sp.conv_out.bn.num_batches_tracked', 'ffm.convblk.conv.weight', 'ffm.convblk.bn.weight', 'ffm.convblk.bn.bias', 'ffm.convblk.bn.running_mean', 'ffm.convblk.bn.running_var', 'ffm.convblk.bn.num_batches_tracked', 'ffm.conv.weight', 'ffm.bn.weight', 'ffm.bn.bias', 'ffm.bn.running_mean', 'ffm.bn.running_var', 'ffm.bn.num_batches_tracked', 'conv_out.conv.conv.weight', 'conv_out.conv.bn.weight', 'conv_out.conv.bn.bias', 'conv_out.conv.bn.running_mean', 'conv_out.conv.bn.running_var', 'conv_out.conv.bn.num_batches_tracked', 'conv_out.conv_out.weight', 'conv_out.conv_out.bias', 'conv_out16.conv.conv.weight', 'conv_out16.conv.bn.weight', 'conv_out16.conv.bn.bias', 'conv_out16.conv.bn.running_mean', 'conv_out16.conv.bn.running_var', 'conv_out16.conv.bn.num_batches_tracked', 'conv_out16.conv_out.weight', 'conv_out16.conv_out.bias', 'conv_out32.conv.conv.weight', 'conv_out32.conv.bn.weight', 'conv_out32.conv.bn.bias', 'conv_out32.conv.bn.running_mean', 'conv_out32.conv.bn.running_var', 'conv_out32.conv.bn.num_batches_tracked', 'conv_out32.conv_out.weight', 'conv_out32.conv_out.bias'])\n",
            "odict_keys(['cp.resnet.conv1.weight', 'cp.resnet.bn1.weight', 'cp.resnet.bn1.bias', 'cp.resnet.bn1.running_mean', 'cp.resnet.bn1.running_var', 'cp.resnet.bn1.num_batches_tracked', 'cp.resnet.layer1.0.conv1.weight', 'cp.resnet.layer1.0.bn1.weight', 'cp.resnet.layer1.0.bn1.bias', 'cp.resnet.layer1.0.bn1.running_mean', 'cp.resnet.layer1.0.bn1.running_var', 'cp.resnet.layer1.0.bn1.num_batches_tracked', 'cp.resnet.layer1.0.conv2.weight', 'cp.resnet.layer1.0.bn2.weight', 'cp.resnet.layer1.0.bn2.bias', 'cp.resnet.layer1.0.bn2.running_mean', 'cp.resnet.layer1.0.bn2.running_var', 'cp.resnet.layer1.0.bn2.num_batches_tracked', 'cp.resnet.layer1.1.conv1.weight', 'cp.resnet.layer1.1.bn1.weight', 'cp.resnet.layer1.1.bn1.bias', 'cp.resnet.layer1.1.bn1.running_mean', 'cp.resnet.layer1.1.bn1.running_var', 'cp.resnet.layer1.1.bn1.num_batches_tracked', 'cp.resnet.layer1.1.conv2.weight', 'cp.resnet.layer1.1.bn2.weight', 'cp.resnet.layer1.1.bn2.bias', 'cp.resnet.layer1.1.bn2.running_mean', 'cp.resnet.layer1.1.bn2.running_var', 'cp.resnet.layer1.1.bn2.num_batches_tracked', 'cp.resnet.layer2.0.conv1.weight', 'cp.resnet.layer2.0.bn1.weight', 'cp.resnet.layer2.0.bn1.bias', 'cp.resnet.layer2.0.bn1.running_mean', 'cp.resnet.layer2.0.bn1.running_var', 'cp.resnet.layer2.0.bn1.num_batches_tracked', 'cp.resnet.layer2.0.conv2.weight', 'cp.resnet.layer2.0.bn2.weight', 'cp.resnet.layer2.0.bn2.bias', 'cp.resnet.layer2.0.bn2.running_mean', 'cp.resnet.layer2.0.bn2.running_var', 'cp.resnet.layer2.0.bn2.num_batches_tracked', 'cp.resnet.layer2.0.downsample.0.weight', 'cp.resnet.layer2.0.downsample.1.weight', 'cp.resnet.layer2.0.downsample.1.bias', 'cp.resnet.layer2.0.downsample.1.running_mean', 'cp.resnet.layer2.0.downsample.1.running_var', 'cp.resnet.layer2.0.downsample.1.num_batches_tracked', 'cp.resnet.layer2.1.conv1.weight', 'cp.resnet.layer2.1.bn1.weight', 'cp.resnet.layer2.1.bn1.bias', 'cp.resnet.layer2.1.bn1.running_mean', 'cp.resnet.layer2.1.bn1.running_var', 'cp.resnet.layer2.1.bn1.num_batches_tracked', 'cp.resnet.layer2.1.conv2.weight', 'cp.resnet.layer2.1.bn2.weight', 'cp.resnet.layer2.1.bn2.bias', 'cp.resnet.layer2.1.bn2.running_mean', 'cp.resnet.layer2.1.bn2.running_var', 'cp.resnet.layer2.1.bn2.num_batches_tracked', 'cp.resnet.layer3.0.conv1.weight', 'cp.resnet.layer3.0.bn1.weight', 'cp.resnet.layer3.0.bn1.bias', 'cp.resnet.layer3.0.bn1.running_mean', 'cp.resnet.layer3.0.bn1.running_var', 'cp.resnet.layer3.0.bn1.num_batches_tracked', 'cp.resnet.layer3.0.conv2.weight', 'cp.resnet.layer3.0.bn2.weight', 'cp.resnet.layer3.0.bn2.bias', 'cp.resnet.layer3.0.bn2.running_mean', 'cp.resnet.layer3.0.bn2.running_var', 'cp.resnet.layer3.0.bn2.num_batches_tracked', 'cp.resnet.layer3.0.downsample.0.weight', 'cp.resnet.layer3.0.downsample.1.weight', 'cp.resnet.layer3.0.downsample.1.bias', 'cp.resnet.layer3.0.downsample.1.running_mean', 'cp.resnet.layer3.0.downsample.1.running_var', 'cp.resnet.layer3.0.downsample.1.num_batches_tracked', 'cp.resnet.layer3.1.conv1.weight', 'cp.resnet.layer3.1.bn1.weight', 'cp.resnet.layer3.1.bn1.bias', 'cp.resnet.layer3.1.bn1.running_mean', 'cp.resnet.layer3.1.bn1.running_var', 'cp.resnet.layer3.1.bn1.num_batches_tracked', 'cp.resnet.layer3.1.conv2.weight', 'cp.resnet.layer3.1.bn2.weight', 'cp.resnet.layer3.1.bn2.bias', 'cp.resnet.layer3.1.bn2.running_mean', 'cp.resnet.layer3.1.bn2.running_var', 'cp.resnet.layer3.1.bn2.num_batches_tracked', 'cp.resnet.layer4.0.conv1.weight', 'cp.resnet.layer4.0.bn1.weight', 'cp.resnet.layer4.0.bn1.bias', 'cp.resnet.layer4.0.bn1.running_mean', 'cp.resnet.layer4.0.bn1.running_var', 'cp.resnet.layer4.0.bn1.num_batches_tracked', 'cp.resnet.layer4.0.conv2.weight', 'cp.resnet.layer4.0.bn2.weight', 'cp.resnet.layer4.0.bn2.bias', 'cp.resnet.layer4.0.bn2.running_mean', 'cp.resnet.layer4.0.bn2.running_var', 'cp.resnet.layer4.0.bn2.num_batches_tracked', 'cp.resnet.layer4.0.downsample.0.weight', 'cp.resnet.layer4.0.downsample.1.weight', 'cp.resnet.layer4.0.downsample.1.bias', 'cp.resnet.layer4.0.downsample.1.running_mean', 'cp.resnet.layer4.0.downsample.1.running_var', 'cp.resnet.layer4.0.downsample.1.num_batches_tracked', 'cp.resnet.layer4.1.conv1.weight', 'cp.resnet.layer4.1.bn1.weight', 'cp.resnet.layer4.1.bn1.bias', 'cp.resnet.layer4.1.bn1.running_mean', 'cp.resnet.layer4.1.bn1.running_var', 'cp.resnet.layer4.1.bn1.num_batches_tracked', 'cp.resnet.layer4.1.conv2.weight', 'cp.resnet.layer4.1.bn2.weight', 'cp.resnet.layer4.1.bn2.bias', 'cp.resnet.layer4.1.bn2.running_mean', 'cp.resnet.layer4.1.bn2.running_var', 'cp.resnet.layer4.1.bn2.num_batches_tracked', 'cp.arm16.conv.conv.weight', 'cp.arm16.conv.bn.weight', 'cp.arm16.conv.bn.bias', 'cp.arm16.conv.bn.running_mean', 'cp.arm16.conv.bn.running_var', 'cp.arm16.conv.bn.num_batches_tracked', 'cp.arm16.conv_atten.weight', 'cp.arm16.bn_atten.weight', 'cp.arm16.bn_atten.bias', 'cp.arm16.bn_atten.running_mean', 'cp.arm16.bn_atten.running_var', 'cp.arm16.bn_atten.num_batches_tracked', 'cp.arm32.conv.conv.weight', 'cp.arm32.conv.bn.weight', 'cp.arm32.conv.bn.bias', 'cp.arm32.conv.bn.running_mean', 'cp.arm32.conv.bn.running_var', 'cp.arm32.conv.bn.num_batches_tracked', 'cp.arm32.conv_atten.weight', 'cp.arm32.bn_atten.weight', 'cp.arm32.bn_atten.bias', 'cp.arm32.bn_atten.running_mean', 'cp.arm32.bn_atten.running_var', 'cp.arm32.bn_atten.num_batches_tracked', 'cp.conv_head32.conv.weight', 'cp.conv_head32.bn.weight', 'cp.conv_head32.bn.bias', 'cp.conv_head32.bn.running_mean', 'cp.conv_head32.bn.running_var', 'cp.conv_head32.bn.num_batches_tracked', 'cp.conv_head16.conv.weight', 'cp.conv_head16.bn.weight', 'cp.conv_head16.bn.bias', 'cp.conv_head16.bn.running_mean', 'cp.conv_head16.bn.running_var', 'cp.conv_head16.bn.num_batches_tracked', 'cp.conv_avg.conv.weight', 'cp.conv_avg.bn.weight', 'cp.conv_avg.bn.bias', 'cp.conv_avg.bn.running_mean', 'cp.conv_avg.bn.running_var', 'cp.conv_avg.bn.num_batches_tracked', 'sp.conv1.conv.weight', 'sp.conv1.bn.weight', 'sp.conv1.bn.bias', 'sp.conv1.bn.running_mean', 'sp.conv1.bn.running_var', 'sp.conv1.bn.num_batches_tracked', 'sp.conv2.conv.weight', 'sp.conv2.bn.weight', 'sp.conv2.bn.bias', 'sp.conv2.bn.running_mean', 'sp.conv2.bn.running_var', 'sp.conv2.bn.num_batches_tracked', 'sp.conv3.conv.weight', 'sp.conv3.bn.weight', 'sp.conv3.bn.bias', 'sp.conv3.bn.running_mean', 'sp.conv3.bn.running_var', 'sp.conv3.bn.num_batches_tracked', 'sp.conv_out.conv.weight', 'sp.conv_out.bn.weight', 'sp.conv_out.bn.bias', 'sp.conv_out.bn.running_mean', 'sp.conv_out.bn.running_var', 'sp.conv_out.bn.num_batches_tracked', 'ffm.convblk.conv.weight', 'ffm.convblk.bn.weight', 'ffm.convblk.bn.bias', 'ffm.convblk.bn.running_mean', 'ffm.convblk.bn.running_var', 'ffm.convblk.bn.num_batches_tracked', 'ffm.conv.weight', 'ffm.bn.weight', 'ffm.bn.bias', 'ffm.bn.running_mean', 'ffm.bn.running_var', 'ffm.bn.num_batches_tracked', 'conv_out.conv.conv.weight', 'conv_out.conv.bn.weight', 'conv_out.conv.bn.bias', 'conv_out.conv.bn.running_mean', 'conv_out.conv.bn.running_var', 'conv_out.conv.bn.num_batches_tracked', 'conv_out.conv_out.weight', 'conv_out.conv_out.bias', 'conv_out16.conv.conv.weight', 'conv_out16.conv.bn.weight', 'conv_out16.conv.bn.bias', 'conv_out16.conv.bn.running_mean', 'conv_out16.conv.bn.running_var', 'conv_out16.conv.bn.num_batches_tracked', 'conv_out16.conv_out.weight', 'conv_out16.conv_out.bias', 'conv_out32.conv.conv.weight', 'conv_out32.conv.bn.weight', 'conv_out32.conv.bn.bias', 'conv_out32.conv.bn.running_mean', 'conv_out32.conv.bn.running_var', 'conv_out32.conv.bn.num_batches_tracked', 'conv_out32.conv_out.weight', 'conv_out32.conv_out.bias'])\n",
            "conv_out.conv_out.weight in own_state\n",
            "Size mismatch for conv_out.conv_out.weight: torch.Size([20, 256, 1, 1]) vs torch.Size([19, 256, 1, 1])\n",
            "conv_out.conv_out.bias in own_state\n",
            "Size mismatch for conv_out.conv_out.bias: torch.Size([20]) vs torch.Size([19])\n",
            "conv_out16.conv_out.weight in own_state\n",
            "Size mismatch for conv_out16.conv_out.weight: torch.Size([20, 64, 1, 1]) vs torch.Size([19, 64, 1, 1])\n",
            "conv_out16.conv_out.bias in own_state\n",
            "Size mismatch for conv_out16.conv_out.bias: torch.Size([20]) vs torch.Size([19])\n",
            "conv_out32.conv_out.weight in own_state\n",
            "Size mismatch for conv_out32.conv_out.weight: torch.Size([20, 64, 1, 1]) vs torch.Size([19, 64, 1, 1])\n",
            "conv_out32.conv_out.bias in own_state\n",
            "Size mismatch for conv_out32.conv_out.bias: torch.Size([20]) vs torch.Size([19])\n",
            "Import Model bisenet with weights bisenetv1_pretrained.pth to FineTune\n",
            "========== TRAINING ===========\n",
            "/content/cityscapes/leftImg8bit/train\n",
            "/content/cityscapes/leftImg8bit/val\n",
            "<class '__main__.CrossEntropyLoss2d'>\n",
            "----- TRAINING - EPOCH 1 -----\n",
            "LEARNING RATE:  0.0025\n",
            "loss: 0.4135 (epoch: 1, step: 0) // Avg time/img: 0.5223 s\n",
            "loss: 0.3298 (epoch: 1, step: 50) // Avg time/img: 0.0438 s\n",
            "loss: 0.3058 (epoch: 1, step: 100) // Avg time/img: 0.0390 s\n",
            "loss: 0.2876 (epoch: 1, step: 150) // Avg time/img: 0.0375 s\n",
            "loss: 0.2787 (epoch: 1, step: 200) // Avg time/img: 0.0369 s\n",
            "loss: 0.2708 (epoch: 1, step: 250) // Avg time/img: 0.0365 s\n",
            "loss: 0.2649 (epoch: 1, step: 300) // Avg time/img: 0.0364 s\n",
            "loss: 0.2594 (epoch: 1, step: 350) // Avg time/img: 0.0362 s\n",
            "loss: 0.256 (epoch: 1, step: 400) // Avg time/img: 0.0362 s\n",
            "loss: 0.2529 (epoch: 1, step: 450) // Avg time/img: 0.0361 s\n",
            "----- VALIDATING - EPOCH 1 -----\n",
            "VAL loss: 0.1632 (epoch: 1, step: 0) // Avg time/img: 0.0359 s\n",
            "VAL loss: 0.3407 (epoch: 1, step: 50) // Avg time/img: 0.0223 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m65.55\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/bisenet_training1/model-001.pth (epoch: 1)\n",
            "save: ../save/bisenet_training1/model_best.pth (epoch: 1)\n",
            "----- TRAINING - EPOCH 2 -----\n",
            "LEARNING RATE:  0.0023872134540537495\n",
            "loss: 0.2414 (epoch: 2, step: 0) // Avg time/img: 0.0425 s\n",
            "loss: 0.2249 (epoch: 2, step: 50) // Avg time/img: 0.0361 s\n",
            "loss: 0.2301 (epoch: 2, step: 100) // Avg time/img: 0.0355 s\n",
            "loss: 0.2275 (epoch: 2, step: 150) // Avg time/img: 0.0354 s\n",
            "loss: 0.2262 (epoch: 2, step: 200) // Avg time/img: 0.0355 s\n",
            "loss: 0.2259 (epoch: 2, step: 250) // Avg time/img: 0.0356 s\n",
            "loss: 0.2256 (epoch: 2, step: 300) // Avg time/img: 0.0355 s\n",
            "loss: 0.2255 (epoch: 2, step: 350) // Avg time/img: 0.0355 s\n",
            "loss: 0.2256 (epoch: 2, step: 400) // Avg time/img: 0.0355 s\n",
            "loss: 0.2261 (epoch: 2, step: 450) // Avg time/img: 0.0355 s\n",
            "----- VALIDATING - EPOCH 2 -----\n",
            "VAL loss: 0.1606 (epoch: 2, step: 0) // Avg time/img: 0.0290 s\n",
            "VAL loss: 0.3336 (epoch: 2, step: 50) // Avg time/img: 0.0217 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m65.41\u001b[0m %\n",
            "save: ../save/bisenet_training1/model-002.pth (epoch: 2)\n",
            "----- TRAINING - EPOCH 3 -----\n",
            "LEARNING RATE:  0.0022738314402074057\n",
            "loss: 0.1743 (epoch: 3, step: 0) // Avg time/img: 0.0431 s\n",
            "loss: 0.2214 (epoch: 3, step: 50) // Avg time/img: 0.0356 s\n",
            "loss: 0.221 (epoch: 3, step: 100) // Avg time/img: 0.0356 s\n",
            "loss: 0.2208 (epoch: 3, step: 150) // Avg time/img: 0.0357 s\n",
            "loss: 0.2177 (epoch: 3, step: 200) // Avg time/img: 0.0357 s\n",
            "loss: 0.2175 (epoch: 3, step: 250) // Avg time/img: 0.0357 s\n",
            "loss: 0.2194 (epoch: 3, step: 300) // Avg time/img: 0.0358 s\n",
            "loss: 0.2205 (epoch: 3, step: 350) // Avg time/img: 0.0358 s\n",
            "loss: 0.2193 (epoch: 3, step: 400) // Avg time/img: 0.0358 s\n",
            "loss: 0.2197 (epoch: 3, step: 450) // Avg time/img: 0.0356 s\n",
            "----- VALIDATING - EPOCH 3 -----\n",
            "VAL loss: 0.1585 (epoch: 3, step: 0) // Avg time/img: 0.0334 s\n",
            "VAL loss: 0.329 (epoch: 3, step: 50) // Avg time/img: 0.0221 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m65.52\u001b[0m %\n",
            "save: ../save/bisenet_training1/model-003.pth (epoch: 3)\n",
            "----- TRAINING - EPOCH 4 -----\n",
            "LEARNING RATE:  0.0021598174307570477\n",
            "loss: 0.1685 (epoch: 4, step: 0) // Avg time/img: 0.0411 s\n",
            "loss: 0.2234 (epoch: 4, step: 50) // Avg time/img: 0.0359 s\n",
            "loss: 0.219 (epoch: 4, step: 100) // Avg time/img: 0.0354 s\n",
            "loss: 0.2168 (epoch: 4, step: 150) // Avg time/img: 0.0355 s\n",
            "loss: 0.2165 (epoch: 4, step: 200) // Avg time/img: 0.0356 s\n",
            "loss: 0.2163 (epoch: 4, step: 250) // Avg time/img: 0.0355 s\n",
            "loss: 0.2171 (epoch: 4, step: 300) // Avg time/img: 0.0356 s\n",
            "loss: 0.2179 (epoch: 4, step: 350) // Avg time/img: 0.0355 s\n",
            "loss: 0.2172 (epoch: 4, step: 400) // Avg time/img: 0.0354 s\n",
            "loss: 0.2177 (epoch: 4, step: 450) // Avg time/img: 0.0355 s\n",
            "----- VALIDATING - EPOCH 4 -----\n",
            "VAL loss: 0.1574 (epoch: 4, step: 0) // Avg time/img: 0.0295 s\n",
            "VAL loss: 0.3274 (epoch: 4, step: 50) // Avg time/img: 0.0215 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m65.54\u001b[0m %\n",
            "save: ../save/bisenet_training1/model-004.pth (epoch: 4)\n",
            "----- TRAINING - EPOCH 5 -----\n",
            "LEARNING RATE:  0.002045130365127146\n",
            "loss: 0.2186 (epoch: 5, step: 0) // Avg time/img: 0.0397 s\n",
            "loss: 0.2124 (epoch: 5, step: 50) // Avg time/img: 0.0352 s\n",
            "loss: 0.214 (epoch: 5, step: 100) // Avg time/img: 0.0353 s\n",
            "loss: 0.2166 (epoch: 5, step: 150) // Avg time/img: 0.0352 s\n",
            "loss: 0.217 (epoch: 5, step: 200) // Avg time/img: 0.0353 s\n",
            "loss: 0.2162 (epoch: 5, step: 250) // Avg time/img: 0.0353 s\n",
            "loss: 0.2159 (epoch: 5, step: 300) // Avg time/img: 0.0354 s\n",
            "loss: 0.2166 (epoch: 5, step: 350) // Avg time/img: 0.0355 s\n",
            "loss: 0.2163 (epoch: 5, step: 400) // Avg time/img: 0.0356 s\n",
            "loss: 0.2159 (epoch: 5, step: 450) // Avg time/img: 0.0356 s\n",
            "----- VALIDATING - EPOCH 5 -----\n",
            "VAL loss: 0.156 (epoch: 5, step: 0) // Avg time/img: 0.0352 s\n",
            "VAL loss: 0.3222 (epoch: 5, step: 50) // Avg time/img: 0.0222 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m65.76\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/bisenet_training1/model-005.pth (epoch: 5)\n",
            "save: ../save/bisenet_training1/model_best.pth (epoch: 5)\n",
            "----- TRAINING - EPOCH 6 -----\n",
            "LEARNING RATE:  0.0019297237668089262\n",
            "loss: 0.2863 (epoch: 6, step: 0) // Avg time/img: 0.0442 s\n",
            "loss: 0.2075 (epoch: 6, step: 50) // Avg time/img: 0.0361 s\n",
            "loss: 0.2091 (epoch: 6, step: 100) // Avg time/img: 0.0356 s\n",
            "loss: 0.2093 (epoch: 6, step: 150) // Avg time/img: 0.0354 s\n",
            "loss: 0.2104 (epoch: 6, step: 200) // Avg time/img: 0.0356 s\n",
            "loss: 0.2115 (epoch: 6, step: 250) // Avg time/img: 0.0355 s\n",
            "loss: 0.2119 (epoch: 6, step: 300) // Avg time/img: 0.0355 s\n",
            "loss: 0.2116 (epoch: 6, step: 350) // Avg time/img: 0.0355 s\n",
            "loss: 0.2126 (epoch: 6, step: 400) // Avg time/img: 0.0355 s\n",
            "loss: 0.2138 (epoch: 6, step: 450) // Avg time/img: 0.0354 s\n",
            "----- VALIDATING - EPOCH 6 -----\n",
            "VAL loss: 0.1564 (epoch: 6, step: 0) // Avg time/img: 0.0370 s\n",
            "VAL loss: 0.3225 (epoch: 6, step: 50) // Avg time/img: 0.0215 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m65.89\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/bisenet_training1/model-006.pth (epoch: 6)\n",
            "save: ../save/bisenet_training1/model_best.pth (epoch: 6)\n",
            "----- TRAINING - EPOCH 7 -----\n",
            "LEARNING RATE:  0.0018135446173430498\n",
            "loss: 0.2585 (epoch: 7, step: 0) // Avg time/img: 0.0420 s\n",
            "loss: 0.2074 (epoch: 7, step: 50) // Avg time/img: 0.0358 s\n",
            "loss: 0.2106 (epoch: 7, step: 100) // Avg time/img: 0.0356 s\n",
            "loss: 0.2117 (epoch: 7, step: 150) // Avg time/img: 0.0356 s\n",
            "loss: 0.2117 (epoch: 7, step: 200) // Avg time/img: 0.0356 s\n",
            "loss: 0.2128 (epoch: 7, step: 250) // Avg time/img: 0.0357 s\n",
            "loss: 0.2109 (epoch: 7, step: 300) // Avg time/img: 0.0355 s\n",
            "loss: 0.2104 (epoch: 7, step: 350) // Avg time/img: 0.0357 s\n",
            "loss: 0.2113 (epoch: 7, step: 400) // Avg time/img: 0.0357 s\n",
            "loss: 0.2118 (epoch: 7, step: 450) // Avg time/img: 0.0358 s\n",
            "----- VALIDATING - EPOCH 7 -----\n",
            "VAL loss: 0.1558 (epoch: 7, step: 0) // Avg time/img: 0.0326 s\n",
            "VAL loss: 0.3235 (epoch: 7, step: 50) // Avg time/img: 0.0222 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m65.54\u001b[0m %\n",
            "save: ../save/bisenet_training1/model-007.pth (epoch: 7)\n",
            "----- TRAINING - EPOCH 8 -----\n",
            "LEARNING RATE:  0.0016965318981453127\n",
            "loss: 0.1711 (epoch: 8, step: 0) // Avg time/img: 0.0424 s\n",
            "loss: 0.2119 (epoch: 8, step: 50) // Avg time/img: 0.0361 s\n",
            "loss: 0.2098 (epoch: 8, step: 100) // Avg time/img: 0.0359 s\n",
            "loss: 0.2081 (epoch: 8, step: 150) // Avg time/img: 0.0356 s\n",
            "loss: 0.2071 (epoch: 8, step: 200) // Avg time/img: 0.0355 s\n",
            "loss: 0.2081 (epoch: 8, step: 250) // Avg time/img: 0.0356 s\n",
            "loss: 0.2087 (epoch: 8, step: 300) // Avg time/img: 0.0354 s\n",
            "loss: 0.2094 (epoch: 8, step: 350) // Avg time/img: 0.0355 s\n",
            "loss: 0.21 (epoch: 8, step: 400) // Avg time/img: 0.0356 s\n",
            "loss: 0.211 (epoch: 8, step: 450) // Avg time/img: 0.0356 s\n",
            "----- VALIDATING - EPOCH 8 -----\n",
            "VAL loss: 0.1525 (epoch: 8, step: 0) // Avg time/img: 0.0369 s\n",
            "VAL loss: 0.3211 (epoch: 8, step: 50) // Avg time/img: 0.0216 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m65.88\u001b[0m %\n",
            "save: ../save/bisenet_training1/model-008.pth (epoch: 8)\n",
            "----- TRAINING - EPOCH 9 -----\n",
            "LEARNING RATE:  0.0015786146687233882\n",
            "loss: 0.2422 (epoch: 9, step: 0) // Avg time/img: 0.0475 s\n",
            "loss: 0.2092 (epoch: 9, step: 50) // Avg time/img: 0.0353 s\n",
            "loss: 0.2116 (epoch: 9, step: 100) // Avg time/img: 0.0354 s\n",
            "loss: 0.2135 (epoch: 9, step: 150) // Avg time/img: 0.0357 s\n",
            "loss: 0.2141 (epoch: 9, step: 200) // Avg time/img: 0.0356 s\n",
            "loss: 0.2124 (epoch: 9, step: 250) // Avg time/img: 0.0356 s\n",
            "loss: 0.2117 (epoch: 9, step: 300) // Avg time/img: 0.0356 s\n",
            "loss: 0.2122 (epoch: 9, step: 350) // Avg time/img: 0.0355 s\n",
            "loss: 0.2128 (epoch: 9, step: 400) // Avg time/img: 0.0356 s\n",
            "loss: 0.2121 (epoch: 9, step: 450) // Avg time/img: 0.0355 s\n",
            "----- VALIDATING - EPOCH 9 -----\n",
            "VAL loss: 0.1554 (epoch: 9, step: 0) // Avg time/img: 0.0423 s\n",
            "VAL loss: 0.325 (epoch: 9, step: 50) // Avg time/img: 0.0227 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m65.64\u001b[0m %\n",
            "save: ../save/bisenet_training1/model-009.pth (epoch: 9)\n",
            "----- TRAINING - EPOCH 10 -----\n",
            "LEARNING RATE:  0.0014597094822999506\n",
            "loss: 0.1769 (epoch: 10, step: 0) // Avg time/img: 0.0488 s\n",
            "loss: 0.2227 (epoch: 10, step: 50) // Avg time/img: 0.0361 s\n",
            "loss: 0.2171 (epoch: 10, step: 100) // Avg time/img: 0.0358 s\n",
            "loss: 0.216 (epoch: 10, step: 150) // Avg time/img: 0.0357 s\n",
            "loss: 0.215 (epoch: 10, step: 200) // Avg time/img: 0.0356 s\n",
            "loss: 0.214 (epoch: 10, step: 250) // Avg time/img: 0.0358 s\n",
            "loss: 0.2138 (epoch: 10, step: 300) // Avg time/img: 0.0357 s\n",
            "loss: 0.2132 (epoch: 10, step: 350) // Avg time/img: 0.0356 s\n",
            "loss: 0.2114 (epoch: 10, step: 400) // Avg time/img: 0.0357 s\n",
            "loss: 0.211 (epoch: 10, step: 450) // Avg time/img: 0.0356 s\n",
            "----- VALIDATING - EPOCH 10 -----\n",
            "VAL loss: 0.1543 (epoch: 10, step: 0) // Avg time/img: 0.0315 s\n",
            "VAL loss: 0.3228 (epoch: 10, step: 50) // Avg time/img: 0.0233 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m65.71\u001b[0m %\n",
            "save: ../save/bisenet_training1/model-010.pth (epoch: 10)\n",
            "----- TRAINING - EPOCH 11 -----\n",
            "LEARNING RATE:  0.0013397168281703664\n",
            "loss: 0.1687 (epoch: 11, step: 0) // Avg time/img: 0.0443 s\n",
            "loss: 0.2149 (epoch: 11, step: 50) // Avg time/img: 0.0371 s\n",
            "loss: 0.2125 (epoch: 11, step: 100) // Avg time/img: 0.0362 s\n",
            "loss: 0.2104 (epoch: 11, step: 150) // Avg time/img: 0.0359 s\n",
            "loss: 0.2095 (epoch: 11, step: 200) // Avg time/img: 0.0360 s\n",
            "loss: 0.2107 (epoch: 11, step: 250) // Avg time/img: 0.0359 s\n",
            "loss: 0.2111 (epoch: 11, step: 300) // Avg time/img: 0.0360 s\n",
            "loss: 0.2103 (epoch: 11, step: 350) // Avg time/img: 0.0360 s\n",
            "loss: 0.2094 (epoch: 11, step: 400) // Avg time/img: 0.0359 s\n",
            "loss: 0.2096 (epoch: 11, step: 450) // Avg time/img: 0.0360 s\n",
            "----- VALIDATING - EPOCH 11 -----\n",
            "VAL loss: 0.1542 (epoch: 11, step: 0) // Avg time/img: 0.0316 s\n",
            "VAL loss: 0.32 (epoch: 11, step: 50) // Avg time/img: 0.0223 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m65.99\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/bisenet_training1/model-011.pth (epoch: 11)\n",
            "save: ../save/bisenet_training1/model_best.pth (epoch: 11)\n",
            "----- TRAINING - EPOCH 12 -----\n",
            "LEARNING RATE:  0.0012185160979474884\n",
            "loss: 0.1901 (epoch: 12, step: 0) // Avg time/img: 0.0421 s\n",
            "loss: 0.2048 (epoch: 12, step: 50) // Avg time/img: 0.0360 s\n",
            "loss: 0.2075 (epoch: 12, step: 100) // Avg time/img: 0.0355 s\n",
            "loss: 0.2087 (epoch: 12, step: 150) // Avg time/img: 0.0352 s\n",
            "loss: 0.2099 (epoch: 12, step: 200) // Avg time/img: 0.0354 s\n",
            "loss: 0.2087 (epoch: 12, step: 250) // Avg time/img: 0.0354 s\n",
            "loss: 0.2089 (epoch: 12, step: 300) // Avg time/img: 0.0355 s\n",
            "loss: 0.2104 (epoch: 12, step: 350) // Avg time/img: 0.0355 s\n",
            "loss: 0.2107 (epoch: 12, step: 400) // Avg time/img: 0.0354 s\n",
            "loss: 0.2108 (epoch: 12, step: 450) // Avg time/img: 0.0355 s\n",
            "----- VALIDATING - EPOCH 12 -----\n",
            "VAL loss: 0.1537 (epoch: 12, step: 0) // Avg time/img: 0.0315 s\n",
            "VAL loss: 0.3193 (epoch: 12, step: 50) // Avg time/img: 0.0224 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m66.09\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/bisenet_training1/model-012.pth (epoch: 12)\n",
            "save: ../save/bisenet_training1/model_best.pth (epoch: 12)\n",
            "----- TRAINING - EPOCH 13 -----\n",
            "LEARNING RATE:  0.0010959582263852174\n",
            "loss: 0.1624 (epoch: 13, step: 0) // Avg time/img: 0.0457 s\n",
            "loss: 0.2108 (epoch: 13, step: 50) // Avg time/img: 0.0357 s\n",
            "loss: 0.2099 (epoch: 13, step: 100) // Avg time/img: 0.0353 s\n",
            "loss: 0.2107 (epoch: 13, step: 150) // Avg time/img: 0.0350 s\n",
            "loss: 0.2084 (epoch: 13, step: 200) // Avg time/img: 0.0351 s\n",
            "loss: 0.2089 (epoch: 13, step: 250) // Avg time/img: 0.0353 s\n",
            "loss: 0.2091 (epoch: 13, step: 300) // Avg time/img: 0.0354 s\n",
            "loss: 0.2094 (epoch: 13, step: 350) // Avg time/img: 0.0354 s\n",
            "loss: 0.2092 (epoch: 13, step: 400) // Avg time/img: 0.0354 s\n",
            "loss: 0.2093 (epoch: 13, step: 450) // Avg time/img: 0.0355 s\n",
            "----- VALIDATING - EPOCH 13 -----\n",
            "VAL loss: 0.1536 (epoch: 13, step: 0) // Avg time/img: 0.0320 s\n",
            "VAL loss: 0.3193 (epoch: 13, step: 50) // Avg time/img: 0.0221 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m65.97\u001b[0m %\n",
            "save: ../save/bisenet_training1/model-013.pth (epoch: 13)\n",
            "----- TRAINING - EPOCH 14 -----\n",
            "LEARNING RATE:  0.0009718544969969087\n",
            "loss: 0.2078 (epoch: 14, step: 0) // Avg time/img: 0.0454 s\n",
            "loss: 0.2052 (epoch: 14, step: 50) // Avg time/img: 0.0349 s\n",
            "loss: 0.2044 (epoch: 14, step: 100) // Avg time/img: 0.0350 s\n",
            "loss: 0.2077 (epoch: 14, step: 150) // Avg time/img: 0.0350 s\n",
            "loss: 0.206 (epoch: 14, step: 200) // Avg time/img: 0.0351 s\n",
            "loss: 0.2077 (epoch: 14, step: 250) // Avg time/img: 0.0354 s\n",
            "loss: 0.2067 (epoch: 14, step: 300) // Avg time/img: 0.0354 s\n",
            "loss: 0.2076 (epoch: 14, step: 350) // Avg time/img: 0.0355 s\n",
            "loss: 0.2078 (epoch: 14, step: 400) // Avg time/img: 0.0354 s\n",
            "loss: 0.2074 (epoch: 14, step: 450) // Avg time/img: 0.0355 s\n",
            "----- VALIDATING - EPOCH 14 -----\n",
            "VAL loss: 0.153 (epoch: 14, step: 0) // Avg time/img: 0.0332 s\n",
            "VAL loss: 0.3172 (epoch: 14, step: 50) // Avg time/img: 0.0218 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m66.11\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/bisenet_training1/model-014.pth (epoch: 14)\n",
            "save: ../save/bisenet_training1/model_best.pth (epoch: 14)\n",
            "----- TRAINING - EPOCH 15 -----\n",
            "LEARNING RATE:  0.0008459586547541247\n",
            "loss: 0.2014 (epoch: 15, step: 0) // Avg time/img: 0.0463 s\n",
            "loss: 0.2115 (epoch: 15, step: 50) // Avg time/img: 0.0356 s\n",
            "loss: 0.2117 (epoch: 15, step: 100) // Avg time/img: 0.0354 s\n",
            "loss: 0.2107 (epoch: 15, step: 150) // Avg time/img: 0.0353 s\n",
            "loss: 0.2101 (epoch: 15, step: 200) // Avg time/img: 0.0355 s\n",
            "loss: 0.2095 (epoch: 15, step: 250) // Avg time/img: 0.0355 s\n",
            "loss: 0.2104 (epoch: 15, step: 300) // Avg time/img: 0.0355 s\n",
            "loss: 0.209 (epoch: 15, step: 350) // Avg time/img: 0.0356 s\n",
            "loss: 0.2081 (epoch: 15, step: 400) // Avg time/img: 0.0356 s\n",
            "loss: 0.2083 (epoch: 15, step: 450) // Avg time/img: 0.0357 s\n",
            "----- VALIDATING - EPOCH 15 -----\n",
            "VAL loss: 0.1524 (epoch: 15, step: 0) // Avg time/img: 0.0346 s\n",
            "VAL loss: 0.3183 (epoch: 15, step: 50) // Avg time/img: 0.0228 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m65.96\u001b[0m %\n",
            "save: ../save/bisenet_training1/model-015.pth (epoch: 15)\n",
            "----- TRAINING - EPOCH 16 -----\n",
            "LEARNING RATE:  0.0007179364718731469\n",
            "loss: 0.3048 (epoch: 16, step: 0) // Avg time/img: 0.0425 s\n",
            "loss: 0.2035 (epoch: 16, step: 50) // Avg time/img: 0.0361 s\n",
            "loss: 0.2108 (epoch: 16, step: 100) // Avg time/img: 0.0363 s\n",
            "loss: 0.211 (epoch: 16, step: 150) // Avg time/img: 0.0361 s\n",
            "loss: 0.2099 (epoch: 16, step: 200) // Avg time/img: 0.0359 s\n",
            "loss: 0.2112 (epoch: 16, step: 250) // Avg time/img: 0.0358 s\n",
            "loss: 0.2081 (epoch: 16, step: 300) // Avg time/img: 0.0357 s\n",
            "loss: 0.2086 (epoch: 16, step: 350) // Avg time/img: 0.0357 s\n",
            "loss: 0.2085 (epoch: 16, step: 400) // Avg time/img: 0.0359 s\n",
            "loss: 0.2085 (epoch: 16, step: 450) // Avg time/img: 0.0358 s\n",
            "----- VALIDATING - EPOCH 16 -----\n",
            "VAL loss: 0.1535 (epoch: 16, step: 0) // Avg time/img: 0.0299 s\n",
            "VAL loss: 0.3203 (epoch: 16, step: 50) // Avg time/img: 0.0224 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m66.11\u001b[0m %\n",
            "save: ../save/bisenet_training1/model-016.pth (epoch: 16)\n",
            "----- TRAINING - EPOCH 17 -----\n",
            "LEARNING RATE:  0.0005873094715440094\n",
            "loss: 0.2015 (epoch: 17, step: 0) // Avg time/img: 0.0442 s\n",
            "loss: 0.2058 (epoch: 17, step: 50) // Avg time/img: 0.0352 s\n",
            "loss: 0.2023 (epoch: 17, step: 100) // Avg time/img: 0.0354 s\n",
            "loss: 0.2049 (epoch: 17, step: 150) // Avg time/img: 0.0355 s\n",
            "loss: 0.209 (epoch: 17, step: 200) // Avg time/img: 0.0355 s\n",
            "loss: 0.2062 (epoch: 17, step: 250) // Avg time/img: 0.0357 s\n",
            "loss: 0.2065 (epoch: 17, step: 300) // Avg time/img: 0.0357 s\n",
            "loss: 0.2067 (epoch: 17, step: 350) // Avg time/img: 0.0356 s\n",
            "loss: 0.208 (epoch: 17, step: 400) // Avg time/img: 0.0356 s\n",
            "loss: 0.2091 (epoch: 17, step: 450) // Avg time/img: 0.0356 s\n",
            "----- VALIDATING - EPOCH 17 -----\n",
            "VAL loss: 0.1522 (epoch: 17, step: 0) // Avg time/img: 0.0317 s\n",
            "VAL loss: 0.3166 (epoch: 17, step: 50) // Avg time/img: 0.0230 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m66.01\u001b[0m %\n",
            "save: ../save/bisenet_training1/model-017.pth (epoch: 17)\n",
            "----- TRAINING - EPOCH 18 -----\n",
            "LEARNING RATE:  0.0004533380182841864\n",
            "loss: 0.3121 (epoch: 18, step: 0) // Avg time/img: 0.0388 s\n",
            "loss: 0.2069 (epoch: 18, step: 50) // Avg time/img: 0.0361 s\n",
            "loss: 0.2094 (epoch: 18, step: 100) // Avg time/img: 0.0356 s\n",
            "loss: 0.2082 (epoch: 18, step: 150) // Avg time/img: 0.0358 s\n",
            "loss: 0.2103 (epoch: 18, step: 200) // Avg time/img: 0.0356 s\n",
            "loss: 0.2088 (epoch: 18, step: 250) // Avg time/img: 0.0357 s\n",
            "loss: 0.2087 (epoch: 18, step: 300) // Avg time/img: 0.0359 s\n",
            "loss: 0.2089 (epoch: 18, step: 350) // Avg time/img: 0.0357 s\n",
            "loss: 0.2083 (epoch: 18, step: 400) // Avg time/img: 0.0358 s\n",
            "loss: 0.2086 (epoch: 18, step: 450) // Avg time/img: 0.0357 s\n",
            "----- VALIDATING - EPOCH 18 -----\n",
            "VAL loss: 0.1513 (epoch: 18, step: 0) // Avg time/img: 0.0351 s\n",
            "VAL loss: 0.3182 (epoch: 18, step: 50) // Avg time/img: 0.0224 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m66.04\u001b[0m %\n",
            "save: ../save/bisenet_training1/model-018.pth (epoch: 18)\n",
            "----- TRAINING - EPOCH 19 -----\n",
            "LEARNING RATE:  0.00031473135294854176\n",
            "loss: 0.1791 (epoch: 19, step: 0) // Avg time/img: 0.0426 s\n",
            "loss: 0.2055 (epoch: 19, step: 50) // Avg time/img: 0.0357 s\n",
            "loss: 0.2094 (epoch: 19, step: 100) // Avg time/img: 0.0355 s\n",
            "loss: 0.2082 (epoch: 19, step: 150) // Avg time/img: 0.0355 s\n",
            "loss: 0.2088 (epoch: 19, step: 200) // Avg time/img: 0.0358 s\n",
            "loss: 0.2079 (epoch: 19, step: 250) // Avg time/img: 0.0357 s\n",
            "loss: 0.2084 (epoch: 19, step: 300) // Avg time/img: 0.0357 s\n",
            "loss: 0.2084 (epoch: 19, step: 350) // Avg time/img: 0.0357 s\n",
            "loss: 0.2074 (epoch: 19, step: 400) // Avg time/img: 0.0357 s\n",
            "loss: 0.2067 (epoch: 19, step: 450) // Avg time/img: 0.0357 s\n",
            "----- VALIDATING - EPOCH 19 -----\n",
            "VAL loss: 0.1514 (epoch: 19, step: 0) // Avg time/img: 0.0315 s\n",
            "VAL loss: 0.3149 (epoch: 19, step: 50) // Avg time/img: 0.0218 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m66.26\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/bisenet_training1/model-019.pth (epoch: 19)\n",
            "save: ../save/bisenet_training1/model_best.pth (epoch: 19)\n",
            "----- TRAINING - EPOCH 20 -----\n",
            "LEARNING RATE:  0.00016866035595919555\n",
            "loss: 0.188 (epoch: 20, step: 0) // Avg time/img: 0.0450 s\n",
            "loss: 0.2009 (epoch: 20, step: 50) // Avg time/img: 0.0351 s\n",
            "loss: 0.2046 (epoch: 20, step: 100) // Avg time/img: 0.0353 s\n",
            "loss: 0.2019 (epoch: 20, step: 150) // Avg time/img: 0.0354 s\n",
            "loss: 0.2026 (epoch: 20, step: 200) // Avg time/img: 0.0353 s\n",
            "loss: 0.2035 (epoch: 20, step: 250) // Avg time/img: 0.0355 s\n",
            "loss: 0.2047 (epoch: 20, step: 300) // Avg time/img: 0.0354 s\n",
            "loss: 0.2058 (epoch: 20, step: 350) // Avg time/img: 0.0353 s\n",
            "loss: 0.2065 (epoch: 20, step: 400) // Avg time/img: 0.0355 s\n",
            "loss: 0.2072 (epoch: 20, step: 450) // Avg time/img: 0.0355 s\n",
            "----- VALIDATING - EPOCH 20 -----\n",
            "VAL loss: 0.1508 (epoch: 20, step: 0) // Avg time/img: 0.0293 s\n",
            "VAL loss: 0.3149 (epoch: 20, step: 50) // Avg time/img: 0.0229 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m66.37\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/bisenet_training1/model-020.pth (epoch: 20)\n",
            "save: ../save/bisenet_training1/model_best.pth (epoch: 20)\n",
            "========== TRAINING FINISHED ===========\n",
            "updating: content/AnomalySegmentation/save/bisenet_training1/ (stored 0%)\n",
            "updating: content/AnomalySegmentation/save/bisenet_training1/automated_log.txt (deflated 66%)\n",
            "updating: content/AnomalySegmentation/save/bisenet_training1/bisenet.py (deflated 82%)\n",
            "updating: content/AnomalySegmentation/save/bisenet_training1/model-010.pth (deflated 7%)\n",
            "updating: content/AnomalySegmentation/save/bisenet_training1/model.txt (deflated 91%)\n",
            "updating: content/AnomalySegmentation/save/bisenet_training1/opts.txt (deflated 37%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training1/model-014.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training1/model_best.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training1/model-002.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training1/model-011.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training1/model-001.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training1/model-003.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training1/model-020.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training1/model-006.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training1/model-015.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training1/model-005.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training1/model-007.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training1/model-012.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training1/model-018.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training1/best.txt (stored 0%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training1/model_best.pth.tar (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training1/model-016.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training1/model-019.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training1/model-013.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training1/model-008.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training1/model-017.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training1/model-009.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training1/checkpoint.pth.tar (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training1/model-004.pth (deflated 7%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation"
      ],
      "metadata": {
        "id": "mMrPCZ56IShf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "no_execute = False\n",
        "just_once = False\n",
        "\n",
        "for net in [\"erfnet\", \"enet\", \"bisenet\"]:\n",
        "  print(\"----------------------------\")\n",
        "  for dataset_dir in ['RoadAnomaly21', 'RoadObsticle21', 'FS_LostFound_full', 'fs_static', 'RoadAnomaly']:\n",
        "\n",
        "    if no_execute:\n",
        "      break\n",
        "\n",
        "    load_dir = f'/content/AnomalySegmentation/save/{net}_training1'\n",
        "    weights = f'/model_best.pth'\n",
        "    format_file = os.listdir(f'/content/Validation_Dataset/{dataset_dir}/images')[0].split(\".\")[1]\n",
        "    input =f'/content/Validation_Dataset/{dataset_dir}/images/\\*.{format_file}'\n",
        "    print(f\"\\nDataset: {dataset_dir} net: {net}\")\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "      !python -W ignore /content/AnomalySegmentation/eval/evalAnomaly.py --input {input} --void --model {net} --loadDir {load_dir} --loadWeights {weights} | tail -n 2\n",
        "    else:\n",
        "      !python -W ignore /content/AnomalySegmentation/eval/evalAnomaly.py --input {input} --void --model {net} --loadDir {load_dir} --loadWeights {weights} --cpu | tail -n 2\n",
        "\n",
        "    if just_once:\n",
        "      no_execute = True\n",
        "      just_once = False"
      ],
      "metadata": {
        "id": "cchB40LlIT9a",
        "outputId": "30674a33-f5d0-47a4-a215-15cf17f372ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------\n",
            "\n",
            "Dataset: RoadAnomaly21 net: erfnet\n",
            "AUPRC score: 23.066538132595678\n",
            "FPR@TPR95: 83.00218872131218\n",
            "\n",
            "Dataset: RoadObsticle21 net: erfnet\n",
            "AUPRC score: 1.2479619783498568\n",
            "FPR@TPR95: 98.77720541475112\n",
            "\n",
            "Dataset: FS_LostFound_full net: erfnet\n",
            "AUPRC score: 3.9670465669000987\n",
            "FPR@TPR95: 37.15986952996774\n",
            "\n",
            "Dataset: fs_static net: erfnet\n",
            "AUPRC score: 12.255619101525266\n",
            "FPR@TPR95: 82.25043369836908\n",
            "\n",
            "Dataset: RoadAnomaly net: erfnet\n",
            "AUPRC score: 10.108372222814976\n",
            "FPR@TPR95: 97.9176876261683\n",
            "----------------------------\n",
            "\n",
            "Dataset: RoadAnomaly21 net: enet\n",
            "AUPRC score: 17.710540775846635\n",
            "FPR@TPR95: 93.9867224686995\n",
            "\n",
            "Dataset: RoadObsticle21 net: enet\n",
            "AUPRC score: 1.3713287968833945\n",
            "FPR@TPR95: 91.93726483525218\n",
            "\n",
            "Dataset: FS_LostFound_full net: enet\n",
            "AUPRC score: 0.9859109956888955\n",
            "FPR@TPR95: 60.28533261005652\n",
            "\n",
            "Dataset: fs_static net: enet\n",
            "AUPRC score: 7.72351282202996\n",
            "FPR@TPR95: 78.34549796336687\n",
            "\n",
            "Dataset: RoadAnomaly net: enet\n",
            "AUPRC score: 12.788489618794117\n",
            "FPR@TPR95: 86.93573427839908\n",
            "----------------------------\n",
            "\n",
            "Dataset: RoadAnomaly21 net: bisenet\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n",
            "100% 44.7M/44.7M [00:00<00:00, 352MB/s]\n",
            "AUPRC score: 21.81055194591118\n",
            "FPR@TPR95: 91.87575225697388\n",
            "\n",
            "Dataset: RoadObsticle21 net: bisenet\n",
            "AUPRC score: 10.298027408646206\n",
            "FPR@TPR95: 44.90818638296744\n",
            "\n",
            "Dataset: FS_LostFound_full net: bisenet\n",
            "AUPRC score: 3.5811069764660606\n",
            "FPR@TPR95: 44.204070137081445\n",
            "\n",
            "Dataset: fs_static net: bisenet\n",
            "AUPRC score: 9.766178585596988\n",
            "FPR@TPR95: 50.448052314843984\n",
            "\n",
            "Dataset: RoadAnomaly net: bisenet\n",
            "AUPRC score: 12.153410762011895\n",
            "FPR@TPR95: 91.2516594411978\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extensions"
      ],
      "metadata": {
        "id": "8xviXChktBk0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Mahalanobis"
      ],
      "metadata": {
        "id": "uVugUIVbxMSq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/AnomalySegmentation && git pull -q"
      ],
      "metadata": {
        "id": "vZiXsIPdxnUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datadir = '/content/cityscapes'\n",
        "loadWeights = '/save/erfnet_training1/model_best.pth'\n",
        "loadDir = '/content/AnomalySegmentation'\n",
        "\n",
        "!python /content/AnomalySegmentation/eval/mahalanobis.py --datadir {datadir} --model erfnet --loadDir {loadDir} --loadWeights {loadWeights}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "S70ucntWxy4F",
        "outputId": "69ec9f39-9a4d-4964-c600-7ed1e7a241ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model: erfnet\n",
            "Loading weights: /content/AnomalySegmentation/save/erfnet_training1/model_best.pth\n",
            "/content/cityscapes/leftImg8bit/train\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/content/AnomalySegmentation/eval/mahalanobis.py:175: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model = load_my_state_dict(model, torch.load(weightspath, map_location=lambda storage, loc: storage))\n",
            "odict_keys(['module.encoder.initial_block.conv.weight', 'module.encoder.initial_block.conv.bias', 'module.encoder.initial_block.bn.weight', 'module.encoder.initial_block.bn.bias', 'module.encoder.initial_block.bn.running_mean', 'module.encoder.initial_block.bn.running_var', 'module.encoder.initial_block.bn.num_batches_tracked', 'module.encoder.layers.0.conv.weight', 'module.encoder.layers.0.conv.bias', 'module.encoder.layers.0.bn.weight', 'module.encoder.layers.0.bn.bias', 'module.encoder.layers.0.bn.running_mean', 'module.encoder.layers.0.bn.running_var', 'module.encoder.layers.0.bn.num_batches_tracked', 'module.encoder.layers.1.conv3x1_1.weight', 'module.encoder.layers.1.conv3x1_1.bias', 'module.encoder.layers.1.conv1x3_1.weight', 'module.encoder.layers.1.conv1x3_1.bias', 'module.encoder.layers.1.bn1.weight', 'module.encoder.layers.1.bn1.bias', 'module.encoder.layers.1.bn1.running_mean', 'module.encoder.layers.1.bn1.running_var', 'module.encoder.layers.1.bn1.num_batches_tracked', 'module.encoder.layers.1.conv3x1_2.weight', 'module.encoder.layers.1.conv3x1_2.bias', 'module.encoder.layers.1.conv1x3_2.weight', 'module.encoder.layers.1.conv1x3_2.bias', 'module.encoder.layers.1.bn2.weight', 'module.encoder.layers.1.bn2.bias', 'module.encoder.layers.1.bn2.running_mean', 'module.encoder.layers.1.bn2.running_var', 'module.encoder.layers.1.bn2.num_batches_tracked', 'module.encoder.layers.2.conv3x1_1.weight', 'module.encoder.layers.2.conv3x1_1.bias', 'module.encoder.layers.2.conv1x3_1.weight', 'module.encoder.layers.2.conv1x3_1.bias', 'module.encoder.layers.2.bn1.weight', 'module.encoder.layers.2.bn1.bias', 'module.encoder.layers.2.bn1.running_mean', 'module.encoder.layers.2.bn1.running_var', 'module.encoder.layers.2.bn1.num_batches_tracked', 'module.encoder.layers.2.conv3x1_2.weight', 'module.encoder.layers.2.conv3x1_2.bias', 'module.encoder.layers.2.conv1x3_2.weight', 'module.encoder.layers.2.conv1x3_2.bias', 'module.encoder.layers.2.bn2.weight', 'module.encoder.layers.2.bn2.bias', 'module.encoder.layers.2.bn2.running_mean', 'module.encoder.layers.2.bn2.running_var', 'module.encoder.layers.2.bn2.num_batches_tracked', 'module.encoder.layers.3.conv3x1_1.weight', 'module.encoder.layers.3.conv3x1_1.bias', 'module.encoder.layers.3.conv1x3_1.weight', 'module.encoder.layers.3.conv1x3_1.bias', 'module.encoder.layers.3.bn1.weight', 'module.encoder.layers.3.bn1.bias', 'module.encoder.layers.3.bn1.running_mean', 'module.encoder.layers.3.bn1.running_var', 'module.encoder.layers.3.bn1.num_batches_tracked', 'module.encoder.layers.3.conv3x1_2.weight', 'module.encoder.layers.3.conv3x1_2.bias', 'module.encoder.layers.3.conv1x3_2.weight', 'module.encoder.layers.3.conv1x3_2.bias', 'module.encoder.layers.3.bn2.weight', 'module.encoder.layers.3.bn2.bias', 'module.encoder.layers.3.bn2.running_mean', 'module.encoder.layers.3.bn2.running_var', 'module.encoder.layers.3.bn2.num_batches_tracked', 'module.encoder.layers.4.conv3x1_1.weight', 'module.encoder.layers.4.conv3x1_1.bias', 'module.encoder.layers.4.conv1x3_1.weight', 'module.encoder.layers.4.conv1x3_1.bias', 'module.encoder.layers.4.bn1.weight', 'module.encoder.layers.4.bn1.bias', 'module.encoder.layers.4.bn1.running_mean', 'module.encoder.layers.4.bn1.running_var', 'module.encoder.layers.4.bn1.num_batches_tracked', 'module.encoder.layers.4.conv3x1_2.weight', 'module.encoder.layers.4.conv3x1_2.bias', 'module.encoder.layers.4.conv1x3_2.weight', 'module.encoder.layers.4.conv1x3_2.bias', 'module.encoder.layers.4.bn2.weight', 'module.encoder.layers.4.bn2.bias', 'module.encoder.layers.4.bn2.running_mean', 'module.encoder.layers.4.bn2.running_var', 'module.encoder.layers.4.bn2.num_batches_tracked', 'module.encoder.layers.5.conv3x1_1.weight', 'module.encoder.layers.5.conv3x1_1.bias', 'module.encoder.layers.5.conv1x3_1.weight', 'module.encoder.layers.5.conv1x3_1.bias', 'module.encoder.layers.5.bn1.weight', 'module.encoder.layers.5.bn1.bias', 'module.encoder.layers.5.bn1.running_mean', 'module.encoder.layers.5.bn1.running_var', 'module.encoder.layers.5.bn1.num_batches_tracked', 'module.encoder.layers.5.conv3x1_2.weight', 'module.encoder.layers.5.conv3x1_2.bias', 'module.encoder.layers.5.conv1x3_2.weight', 'module.encoder.layers.5.conv1x3_2.bias', 'module.encoder.layers.5.bn2.weight', 'module.encoder.layers.5.bn2.bias', 'module.encoder.layers.5.bn2.running_mean', 'module.encoder.layers.5.bn2.running_var', 'module.encoder.layers.5.bn2.num_batches_tracked', 'module.encoder.layers.6.conv.weight', 'module.encoder.layers.6.conv.bias', 'module.encoder.layers.6.bn.weight', 'module.encoder.layers.6.bn.bias', 'module.encoder.layers.6.bn.running_mean', 'module.encoder.layers.6.bn.running_var', 'module.encoder.layers.6.bn.num_batches_tracked', 'module.encoder.layers.7.conv3x1_1.weight', 'module.encoder.layers.7.conv3x1_1.bias', 'module.encoder.layers.7.conv1x3_1.weight', 'module.encoder.layers.7.conv1x3_1.bias', 'module.encoder.layers.7.bn1.weight', 'module.encoder.layers.7.bn1.bias', 'module.encoder.layers.7.bn1.running_mean', 'module.encoder.layers.7.bn1.running_var', 'module.encoder.layers.7.bn1.num_batches_tracked', 'module.encoder.layers.7.conv3x1_2.weight', 'module.encoder.layers.7.conv3x1_2.bias', 'module.encoder.layers.7.conv1x3_2.weight', 'module.encoder.layers.7.conv1x3_2.bias', 'module.encoder.layers.7.bn2.weight', 'module.encoder.layers.7.bn2.bias', 'module.encoder.layers.7.bn2.running_mean', 'module.encoder.layers.7.bn2.running_var', 'module.encoder.layers.7.bn2.num_batches_tracked', 'module.encoder.layers.8.conv3x1_1.weight', 'module.encoder.layers.8.conv3x1_1.bias', 'module.encoder.layers.8.conv1x3_1.weight', 'module.encoder.layers.8.conv1x3_1.bias', 'module.encoder.layers.8.bn1.weight', 'module.encoder.layers.8.bn1.bias', 'module.encoder.layers.8.bn1.running_mean', 'module.encoder.layers.8.bn1.running_var', 'module.encoder.layers.8.bn1.num_batches_tracked', 'module.encoder.layers.8.conv3x1_2.weight', 'module.encoder.layers.8.conv3x1_2.bias', 'module.encoder.layers.8.conv1x3_2.weight', 'module.encoder.layers.8.conv1x3_2.bias', 'module.encoder.layers.8.bn2.weight', 'module.encoder.layers.8.bn2.bias', 'module.encoder.layers.8.bn2.running_mean', 'module.encoder.layers.8.bn2.running_var', 'module.encoder.layers.8.bn2.num_batches_tracked', 'module.encoder.layers.9.conv3x1_1.weight', 'module.encoder.layers.9.conv3x1_1.bias', 'module.encoder.layers.9.conv1x3_1.weight', 'module.encoder.layers.9.conv1x3_1.bias', 'module.encoder.layers.9.bn1.weight', 'module.encoder.layers.9.bn1.bias', 'module.encoder.layers.9.bn1.running_mean', 'module.encoder.layers.9.bn1.running_var', 'module.encoder.layers.9.bn1.num_batches_tracked', 'module.encoder.layers.9.conv3x1_2.weight', 'module.encoder.layers.9.conv3x1_2.bias', 'module.encoder.layers.9.conv1x3_2.weight', 'module.encoder.layers.9.conv1x3_2.bias', 'module.encoder.layers.9.bn2.weight', 'module.encoder.layers.9.bn2.bias', 'module.encoder.layers.9.bn2.running_mean', 'module.encoder.layers.9.bn2.running_var', 'module.encoder.layers.9.bn2.num_batches_tracked', 'module.encoder.layers.10.conv3x1_1.weight', 'module.encoder.layers.10.conv3x1_1.bias', 'module.encoder.layers.10.conv1x3_1.weight', 'module.encoder.layers.10.conv1x3_1.bias', 'module.encoder.layers.10.bn1.weight', 'module.encoder.layers.10.bn1.bias', 'module.encoder.layers.10.bn1.running_mean', 'module.encoder.layers.10.bn1.running_var', 'module.encoder.layers.10.bn1.num_batches_tracked', 'module.encoder.layers.10.conv3x1_2.weight', 'module.encoder.layers.10.conv3x1_2.bias', 'module.encoder.layers.10.conv1x3_2.weight', 'module.encoder.layers.10.conv1x3_2.bias', 'module.encoder.layers.10.bn2.weight', 'module.encoder.layers.10.bn2.bias', 'module.encoder.layers.10.bn2.running_mean', 'module.encoder.layers.10.bn2.running_var', 'module.encoder.layers.10.bn2.num_batches_tracked', 'module.encoder.layers.11.conv3x1_1.weight', 'module.encoder.layers.11.conv3x1_1.bias', 'module.encoder.layers.11.conv1x3_1.weight', 'module.encoder.layers.11.conv1x3_1.bias', 'module.encoder.layers.11.bn1.weight', 'module.encoder.layers.11.bn1.bias', 'module.encoder.layers.11.bn1.running_mean', 'module.encoder.layers.11.bn1.running_var', 'module.encoder.layers.11.bn1.num_batches_tracked', 'module.encoder.layers.11.conv3x1_2.weight', 'module.encoder.layers.11.conv3x1_2.bias', 'module.encoder.layers.11.conv1x3_2.weight', 'module.encoder.layers.11.conv1x3_2.bias', 'module.encoder.layers.11.bn2.weight', 'module.encoder.layers.11.bn2.bias', 'module.encoder.layers.11.bn2.running_mean', 'module.encoder.layers.11.bn2.running_var', 'module.encoder.layers.11.bn2.num_batches_tracked', 'module.encoder.layers.12.conv3x1_1.weight', 'module.encoder.layers.12.conv3x1_1.bias', 'module.encoder.layers.12.conv1x3_1.weight', 'module.encoder.layers.12.conv1x3_1.bias', 'module.encoder.layers.12.bn1.weight', 'module.encoder.layers.12.bn1.bias', 'module.encoder.layers.12.bn1.running_mean', 'module.encoder.layers.12.bn1.running_var', 'module.encoder.layers.12.bn1.num_batches_tracked', 'module.encoder.layers.12.conv3x1_2.weight', 'module.encoder.layers.12.conv3x1_2.bias', 'module.encoder.layers.12.conv1x3_2.weight', 'module.encoder.layers.12.conv1x3_2.bias', 'module.encoder.layers.12.bn2.weight', 'module.encoder.layers.12.bn2.bias', 'module.encoder.layers.12.bn2.running_mean', 'module.encoder.layers.12.bn2.running_var', 'module.encoder.layers.12.bn2.num_batches_tracked', 'module.encoder.layers.13.conv3x1_1.weight', 'module.encoder.layers.13.conv3x1_1.bias', 'module.encoder.layers.13.conv1x3_1.weight', 'module.encoder.layers.13.conv1x3_1.bias', 'module.encoder.layers.13.bn1.weight', 'module.encoder.layers.13.bn1.bias', 'module.encoder.layers.13.bn1.running_mean', 'module.encoder.layers.13.bn1.running_var', 'module.encoder.layers.13.bn1.num_batches_tracked', 'module.encoder.layers.13.conv3x1_2.weight', 'module.encoder.layers.13.conv3x1_2.bias', 'module.encoder.layers.13.conv1x3_2.weight', 'module.encoder.layers.13.conv1x3_2.bias', 'module.encoder.layers.13.bn2.weight', 'module.encoder.layers.13.bn2.bias', 'module.encoder.layers.13.bn2.running_mean', 'module.encoder.layers.13.bn2.running_var', 'module.encoder.layers.13.bn2.num_batches_tracked', 'module.encoder.layers.14.conv3x1_1.weight', 'module.encoder.layers.14.conv3x1_1.bias', 'module.encoder.layers.14.conv1x3_1.weight', 'module.encoder.layers.14.conv1x3_1.bias', 'module.encoder.layers.14.bn1.weight', 'module.encoder.layers.14.bn1.bias', 'module.encoder.layers.14.bn1.running_mean', 'module.encoder.layers.14.bn1.running_var', 'module.encoder.layers.14.bn1.num_batches_tracked', 'module.encoder.layers.14.conv3x1_2.weight', 'module.encoder.layers.14.conv3x1_2.bias', 'module.encoder.layers.14.conv1x3_2.weight', 'module.encoder.layers.14.conv1x3_2.bias', 'module.encoder.layers.14.bn2.weight', 'module.encoder.layers.14.bn2.bias', 'module.encoder.layers.14.bn2.running_mean', 'module.encoder.layers.14.bn2.running_var', 'module.encoder.layers.14.bn2.num_batches_tracked', 'module.encoder.output_conv.weight', 'module.encoder.output_conv.bias', 'module.decoder.layers.0.conv.weight', 'module.decoder.layers.0.conv.bias', 'module.decoder.layers.0.bn.weight', 'module.decoder.layers.0.bn.bias', 'module.decoder.layers.0.bn.running_mean', 'module.decoder.layers.0.bn.running_var', 'module.decoder.layers.0.bn.num_batches_tracked', 'module.decoder.layers.1.conv3x1_1.weight', 'module.decoder.layers.1.conv3x1_1.bias', 'module.decoder.layers.1.conv1x3_1.weight', 'module.decoder.layers.1.conv1x3_1.bias', 'module.decoder.layers.1.bn1.weight', 'module.decoder.layers.1.bn1.bias', 'module.decoder.layers.1.bn1.running_mean', 'module.decoder.layers.1.bn1.running_var', 'module.decoder.layers.1.bn1.num_batches_tracked', 'module.decoder.layers.1.conv3x1_2.weight', 'module.decoder.layers.1.conv3x1_2.bias', 'module.decoder.layers.1.conv1x3_2.weight', 'module.decoder.layers.1.conv1x3_2.bias', 'module.decoder.layers.1.bn2.weight', 'module.decoder.layers.1.bn2.bias', 'module.decoder.layers.1.bn2.running_mean', 'module.decoder.layers.1.bn2.running_var', 'module.decoder.layers.1.bn2.num_batches_tracked', 'module.decoder.layers.2.conv3x1_1.weight', 'module.decoder.layers.2.conv3x1_1.bias', 'module.decoder.layers.2.conv1x3_1.weight', 'module.decoder.layers.2.conv1x3_1.bias', 'module.decoder.layers.2.bn1.weight', 'module.decoder.layers.2.bn1.bias', 'module.decoder.layers.2.bn1.running_mean', 'module.decoder.layers.2.bn1.running_var', 'module.decoder.layers.2.bn1.num_batches_tracked', 'module.decoder.layers.2.conv3x1_2.weight', 'module.decoder.layers.2.conv3x1_2.bias', 'module.decoder.layers.2.conv1x3_2.weight', 'module.decoder.layers.2.conv1x3_2.bias', 'module.decoder.layers.2.bn2.weight', 'module.decoder.layers.2.bn2.bias', 'module.decoder.layers.2.bn2.running_mean', 'module.decoder.layers.2.bn2.running_var', 'module.decoder.layers.2.bn2.num_batches_tracked', 'module.decoder.layers.3.conv.weight', 'module.decoder.layers.3.conv.bias', 'module.decoder.layers.3.bn.weight', 'module.decoder.layers.3.bn.bias', 'module.decoder.layers.3.bn.running_mean', 'module.decoder.layers.3.bn.running_var', 'module.decoder.layers.3.bn.num_batches_tracked', 'module.decoder.layers.4.conv3x1_1.weight', 'module.decoder.layers.4.conv3x1_1.bias', 'module.decoder.layers.4.conv1x3_1.weight', 'module.decoder.layers.4.conv1x3_1.bias', 'module.decoder.layers.4.bn1.weight', 'module.decoder.layers.4.bn1.bias', 'module.decoder.layers.4.bn1.running_mean', 'module.decoder.layers.4.bn1.running_var', 'module.decoder.layers.4.bn1.num_batches_tracked', 'module.decoder.layers.4.conv3x1_2.weight', 'module.decoder.layers.4.conv3x1_2.bias', 'module.decoder.layers.4.conv1x3_2.weight', 'module.decoder.layers.4.conv1x3_2.bias', 'module.decoder.layers.4.bn2.weight', 'module.decoder.layers.4.bn2.bias', 'module.decoder.layers.4.bn2.running_mean', 'module.decoder.layers.4.bn2.running_var', 'module.decoder.layers.4.bn2.num_batches_tracked', 'module.decoder.layers.5.conv3x1_1.weight', 'module.decoder.layers.5.conv3x1_1.bias', 'module.decoder.layers.5.conv1x3_1.weight', 'module.decoder.layers.5.conv1x3_1.bias', 'module.decoder.layers.5.bn1.weight', 'module.decoder.layers.5.bn1.bias', 'module.decoder.layers.5.bn1.running_mean', 'module.decoder.layers.5.bn1.running_var', 'module.decoder.layers.5.bn1.num_batches_tracked', 'module.decoder.layers.5.conv3x1_2.weight', 'module.decoder.layers.5.conv3x1_2.bias', 'module.decoder.layers.5.conv1x3_2.weight', 'module.decoder.layers.5.conv1x3_2.bias', 'module.decoder.layers.5.bn2.weight', 'module.decoder.layers.5.bn2.bias', 'module.decoder.layers.5.bn2.running_mean', 'module.decoder.layers.5.bn2.running_var', 'module.decoder.layers.5.bn2.num_batches_tracked', 'module.decoder.output_conv.weight', 'module.decoder.output_conv.bias'])\n",
            "odict_keys(['module.encoder.initial_block.conv.weight', 'module.encoder.initial_block.conv.bias', 'module.encoder.initial_block.bn.weight', 'module.encoder.initial_block.bn.bias', 'module.encoder.initial_block.bn.running_mean', 'module.encoder.initial_block.bn.running_var', 'module.encoder.initial_block.bn.num_batches_tracked', 'module.encoder.layers.0.conv.weight', 'module.encoder.layers.0.conv.bias', 'module.encoder.layers.0.bn.weight', 'module.encoder.layers.0.bn.bias', 'module.encoder.layers.0.bn.running_mean', 'module.encoder.layers.0.bn.running_var', 'module.encoder.layers.0.bn.num_batches_tracked', 'module.encoder.layers.1.conv3x1_1.weight', 'module.encoder.layers.1.conv3x1_1.bias', 'module.encoder.layers.1.conv1x3_1.weight', 'module.encoder.layers.1.conv1x3_1.bias', 'module.encoder.layers.1.bn1.weight', 'module.encoder.layers.1.bn1.bias', 'module.encoder.layers.1.bn1.running_mean', 'module.encoder.layers.1.bn1.running_var', 'module.encoder.layers.1.bn1.num_batches_tracked', 'module.encoder.layers.1.conv3x1_2.weight', 'module.encoder.layers.1.conv3x1_2.bias', 'module.encoder.layers.1.conv1x3_2.weight', 'module.encoder.layers.1.conv1x3_2.bias', 'module.encoder.layers.1.bn2.weight', 'module.encoder.layers.1.bn2.bias', 'module.encoder.layers.1.bn2.running_mean', 'module.encoder.layers.1.bn2.running_var', 'module.encoder.layers.1.bn2.num_batches_tracked', 'module.encoder.layers.2.conv3x1_1.weight', 'module.encoder.layers.2.conv3x1_1.bias', 'module.encoder.layers.2.conv1x3_1.weight', 'module.encoder.layers.2.conv1x3_1.bias', 'module.encoder.layers.2.bn1.weight', 'module.encoder.layers.2.bn1.bias', 'module.encoder.layers.2.bn1.running_mean', 'module.encoder.layers.2.bn1.running_var', 'module.encoder.layers.2.bn1.num_batches_tracked', 'module.encoder.layers.2.conv3x1_2.weight', 'module.encoder.layers.2.conv3x1_2.bias', 'module.encoder.layers.2.conv1x3_2.weight', 'module.encoder.layers.2.conv1x3_2.bias', 'module.encoder.layers.2.bn2.weight', 'module.encoder.layers.2.bn2.bias', 'module.encoder.layers.2.bn2.running_mean', 'module.encoder.layers.2.bn2.running_var', 'module.encoder.layers.2.bn2.num_batches_tracked', 'module.encoder.layers.3.conv3x1_1.weight', 'module.encoder.layers.3.conv3x1_1.bias', 'module.encoder.layers.3.conv1x3_1.weight', 'module.encoder.layers.3.conv1x3_1.bias', 'module.encoder.layers.3.bn1.weight', 'module.encoder.layers.3.bn1.bias', 'module.encoder.layers.3.bn1.running_mean', 'module.encoder.layers.3.bn1.running_var', 'module.encoder.layers.3.bn1.num_batches_tracked', 'module.encoder.layers.3.conv3x1_2.weight', 'module.encoder.layers.3.conv3x1_2.bias', 'module.encoder.layers.3.conv1x3_2.weight', 'module.encoder.layers.3.conv1x3_2.bias', 'module.encoder.layers.3.bn2.weight', 'module.encoder.layers.3.bn2.bias', 'module.encoder.layers.3.bn2.running_mean', 'module.encoder.layers.3.bn2.running_var', 'module.encoder.layers.3.bn2.num_batches_tracked', 'module.encoder.layers.4.conv3x1_1.weight', 'module.encoder.layers.4.conv3x1_1.bias', 'module.encoder.layers.4.conv1x3_1.weight', 'module.encoder.layers.4.conv1x3_1.bias', 'module.encoder.layers.4.bn1.weight', 'module.encoder.layers.4.bn1.bias', 'module.encoder.layers.4.bn1.running_mean', 'module.encoder.layers.4.bn1.running_var', 'module.encoder.layers.4.bn1.num_batches_tracked', 'module.encoder.layers.4.conv3x1_2.weight', 'module.encoder.layers.4.conv3x1_2.bias', 'module.encoder.layers.4.conv1x3_2.weight', 'module.encoder.layers.4.conv1x3_2.bias', 'module.encoder.layers.4.bn2.weight', 'module.encoder.layers.4.bn2.bias', 'module.encoder.layers.4.bn2.running_mean', 'module.encoder.layers.4.bn2.running_var', 'module.encoder.layers.4.bn2.num_batches_tracked', 'module.encoder.layers.5.conv3x1_1.weight', 'module.encoder.layers.5.conv3x1_1.bias', 'module.encoder.layers.5.conv1x3_1.weight', 'module.encoder.layers.5.conv1x3_1.bias', 'module.encoder.layers.5.bn1.weight', 'module.encoder.layers.5.bn1.bias', 'module.encoder.layers.5.bn1.running_mean', 'module.encoder.layers.5.bn1.running_var', 'module.encoder.layers.5.bn1.num_batches_tracked', 'module.encoder.layers.5.conv3x1_2.weight', 'module.encoder.layers.5.conv3x1_2.bias', 'module.encoder.layers.5.conv1x3_2.weight', 'module.encoder.layers.5.conv1x3_2.bias', 'module.encoder.layers.5.bn2.weight', 'module.encoder.layers.5.bn2.bias', 'module.encoder.layers.5.bn2.running_mean', 'module.encoder.layers.5.bn2.running_var', 'module.encoder.layers.5.bn2.num_batches_tracked', 'module.encoder.layers.6.conv.weight', 'module.encoder.layers.6.conv.bias', 'module.encoder.layers.6.bn.weight', 'module.encoder.layers.6.bn.bias', 'module.encoder.layers.6.bn.running_mean', 'module.encoder.layers.6.bn.running_var', 'module.encoder.layers.6.bn.num_batches_tracked', 'module.encoder.layers.7.conv3x1_1.weight', 'module.encoder.layers.7.conv3x1_1.bias', 'module.encoder.layers.7.conv1x3_1.weight', 'module.encoder.layers.7.conv1x3_1.bias', 'module.encoder.layers.7.bn1.weight', 'module.encoder.layers.7.bn1.bias', 'module.encoder.layers.7.bn1.running_mean', 'module.encoder.layers.7.bn1.running_var', 'module.encoder.layers.7.bn1.num_batches_tracked', 'module.encoder.layers.7.conv3x1_2.weight', 'module.encoder.layers.7.conv3x1_2.bias', 'module.encoder.layers.7.conv1x3_2.weight', 'module.encoder.layers.7.conv1x3_2.bias', 'module.encoder.layers.7.bn2.weight', 'module.encoder.layers.7.bn2.bias', 'module.encoder.layers.7.bn2.running_mean', 'module.encoder.layers.7.bn2.running_var', 'module.encoder.layers.7.bn2.num_batches_tracked', 'module.encoder.layers.8.conv3x1_1.weight', 'module.encoder.layers.8.conv3x1_1.bias', 'module.encoder.layers.8.conv1x3_1.weight', 'module.encoder.layers.8.conv1x3_1.bias', 'module.encoder.layers.8.bn1.weight', 'module.encoder.layers.8.bn1.bias', 'module.encoder.layers.8.bn1.running_mean', 'module.encoder.layers.8.bn1.running_var', 'module.encoder.layers.8.bn1.num_batches_tracked', 'module.encoder.layers.8.conv3x1_2.weight', 'module.encoder.layers.8.conv3x1_2.bias', 'module.encoder.layers.8.conv1x3_2.weight', 'module.encoder.layers.8.conv1x3_2.bias', 'module.encoder.layers.8.bn2.weight', 'module.encoder.layers.8.bn2.bias', 'module.encoder.layers.8.bn2.running_mean', 'module.encoder.layers.8.bn2.running_var', 'module.encoder.layers.8.bn2.num_batches_tracked', 'module.encoder.layers.9.conv3x1_1.weight', 'module.encoder.layers.9.conv3x1_1.bias', 'module.encoder.layers.9.conv1x3_1.weight', 'module.encoder.layers.9.conv1x3_1.bias', 'module.encoder.layers.9.bn1.weight', 'module.encoder.layers.9.bn1.bias', 'module.encoder.layers.9.bn1.running_mean', 'module.encoder.layers.9.bn1.running_var', 'module.encoder.layers.9.bn1.num_batches_tracked', 'module.encoder.layers.9.conv3x1_2.weight', 'module.encoder.layers.9.conv3x1_2.bias', 'module.encoder.layers.9.conv1x3_2.weight', 'module.encoder.layers.9.conv1x3_2.bias', 'module.encoder.layers.9.bn2.weight', 'module.encoder.layers.9.bn2.bias', 'module.encoder.layers.9.bn2.running_mean', 'module.encoder.layers.9.bn2.running_var', 'module.encoder.layers.9.bn2.num_batches_tracked', 'module.encoder.layers.10.conv3x1_1.weight', 'module.encoder.layers.10.conv3x1_1.bias', 'module.encoder.layers.10.conv1x3_1.weight', 'module.encoder.layers.10.conv1x3_1.bias', 'module.encoder.layers.10.bn1.weight', 'module.encoder.layers.10.bn1.bias', 'module.encoder.layers.10.bn1.running_mean', 'module.encoder.layers.10.bn1.running_var', 'module.encoder.layers.10.bn1.num_batches_tracked', 'module.encoder.layers.10.conv3x1_2.weight', 'module.encoder.layers.10.conv3x1_2.bias', 'module.encoder.layers.10.conv1x3_2.weight', 'module.encoder.layers.10.conv1x3_2.bias', 'module.encoder.layers.10.bn2.weight', 'module.encoder.layers.10.bn2.bias', 'module.encoder.layers.10.bn2.running_mean', 'module.encoder.layers.10.bn2.running_var', 'module.encoder.layers.10.bn2.num_batches_tracked', 'module.encoder.layers.11.conv3x1_1.weight', 'module.encoder.layers.11.conv3x1_1.bias', 'module.encoder.layers.11.conv1x3_1.weight', 'module.encoder.layers.11.conv1x3_1.bias', 'module.encoder.layers.11.bn1.weight', 'module.encoder.layers.11.bn1.bias', 'module.encoder.layers.11.bn1.running_mean', 'module.encoder.layers.11.bn1.running_var', 'module.encoder.layers.11.bn1.num_batches_tracked', 'module.encoder.layers.11.conv3x1_2.weight', 'module.encoder.layers.11.conv3x1_2.bias', 'module.encoder.layers.11.conv1x3_2.weight', 'module.encoder.layers.11.conv1x3_2.bias', 'module.encoder.layers.11.bn2.weight', 'module.encoder.layers.11.bn2.bias', 'module.encoder.layers.11.bn2.running_mean', 'module.encoder.layers.11.bn2.running_var', 'module.encoder.layers.11.bn2.num_batches_tracked', 'module.encoder.layers.12.conv3x1_1.weight', 'module.encoder.layers.12.conv3x1_1.bias', 'module.encoder.layers.12.conv1x3_1.weight', 'module.encoder.layers.12.conv1x3_1.bias', 'module.encoder.layers.12.bn1.weight', 'module.encoder.layers.12.bn1.bias', 'module.encoder.layers.12.bn1.running_mean', 'module.encoder.layers.12.bn1.running_var', 'module.encoder.layers.12.bn1.num_batches_tracked', 'module.encoder.layers.12.conv3x1_2.weight', 'module.encoder.layers.12.conv3x1_2.bias', 'module.encoder.layers.12.conv1x3_2.weight', 'module.encoder.layers.12.conv1x3_2.bias', 'module.encoder.layers.12.bn2.weight', 'module.encoder.layers.12.bn2.bias', 'module.encoder.layers.12.bn2.running_mean', 'module.encoder.layers.12.bn2.running_var', 'module.encoder.layers.12.bn2.num_batches_tracked', 'module.encoder.layers.13.conv3x1_1.weight', 'module.encoder.layers.13.conv3x1_1.bias', 'module.encoder.layers.13.conv1x3_1.weight', 'module.encoder.layers.13.conv1x3_1.bias', 'module.encoder.layers.13.bn1.weight', 'module.encoder.layers.13.bn1.bias', 'module.encoder.layers.13.bn1.running_mean', 'module.encoder.layers.13.bn1.running_var', 'module.encoder.layers.13.bn1.num_batches_tracked', 'module.encoder.layers.13.conv3x1_2.weight', 'module.encoder.layers.13.conv3x1_2.bias', 'module.encoder.layers.13.conv1x3_2.weight', 'module.encoder.layers.13.conv1x3_2.bias', 'module.encoder.layers.13.bn2.weight', 'module.encoder.layers.13.bn2.bias', 'module.encoder.layers.13.bn2.running_mean', 'module.encoder.layers.13.bn2.running_var', 'module.encoder.layers.13.bn2.num_batches_tracked', 'module.encoder.layers.14.conv3x1_1.weight', 'module.encoder.layers.14.conv3x1_1.bias', 'module.encoder.layers.14.conv1x3_1.weight', 'module.encoder.layers.14.conv1x3_1.bias', 'module.encoder.layers.14.bn1.weight', 'module.encoder.layers.14.bn1.bias', 'module.encoder.layers.14.bn1.running_mean', 'module.encoder.layers.14.bn1.running_var', 'module.encoder.layers.14.bn1.num_batches_tracked', 'module.encoder.layers.14.conv3x1_2.weight', 'module.encoder.layers.14.conv3x1_2.bias', 'module.encoder.layers.14.conv1x3_2.weight', 'module.encoder.layers.14.conv1x3_2.bias', 'module.encoder.layers.14.bn2.weight', 'module.encoder.layers.14.bn2.bias', 'module.encoder.layers.14.bn2.running_mean', 'module.encoder.layers.14.bn2.running_var', 'module.encoder.layers.14.bn2.num_batches_tracked', 'module.encoder.output_conv.weight', 'module.encoder.output_conv.bias', 'module.decoder.layers.0.conv.weight', 'module.decoder.layers.0.conv.bias', 'module.decoder.layers.0.bn.weight', 'module.decoder.layers.0.bn.bias', 'module.decoder.layers.0.bn.running_mean', 'module.decoder.layers.0.bn.running_var', 'module.decoder.layers.0.bn.num_batches_tracked', 'module.decoder.layers.1.conv3x1_1.weight', 'module.decoder.layers.1.conv3x1_1.bias', 'module.decoder.layers.1.conv1x3_1.weight', 'module.decoder.layers.1.conv1x3_1.bias', 'module.decoder.layers.1.bn1.weight', 'module.decoder.layers.1.bn1.bias', 'module.decoder.layers.1.bn1.running_mean', 'module.decoder.layers.1.bn1.running_var', 'module.decoder.layers.1.bn1.num_batches_tracked', 'module.decoder.layers.1.conv3x1_2.weight', 'module.decoder.layers.1.conv3x1_2.bias', 'module.decoder.layers.1.conv1x3_2.weight', 'module.decoder.layers.1.conv1x3_2.bias', 'module.decoder.layers.1.bn2.weight', 'module.decoder.layers.1.bn2.bias', 'module.decoder.layers.1.bn2.running_mean', 'module.decoder.layers.1.bn2.running_var', 'module.decoder.layers.1.bn2.num_batches_tracked', 'module.decoder.layers.2.conv3x1_1.weight', 'module.decoder.layers.2.conv3x1_1.bias', 'module.decoder.layers.2.conv1x3_1.weight', 'module.decoder.layers.2.conv1x3_1.bias', 'module.decoder.layers.2.bn1.weight', 'module.decoder.layers.2.bn1.bias', 'module.decoder.layers.2.bn1.running_mean', 'module.decoder.layers.2.bn1.running_var', 'module.decoder.layers.2.bn1.num_batches_tracked', 'module.decoder.layers.2.conv3x1_2.weight', 'module.decoder.layers.2.conv3x1_2.bias', 'module.decoder.layers.2.conv1x3_2.weight', 'module.decoder.layers.2.conv1x3_2.bias', 'module.decoder.layers.2.bn2.weight', 'module.decoder.layers.2.bn2.bias', 'module.decoder.layers.2.bn2.running_mean', 'module.decoder.layers.2.bn2.running_var', 'module.decoder.layers.2.bn2.num_batches_tracked', 'module.decoder.layers.3.conv.weight', 'module.decoder.layers.3.conv.bias', 'module.decoder.layers.3.bn.weight', 'module.decoder.layers.3.bn.bias', 'module.decoder.layers.3.bn.running_mean', 'module.decoder.layers.3.bn.running_var', 'module.decoder.layers.3.bn.num_batches_tracked', 'module.decoder.layers.4.conv3x1_1.weight', 'module.decoder.layers.4.conv3x1_1.bias', 'module.decoder.layers.4.conv1x3_1.weight', 'module.decoder.layers.4.conv1x3_1.bias', 'module.decoder.layers.4.bn1.weight', 'module.decoder.layers.4.bn1.bias', 'module.decoder.layers.4.bn1.running_mean', 'module.decoder.layers.4.bn1.running_var', 'module.decoder.layers.4.bn1.num_batches_tracked', 'module.decoder.layers.4.conv3x1_2.weight', 'module.decoder.layers.4.conv3x1_2.bias', 'module.decoder.layers.4.conv1x3_2.weight', 'module.decoder.layers.4.conv1x3_2.bias', 'module.decoder.layers.4.bn2.weight', 'module.decoder.layers.4.bn2.bias', 'module.decoder.layers.4.bn2.running_mean', 'module.decoder.layers.4.bn2.running_var', 'module.decoder.layers.4.bn2.num_batches_tracked', 'module.decoder.layers.5.conv3x1_1.weight', 'module.decoder.layers.5.conv3x1_1.bias', 'module.decoder.layers.5.conv1x3_1.weight', 'module.decoder.layers.5.conv1x3_1.bias', 'module.decoder.layers.5.bn1.weight', 'module.decoder.layers.5.bn1.bias', 'module.decoder.layers.5.bn1.running_mean', 'module.decoder.layers.5.bn1.running_var', 'module.decoder.layers.5.bn1.num_batches_tracked', 'module.decoder.layers.5.conv3x1_2.weight', 'module.decoder.layers.5.conv3x1_2.bias', 'module.decoder.layers.5.conv1x3_2.weight', 'module.decoder.layers.5.conv1x3_2.bias', 'module.decoder.layers.5.bn2.weight', 'module.decoder.layers.5.bn2.bias', 'module.decoder.layers.5.bn2.running_mean', 'module.decoder.layers.5.bn2.running_var', 'module.decoder.layers.5.bn2.num_batches_tracked', 'module.decoder.output_conv.weight', 'module.decoder.output_conv.bias'])\n",
            "Model and weights LOADED successfully\n",
            "-----------0.0-----------\n",
            "-----------0.03362474781439139-----------\n",
            "-----------0.06724949562878278-----------\n",
            "-----------0.10087424344317418-----------\n",
            "-----------0.13449899125756556-----------\n",
            "-----------0.16812373907195696-----------\n",
            "-----------0.20174848688634836-----------\n",
            "-----------0.23537323470073976-----------\n",
            "-----------0.26899798251513113-----------\n",
            "-----------0.3026227303295225-----------\n",
            "-----------0.3362474781439139-----------\n",
            "-----------0.3698722259583053-----------\n",
            "-----------0.4034969737726967-----------\n",
            "-----------0.4371217215870881-----------\n",
            "-----------0.4707464694014795-----------\n",
            "-----------0.5043712172158709-----------\n",
            "-----------0.5379959650302623-----------\n",
            "-----------0.5716207128446537-----------\n",
            "-----------0.605245460659045-----------\n",
            "-----------0.6388702084734365-----------\n",
            "-----------0.6724949562878278-----------\n",
            "-----------0.7061197041022192-----------\n",
            "-----------0.7397444519166106-----------\n",
            "-----------0.773369199731002-----------\n",
            "-----------0.8069939475453934-----------\n",
            "-----------0.8406186953597847-----------\n",
            "-----------0.8742434431741762-----------\n",
            "-----------0.9078681909885675-----------\n",
            "-----------0.941492938802959-----------\n",
            "-----------0.9751176866173503-----------\n",
            "-----------1.0087424344317417-----------\n",
            "-----------1.0423671822461333-----------\n",
            "-----------1.0759919300605245-----------\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7c71d1665a20>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1568, in _shutdown_workers\n",
            "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 149, in join\n",
            "    res = self._popen.wait(timeout)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/popen_fork.py\", line 40, in wait\n",
            "    if not wait([self.sentinel], timeout):\n",
            "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 931, in wait\n",
            "    ready = selector.select(timeout)\n",
            "  File \"/usr/lib/python3.10/selectors.py\", line 416, in select\n",
            "    fd_event_list = self._selector.poll(timeout)\n",
            "KeyboardInterrupt: \n",
            "Traceback (most recent call last):\n",
            "  File \"/content/AnomalySegmentation/eval/mahalanobis.py\", line 242, in <module>\n",
            "    main()\n",
            "  File \"/content/AnomalySegmentation/eval/mahalanobis.py\", line 199, in main\n",
            "    with torch.no_grad():\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 154, in __new__\n",
            "    def __new__(cls, orig_func=None):\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datadir = '/content/cityscapes'\n",
        "loadWeights = '/save/erfnet_training1/model_best.pth'\n",
        "loadDir = '/content/AnomalySegmentation'\n",
        "mean = \"/save/mean_cityscapes_erfnet.npy\"\n",
        "\n",
        "!python /content/AnomalySegmentation/eval/mahalanobis.py --datadir {datadir} --model erfnet --loadDir {loadDir} --loadWeights {loadWeights} --mean {mean} --num-workers 2"
      ],
      "metadata": {
        "id": "h5yP-t0skBGb",
        "outputId": "118fb1c8-de02-4154-e1d1-bb6a5a8419a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/AnomalySegmentation/eval/mahalanobis.py\", line 3, in <module>\n",
            "    import torch\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2486, in <module>\n",
            "    from torch import _meta_registrations\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_meta_registrations.py\", line 10, in <module>\n",
            "    from torch._decomp import (\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_decomp/__init__.py\", line 249, in <module>\n",
            "    import torch._decomp.decompositions\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_decomp/decompositions.py\", line 15, in <module>\n",
            "    import torch._prims as prims\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_prims/__init__.py\", line 3090, in <module>\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_prims/__init__.py\", line 319, in _make_prim\n",
            "    prim_def = torch.library.custom_op(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_library/custom_ops.py\", line 157, in custom_op\n",
            "    return inner(fn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_library/custom_ops.py\", line 138, in inner\n",
            "    result = CustomOpDef(namespace, opname, schema_str, fn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_library/custom_ops.py\", line 186, in __init__\n",
            "    self._register_to_dispatcher()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_library/custom_ops.py\", line 618, in _register_to_dispatcher\n",
            "    autograd_impl = autograd.make_autograd_impl(self._opoverload, self)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_library/autograd.py\", line 99, in make_autograd_impl\n",
            "    if any(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_library/autograd.py\", line 100, in <genexpr>\n",
            "    utils.is_tensorlist_like_type(a.type)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_library/utils.py\", line 113, in is_tensorlist_like_type\n",
            "    or typ == _C.ListType(_C.OptionalType(_C.TensorType.get()))\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbd9d0b7-76db-4494-e795-99fa94b86e18",
        "id": "wxqrO7DO30IF"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------\n",
            "\n",
            "Dataset: RoadAnomaly21 method: Mahalanobis\n",
            "Loading model: /content/AnomalySegmentation/trained_models//erfnet.py\n",
            "Loading weights: /content/AnomalySegmentation/trained_models/erfnet_pretrained.pth\n",
            "Model and weights LOADED successfully\n",
            "mean shape:  [[ 9.47391605e+00 -1.53364345e-01 -4.28085232e+00 -2.60957479e+00\n",
            "  -1.88721311e+00 -3.01505017e+00 -3.05055714e+00 -1.75006449e+00\n",
            "  -1.58511269e+00 -7.84052491e-01 -1.80338085e+00 -6.06463492e-01\n",
            "  -1.65718365e+00 -1.76602602e+00 -1.47501361e+00 -1.52931595e+00\n",
            "  -1.21098173e+00 -1.93728256e+00 -2.91776657e+00 -2.16307616e+00]\n",
            " [ 1.17828500e+00  8.86781502e+00 -1.62400520e+00 -6.53047502e-01\n",
            "  -3.00401568e+00 -1.04693687e+00 -5.53424025e+00 -4.88593483e+00\n",
            "  -3.52651858e+00  2.95564532e-01 -2.10292721e+00 -1.66624975e+00\n",
            "  -4.55403805e+00 -1.38096905e+00 -3.82047367e+00 -6.94308901e+00\n",
            "  -2.99339676e+00 -3.36712003e+00 -9.66851532e-01 -3.66484404e+00]\n",
            " [-6.09100914e+00 -3.33458328e+00  8.18830585e+00 -1.40364635e+00\n",
            "  -1.62133443e+00  4.74281013e-01 -1.76999891e+00 -1.25186145e+00\n",
            "   6.61789775e-02 -5.81029940e+00 -1.58107972e+00 -2.13845229e+00\n",
            "  -3.35688663e+00 -3.09009027e+00 -1.91846132e+00 -4.18830347e+00\n",
            "  -2.03108287e+00 -4.55380440e+00 -2.78936410e+00 -3.64918423e+00]\n",
            " [-3.43343568e+00 -2.17381552e-01 -3.75593275e-01  7.41109705e+00\n",
            "   1.17263626e-02 -2.03283882e+00 -8.64830971e+00 -6.25905991e+00\n",
            "  -5.18851588e-03 -1.11671484e+00 -8.65121269e+00 -4.44218063e+00\n",
            "  -6.44711542e+00 -2.20859575e+00 -1.84761965e+00 -6.51903057e+00\n",
            "  -1.05142155e+01 -5.33923686e-01 -3.95114374e+00 -5.54778147e+00]\n",
            " [-3.08720231e+00 -3.48581982e+00 -1.56796932e+00 -7.79400527e-01\n",
            "   5.99547958e+00 -1.35234177e+00 -3.65560150e+00 -2.62476373e+00\n",
            "  -1.94557059e+00 -2.16301942e+00 -1.05322256e+01 -2.35526514e+00\n",
            "  -3.90873647e+00 -4.44750977e+00 -6.68696404e+00 -3.35921288e+00\n",
            "  -6.26932049e+00 -4.63401079e+00 -1.97871137e+00 -5.18198681e+00]\n",
            " [-5.29459190e+00 -2.33917093e+00  9.86048341e-01 -3.31411481e+00\n",
            "  -1.99832904e+00  6.11520863e+00 -1.90763748e+00 -1.08723712e+00\n",
            "   2.67463867e-02 -4.09403944e+00 -3.02436209e+00 -2.16592240e+00\n",
            "  -5.30932665e+00 -3.12803650e+00 -4.95815563e+00 -6.24611521e+00\n",
            "  -4.93330669e+00 -6.72880363e+00 -2.06649089e+00 -5.28043747e+00]\n",
            " [-7.23020506e+00 -7.68683672e+00  3.38047767e+00 -8.29992676e+00\n",
            "  -3.10841346e+00  3.29398656e+00  7.74280357e+00  1.59127808e+00\n",
            "   2.66655755e+00 -4.26323462e+00  2.26926282e-01  1.08719893e-01\n",
            "  -3.80924797e+00 -7.50573730e+00 -5.18605137e+00 -1.40789852e-01\n",
            "  -2.16258407e+00 -1.05069256e+01 -7.31151009e+00 -4.90072489e+00]\n",
            " [-3.65513182e+00 -6.21735764e+00  1.29654014e+00 -3.38319778e+00\n",
            "  -1.56025076e+00  8.31872165e-01 -8.76031935e-01  7.11746454e+00\n",
            "  -3.56169790e-01 -4.57800055e+00 -9.99007761e-01 -2.41856122e+00\n",
            "  -1.87134600e+00 -1.76173639e+00 -1.79829955e+00 -5.05804110e+00\n",
            "  -9.71121883e+00 -1.09394817e+01 -6.06652689e+00 -5.51081085e+00]\n",
            " [-2.89445901e+00 -3.84487581e+00  8.77959967e-01 -1.82086742e+00\n",
            "  -2.42468762e+00  2.00544596e-01 -1.50937200e+00 -1.84290731e+00\n",
            "   8.52589607e+00 -5.35762548e-01 -1.07255459e+00 -3.12493777e+00\n",
            "  -3.10031033e+00 -1.16505945e+00 -3.64628243e+00 -2.73300648e+00\n",
            "  -3.26817203e+00 -4.57943010e+00 -3.38852286e+00 -3.17749810e+00]\n",
            " [ 2.27725375e-02  1.06740928e+00 -7.09144831e+00 -1.10042179e+00\n",
            "  -5.47922730e-01 -2.65664315e+00 -4.39860058e+00 -3.80578494e+00\n",
            "   6.61651194e-01  7.57295465e+00 -2.21560860e+00 -2.35329795e+00\n",
            "  -2.85510850e+00 -3.72381616e+00 -7.21391153e+00 -2.73804092e+00\n",
            "  -7.70631742e+00 -3.50845861e+00 -1.04953349e+00 -4.17231274e+00]\n",
            " [-1.05754994e-01 -1.82859793e-01  1.45439720e+00 -4.29355955e+00\n",
            "  -6.67381716e+00  1.13282502e+00 -9.85899687e-01  4.38281685e-01\n",
            "   7.89561331e-01 -1.51466236e-01  9.88578892e+00 -4.91721392e+00\n",
            "  -1.24033380e+00 -5.07104158e+00 -1.17861736e+00 -3.60034990e+00\n",
            "  -2.00458789e+00 -6.58753824e+00 -2.39760113e+00 -2.81982207e+00]\n",
            " [-2.35331821e+00 -2.07986498e+00 -7.82505691e-01 -4.48104906e+00\n",
            "  -2.89769292e+00 -2.25190663e+00 -3.87249112e+00 -3.68551278e+00\n",
            "  -3.64397860e+00 -3.99596190e+00 -9.33475971e+00  5.84995842e+00\n",
            "  -1.30550921e+00 -2.38445330e+00 -7.41336107e+00 -6.11098528e+00\n",
            "  -6.36851025e+00 -3.16688824e+00 -2.98703504e+00 -5.95539618e+00]\n",
            " [-4.94513178e+00 -6.54697609e+00 -3.21434212e+00 -6.00414562e+00\n",
            "  -4.78725386e+00 -6.91609430e+00 -1.00187864e+01 -3.68027568e+00\n",
            "  -4.63803720e+00 -7.57164860e+00 -5.95008039e+00 -1.55047381e+00\n",
            "   3.86343646e+00 -4.80199194e+00 -2.99728155e+00 -4.63549423e+00\n",
            "  -6.90959978e+00 -2.21650577e+00 -5.30939817e-01 -7.19778204e+00]\n",
            " [-1.21962357e+00 -1.29279220e+00 -3.03141499e+00 -2.76665354e+00\n",
            "  -4.46767712e+00 -1.60576177e+00 -6.74284410e+00 -2.10086823e+00\n",
            "  -1.62678826e+00 -3.02621269e+00 -6.71150541e+00 -1.72396135e+00\n",
            "  -4.32491112e+00  8.27347279e+00 -1.35729730e+00 -2.70669436e+00\n",
            "  -4.91638231e+00 -2.30103970e+00 -2.44592738e+00 -4.28046036e+00]\n",
            " [-1.97135699e+00 -2.85817814e+00 -1.08225369e+00 -1.61517560e+00\n",
            "  -7.53079271e+00 -4.00180864e+00 -6.59525347e+00 -2.00592446e+00\n",
            "  -6.59183931e+00 -1.01455154e+01 -1.96822095e+00 -6.25470066e+00\n",
            "  -1.43799818e+00 -4.17478204e-01  7.41051769e+00 -5.24967551e-01\n",
            "  -2.90299869e+00 -1.33700454e+00 -5.46037149e+00 -4.87532091e+00]\n",
            " [-4.53587770e+00 -1.14588604e+01 -5.52899933e+00 -9.42649364e+00\n",
            "  -4.02583456e+00 -4.02468920e+00 -2.60906100e+00 -4.42482042e+00\n",
            "  -2.14630556e+00 -4.35607672e+00 -6.27299166e+00 -5.77375269e+00\n",
            "  -2.83521867e+00 -1.45329297e+00 -1.43666184e+00  5.91057682e+00\n",
            "  -1.30376101e+00 -3.20994091e+00 -4.33077621e+00 -5.93252230e+00]\n",
            " [-4.20418352e-01 -2.20519996e+00  8.33943605e-01 -1.30117750e+01\n",
            "  -6.29975700e+00 -1.15953481e+00 -2.89861274e+00 -7.67661095e+00\n",
            "  -3.85714698e+00 -7.83243322e+00 -2.36895752e+00 -3.83022332e+00\n",
            "  -3.67780924e+00 -3.48450756e+00 -3.62164187e+00  2.38630384e-01\n",
            "   7.49788332e+00 -3.67516851e+00  7.76995197e-02 -4.49554682e+00]\n",
            " [-2.82988858e+00 -3.65364170e+00 -3.80217981e+00 -8.97055268e-01\n",
            "  -5.41914511e+00 -8.19533825e+00 -1.37874784e+01 -1.15053196e+01\n",
            "  -5.91966724e+00 -5.86246490e+00 -1.11267157e+01 -1.38881540e+00\n",
            "  -1.61308169e-01 -1.03623486e+00 -1.79844689e+00 -3.28299713e+00\n",
            "  -5.17995739e+00  5.15966415e+00  1.83292225e-01 -6.43559265e+00]\n",
            " [-4.62558985e+00 -1.81449723e+00 -3.06890941e+00 -6.00455093e+00\n",
            "  -3.07814217e+00 -1.99535620e+00 -1.08228073e+01 -7.52263403e+00\n",
            "  -4.32984638e+00 -3.59404469e+00 -6.88565779e+00 -4.09002209e+00\n",
            "  -1.91099548e+00 -3.53804684e+00 -7.21245003e+00 -6.35778999e+00\n",
            "  -4.10250854e+00 -2.38090158e+00  4.20121765e+00 -6.77165890e+00]\n",
            " [ 2.55759072e+00 -3.09470087e-01 -2.27886486e+00 -1.92675447e+00\n",
            "  -1.95014191e+00 -1.67344904e+00 -3.75778222e+00 -2.53304839e+00\n",
            "  -1.40959263e+00 -8.34550381e-01 -2.67928123e+00 -1.69567299e+00\n",
            "  -2.83534217e+00 -1.87789810e+00 -2.69884896e+00 -2.87712717e+00\n",
            "  -3.34889007e+00 -2.88919926e+00 -2.40740657e+00 -3.45654058e+00]]\n",
            "cov shape:  [[ 4.86217165e+00  9.01867628e-01 -2.53227162e+00 -2.27063790e-01\n",
            "  -3.07615519e-01 -1.06444204e+00 -1.02834567e-01  2.36858670e-02\n",
            "  -3.20682645e-01  1.25855803e+00  8.09911489e-01  3.85140181e-01\n",
            "   5.42841315e-01  2.86023498e-01  5.34947991e-01  8.62877488e-01\n",
            "   8.33933115e-01  8.28830540e-01 -9.18189809e-02  7.34081209e-01]\n",
            " [ 9.01867628e-01  2.83106661e+00 -3.98538649e-01  4.22827780e-01\n",
            "  -3.75586092e-01  3.72506939e-02 -6.86935961e-01 -7.07577467e-01\n",
            "  -6.80343986e-01  8.14784169e-01  3.68099064e-01  9.62772686e-03\n",
            "  -5.21943569e-01  2.20938876e-01 -2.89078623e-01 -1.06530464e+00\n",
            "   1.49575964e-01  4.78993393e-02  4.55110252e-01  7.55973384e-02]\n",
            " [-2.53227162e+00 -3.98538649e-01  4.24507236e+00  5.45557141e-01\n",
            "   8.07865784e-02  7.72379518e-01  8.61060500e-01  4.90417749e-01\n",
            "   5.00336111e-01 -1.76889694e+00  7.75323570e-01 -1.44662276e-01\n",
            "  -3.78167152e-01 -5.38629234e-01  5.46711743e-01 -6.66874826e-01\n",
            "   7.49345899e-01 -6.26119912e-01 -3.04575145e-01  1.12466276e-01]\n",
            " [-2.27063790e-01  4.22827780e-01  5.45557141e-01  1.53814733e+00\n",
            "   5.57096720e-01 -2.79044718e-01 -4.37022835e-01 -2.60150701e-01\n",
            "   2.32952684e-01  2.69132286e-01 -4.74379301e-01 -1.97529629e-01\n",
            "  -2.98771173e-01  6.65179193e-02  3.19413096e-01 -3.14831495e-01\n",
            "  -6.07027054e-01  4.94130820e-01 -1.44095346e-01  8.48548189e-02]\n",
            " [-3.07615519e-01 -3.75586092e-01  8.07865784e-02  5.57096720e-01\n",
            "   1.11160088e+00 -2.31633186e-02  8.86088982e-02  1.45019442e-01\n",
            "  -9.27994624e-02  1.06651716e-01 -8.36634338e-01  6.90098330e-02\n",
            "  -3.66689749e-02 -4.32733774e-01 -3.07958961e-01  2.04425789e-02\n",
            "  -4.79044974e-01  1.61138009e-02  1.87468510e-02 -6.40841275e-02]\n",
            " [-1.06444204e+00  3.72506939e-02  7.72379518e-01 -2.79044718e-01\n",
            "  -2.31633186e-02  1.41814721e+00  5.91644645e-01  4.77879316e-01\n",
            "   2.31971979e-01 -4.96109962e-01  3.81510407e-01 -8.82847384e-02\n",
            "  -3.93816382e-01 -1.32841825e-01 -1.76587760e-01 -4.89817381e-01\n",
            "   8.89823958e-02 -7.53069043e-01  6.27177656e-02 -1.10244699e-01]\n",
            " [-1.02834567e-01 -6.86935961e-01  8.61060500e-01 -4.37022835e-01\n",
            "   8.86088982e-02  5.91644645e-01  1.39858723e+00  8.91589284e-01\n",
            "   6.36216164e-01 -3.44848335e-01  9.42835212e-01  2.15093046e-01\n",
            "   2.03517377e-01 -4.77445036e-01  2.09907755e-01  4.32108462e-01\n",
            "   5.97899199e-01 -6.02783680e-01 -4.19413090e-01  2.82130063e-01]\n",
            " [ 2.36858670e-02 -7.07577467e-01  4.90417749e-01 -2.60150701e-01\n",
            "   1.45019442e-01  4.77879316e-01  8.91589284e-01  1.34435844e+00\n",
            "   2.50112116e-01 -4.70255792e-01  7.77488768e-01  5.34376539e-02\n",
            "   2.50056863e-01 -2.13777670e-03  4.37948942e-01  2.12414637e-01\n",
            "   4.14197370e-02 -6.89260781e-01 -4.39750850e-01  1.66964814e-01]\n",
            " [-3.20682645e-01 -6.80343986e-01  5.00336111e-01  2.32952684e-01\n",
            "  -9.27994624e-02  2.31971979e-01  6.36216164e-01  2.50112116e-01\n",
            "   2.40815401e+00  5.00963271e-01  7.38364160e-01 -3.66638422e-01\n",
            "  -6.72785640e-02  1.97175905e-01 -3.35109723e-03  2.79865354e-01\n",
            "   1.20381311e-01 -2.59081990e-01 -3.80139261e-01  2.76157498e-01]\n",
            " [ 1.25855803e+00  8.14784169e-01 -1.76889694e+00  2.69132286e-01\n",
            "   1.06651716e-01 -4.96109962e-01 -3.44848335e-01 -4.70255792e-01\n",
            "   5.00963271e-01  1.96146631e+00  1.80940092e-01 -9.56172124e-02\n",
            "  -2.81864461e-02  1.86310224e-02 -6.34221196e-01  1.60656825e-01\n",
            "  -4.95728850e-01  2.67904252e-01  2.24283740e-01  1.11139603e-01]\n",
            " [ 8.09911489e-01  3.68099064e-01  7.75323570e-01 -4.74379301e-01\n",
            "  -8.36634338e-01  3.81510407e-01  9.42835212e-01  7.77488768e-01\n",
            "   7.38364160e-01  1.80940092e-01  2.96772456e+00 -5.11886835e-01\n",
            "   3.32706779e-01 -5.44660032e-01  6.69295371e-01  2.18011320e-01\n",
            "   1.12244391e+00 -5.33072531e-01 -1.44187748e-01  5.70582688e-01]\n",
            " [ 3.85140181e-01  9.62772686e-03 -1.44662276e-01 -1.97529629e-01\n",
            "   6.90098330e-02 -8.82847384e-02  2.15093046e-01  5.34376539e-02\n",
            "  -3.66638422e-01 -9.56172124e-02 -5.11886835e-01  9.04819548e-01\n",
            "   2.38732189e-01  2.74756216e-02 -2.49173149e-01 -3.46569940e-02\n",
            "  -1.38684316e-02  1.35810003e-01 -7.12975487e-02 -4.36910335e-03]\n",
            " [ 5.42841315e-01 -5.21943569e-01 -3.78167152e-01 -2.98771173e-01\n",
            "  -3.66689749e-02 -3.93816382e-01  2.03517377e-01  2.50056863e-01\n",
            "  -6.72785640e-02 -2.81864461e-02  3.32706779e-01  2.38732189e-01\n",
            "   6.98428631e-01 -2.41519228e-01  2.53673702e-01  5.04576743e-01\n",
            "   2.08380073e-01  2.33126193e-01 -6.58003241e-03  1.52886823e-01]\n",
            " [ 2.86023498e-01  2.20938876e-01 -5.38629234e-01  6.65179193e-02\n",
            "  -4.32733774e-01 -1.32841825e-01 -4.77445036e-01 -2.13777670e-03\n",
            "   1.97175905e-01  1.86310224e-02 -5.44660032e-01  2.74756216e-02\n",
            "  -2.41519228e-01  1.74149096e+00  3.99985075e-01  1.58893168e-01\n",
            "  -8.90314206e-02  3.07576329e-01 -8.54879916e-02  5.77470697e-02]\n",
            " [ 5.34947991e-01 -2.89078623e-01  5.46711743e-01  3.19413096e-01\n",
            "  -3.07958961e-01 -1.76587760e-01  2.09907755e-01  4.37948942e-01\n",
            "  -3.35109723e-03 -6.34221196e-01  6.69295371e-01 -2.49173149e-01\n",
            "   2.53673702e-01  3.99985075e-01  1.29401672e+00  5.41884661e-01\n",
            "   5.69690347e-01  3.58934283e-01 -3.42244297e-01  3.81587952e-01]\n",
            " [ 8.62877488e-01 -1.06530464e+00 -6.66874826e-01 -3.14831495e-01\n",
            "   2.04425789e-02 -4.89817381e-01  4.32108462e-01  2.12414637e-01\n",
            "   2.79865354e-01  1.60656825e-01  2.18011320e-01 -3.46569940e-02\n",
            "   5.04576743e-01  1.58893168e-01  5.41884661e-01  1.24961579e+00\n",
            "   4.75901812e-01  4.49608892e-01 -2.04467401e-01  3.07554513e-01]\n",
            " [ 8.33933115e-01  1.49575964e-01  7.49345899e-01 -6.07027054e-01\n",
            "  -4.79044974e-01  8.89823958e-02  5.97899199e-01  4.14197370e-02\n",
            "   1.20381311e-01 -4.95728850e-01  1.12244391e+00 -1.38684316e-02\n",
            "   2.08380073e-01 -8.90314206e-02  5.69690347e-01  4.75901812e-01\n",
            "   1.55777621e+00  1.39711738e-01  7.93709606e-02  4.76549059e-01]\n",
            " [ 8.28830540e-01  4.78993393e-02 -6.26119912e-01  4.94130820e-01\n",
            "   1.61138009e-02 -7.53069043e-01 -6.02783680e-01 -6.89260781e-01\n",
            "  -2.59081990e-01  2.67904252e-01 -5.33072531e-01  1.35810003e-01\n",
            "   2.33126193e-01  3.07576329e-01  3.58934283e-01  4.49608892e-01\n",
            "   1.39711738e-01  1.09693754e+00  1.70832619e-01  1.74041554e-01]\n",
            " [-9.18189809e-02  4.55110252e-01 -3.04575145e-01 -1.44095346e-01\n",
            "   1.87468510e-02  6.27177656e-02 -4.19413090e-01 -4.39750850e-01\n",
            "  -3.80139261e-01  2.24283740e-01 -1.44187748e-01 -7.12975487e-02\n",
            "  -6.58003241e-03 -8.54879916e-02 -3.42244297e-01 -2.04467401e-01\n",
            "   7.93709606e-02  1.70832619e-01  5.53345978e-01 -8.93651322e-02]\n",
            " [ 7.34081209e-01  7.55973384e-02  1.12466276e-01  8.48548189e-02\n",
            "  -6.40841275e-02 -1.10244699e-01  2.82130063e-01  1.66964814e-01\n",
            "   2.76157498e-01  1.11139603e-01  5.70582688e-01 -4.36910335e-03\n",
            "   1.52886823e-01  5.77470697e-02  3.81587952e-01  3.07554513e-01\n",
            "   4.76549059e-01  1.74041554e-01 -8.93651322e-02  3.31872404e-01]]\n",
            "  0% 0/10 [00:00<?, ?it/s]scores tensor([ -47.9779,  -70.7957,  -38.9964,  ..., -209.9153, -203.6648,\n",
            "         -21.5322], device='cuda:0')\n",
            "  0% 0/10 [00:01<?, ?it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/AnomalySegmentation/eval/evalAnomaly.py\", line 324, in <module>\n",
            "    main()\n",
            "  File \"/content/AnomalySegmentation/eval/evalAnomaly.py\", line 255, in main\n",
            "    anomaly_result = mahalanobis_distance_score(result, means, cov_inv)\n",
            "  File \"/content/AnomalySegmentation/eval/evalAnomaly.py\", line 142, in mahalanobis_distance_score\n",
            "    M_scores = scores.max(dim=1)[0].reshape(output.size(1), output.size(2))  # (512, 1024)\n",
            "IndexError: Dimension out of range (expected to be in range of [-1, 0], but got 1)\n",
            "----------------------------\n",
            "----------------------------\n",
            "----------------------------\n",
            "----------------------------\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "no_execute = False\n",
        "just_once = True\n",
        "\n",
        "for dataset_dir in ['RoadAnomaly21', 'RoadObsticle21', 'FS_LostFound_full', 'fs_static', 'RoadAnomaly']:\n",
        "  print(\"----------------------------\")\n",
        "  for method in [\"Mahalanobis\", \"MSP\", \"MaxLogit\", \"MaxEntropy\"]:\n",
        "\n",
        "    if no_execute:\n",
        "      break\n",
        "\n",
        "    format_file = os.listdir(f'/content/Validation_Dataset/{dataset_dir}/images')[0].split(\".\")[1]\n",
        "    input =f'/content/Validation_Dataset/{dataset_dir}/images/\\*.{format_file}'\n",
        "    print(f\"\\nDataset: {dataset_dir} method: {method}\")\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "      !python -W ignore /content/AnomalySegmentation/eval/evalAnomaly.py --input {input} --method  {method}\n",
        "    else:\n",
        "      !python -W ignore /content/AnomalySegmentation/eval/evalAnomaly.py --input {input} --method {method}  --cpu | tail -n 2\n",
        "\n",
        "    if just_once:\n",
        "      no_execute = True\n",
        "      just_once = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Losses"
      ],
      "metadata": {
        "id": "iFHIDiwduTGq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine tune ERFNET with different losses\n",
        "\n",
        "losses = [\"Focal\", \"LogitNorm\", \"IsoMaxPlus\"]\n",
        "models = [\"erfnet\", \"erfnet\", \"erfnet_isomaxplus\"]\n",
        "savedirs = [\"erfnet_training_focal_loss\", \"erfnet_training_logitnorm_loss\", \"erfnet_training_isomaxplus_loss\"]\n",
        "epochs = 20\n",
        "\n",
        "# Base directory of the project\n",
        "base_dir = \"/content/AnomalySegmentation/train\"\n",
        "# Dataset directory\n",
        "data_dir = \"/content/cityscapes\"\n",
        "pretrained_weights = \"erfnet_pretrained.pth\"\n",
        "\n",
        "# Loop to execute fine-tuning\n",
        "for loss, model, savedir in zip(losses, models, savedirs):\n",
        "    print(f\"\\n\\n----- Fine-tuning with {loss} loss -----\")\n",
        "    !cd {base_dir} && python -W ignore main.py --savedir {savedir} --loss {loss} --datadir {data_dir} --model {model} --cuda --num-epochs=20 --epochs-save=1 --FineTune --decoder --loadWeights={pretrained_weights}\n",
        "    print(f\"Model saved in /content/AnomalySegmentation/save/{savedir}\")\n",
        "    # zip folder\n",
        "    !zip -r save_{savedir}.zip /content/AnomalySegmentation/save/{savedir}"
      ],
      "metadata": {
        "id": "95d_Y3uG6ktP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "233bd51a-c6c5-4060-b9ac-93595b7511a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "----- Fine-tuning with Focal loss -----\n",
            "odict_keys(['module.encoder.initial_block.conv.weight', 'module.encoder.initial_block.conv.bias', 'module.encoder.initial_block.bn.weight', 'module.encoder.initial_block.bn.bias', 'module.encoder.initial_block.bn.running_mean', 'module.encoder.initial_block.bn.running_var', 'module.encoder.layers.0.conv.weight', 'module.encoder.layers.0.conv.bias', 'module.encoder.layers.0.bn.weight', 'module.encoder.layers.0.bn.bias', 'module.encoder.layers.0.bn.running_mean', 'module.encoder.layers.0.bn.running_var', 'module.encoder.layers.1.conv3x1_1.weight', 'module.encoder.layers.1.conv3x1_1.bias', 'module.encoder.layers.1.conv1x3_1.weight', 'module.encoder.layers.1.conv1x3_1.bias', 'module.encoder.layers.1.conv3x1_2.weight', 'module.encoder.layers.1.conv3x1_2.bias', 'module.encoder.layers.1.conv1x3_2.weight', 'module.encoder.layers.1.conv1x3_2.bias', 'module.encoder.layers.1.bn1.weight', 'module.encoder.layers.1.bn1.bias', 'module.encoder.layers.1.bn1.running_mean', 'module.encoder.layers.1.bn1.running_var', 'module.encoder.layers.1.bn2.weight', 'module.encoder.layers.1.bn2.bias', 'module.encoder.layers.1.bn2.running_mean', 'module.encoder.layers.1.bn2.running_var', 'module.encoder.layers.2.conv3x1_1.weight', 'module.encoder.layers.2.conv3x1_1.bias', 'module.encoder.layers.2.conv1x3_1.weight', 'module.encoder.layers.2.conv1x3_1.bias', 'module.encoder.layers.2.conv3x1_2.weight', 'module.encoder.layers.2.conv3x1_2.bias', 'module.encoder.layers.2.conv1x3_2.weight', 'module.encoder.layers.2.conv1x3_2.bias', 'module.encoder.layers.2.bn1.weight', 'module.encoder.layers.2.bn1.bias', 'module.encoder.layers.2.bn1.running_mean', 'module.encoder.layers.2.bn1.running_var', 'module.encoder.layers.2.bn2.weight', 'module.encoder.layers.2.bn2.bias', 'module.encoder.layers.2.bn2.running_mean', 'module.encoder.layers.2.bn2.running_var', 'module.encoder.layers.3.conv3x1_1.weight', 'module.encoder.layers.3.conv3x1_1.bias', 'module.encoder.layers.3.conv1x3_1.weight', 'module.encoder.layers.3.conv1x3_1.bias', 'module.encoder.layers.3.conv3x1_2.weight', 'module.encoder.layers.3.conv3x1_2.bias', 'module.encoder.layers.3.conv1x3_2.weight', 'module.encoder.layers.3.conv1x3_2.bias', 'module.encoder.layers.3.bn1.weight', 'module.encoder.layers.3.bn1.bias', 'module.encoder.layers.3.bn1.running_mean', 'module.encoder.layers.3.bn1.running_var', 'module.encoder.layers.3.bn2.weight', 'module.encoder.layers.3.bn2.bias', 'module.encoder.layers.3.bn2.running_mean', 'module.encoder.layers.3.bn2.running_var', 'module.encoder.layers.4.conv3x1_1.weight', 'module.encoder.layers.4.conv3x1_1.bias', 'module.encoder.layers.4.conv1x3_1.weight', 'module.encoder.layers.4.conv1x3_1.bias', 'module.encoder.layers.4.conv3x1_2.weight', 'module.encoder.layers.4.conv3x1_2.bias', 'module.encoder.layers.4.conv1x3_2.weight', 'module.encoder.layers.4.conv1x3_2.bias', 'module.encoder.layers.4.bn1.weight', 'module.encoder.layers.4.bn1.bias', 'module.encoder.layers.4.bn1.running_mean', 'module.encoder.layers.4.bn1.running_var', 'module.encoder.layers.4.bn2.weight', 'module.encoder.layers.4.bn2.bias', 'module.encoder.layers.4.bn2.running_mean', 'module.encoder.layers.4.bn2.running_var', 'module.encoder.layers.5.conv3x1_1.weight', 'module.encoder.layers.5.conv3x1_1.bias', 'module.encoder.layers.5.conv1x3_1.weight', 'module.encoder.layers.5.conv1x3_1.bias', 'module.encoder.layers.5.conv3x1_2.weight', 'module.encoder.layers.5.conv3x1_2.bias', 'module.encoder.layers.5.conv1x3_2.weight', 'module.encoder.layers.5.conv1x3_2.bias', 'module.encoder.layers.5.bn1.weight', 'module.encoder.layers.5.bn1.bias', 'module.encoder.layers.5.bn1.running_mean', 'module.encoder.layers.5.bn1.running_var', 'module.encoder.layers.5.bn2.weight', 'module.encoder.layers.5.bn2.bias', 'module.encoder.layers.5.bn2.running_mean', 'module.encoder.layers.5.bn2.running_var', 'module.encoder.layers.6.conv.weight', 'module.encoder.layers.6.conv.bias', 'module.encoder.layers.6.bn.weight', 'module.encoder.layers.6.bn.bias', 'module.encoder.layers.6.bn.running_mean', 'module.encoder.layers.6.bn.running_var', 'module.encoder.layers.7.conv3x1_1.weight', 'module.encoder.layers.7.conv3x1_1.bias', 'module.encoder.layers.7.conv1x3_1.weight', 'module.encoder.layers.7.conv1x3_1.bias', 'module.encoder.layers.7.conv3x1_2.weight', 'module.encoder.layers.7.conv3x1_2.bias', 'module.encoder.layers.7.conv1x3_2.weight', 'module.encoder.layers.7.conv1x3_2.bias', 'module.encoder.layers.7.bn1.weight', 'module.encoder.layers.7.bn1.bias', 'module.encoder.layers.7.bn1.running_mean', 'module.encoder.layers.7.bn1.running_var', 'module.encoder.layers.7.bn2.weight', 'module.encoder.layers.7.bn2.bias', 'module.encoder.layers.7.bn2.running_mean', 'module.encoder.layers.7.bn2.running_var', 'module.encoder.layers.8.conv3x1_1.weight', 'module.encoder.layers.8.conv3x1_1.bias', 'module.encoder.layers.8.conv1x3_1.weight', 'module.encoder.layers.8.conv1x3_1.bias', 'module.encoder.layers.8.conv3x1_2.weight', 'module.encoder.layers.8.conv3x1_2.bias', 'module.encoder.layers.8.conv1x3_2.weight', 'module.encoder.layers.8.conv1x3_2.bias', 'module.encoder.layers.8.bn1.weight', 'module.encoder.layers.8.bn1.bias', 'module.encoder.layers.8.bn1.running_mean', 'module.encoder.layers.8.bn1.running_var', 'module.encoder.layers.8.bn2.weight', 'module.encoder.layers.8.bn2.bias', 'module.encoder.layers.8.bn2.running_mean', 'module.encoder.layers.8.bn2.running_var', 'module.encoder.layers.9.conv3x1_1.weight', 'module.encoder.layers.9.conv3x1_1.bias', 'module.encoder.layers.9.conv1x3_1.weight', 'module.encoder.layers.9.conv1x3_1.bias', 'module.encoder.layers.9.conv3x1_2.weight', 'module.encoder.layers.9.conv3x1_2.bias', 'module.encoder.layers.9.conv1x3_2.weight', 'module.encoder.layers.9.conv1x3_2.bias', 'module.encoder.layers.9.bn1.weight', 'module.encoder.layers.9.bn1.bias', 'module.encoder.layers.9.bn1.running_mean', 'module.encoder.layers.9.bn1.running_var', 'module.encoder.layers.9.bn2.weight', 'module.encoder.layers.9.bn2.bias', 'module.encoder.layers.9.bn2.running_mean', 'module.encoder.layers.9.bn2.running_var', 'module.encoder.layers.10.conv3x1_1.weight', 'module.encoder.layers.10.conv3x1_1.bias', 'module.encoder.layers.10.conv1x3_1.weight', 'module.encoder.layers.10.conv1x3_1.bias', 'module.encoder.layers.10.conv3x1_2.weight', 'module.encoder.layers.10.conv3x1_2.bias', 'module.encoder.layers.10.conv1x3_2.weight', 'module.encoder.layers.10.conv1x3_2.bias', 'module.encoder.layers.10.bn1.weight', 'module.encoder.layers.10.bn1.bias', 'module.encoder.layers.10.bn1.running_mean', 'module.encoder.layers.10.bn1.running_var', 'module.encoder.layers.10.bn2.weight', 'module.encoder.layers.10.bn2.bias', 'module.encoder.layers.10.bn2.running_mean', 'module.encoder.layers.10.bn2.running_var', 'module.encoder.layers.11.conv3x1_1.weight', 'module.encoder.layers.11.conv3x1_1.bias', 'module.encoder.layers.11.conv1x3_1.weight', 'module.encoder.layers.11.conv1x3_1.bias', 'module.encoder.layers.11.conv3x1_2.weight', 'module.encoder.layers.11.conv3x1_2.bias', 'module.encoder.layers.11.conv1x3_2.weight', 'module.encoder.layers.11.conv1x3_2.bias', 'module.encoder.layers.11.bn1.weight', 'module.encoder.layers.11.bn1.bias', 'module.encoder.layers.11.bn1.running_mean', 'module.encoder.layers.11.bn1.running_var', 'module.encoder.layers.11.bn2.weight', 'module.encoder.layers.11.bn2.bias', 'module.encoder.layers.11.bn2.running_mean', 'module.encoder.layers.11.bn2.running_var', 'module.encoder.layers.12.conv3x1_1.weight', 'module.encoder.layers.12.conv3x1_1.bias', 'module.encoder.layers.12.conv1x3_1.weight', 'module.encoder.layers.12.conv1x3_1.bias', 'module.encoder.layers.12.conv3x1_2.weight', 'module.encoder.layers.12.conv3x1_2.bias', 'module.encoder.layers.12.conv1x3_2.weight', 'module.encoder.layers.12.conv1x3_2.bias', 'module.encoder.layers.12.bn1.weight', 'module.encoder.layers.12.bn1.bias', 'module.encoder.layers.12.bn1.running_mean', 'module.encoder.layers.12.bn1.running_var', 'module.encoder.layers.12.bn2.weight', 'module.encoder.layers.12.bn2.bias', 'module.encoder.layers.12.bn2.running_mean', 'module.encoder.layers.12.bn2.running_var', 'module.encoder.layers.13.conv3x1_1.weight', 'module.encoder.layers.13.conv3x1_1.bias', 'module.encoder.layers.13.conv1x3_1.weight', 'module.encoder.layers.13.conv1x3_1.bias', 'module.encoder.layers.13.conv3x1_2.weight', 'module.encoder.layers.13.conv3x1_2.bias', 'module.encoder.layers.13.conv1x3_2.weight', 'module.encoder.layers.13.conv1x3_2.bias', 'module.encoder.layers.13.bn1.weight', 'module.encoder.layers.13.bn1.bias', 'module.encoder.layers.13.bn1.running_mean', 'module.encoder.layers.13.bn1.running_var', 'module.encoder.layers.13.bn2.weight', 'module.encoder.layers.13.bn2.bias', 'module.encoder.layers.13.bn2.running_mean', 'module.encoder.layers.13.bn2.running_var', 'module.encoder.layers.14.conv3x1_1.weight', 'module.encoder.layers.14.conv3x1_1.bias', 'module.encoder.layers.14.conv1x3_1.weight', 'module.encoder.layers.14.conv1x3_1.bias', 'module.encoder.layers.14.conv3x1_2.weight', 'module.encoder.layers.14.conv3x1_2.bias', 'module.encoder.layers.14.conv1x3_2.weight', 'module.encoder.layers.14.conv1x3_2.bias', 'module.encoder.layers.14.bn1.weight', 'module.encoder.layers.14.bn1.bias', 'module.encoder.layers.14.bn1.running_mean', 'module.encoder.layers.14.bn1.running_var', 'module.encoder.layers.14.bn2.weight', 'module.encoder.layers.14.bn2.bias', 'module.encoder.layers.14.bn2.running_mean', 'module.encoder.layers.14.bn2.running_var', 'module.decoder.layers.0.conv.weight', 'module.decoder.layers.0.conv.bias', 'module.decoder.layers.0.bn.weight', 'module.decoder.layers.0.bn.bias', 'module.decoder.layers.0.bn.running_mean', 'module.decoder.layers.0.bn.running_var', 'module.decoder.layers.1.conv3x1_1.weight', 'module.decoder.layers.1.conv3x1_1.bias', 'module.decoder.layers.1.conv1x3_1.weight', 'module.decoder.layers.1.conv1x3_1.bias', 'module.decoder.layers.1.bn1.weight', 'module.decoder.layers.1.bn1.bias', 'module.decoder.layers.1.bn1.running_mean', 'module.decoder.layers.1.bn1.running_var', 'module.decoder.layers.1.conv3x1_2.weight', 'module.decoder.layers.1.conv3x1_2.bias', 'module.decoder.layers.1.conv1x3_2.weight', 'module.decoder.layers.1.conv1x3_2.bias', 'module.decoder.layers.1.bn2.weight', 'module.decoder.layers.1.bn2.bias', 'module.decoder.layers.1.bn2.running_mean', 'module.decoder.layers.1.bn2.running_var', 'module.decoder.layers.2.conv3x1_1.weight', 'module.decoder.layers.2.conv3x1_1.bias', 'module.decoder.layers.2.conv1x3_1.weight', 'module.decoder.layers.2.conv1x3_1.bias', 'module.decoder.layers.2.bn1.weight', 'module.decoder.layers.2.bn1.bias', 'module.decoder.layers.2.bn1.running_mean', 'module.decoder.layers.2.bn1.running_var', 'module.decoder.layers.2.conv3x1_2.weight', 'module.decoder.layers.2.conv3x1_2.bias', 'module.decoder.layers.2.conv1x3_2.weight', 'module.decoder.layers.2.conv1x3_2.bias', 'module.decoder.layers.2.bn2.weight', 'module.decoder.layers.2.bn2.bias', 'module.decoder.layers.2.bn2.running_mean', 'module.decoder.layers.2.bn2.running_var', 'module.decoder.layers.3.conv.weight', 'module.decoder.layers.3.conv.bias', 'module.decoder.layers.3.bn.weight', 'module.decoder.layers.3.bn.bias', 'module.decoder.layers.3.bn.running_mean', 'module.decoder.layers.3.bn.running_var', 'module.decoder.layers.4.conv3x1_1.weight', 'module.decoder.layers.4.conv3x1_1.bias', 'module.decoder.layers.4.conv1x3_1.weight', 'module.decoder.layers.4.conv1x3_1.bias', 'module.decoder.layers.4.bn1.weight', 'module.decoder.layers.4.bn1.bias', 'module.decoder.layers.4.bn1.running_mean', 'module.decoder.layers.4.bn1.running_var', 'module.decoder.layers.4.conv3x1_2.weight', 'module.decoder.layers.4.conv3x1_2.bias', 'module.decoder.layers.4.conv1x3_2.weight', 'module.decoder.layers.4.conv1x3_2.bias', 'module.decoder.layers.4.bn2.weight', 'module.decoder.layers.4.bn2.bias', 'module.decoder.layers.4.bn2.running_mean', 'module.decoder.layers.4.bn2.running_var', 'module.decoder.layers.5.conv3x1_1.weight', 'module.decoder.layers.5.conv3x1_1.bias', 'module.decoder.layers.5.conv1x3_1.weight', 'module.decoder.layers.5.conv1x3_1.bias', 'module.decoder.layers.5.bn1.weight', 'module.decoder.layers.5.bn1.bias', 'module.decoder.layers.5.bn1.running_mean', 'module.decoder.layers.5.bn1.running_var', 'module.decoder.layers.5.conv3x1_2.weight', 'module.decoder.layers.5.conv3x1_2.bias', 'module.decoder.layers.5.conv1x3_2.weight', 'module.decoder.layers.5.conv1x3_2.bias', 'module.decoder.layers.5.bn2.weight', 'module.decoder.layers.5.bn2.bias', 'module.decoder.layers.5.bn2.running_mean', 'module.decoder.layers.5.bn2.running_var', 'module.decoder.output_conv.weight', 'module.decoder.output_conv.bias'])\n",
            "odict_keys(['encoder.initial_block.conv.weight', 'encoder.initial_block.conv.bias', 'encoder.initial_block.bn.weight', 'encoder.initial_block.bn.bias', 'encoder.initial_block.bn.running_mean', 'encoder.initial_block.bn.running_var', 'encoder.initial_block.bn.num_batches_tracked', 'encoder.layers.0.conv.weight', 'encoder.layers.0.conv.bias', 'encoder.layers.0.bn.weight', 'encoder.layers.0.bn.bias', 'encoder.layers.0.bn.running_mean', 'encoder.layers.0.bn.running_var', 'encoder.layers.0.bn.num_batches_tracked', 'encoder.layers.1.conv3x1_1.weight', 'encoder.layers.1.conv3x1_1.bias', 'encoder.layers.1.conv1x3_1.weight', 'encoder.layers.1.conv1x3_1.bias', 'encoder.layers.1.bn1.weight', 'encoder.layers.1.bn1.bias', 'encoder.layers.1.bn1.running_mean', 'encoder.layers.1.bn1.running_var', 'encoder.layers.1.bn1.num_batches_tracked', 'encoder.layers.1.conv3x1_2.weight', 'encoder.layers.1.conv3x1_2.bias', 'encoder.layers.1.conv1x3_2.weight', 'encoder.layers.1.conv1x3_2.bias', 'encoder.layers.1.bn2.weight', 'encoder.layers.1.bn2.bias', 'encoder.layers.1.bn2.running_mean', 'encoder.layers.1.bn2.running_var', 'encoder.layers.1.bn2.num_batches_tracked', 'encoder.layers.2.conv3x1_1.weight', 'encoder.layers.2.conv3x1_1.bias', 'encoder.layers.2.conv1x3_1.weight', 'encoder.layers.2.conv1x3_1.bias', 'encoder.layers.2.bn1.weight', 'encoder.layers.2.bn1.bias', 'encoder.layers.2.bn1.running_mean', 'encoder.layers.2.bn1.running_var', 'encoder.layers.2.bn1.num_batches_tracked', 'encoder.layers.2.conv3x1_2.weight', 'encoder.layers.2.conv3x1_2.bias', 'encoder.layers.2.conv1x3_2.weight', 'encoder.layers.2.conv1x3_2.bias', 'encoder.layers.2.bn2.weight', 'encoder.layers.2.bn2.bias', 'encoder.layers.2.bn2.running_mean', 'encoder.layers.2.bn2.running_var', 'encoder.layers.2.bn2.num_batches_tracked', 'encoder.layers.3.conv3x1_1.weight', 'encoder.layers.3.conv3x1_1.bias', 'encoder.layers.3.conv1x3_1.weight', 'encoder.layers.3.conv1x3_1.bias', 'encoder.layers.3.bn1.weight', 'encoder.layers.3.bn1.bias', 'encoder.layers.3.bn1.running_mean', 'encoder.layers.3.bn1.running_var', 'encoder.layers.3.bn1.num_batches_tracked', 'encoder.layers.3.conv3x1_2.weight', 'encoder.layers.3.conv3x1_2.bias', 'encoder.layers.3.conv1x3_2.weight', 'encoder.layers.3.conv1x3_2.bias', 'encoder.layers.3.bn2.weight', 'encoder.layers.3.bn2.bias', 'encoder.layers.3.bn2.running_mean', 'encoder.layers.3.bn2.running_var', 'encoder.layers.3.bn2.num_batches_tracked', 'encoder.layers.4.conv3x1_1.weight', 'encoder.layers.4.conv3x1_1.bias', 'encoder.layers.4.conv1x3_1.weight', 'encoder.layers.4.conv1x3_1.bias', 'encoder.layers.4.bn1.weight', 'encoder.layers.4.bn1.bias', 'encoder.layers.4.bn1.running_mean', 'encoder.layers.4.bn1.running_var', 'encoder.layers.4.bn1.num_batches_tracked', 'encoder.layers.4.conv3x1_2.weight', 'encoder.layers.4.conv3x1_2.bias', 'encoder.layers.4.conv1x3_2.weight', 'encoder.layers.4.conv1x3_2.bias', 'encoder.layers.4.bn2.weight', 'encoder.layers.4.bn2.bias', 'encoder.layers.4.bn2.running_mean', 'encoder.layers.4.bn2.running_var', 'encoder.layers.4.bn2.num_batches_tracked', 'encoder.layers.5.conv3x1_1.weight', 'encoder.layers.5.conv3x1_1.bias', 'encoder.layers.5.conv1x3_1.weight', 'encoder.layers.5.conv1x3_1.bias', 'encoder.layers.5.bn1.weight', 'encoder.layers.5.bn1.bias', 'encoder.layers.5.bn1.running_mean', 'encoder.layers.5.bn1.running_var', 'encoder.layers.5.bn1.num_batches_tracked', 'encoder.layers.5.conv3x1_2.weight', 'encoder.layers.5.conv3x1_2.bias', 'encoder.layers.5.conv1x3_2.weight', 'encoder.layers.5.conv1x3_2.bias', 'encoder.layers.5.bn2.weight', 'encoder.layers.5.bn2.bias', 'encoder.layers.5.bn2.running_mean', 'encoder.layers.5.bn2.running_var', 'encoder.layers.5.bn2.num_batches_tracked', 'encoder.layers.6.conv.weight', 'encoder.layers.6.conv.bias', 'encoder.layers.6.bn.weight', 'encoder.layers.6.bn.bias', 'encoder.layers.6.bn.running_mean', 'encoder.layers.6.bn.running_var', 'encoder.layers.6.bn.num_batches_tracked', 'encoder.layers.7.conv3x1_1.weight', 'encoder.layers.7.conv3x1_1.bias', 'encoder.layers.7.conv1x3_1.weight', 'encoder.layers.7.conv1x3_1.bias', 'encoder.layers.7.bn1.weight', 'encoder.layers.7.bn1.bias', 'encoder.layers.7.bn1.running_mean', 'encoder.layers.7.bn1.running_var', 'encoder.layers.7.bn1.num_batches_tracked', 'encoder.layers.7.conv3x1_2.weight', 'encoder.layers.7.conv3x1_2.bias', 'encoder.layers.7.conv1x3_2.weight', 'encoder.layers.7.conv1x3_2.bias', 'encoder.layers.7.bn2.weight', 'encoder.layers.7.bn2.bias', 'encoder.layers.7.bn2.running_mean', 'encoder.layers.7.bn2.running_var', 'encoder.layers.7.bn2.num_batches_tracked', 'encoder.layers.8.conv3x1_1.weight', 'encoder.layers.8.conv3x1_1.bias', 'encoder.layers.8.conv1x3_1.weight', 'encoder.layers.8.conv1x3_1.bias', 'encoder.layers.8.bn1.weight', 'encoder.layers.8.bn1.bias', 'encoder.layers.8.bn1.running_mean', 'encoder.layers.8.bn1.running_var', 'encoder.layers.8.bn1.num_batches_tracked', 'encoder.layers.8.conv3x1_2.weight', 'encoder.layers.8.conv3x1_2.bias', 'encoder.layers.8.conv1x3_2.weight', 'encoder.layers.8.conv1x3_2.bias', 'encoder.layers.8.bn2.weight', 'encoder.layers.8.bn2.bias', 'encoder.layers.8.bn2.running_mean', 'encoder.layers.8.bn2.running_var', 'encoder.layers.8.bn2.num_batches_tracked', 'encoder.layers.9.conv3x1_1.weight', 'encoder.layers.9.conv3x1_1.bias', 'encoder.layers.9.conv1x3_1.weight', 'encoder.layers.9.conv1x3_1.bias', 'encoder.layers.9.bn1.weight', 'encoder.layers.9.bn1.bias', 'encoder.layers.9.bn1.running_mean', 'encoder.layers.9.bn1.running_var', 'encoder.layers.9.bn1.num_batches_tracked', 'encoder.layers.9.conv3x1_2.weight', 'encoder.layers.9.conv3x1_2.bias', 'encoder.layers.9.conv1x3_2.weight', 'encoder.layers.9.conv1x3_2.bias', 'encoder.layers.9.bn2.weight', 'encoder.layers.9.bn2.bias', 'encoder.layers.9.bn2.running_mean', 'encoder.layers.9.bn2.running_var', 'encoder.layers.9.bn2.num_batches_tracked', 'encoder.layers.10.conv3x1_1.weight', 'encoder.layers.10.conv3x1_1.bias', 'encoder.layers.10.conv1x3_1.weight', 'encoder.layers.10.conv1x3_1.bias', 'encoder.layers.10.bn1.weight', 'encoder.layers.10.bn1.bias', 'encoder.layers.10.bn1.running_mean', 'encoder.layers.10.bn1.running_var', 'encoder.layers.10.bn1.num_batches_tracked', 'encoder.layers.10.conv3x1_2.weight', 'encoder.layers.10.conv3x1_2.bias', 'encoder.layers.10.conv1x3_2.weight', 'encoder.layers.10.conv1x3_2.bias', 'encoder.layers.10.bn2.weight', 'encoder.layers.10.bn2.bias', 'encoder.layers.10.bn2.running_mean', 'encoder.layers.10.bn2.running_var', 'encoder.layers.10.bn2.num_batches_tracked', 'encoder.layers.11.conv3x1_1.weight', 'encoder.layers.11.conv3x1_1.bias', 'encoder.layers.11.conv1x3_1.weight', 'encoder.layers.11.conv1x3_1.bias', 'encoder.layers.11.bn1.weight', 'encoder.layers.11.bn1.bias', 'encoder.layers.11.bn1.running_mean', 'encoder.layers.11.bn1.running_var', 'encoder.layers.11.bn1.num_batches_tracked', 'encoder.layers.11.conv3x1_2.weight', 'encoder.layers.11.conv3x1_2.bias', 'encoder.layers.11.conv1x3_2.weight', 'encoder.layers.11.conv1x3_2.bias', 'encoder.layers.11.bn2.weight', 'encoder.layers.11.bn2.bias', 'encoder.layers.11.bn2.running_mean', 'encoder.layers.11.bn2.running_var', 'encoder.layers.11.bn2.num_batches_tracked', 'encoder.layers.12.conv3x1_1.weight', 'encoder.layers.12.conv3x1_1.bias', 'encoder.layers.12.conv1x3_1.weight', 'encoder.layers.12.conv1x3_1.bias', 'encoder.layers.12.bn1.weight', 'encoder.layers.12.bn1.bias', 'encoder.layers.12.bn1.running_mean', 'encoder.layers.12.bn1.running_var', 'encoder.layers.12.bn1.num_batches_tracked', 'encoder.layers.12.conv3x1_2.weight', 'encoder.layers.12.conv3x1_2.bias', 'encoder.layers.12.conv1x3_2.weight', 'encoder.layers.12.conv1x3_2.bias', 'encoder.layers.12.bn2.weight', 'encoder.layers.12.bn2.bias', 'encoder.layers.12.bn2.running_mean', 'encoder.layers.12.bn2.running_var', 'encoder.layers.12.bn2.num_batches_tracked', 'encoder.layers.13.conv3x1_1.weight', 'encoder.layers.13.conv3x1_1.bias', 'encoder.layers.13.conv1x3_1.weight', 'encoder.layers.13.conv1x3_1.bias', 'encoder.layers.13.bn1.weight', 'encoder.layers.13.bn1.bias', 'encoder.layers.13.bn1.running_mean', 'encoder.layers.13.bn1.running_var', 'encoder.layers.13.bn1.num_batches_tracked', 'encoder.layers.13.conv3x1_2.weight', 'encoder.layers.13.conv3x1_2.bias', 'encoder.layers.13.conv1x3_2.weight', 'encoder.layers.13.conv1x3_2.bias', 'encoder.layers.13.bn2.weight', 'encoder.layers.13.bn2.bias', 'encoder.layers.13.bn2.running_mean', 'encoder.layers.13.bn2.running_var', 'encoder.layers.13.bn2.num_batches_tracked', 'encoder.layers.14.conv3x1_1.weight', 'encoder.layers.14.conv3x1_1.bias', 'encoder.layers.14.conv1x3_1.weight', 'encoder.layers.14.conv1x3_1.bias', 'encoder.layers.14.bn1.weight', 'encoder.layers.14.bn1.bias', 'encoder.layers.14.bn1.running_mean', 'encoder.layers.14.bn1.running_var', 'encoder.layers.14.bn1.num_batches_tracked', 'encoder.layers.14.conv3x1_2.weight', 'encoder.layers.14.conv3x1_2.bias', 'encoder.layers.14.conv1x3_2.weight', 'encoder.layers.14.conv1x3_2.bias', 'encoder.layers.14.bn2.weight', 'encoder.layers.14.bn2.bias', 'encoder.layers.14.bn2.running_mean', 'encoder.layers.14.bn2.running_var', 'encoder.layers.14.bn2.num_batches_tracked', 'encoder.output_conv.weight', 'encoder.output_conv.bias', 'decoder.layers.0.conv.weight', 'decoder.layers.0.conv.bias', 'decoder.layers.0.bn.weight', 'decoder.layers.0.bn.bias', 'decoder.layers.0.bn.running_mean', 'decoder.layers.0.bn.running_var', 'decoder.layers.0.bn.num_batches_tracked', 'decoder.layers.1.conv3x1_1.weight', 'decoder.layers.1.conv3x1_1.bias', 'decoder.layers.1.conv1x3_1.weight', 'decoder.layers.1.conv1x3_1.bias', 'decoder.layers.1.bn1.weight', 'decoder.layers.1.bn1.bias', 'decoder.layers.1.bn1.running_mean', 'decoder.layers.1.bn1.running_var', 'decoder.layers.1.bn1.num_batches_tracked', 'decoder.layers.1.conv3x1_2.weight', 'decoder.layers.1.conv3x1_2.bias', 'decoder.layers.1.conv1x3_2.weight', 'decoder.layers.1.conv1x3_2.bias', 'decoder.layers.1.bn2.weight', 'decoder.layers.1.bn2.bias', 'decoder.layers.1.bn2.running_mean', 'decoder.layers.1.bn2.running_var', 'decoder.layers.1.bn2.num_batches_tracked', 'decoder.layers.2.conv3x1_1.weight', 'decoder.layers.2.conv3x1_1.bias', 'decoder.layers.2.conv1x3_1.weight', 'decoder.layers.2.conv1x3_1.bias', 'decoder.layers.2.bn1.weight', 'decoder.layers.2.bn1.bias', 'decoder.layers.2.bn1.running_mean', 'decoder.layers.2.bn1.running_var', 'decoder.layers.2.bn1.num_batches_tracked', 'decoder.layers.2.conv3x1_2.weight', 'decoder.layers.2.conv3x1_2.bias', 'decoder.layers.2.conv1x3_2.weight', 'decoder.layers.2.conv1x3_2.bias', 'decoder.layers.2.bn2.weight', 'decoder.layers.2.bn2.bias', 'decoder.layers.2.bn2.running_mean', 'decoder.layers.2.bn2.running_var', 'decoder.layers.2.bn2.num_batches_tracked', 'decoder.layers.3.conv.weight', 'decoder.layers.3.conv.bias', 'decoder.layers.3.bn.weight', 'decoder.layers.3.bn.bias', 'decoder.layers.3.bn.running_mean', 'decoder.layers.3.bn.running_var', 'decoder.layers.3.bn.num_batches_tracked', 'decoder.layers.4.conv3x1_1.weight', 'decoder.layers.4.conv3x1_1.bias', 'decoder.layers.4.conv1x3_1.weight', 'decoder.layers.4.conv1x3_1.bias', 'decoder.layers.4.bn1.weight', 'decoder.layers.4.bn1.bias', 'decoder.layers.4.bn1.running_mean', 'decoder.layers.4.bn1.running_var', 'decoder.layers.4.bn1.num_batches_tracked', 'decoder.layers.4.conv3x1_2.weight', 'decoder.layers.4.conv3x1_2.bias', 'decoder.layers.4.conv1x3_2.weight', 'decoder.layers.4.conv1x3_2.bias', 'decoder.layers.4.bn2.weight', 'decoder.layers.4.bn2.bias', 'decoder.layers.4.bn2.running_mean', 'decoder.layers.4.bn2.running_var', 'decoder.layers.4.bn2.num_batches_tracked', 'decoder.layers.5.conv3x1_1.weight', 'decoder.layers.5.conv3x1_1.bias', 'decoder.layers.5.conv1x3_1.weight', 'decoder.layers.5.conv1x3_1.bias', 'decoder.layers.5.bn1.weight', 'decoder.layers.5.bn1.bias', 'decoder.layers.5.bn1.running_mean', 'decoder.layers.5.bn1.running_var', 'decoder.layers.5.bn1.num_batches_tracked', 'decoder.layers.5.conv3x1_2.weight', 'decoder.layers.5.conv3x1_2.bias', 'decoder.layers.5.conv1x3_2.weight', 'decoder.layers.5.conv1x3_2.bias', 'decoder.layers.5.bn2.weight', 'decoder.layers.5.bn2.bias', 'decoder.layers.5.bn2.running_mean', 'decoder.layers.5.bn2.running_var', 'decoder.layers.5.bn2.num_batches_tracked', 'decoder.output_conv.weight', 'decoder.output_conv.bias'])\n",
            "Import Model erfnet with weights erfnet_pretrained.pth to FineTune\n",
            "========== TRAINING ===========\n",
            "/content/cityscapes/leftImg8bit/train\n",
            "/content/cityscapes/leftImg8bit/val\n",
            "----- TRAINING - EPOCH 1 -----\n",
            "LEARNING RATE:  5e-05\n",
            "loss: 0.8898 (epoch: 1, step: 0) // Avg time/img: 0.3255 s\n",
            "loss: 1.234 (epoch: 1, step: 50) // Avg time/img: 0.0495 s\n",
            "loss: 1.208 (epoch: 1, step: 100) // Avg time/img: 0.0470 s\n",
            "loss: 1.209 (epoch: 1, step: 150) // Avg time/img: 0.0462 s\n",
            "loss: 1.19 (epoch: 1, step: 200) // Avg time/img: 0.0459 s\n",
            "loss: 1.188 (epoch: 1, step: 250) // Avg time/img: 0.0458 s\n",
            "loss: 1.185 (epoch: 1, step: 300) // Avg time/img: 0.0456 s\n",
            "loss: 1.169 (epoch: 1, step: 350) // Avg time/img: 0.0456 s\n",
            "loss: 1.169 (epoch: 1, step: 400) // Avg time/img: 0.0455 s\n",
            "loss: 1.16 (epoch: 1, step: 450) // Avg time/img: 0.0455 s\n",
            "----- VALIDATING - EPOCH 1 -----\n",
            "VAL loss: 1.152 (epoch: 1, step: 0) // Avg time/img: 0.0486 s\n",
            "VAL loss: 1.342 (epoch: 1, step: 50) // Avg time/img: 0.0338 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m72.19\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/erfnet_training_focal_loss/model-001.pth (epoch: 1)\n",
            "save: ../save/erfnet_training_focal_loss/model_best.pth (epoch: 1)\n",
            "----- TRAINING - EPOCH 2 -----\n",
            "LEARNING RATE:  4.774426908107499e-05\n",
            "loss: 1.701 (epoch: 2, step: 0) // Avg time/img: 0.0505 s\n",
            "loss: 1.016 (epoch: 2, step: 50) // Avg time/img: 0.0459 s\n",
            "loss: 1.004 (epoch: 2, step: 100) // Avg time/img: 0.0458 s\n",
            "loss: 1.011 (epoch: 2, step: 150) // Avg time/img: 0.0456 s\n",
            "loss: 1.006 (epoch: 2, step: 200) // Avg time/img: 0.0458 s\n",
            "loss: 0.9969 (epoch: 2, step: 250) // Avg time/img: 0.0459 s\n",
            "loss: 0.9928 (epoch: 2, step: 300) // Avg time/img: 0.0459 s\n",
            "loss: 0.9828 (epoch: 2, step: 350) // Avg time/img: 0.0459 s\n",
            "loss: 0.9711 (epoch: 2, step: 400) // Avg time/img: 0.0460 s\n",
            "loss: 0.9636 (epoch: 2, step: 450) // Avg time/img: 0.0461 s\n",
            "----- VALIDATING - EPOCH 2 -----\n",
            "VAL loss: 0.9424 (epoch: 2, step: 0) // Avg time/img: 0.0463 s\n",
            "VAL loss: 1.113 (epoch: 2, step: 50) // Avg time/img: 0.0341 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.81\u001b[0m %\n",
            "save: ../save/erfnet_training_focal_loss/model-002.pth (epoch: 2)\n",
            "----- TRAINING - EPOCH 3 -----\n",
            "LEARNING RATE:  4.547662880414811e-05\n",
            "loss: 0.7212 (epoch: 3, step: 0) // Avg time/img: 0.0504 s\n",
            "loss: 0.8354 (epoch: 3, step: 50) // Avg time/img: 0.0469 s\n",
            "loss: 0.8059 (epoch: 3, step: 100) // Avg time/img: 0.0468 s\n",
            "loss: 0.8033 (epoch: 3, step: 150) // Avg time/img: 0.0471 s\n",
            "loss: 0.8028 (epoch: 3, step: 200) // Avg time/img: 0.0470 s\n",
            "loss: 0.8009 (epoch: 3, step: 250) // Avg time/img: 0.0474 s\n",
            "loss: 0.7937 (epoch: 3, step: 300) // Avg time/img: 0.0477 s\n",
            "loss: 0.7847 (epoch: 3, step: 350) // Avg time/img: 0.0479 s\n",
            "loss: 0.7761 (epoch: 3, step: 400) // Avg time/img: 0.0480 s\n",
            "loss: 0.7713 (epoch: 3, step: 450) // Avg time/img: 0.0479 s\n",
            "----- VALIDATING - EPOCH 3 -----\n",
            "VAL loss: 0.774 (epoch: 3, step: 0) // Avg time/img: 0.0505 s\n",
            "VAL loss: 0.9109 (epoch: 3, step: 50) // Avg time/img: 0.0351 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m72.20\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/erfnet_training_focal_loss/model-003.pth (epoch: 3)\n",
            "save: ../save/erfnet_training_focal_loss/model_best.pth (epoch: 3)\n",
            "----- TRAINING - EPOCH 4 -----\n",
            "LEARNING RATE:  4.319634861514096e-05\n",
            "loss: 0.616 (epoch: 4, step: 0) // Avg time/img: 0.0644 s\n",
            "loss: 0.6356 (epoch: 4, step: 50) // Avg time/img: 0.0465 s\n",
            "loss: 0.6477 (epoch: 4, step: 100) // Avg time/img: 0.0469 s\n",
            "loss: 0.6538 (epoch: 4, step: 150) // Avg time/img: 0.0468 s\n",
            "loss: 0.6589 (epoch: 4, step: 200) // Avg time/img: 0.0471 s\n",
            "loss: 0.6507 (epoch: 4, step: 250) // Avg time/img: 0.0470 s\n",
            "loss: 0.6461 (epoch: 4, step: 300) // Avg time/img: 0.0471 s\n",
            "loss: 0.6281 (epoch: 4, step: 350) // Avg time/img: 0.0470 s\n",
            "loss: 0.6199 (epoch: 4, step: 400) // Avg time/img: 0.0472 s\n",
            "loss: 0.6182 (epoch: 4, step: 450) // Avg time/img: 0.0471 s\n",
            "----- VALIDATING - EPOCH 4 -----\n",
            "VAL loss: 0.6228 (epoch: 4, step: 0) // Avg time/img: 0.0465 s\n",
            "VAL loss: 0.745 (epoch: 4, step: 50) // Avg time/img: 0.0344 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.65\u001b[0m %\n",
            "save: ../save/erfnet_training_focal_loss/model-004.pth (epoch: 4)\n",
            "----- TRAINING - EPOCH 5 -----\n",
            "LEARNING RATE:  4.090260730254292e-05\n",
            "loss: 0.4657 (epoch: 5, step: 0) // Avg time/img: 0.0532 s\n",
            "loss: 0.5276 (epoch: 5, step: 50) // Avg time/img: 0.0472 s\n",
            "loss: 0.5289 (epoch: 5, step: 100) // Avg time/img: 0.0475 s\n",
            "loss: 0.5409 (epoch: 5, step: 150) // Avg time/img: 0.0473 s\n",
            "loss: 0.5328 (epoch: 5, step: 200) // Avg time/img: 0.0472 s\n",
            "loss: 0.5351 (epoch: 5, step: 250) // Avg time/img: 0.0470 s\n",
            "loss: 0.5301 (epoch: 5, step: 300) // Avg time/img: 0.0470 s\n",
            "loss: 0.5257 (epoch: 5, step: 350) // Avg time/img: 0.0469 s\n",
            "loss: 0.5196 (epoch: 5, step: 400) // Avg time/img: 0.0470 s\n",
            "loss: 0.519 (epoch: 5, step: 450) // Avg time/img: 0.0470 s\n",
            "----- VALIDATING - EPOCH 5 -----\n",
            "VAL loss: 0.5539 (epoch: 5, step: 0) // Avg time/img: 0.0493 s\n",
            "VAL loss: 0.665 (epoch: 5, step: 50) // Avg time/img: 0.0347 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m69.91\u001b[0m %\n",
            "save: ../save/erfnet_training_focal_loss/model-005.pth (epoch: 5)\n",
            "----- TRAINING - EPOCH 6 -----\n",
            "LEARNING RATE:  3.859447533617852e-05\n",
            "loss: 0.4165 (epoch: 6, step: 0) // Avg time/img: 0.0539 s\n",
            "loss: 0.5141 (epoch: 6, step: 50) // Avg time/img: 0.0467 s\n",
            "loss: 0.5019 (epoch: 6, step: 100) // Avg time/img: 0.0470 s\n",
            "loss: 0.4899 (epoch: 6, step: 150) // Avg time/img: 0.0466 s\n",
            "loss: 0.4777 (epoch: 6, step: 200) // Avg time/img: 0.0468 s\n",
            "loss: 0.4737 (epoch: 6, step: 250) // Avg time/img: 0.0467 s\n",
            "loss: 0.4698 (epoch: 6, step: 300) // Avg time/img: 0.0468 s\n",
            "loss: 0.4661 (epoch: 6, step: 350) // Avg time/img: 0.0470 s\n",
            "loss: 0.4636 (epoch: 6, step: 400) // Avg time/img: 0.0470 s\n",
            "loss: 0.4594 (epoch: 6, step: 450) // Avg time/img: 0.0473 s\n",
            "----- VALIDATING - EPOCH 6 -----\n",
            "VAL loss: 0.5128 (epoch: 6, step: 0) // Avg time/img: 0.0502 s\n",
            "VAL loss: 0.6238 (epoch: 6, step: 50) // Avg time/img: 0.0356 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m69.07\u001b[0m %\n",
            "save: ../save/erfnet_training_focal_loss/model-006.pth (epoch: 6)\n",
            "----- TRAINING - EPOCH 7 -----\n",
            "LEARNING RATE:  3.6270892346861e-05\n",
            "loss: 0.3477 (epoch: 7, step: 0) // Avg time/img: 0.0535 s\n",
            "loss: 0.4589 (epoch: 7, step: 50) // Avg time/img: 0.0467 s\n",
            "loss: 0.4558 (epoch: 7, step: 100) // Avg time/img: 0.0473 s\n",
            "loss: 0.4494 (epoch: 7, step: 150) // Avg time/img: 0.0472 s\n",
            "loss: 0.4431 (epoch: 7, step: 200) // Avg time/img: 0.0471 s\n",
            "loss: 0.4403 (epoch: 7, step: 250) // Avg time/img: 0.0471 s\n",
            "loss: 0.4344 (epoch: 7, step: 300) // Avg time/img: 0.0472 s\n",
            "loss: 0.4297 (epoch: 7, step: 350) // Avg time/img: 0.0472 s\n",
            "loss: 0.4278 (epoch: 7, step: 400) // Avg time/img: 0.0473 s\n",
            "loss: 0.4256 (epoch: 7, step: 450) // Avg time/img: 0.0472 s\n",
            "----- VALIDATING - EPOCH 7 -----\n",
            "VAL loss: 0.5099 (epoch: 7, step: 0) // Avg time/img: 0.0545 s\n",
            "VAL loss: 0.599 (epoch: 7, step: 50) // Avg time/img: 0.0344 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m68.56\u001b[0m %\n",
            "save: ../save/erfnet_training_focal_loss/model-007.pth (epoch: 7)\n",
            "----- TRAINING - EPOCH 8 -----\n",
            "LEARNING RATE:  3.393063796290625e-05\n",
            "loss: 0.2477 (epoch: 8, step: 0) // Avg time/img: 0.0552 s\n",
            "loss: 0.4132 (epoch: 8, step: 50) // Avg time/img: 0.0467 s\n",
            "loss: 0.4098 (epoch: 8, step: 100) // Avg time/img: 0.0464 s\n",
            "loss: 0.4061 (epoch: 8, step: 150) // Avg time/img: 0.0467 s\n",
            "loss: 0.4058 (epoch: 8, step: 200) // Avg time/img: 0.0467 s\n",
            "loss: 0.409 (epoch: 8, step: 250) // Avg time/img: 0.0468 s\n",
            "loss: 0.409 (epoch: 8, step: 300) // Avg time/img: 0.0467 s\n",
            "loss: 0.407 (epoch: 8, step: 350) // Avg time/img: 0.0468 s\n",
            "loss: 0.4064 (epoch: 8, step: 400) // Avg time/img: 0.0468 s\n",
            "loss: 0.4037 (epoch: 8, step: 450) // Avg time/img: 0.0469 s\n",
            "----- VALIDATING - EPOCH 8 -----\n",
            "VAL loss: 0.4708 (epoch: 8, step: 0) // Avg time/img: 0.0447 s\n",
            "VAL loss: 0.5627 (epoch: 8, step: 50) // Avg time/img: 0.0343 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m68.52\u001b[0m %\n",
            "save: ../save/erfnet_training_focal_loss/model-008.pth (epoch: 8)\n",
            "----- TRAINING - EPOCH 9 -----\n",
            "LEARNING RATE:  3.157229337446777e-05\n",
            "loss: 0.3547 (epoch: 9, step: 0) // Avg time/img: 0.0556 s\n",
            "loss: 0.3986 (epoch: 9, step: 50) // Avg time/img: 0.0463 s\n",
            "loss: 0.3984 (epoch: 9, step: 100) // Avg time/img: 0.0461 s\n",
            "loss: 0.3989 (epoch: 9, step: 150) // Avg time/img: 0.0464 s\n",
            "loss: 0.397 (epoch: 9, step: 200) // Avg time/img: 0.0464 s\n",
            "loss: 0.3944 (epoch: 9, step: 250) // Avg time/img: 0.0464 s\n",
            "loss: 0.3906 (epoch: 9, step: 300) // Avg time/img: 0.0465 s\n",
            "loss: 0.3895 (epoch: 9, step: 350) // Avg time/img: 0.0464 s\n",
            "loss: 0.3903 (epoch: 9, step: 400) // Avg time/img: 0.0464 s\n",
            "loss: 0.3894 (epoch: 9, step: 450) // Avg time/img: 0.0466 s\n",
            "----- VALIDATING - EPOCH 9 -----\n",
            "VAL loss: 0.4423 (epoch: 9, step: 0) // Avg time/img: 0.0446 s\n",
            "VAL loss: 0.5326 (epoch: 9, step: 50) // Avg time/img: 0.0356 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m68.82\u001b[0m %\n",
            "save: ../save/erfnet_training_focal_loss/model-009.pth (epoch: 9)\n",
            "----- TRAINING - EPOCH 10 -----\n",
            "LEARNING RATE:  2.9194189645999014e-05\n",
            "loss: 0.3467 (epoch: 10, step: 0) // Avg time/img: 0.0578 s\n",
            "loss: 0.3799 (epoch: 10, step: 50) // Avg time/img: 0.0469 s\n",
            "loss: 0.3782 (epoch: 10, step: 100) // Avg time/img: 0.0468 s\n",
            "loss: 0.3825 (epoch: 10, step: 150) // Avg time/img: 0.0471 s\n",
            "loss: 0.384 (epoch: 10, step: 200) // Avg time/img: 0.0470 s\n",
            "loss: 0.3815 (epoch: 10, step: 250) // Avg time/img: 0.0470 s\n",
            "loss: 0.3789 (epoch: 10, step: 300) // Avg time/img: 0.0469 s\n",
            "loss: 0.3749 (epoch: 10, step: 350) // Avg time/img: 0.0470 s\n",
            "loss: 0.3752 (epoch: 10, step: 400) // Avg time/img: 0.0469 s\n",
            "loss: 0.3765 (epoch: 10, step: 450) // Avg time/img: 0.0469 s\n",
            "----- VALIDATING - EPOCH 10 -----\n",
            "VAL loss: 0.4392 (epoch: 10, step: 0) // Avg time/img: 0.0485 s\n",
            "VAL loss: 0.5314 (epoch: 10, step: 50) // Avg time/img: 0.0344 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m69.09\u001b[0m %\n",
            "save: ../save/erfnet_training_focal_loss/model-010.pth (epoch: 10)\n",
            "----- TRAINING - EPOCH 11 -----\n",
            "LEARNING RATE:  2.679433656340733e-05\n",
            "loss: 0.3007 (epoch: 11, step: 0) // Avg time/img: 0.0611 s\n",
            "loss: 0.3743 (epoch: 11, step: 50) // Avg time/img: 0.0463 s\n",
            "loss: 0.3793 (epoch: 11, step: 100) // Avg time/img: 0.0469 s\n",
            "loss: 0.38 (epoch: 11, step: 150) // Avg time/img: 0.0467 s\n",
            "loss: 0.376 (epoch: 11, step: 200) // Avg time/img: 0.0467 s\n",
            "loss: 0.3711 (epoch: 11, step: 250) // Avg time/img: 0.0467 s\n",
            "loss: 0.3708 (epoch: 11, step: 300) // Avg time/img: 0.0467 s\n",
            "loss: 0.3703 (epoch: 11, step: 350) // Avg time/img: 0.0468 s\n",
            "loss: 0.3711 (epoch: 11, step: 400) // Avg time/img: 0.0468 s\n",
            "loss: 0.3705 (epoch: 11, step: 450) // Avg time/img: 0.0468 s\n",
            "----- VALIDATING - EPOCH 11 -----\n",
            "VAL loss: 0.4139 (epoch: 11, step: 0) // Avg time/img: 0.0459 s\n",
            "VAL loss: 0.5157 (epoch: 11, step: 50) // Avg time/img: 0.0344 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m69.41\u001b[0m %\n",
            "save: ../save/erfnet_training_focal_loss/model-011.pth (epoch: 11)\n",
            "----- TRAINING - EPOCH 12 -----\n",
            "LEARNING RATE:  2.437032195894977e-05\n",
            "loss: 0.316 (epoch: 12, step: 0) // Avg time/img: 0.0629 s\n",
            "loss: 0.3704 (epoch: 12, step: 50) // Avg time/img: 0.0471 s\n",
            "loss: 0.3638 (epoch: 12, step: 100) // Avg time/img: 0.0465 s\n",
            "loss: 0.3628 (epoch: 12, step: 150) // Avg time/img: 0.0466 s\n",
            "loss: 0.3653 (epoch: 12, step: 200) // Avg time/img: 0.0465 s\n",
            "loss: 0.3657 (epoch: 12, step: 250) // Avg time/img: 0.0466 s\n",
            "loss: 0.3631 (epoch: 12, step: 300) // Avg time/img: 0.0465 s\n",
            "loss: 0.3662 (epoch: 12, step: 350) // Avg time/img: 0.0466 s\n",
            "loss: 0.3667 (epoch: 12, step: 400) // Avg time/img: 0.0466 s\n",
            "loss: 0.3661 (epoch: 12, step: 450) // Avg time/img: 0.0467 s\n",
            "----- VALIDATING - EPOCH 12 -----\n",
            "VAL loss: 0.421 (epoch: 12, step: 0) // Avg time/img: 0.0510 s\n",
            "VAL loss: 0.5023 (epoch: 12, step: 50) // Avg time/img: 0.0363 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m69.33\u001b[0m %\n",
            "save: ../save/erfnet_training_focal_loss/model-012.pth (epoch: 12)\n",
            "----- TRAINING - EPOCH 13 -----\n",
            "LEARNING RATE:  2.191916452770435e-05\n",
            "loss: 0.3917 (epoch: 13, step: 0) // Avg time/img: 0.0540 s\n",
            "loss: 0.3655 (epoch: 13, step: 50) // Avg time/img: 0.0478 s\n",
            "loss: 0.3657 (epoch: 13, step: 100) // Avg time/img: 0.0476 s\n",
            "loss: 0.3627 (epoch: 13, step: 150) // Avg time/img: 0.0476 s\n",
            "loss: 0.3646 (epoch: 13, step: 200) // Avg time/img: 0.0477 s\n",
            "loss: 0.3629 (epoch: 13, step: 250) // Avg time/img: 0.0476 s\n",
            "loss: 0.3608 (epoch: 13, step: 300) // Avg time/img: 0.0475 s\n",
            "loss: 0.3609 (epoch: 13, step: 350) // Avg time/img: 0.0474 s\n",
            "loss: 0.3602 (epoch: 13, step: 400) // Avg time/img: 0.0473 s\n",
            "loss: 0.3603 (epoch: 13, step: 450) // Avg time/img: 0.0472 s\n",
            "----- VALIDATING - EPOCH 13 -----\n",
            "VAL loss: 0.4108 (epoch: 13, step: 0) // Avg time/img: 0.0499 s\n",
            "VAL loss: 0.4957 (epoch: 13, step: 50) // Avg time/img: 0.0346 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m69.34\u001b[0m %\n",
            "save: ../save/erfnet_training_focal_loss/model-013.pth (epoch: 13)\n",
            "----- TRAINING - EPOCH 14 -----\n",
            "LEARNING RATE:  1.9437089939938174e-05\n",
            "loss: 0.3779 (epoch: 14, step: 0) // Avg time/img: 0.0580 s\n",
            "loss: 0.3632 (epoch: 14, step: 50) // Avg time/img: 0.0470 s\n",
            "loss: 0.3619 (epoch: 14, step: 100) // Avg time/img: 0.0469 s\n",
            "loss: 0.367 (epoch: 14, step: 150) // Avg time/img: 0.0466 s\n",
            "loss: 0.3635 (epoch: 14, step: 200) // Avg time/img: 0.0467 s\n",
            "loss: 0.3611 (epoch: 14, step: 250) // Avg time/img: 0.0467 s\n",
            "loss: 0.3628 (epoch: 14, step: 300) // Avg time/img: 0.0467 s\n",
            "loss: 0.3616 (epoch: 14, step: 350) // Avg time/img: 0.0467 s\n",
            "loss: 0.3594 (epoch: 14, step: 400) // Avg time/img: 0.0468 s\n",
            "loss: 0.3573 (epoch: 14, step: 450) // Avg time/img: 0.0468 s\n",
            "----- VALIDATING - EPOCH 14 -----\n",
            "VAL loss: 0.4113 (epoch: 14, step: 0) // Avg time/img: 0.0494 s\n",
            "VAL loss: 0.4925 (epoch: 14, step: 50) // Avg time/img: 0.0352 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m70.05\u001b[0m %\n",
            "save: ../save/erfnet_training_focal_loss/model-014.pth (epoch: 14)\n",
            "----- TRAINING - EPOCH 15 -----\n",
            "LEARNING RATE:  1.6919173095082493e-05\n",
            "loss: 0.2982 (epoch: 15, step: 0) // Avg time/img: 0.0569 s\n",
            "loss: 0.359 (epoch: 15, step: 50) // Avg time/img: 0.0469 s\n",
            "loss: 0.3553 (epoch: 15, step: 100) // Avg time/img: 0.0467 s\n",
            "loss: 0.3651 (epoch: 15, step: 150) // Avg time/img: 0.0467 s\n",
            "loss: 0.361 (epoch: 15, step: 200) // Avg time/img: 0.0467 s\n",
            "loss: 0.3587 (epoch: 15, step: 250) // Avg time/img: 0.0467 s\n",
            "loss: 0.3584 (epoch: 15, step: 300) // Avg time/img: 0.0468 s\n",
            "loss: 0.3581 (epoch: 15, step: 350) // Avg time/img: 0.0467 s\n",
            "loss: 0.3572 (epoch: 15, step: 400) // Avg time/img: 0.0468 s\n",
            "loss: 0.3553 (epoch: 15, step: 450) // Avg time/img: 0.0467 s\n",
            "----- VALIDATING - EPOCH 15 -----\n",
            "VAL loss: 0.4225 (epoch: 15, step: 0) // Avg time/img: 0.0481 s\n",
            "VAL loss: 0.4985 (epoch: 15, step: 50) // Avg time/img: 0.0335 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m69.60\u001b[0m %\n",
            "save: ../save/erfnet_training_focal_loss/model-015.pth (epoch: 15)\n",
            "----- TRAINING - EPOCH 16 -----\n",
            "LEARNING RATE:  1.4358729437462937e-05\n",
            "loss: 0.3645 (epoch: 16, step: 0) // Avg time/img: 0.0453 s\n",
            "loss: 0.353 (epoch: 16, step: 50) // Avg time/img: 0.0473 s\n",
            "loss: 0.3509 (epoch: 16, step: 100) // Avg time/img: 0.0469 s\n",
            "loss: 0.3513 (epoch: 16, step: 150) // Avg time/img: 0.0472 s\n",
            "loss: 0.35 (epoch: 16, step: 200) // Avg time/img: 0.0473 s\n",
            "loss: 0.3487 (epoch: 16, step: 250) // Avg time/img: 0.0473 s\n",
            "loss: 0.3474 (epoch: 16, step: 300) // Avg time/img: 0.0474 s\n",
            "loss: 0.3482 (epoch: 16, step: 350) // Avg time/img: 0.0477 s\n",
            "loss: 0.3486 (epoch: 16, step: 400) // Avg time/img: 0.0478 s\n",
            "loss: 0.3499 (epoch: 16, step: 450) // Avg time/img: 0.0480 s\n",
            "----- VALIDATING - EPOCH 16 -----\n",
            "VAL loss: 0.4092 (epoch: 16, step: 0) // Avg time/img: 0.0463 s\n",
            "VAL loss: 0.4823 (epoch: 16, step: 50) // Avg time/img: 0.0348 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m69.83\u001b[0m %\n",
            "save: ../save/erfnet_training_focal_loss/model-016.pth (epoch: 16)\n",
            "----- TRAINING - EPOCH 17 -----\n",
            "LEARNING RATE:  1.1746189430880188e-05\n",
            "loss: 0.3241 (epoch: 17, step: 0) // Avg time/img: 0.0530 s\n",
            "loss: 0.3386 (epoch: 17, step: 50) // Avg time/img: 0.0475 s\n",
            "loss: 0.3379 (epoch: 17, step: 100) // Avg time/img: 0.0470 s\n",
            "loss: 0.3438 (epoch: 17, step: 150) // Avg time/img: 0.0471 s\n",
            "loss: 0.3383 (epoch: 17, step: 200) // Avg time/img: 0.0469 s\n",
            "loss: 0.341 (epoch: 17, step: 250) // Avg time/img: 0.0470 s\n",
            "loss: 0.345 (epoch: 17, step: 300) // Avg time/img: 0.0471 s\n",
            "loss: 0.3461 (epoch: 17, step: 350) // Avg time/img: 0.0472 s\n",
            "loss: 0.3483 (epoch: 17, step: 400) // Avg time/img: 0.0473 s\n",
            "loss: 0.348 (epoch: 17, step: 450) // Avg time/img: 0.0472 s\n",
            "----- VALIDATING - EPOCH 17 -----\n",
            "VAL loss: 0.4017 (epoch: 17, step: 0) // Avg time/img: 0.0530 s\n",
            "VAL loss: 0.4826 (epoch: 17, step: 50) // Avg time/img: 0.0342 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m70.03\u001b[0m %\n",
            "save: ../save/erfnet_training_focal_loss/model-017.pth (epoch: 17)\n",
            "----- TRAINING - EPOCH 18 -----\n",
            "LEARNING RATE:  9.066760365683729e-06\n",
            "loss: 0.3123 (epoch: 18, step: 0) // Avg time/img: 0.0519 s\n",
            "loss: 0.3497 (epoch: 18, step: 50) // Avg time/img: 0.0475 s\n",
            "loss: 0.3562 (epoch: 18, step: 100) // Avg time/img: 0.0472 s\n",
            "loss: 0.3535 (epoch: 18, step: 150) // Avg time/img: 0.0471 s\n",
            "loss: 0.3523 (epoch: 18, step: 200) // Avg time/img: 0.0470 s\n",
            "loss: 0.3519 (epoch: 18, step: 250) // Avg time/img: 0.0469 s\n",
            "loss: 0.3517 (epoch: 18, step: 300) // Avg time/img: 0.0468 s\n",
            "loss: 0.3509 (epoch: 18, step: 350) // Avg time/img: 0.0471 s\n",
            "loss: 0.3498 (epoch: 18, step: 400) // Avg time/img: 0.0473 s\n",
            "loss: 0.3499 (epoch: 18, step: 450) // Avg time/img: 0.0476 s\n",
            "----- VALIDATING - EPOCH 18 -----\n",
            "VAL loss: 0.4023 (epoch: 18, step: 0) // Avg time/img: 0.0527 s\n",
            "VAL loss: 0.483 (epoch: 18, step: 50) // Avg time/img: 0.0363 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m69.99\u001b[0m %\n",
            "save: ../save/erfnet_training_focal_loss/model-018.pth (epoch: 18)\n",
            "----- TRAINING - EPOCH 19 -----\n",
            "LEARNING RATE:  6.294627058970836e-06\n",
            "loss: 0.2718 (epoch: 19, step: 0) // Avg time/img: 0.0545 s\n",
            "loss: 0.3506 (epoch: 19, step: 50) // Avg time/img: 0.0475 s\n",
            "loss: 0.3437 (epoch: 19, step: 100) // Avg time/img: 0.0472 s\n",
            "loss: 0.3425 (epoch: 19, step: 150) // Avg time/img: 0.0474 s\n",
            "loss: 0.3446 (epoch: 19, step: 200) // Avg time/img: 0.0472 s\n",
            "loss: 0.3495 (epoch: 19, step: 250) // Avg time/img: 0.0472 s\n",
            "loss: 0.3508 (epoch: 19, step: 300) // Avg time/img: 0.0471 s\n",
            "loss: 0.3501 (epoch: 19, step: 350) // Avg time/img: 0.0471 s\n",
            "loss: 0.3497 (epoch: 19, step: 400) // Avg time/img: 0.0471 s\n",
            "loss: 0.3484 (epoch: 19, step: 450) // Avg time/img: 0.0471 s\n",
            "----- VALIDATING - EPOCH 19 -----\n",
            "VAL loss: 0.4041 (epoch: 19, step: 0) // Avg time/img: 0.0437 s\n",
            "VAL loss: 0.4803 (epoch: 19, step: 50) // Avg time/img: 0.0347 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m69.98\u001b[0m %\n",
            "save: ../save/erfnet_training_focal_loss/model-019.pth (epoch: 19)\n",
            "----- TRAINING - EPOCH 20 -----\n",
            "LEARNING RATE:  3.373207119183911e-06\n",
            "loss: 0.4666 (epoch: 20, step: 0) // Avg time/img: 0.0497 s\n",
            "loss: 0.3457 (epoch: 20, step: 50) // Avg time/img: 0.0458 s\n",
            "loss: 0.3409 (epoch: 20, step: 100) // Avg time/img: 0.0463 s\n",
            "loss: 0.3431 (epoch: 20, step: 150) // Avg time/img: 0.0465 s\n",
            "loss: 0.3445 (epoch: 20, step: 200) // Avg time/img: 0.0466 s\n",
            "loss: 0.3455 (epoch: 20, step: 250) // Avg time/img: 0.0467 s\n",
            "loss: 0.3461 (epoch: 20, step: 300) // Avg time/img: 0.0468 s\n",
            "loss: 0.3464 (epoch: 20, step: 350) // Avg time/img: 0.0468 s\n",
            "loss: 0.3469 (epoch: 20, step: 400) // Avg time/img: 0.0469 s\n",
            "loss: 0.3462 (epoch: 20, step: 450) // Avg time/img: 0.0469 s\n",
            "----- VALIDATING - EPOCH 20 -----\n",
            "VAL loss: 0.3865 (epoch: 20, step: 0) // Avg time/img: 0.0458 s\n",
            "VAL loss: 0.4719 (epoch: 20, step: 50) // Avg time/img: 0.0349 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m69.89\u001b[0m %\n",
            "save: ../save/erfnet_training_focal_loss/model-020.pth (epoch: 20)\n",
            "========== TRAINING FINISHED ===========\n",
            "Model saved in /content/AnomalySegmentation/save/erfnet_training_focal_loss\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_focal_loss/ (stored 0%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_focal_loss/model_best.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_focal_loss/model-007.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_focal_loss/model-015.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_focal_loss/model-008.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_focal_loss/model-004.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_focal_loss/model-005.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_focal_loss/model-002.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_focal_loss/opts.txt (deflated 37%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_focal_loss/model.txt (deflated 92%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_focal_loss/model-014.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_focal_loss/model-013.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_focal_loss/model-012.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_focal_loss/model-016.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_focal_loss/model-011.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_focal_loss/checkpoint.pth.tar (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_focal_loss/model-010.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_focal_loss/model-020.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_focal_loss/model_best.pth.tar (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_focal_loss/model-009.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_focal_loss/model-003.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_focal_loss/model-006.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_focal_loss/automated_log.txt (deflated 64%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_focal_loss/model-019.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_focal_loss/model-017.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_focal_loss/model-018.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_focal_loss/best.txt (stored 0%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_focal_loss/model-001.pth (deflated 10%)\n",
            "\n",
            "\n",
            "----- Fine-tuning with LogitNorm loss -----\n",
            "odict_keys(['module.encoder.initial_block.conv.weight', 'module.encoder.initial_block.conv.bias', 'module.encoder.initial_block.bn.weight', 'module.encoder.initial_block.bn.bias', 'module.encoder.initial_block.bn.running_mean', 'module.encoder.initial_block.bn.running_var', 'module.encoder.layers.0.conv.weight', 'module.encoder.layers.0.conv.bias', 'module.encoder.layers.0.bn.weight', 'module.encoder.layers.0.bn.bias', 'module.encoder.layers.0.bn.running_mean', 'module.encoder.layers.0.bn.running_var', 'module.encoder.layers.1.conv3x1_1.weight', 'module.encoder.layers.1.conv3x1_1.bias', 'module.encoder.layers.1.conv1x3_1.weight', 'module.encoder.layers.1.conv1x3_1.bias', 'module.encoder.layers.1.conv3x1_2.weight', 'module.encoder.layers.1.conv3x1_2.bias', 'module.encoder.layers.1.conv1x3_2.weight', 'module.encoder.layers.1.conv1x3_2.bias', 'module.encoder.layers.1.bn1.weight', 'module.encoder.layers.1.bn1.bias', 'module.encoder.layers.1.bn1.running_mean', 'module.encoder.layers.1.bn1.running_var', 'module.encoder.layers.1.bn2.weight', 'module.encoder.layers.1.bn2.bias', 'module.encoder.layers.1.bn2.running_mean', 'module.encoder.layers.1.bn2.running_var', 'module.encoder.layers.2.conv3x1_1.weight', 'module.encoder.layers.2.conv3x1_1.bias', 'module.encoder.layers.2.conv1x3_1.weight', 'module.encoder.layers.2.conv1x3_1.bias', 'module.encoder.layers.2.conv3x1_2.weight', 'module.encoder.layers.2.conv3x1_2.bias', 'module.encoder.layers.2.conv1x3_2.weight', 'module.encoder.layers.2.conv1x3_2.bias', 'module.encoder.layers.2.bn1.weight', 'module.encoder.layers.2.bn1.bias', 'module.encoder.layers.2.bn1.running_mean', 'module.encoder.layers.2.bn1.running_var', 'module.encoder.layers.2.bn2.weight', 'module.encoder.layers.2.bn2.bias', 'module.encoder.layers.2.bn2.running_mean', 'module.encoder.layers.2.bn2.running_var', 'module.encoder.layers.3.conv3x1_1.weight', 'module.encoder.layers.3.conv3x1_1.bias', 'module.encoder.layers.3.conv1x3_1.weight', 'module.encoder.layers.3.conv1x3_1.bias', 'module.encoder.layers.3.conv3x1_2.weight', 'module.encoder.layers.3.conv3x1_2.bias', 'module.encoder.layers.3.conv1x3_2.weight', 'module.encoder.layers.3.conv1x3_2.bias', 'module.encoder.layers.3.bn1.weight', 'module.encoder.layers.3.bn1.bias', 'module.encoder.layers.3.bn1.running_mean', 'module.encoder.layers.3.bn1.running_var', 'module.encoder.layers.3.bn2.weight', 'module.encoder.layers.3.bn2.bias', 'module.encoder.layers.3.bn2.running_mean', 'module.encoder.layers.3.bn2.running_var', 'module.encoder.layers.4.conv3x1_1.weight', 'module.encoder.layers.4.conv3x1_1.bias', 'module.encoder.layers.4.conv1x3_1.weight', 'module.encoder.layers.4.conv1x3_1.bias', 'module.encoder.layers.4.conv3x1_2.weight', 'module.encoder.layers.4.conv3x1_2.bias', 'module.encoder.layers.4.conv1x3_2.weight', 'module.encoder.layers.4.conv1x3_2.bias', 'module.encoder.layers.4.bn1.weight', 'module.encoder.layers.4.bn1.bias', 'module.encoder.layers.4.bn1.running_mean', 'module.encoder.layers.4.bn1.running_var', 'module.encoder.layers.4.bn2.weight', 'module.encoder.layers.4.bn2.bias', 'module.encoder.layers.4.bn2.running_mean', 'module.encoder.layers.4.bn2.running_var', 'module.encoder.layers.5.conv3x1_1.weight', 'module.encoder.layers.5.conv3x1_1.bias', 'module.encoder.layers.5.conv1x3_1.weight', 'module.encoder.layers.5.conv1x3_1.bias', 'module.encoder.layers.5.conv3x1_2.weight', 'module.encoder.layers.5.conv3x1_2.bias', 'module.encoder.layers.5.conv1x3_2.weight', 'module.encoder.layers.5.conv1x3_2.bias', 'module.encoder.layers.5.bn1.weight', 'module.encoder.layers.5.bn1.bias', 'module.encoder.layers.5.bn1.running_mean', 'module.encoder.layers.5.bn1.running_var', 'module.encoder.layers.5.bn2.weight', 'module.encoder.layers.5.bn2.bias', 'module.encoder.layers.5.bn2.running_mean', 'module.encoder.layers.5.bn2.running_var', 'module.encoder.layers.6.conv.weight', 'module.encoder.layers.6.conv.bias', 'module.encoder.layers.6.bn.weight', 'module.encoder.layers.6.bn.bias', 'module.encoder.layers.6.bn.running_mean', 'module.encoder.layers.6.bn.running_var', 'module.encoder.layers.7.conv3x1_1.weight', 'module.encoder.layers.7.conv3x1_1.bias', 'module.encoder.layers.7.conv1x3_1.weight', 'module.encoder.layers.7.conv1x3_1.bias', 'module.encoder.layers.7.conv3x1_2.weight', 'module.encoder.layers.7.conv3x1_2.bias', 'module.encoder.layers.7.conv1x3_2.weight', 'module.encoder.layers.7.conv1x3_2.bias', 'module.encoder.layers.7.bn1.weight', 'module.encoder.layers.7.bn1.bias', 'module.encoder.layers.7.bn1.running_mean', 'module.encoder.layers.7.bn1.running_var', 'module.encoder.layers.7.bn2.weight', 'module.encoder.layers.7.bn2.bias', 'module.encoder.layers.7.bn2.running_mean', 'module.encoder.layers.7.bn2.running_var', 'module.encoder.layers.8.conv3x1_1.weight', 'module.encoder.layers.8.conv3x1_1.bias', 'module.encoder.layers.8.conv1x3_1.weight', 'module.encoder.layers.8.conv1x3_1.bias', 'module.encoder.layers.8.conv3x1_2.weight', 'module.encoder.layers.8.conv3x1_2.bias', 'module.encoder.layers.8.conv1x3_2.weight', 'module.encoder.layers.8.conv1x3_2.bias', 'module.encoder.layers.8.bn1.weight', 'module.encoder.layers.8.bn1.bias', 'module.encoder.layers.8.bn1.running_mean', 'module.encoder.layers.8.bn1.running_var', 'module.encoder.layers.8.bn2.weight', 'module.encoder.layers.8.bn2.bias', 'module.encoder.layers.8.bn2.running_mean', 'module.encoder.layers.8.bn2.running_var', 'module.encoder.layers.9.conv3x1_1.weight', 'module.encoder.layers.9.conv3x1_1.bias', 'module.encoder.layers.9.conv1x3_1.weight', 'module.encoder.layers.9.conv1x3_1.bias', 'module.encoder.layers.9.conv3x1_2.weight', 'module.encoder.layers.9.conv3x1_2.bias', 'module.encoder.layers.9.conv1x3_2.weight', 'module.encoder.layers.9.conv1x3_2.bias', 'module.encoder.layers.9.bn1.weight', 'module.encoder.layers.9.bn1.bias', 'module.encoder.layers.9.bn1.running_mean', 'module.encoder.layers.9.bn1.running_var', 'module.encoder.layers.9.bn2.weight', 'module.encoder.layers.9.bn2.bias', 'module.encoder.layers.9.bn2.running_mean', 'module.encoder.layers.9.bn2.running_var', 'module.encoder.layers.10.conv3x1_1.weight', 'module.encoder.layers.10.conv3x1_1.bias', 'module.encoder.layers.10.conv1x3_1.weight', 'module.encoder.layers.10.conv1x3_1.bias', 'module.encoder.layers.10.conv3x1_2.weight', 'module.encoder.layers.10.conv3x1_2.bias', 'module.encoder.layers.10.conv1x3_2.weight', 'module.encoder.layers.10.conv1x3_2.bias', 'module.encoder.layers.10.bn1.weight', 'module.encoder.layers.10.bn1.bias', 'module.encoder.layers.10.bn1.running_mean', 'module.encoder.layers.10.bn1.running_var', 'module.encoder.layers.10.bn2.weight', 'module.encoder.layers.10.bn2.bias', 'module.encoder.layers.10.bn2.running_mean', 'module.encoder.layers.10.bn2.running_var', 'module.encoder.layers.11.conv3x1_1.weight', 'module.encoder.layers.11.conv3x1_1.bias', 'module.encoder.layers.11.conv1x3_1.weight', 'module.encoder.layers.11.conv1x3_1.bias', 'module.encoder.layers.11.conv3x1_2.weight', 'module.encoder.layers.11.conv3x1_2.bias', 'module.encoder.layers.11.conv1x3_2.weight', 'module.encoder.layers.11.conv1x3_2.bias', 'module.encoder.layers.11.bn1.weight', 'module.encoder.layers.11.bn1.bias', 'module.encoder.layers.11.bn1.running_mean', 'module.encoder.layers.11.bn1.running_var', 'module.encoder.layers.11.bn2.weight', 'module.encoder.layers.11.bn2.bias', 'module.encoder.layers.11.bn2.running_mean', 'module.encoder.layers.11.bn2.running_var', 'module.encoder.layers.12.conv3x1_1.weight', 'module.encoder.layers.12.conv3x1_1.bias', 'module.encoder.layers.12.conv1x3_1.weight', 'module.encoder.layers.12.conv1x3_1.bias', 'module.encoder.layers.12.conv3x1_2.weight', 'module.encoder.layers.12.conv3x1_2.bias', 'module.encoder.layers.12.conv1x3_2.weight', 'module.encoder.layers.12.conv1x3_2.bias', 'module.encoder.layers.12.bn1.weight', 'module.encoder.layers.12.bn1.bias', 'module.encoder.layers.12.bn1.running_mean', 'module.encoder.layers.12.bn1.running_var', 'module.encoder.layers.12.bn2.weight', 'module.encoder.layers.12.bn2.bias', 'module.encoder.layers.12.bn2.running_mean', 'module.encoder.layers.12.bn2.running_var', 'module.encoder.layers.13.conv3x1_1.weight', 'module.encoder.layers.13.conv3x1_1.bias', 'module.encoder.layers.13.conv1x3_1.weight', 'module.encoder.layers.13.conv1x3_1.bias', 'module.encoder.layers.13.conv3x1_2.weight', 'module.encoder.layers.13.conv3x1_2.bias', 'module.encoder.layers.13.conv1x3_2.weight', 'module.encoder.layers.13.conv1x3_2.bias', 'module.encoder.layers.13.bn1.weight', 'module.encoder.layers.13.bn1.bias', 'module.encoder.layers.13.bn1.running_mean', 'module.encoder.layers.13.bn1.running_var', 'module.encoder.layers.13.bn2.weight', 'module.encoder.layers.13.bn2.bias', 'module.encoder.layers.13.bn2.running_mean', 'module.encoder.layers.13.bn2.running_var', 'module.encoder.layers.14.conv3x1_1.weight', 'module.encoder.layers.14.conv3x1_1.bias', 'module.encoder.layers.14.conv1x3_1.weight', 'module.encoder.layers.14.conv1x3_1.bias', 'module.encoder.layers.14.conv3x1_2.weight', 'module.encoder.layers.14.conv3x1_2.bias', 'module.encoder.layers.14.conv1x3_2.weight', 'module.encoder.layers.14.conv1x3_2.bias', 'module.encoder.layers.14.bn1.weight', 'module.encoder.layers.14.bn1.bias', 'module.encoder.layers.14.bn1.running_mean', 'module.encoder.layers.14.bn1.running_var', 'module.encoder.layers.14.bn2.weight', 'module.encoder.layers.14.bn2.bias', 'module.encoder.layers.14.bn2.running_mean', 'module.encoder.layers.14.bn2.running_var', 'module.decoder.layers.0.conv.weight', 'module.decoder.layers.0.conv.bias', 'module.decoder.layers.0.bn.weight', 'module.decoder.layers.0.bn.bias', 'module.decoder.layers.0.bn.running_mean', 'module.decoder.layers.0.bn.running_var', 'module.decoder.layers.1.conv3x1_1.weight', 'module.decoder.layers.1.conv3x1_1.bias', 'module.decoder.layers.1.conv1x3_1.weight', 'module.decoder.layers.1.conv1x3_1.bias', 'module.decoder.layers.1.bn1.weight', 'module.decoder.layers.1.bn1.bias', 'module.decoder.layers.1.bn1.running_mean', 'module.decoder.layers.1.bn1.running_var', 'module.decoder.layers.1.conv3x1_2.weight', 'module.decoder.layers.1.conv3x1_2.bias', 'module.decoder.layers.1.conv1x3_2.weight', 'module.decoder.layers.1.conv1x3_2.bias', 'module.decoder.layers.1.bn2.weight', 'module.decoder.layers.1.bn2.bias', 'module.decoder.layers.1.bn2.running_mean', 'module.decoder.layers.1.bn2.running_var', 'module.decoder.layers.2.conv3x1_1.weight', 'module.decoder.layers.2.conv3x1_1.bias', 'module.decoder.layers.2.conv1x3_1.weight', 'module.decoder.layers.2.conv1x3_1.bias', 'module.decoder.layers.2.bn1.weight', 'module.decoder.layers.2.bn1.bias', 'module.decoder.layers.2.bn1.running_mean', 'module.decoder.layers.2.bn1.running_var', 'module.decoder.layers.2.conv3x1_2.weight', 'module.decoder.layers.2.conv3x1_2.bias', 'module.decoder.layers.2.conv1x3_2.weight', 'module.decoder.layers.2.conv1x3_2.bias', 'module.decoder.layers.2.bn2.weight', 'module.decoder.layers.2.bn2.bias', 'module.decoder.layers.2.bn2.running_mean', 'module.decoder.layers.2.bn2.running_var', 'module.decoder.layers.3.conv.weight', 'module.decoder.layers.3.conv.bias', 'module.decoder.layers.3.bn.weight', 'module.decoder.layers.3.bn.bias', 'module.decoder.layers.3.bn.running_mean', 'module.decoder.layers.3.bn.running_var', 'module.decoder.layers.4.conv3x1_1.weight', 'module.decoder.layers.4.conv3x1_1.bias', 'module.decoder.layers.4.conv1x3_1.weight', 'module.decoder.layers.4.conv1x3_1.bias', 'module.decoder.layers.4.bn1.weight', 'module.decoder.layers.4.bn1.bias', 'module.decoder.layers.4.bn1.running_mean', 'module.decoder.layers.4.bn1.running_var', 'module.decoder.layers.4.conv3x1_2.weight', 'module.decoder.layers.4.conv3x1_2.bias', 'module.decoder.layers.4.conv1x3_2.weight', 'module.decoder.layers.4.conv1x3_2.bias', 'module.decoder.layers.4.bn2.weight', 'module.decoder.layers.4.bn2.bias', 'module.decoder.layers.4.bn2.running_mean', 'module.decoder.layers.4.bn2.running_var', 'module.decoder.layers.5.conv3x1_1.weight', 'module.decoder.layers.5.conv3x1_1.bias', 'module.decoder.layers.5.conv1x3_1.weight', 'module.decoder.layers.5.conv1x3_1.bias', 'module.decoder.layers.5.bn1.weight', 'module.decoder.layers.5.bn1.bias', 'module.decoder.layers.5.bn1.running_mean', 'module.decoder.layers.5.bn1.running_var', 'module.decoder.layers.5.conv3x1_2.weight', 'module.decoder.layers.5.conv3x1_2.bias', 'module.decoder.layers.5.conv1x3_2.weight', 'module.decoder.layers.5.conv1x3_2.bias', 'module.decoder.layers.5.bn2.weight', 'module.decoder.layers.5.bn2.bias', 'module.decoder.layers.5.bn2.running_mean', 'module.decoder.layers.5.bn2.running_var', 'module.decoder.output_conv.weight', 'module.decoder.output_conv.bias'])\n",
            "odict_keys(['encoder.initial_block.conv.weight', 'encoder.initial_block.conv.bias', 'encoder.initial_block.bn.weight', 'encoder.initial_block.bn.bias', 'encoder.initial_block.bn.running_mean', 'encoder.initial_block.bn.running_var', 'encoder.initial_block.bn.num_batches_tracked', 'encoder.layers.0.conv.weight', 'encoder.layers.0.conv.bias', 'encoder.layers.0.bn.weight', 'encoder.layers.0.bn.bias', 'encoder.layers.0.bn.running_mean', 'encoder.layers.0.bn.running_var', 'encoder.layers.0.bn.num_batches_tracked', 'encoder.layers.1.conv3x1_1.weight', 'encoder.layers.1.conv3x1_1.bias', 'encoder.layers.1.conv1x3_1.weight', 'encoder.layers.1.conv1x3_1.bias', 'encoder.layers.1.bn1.weight', 'encoder.layers.1.bn1.bias', 'encoder.layers.1.bn1.running_mean', 'encoder.layers.1.bn1.running_var', 'encoder.layers.1.bn1.num_batches_tracked', 'encoder.layers.1.conv3x1_2.weight', 'encoder.layers.1.conv3x1_2.bias', 'encoder.layers.1.conv1x3_2.weight', 'encoder.layers.1.conv1x3_2.bias', 'encoder.layers.1.bn2.weight', 'encoder.layers.1.bn2.bias', 'encoder.layers.1.bn2.running_mean', 'encoder.layers.1.bn2.running_var', 'encoder.layers.1.bn2.num_batches_tracked', 'encoder.layers.2.conv3x1_1.weight', 'encoder.layers.2.conv3x1_1.bias', 'encoder.layers.2.conv1x3_1.weight', 'encoder.layers.2.conv1x3_1.bias', 'encoder.layers.2.bn1.weight', 'encoder.layers.2.bn1.bias', 'encoder.layers.2.bn1.running_mean', 'encoder.layers.2.bn1.running_var', 'encoder.layers.2.bn1.num_batches_tracked', 'encoder.layers.2.conv3x1_2.weight', 'encoder.layers.2.conv3x1_2.bias', 'encoder.layers.2.conv1x3_2.weight', 'encoder.layers.2.conv1x3_2.bias', 'encoder.layers.2.bn2.weight', 'encoder.layers.2.bn2.bias', 'encoder.layers.2.bn2.running_mean', 'encoder.layers.2.bn2.running_var', 'encoder.layers.2.bn2.num_batches_tracked', 'encoder.layers.3.conv3x1_1.weight', 'encoder.layers.3.conv3x1_1.bias', 'encoder.layers.3.conv1x3_1.weight', 'encoder.layers.3.conv1x3_1.bias', 'encoder.layers.3.bn1.weight', 'encoder.layers.3.bn1.bias', 'encoder.layers.3.bn1.running_mean', 'encoder.layers.3.bn1.running_var', 'encoder.layers.3.bn1.num_batches_tracked', 'encoder.layers.3.conv3x1_2.weight', 'encoder.layers.3.conv3x1_2.bias', 'encoder.layers.3.conv1x3_2.weight', 'encoder.layers.3.conv1x3_2.bias', 'encoder.layers.3.bn2.weight', 'encoder.layers.3.bn2.bias', 'encoder.layers.3.bn2.running_mean', 'encoder.layers.3.bn2.running_var', 'encoder.layers.3.bn2.num_batches_tracked', 'encoder.layers.4.conv3x1_1.weight', 'encoder.layers.4.conv3x1_1.bias', 'encoder.layers.4.conv1x3_1.weight', 'encoder.layers.4.conv1x3_1.bias', 'encoder.layers.4.bn1.weight', 'encoder.layers.4.bn1.bias', 'encoder.layers.4.bn1.running_mean', 'encoder.layers.4.bn1.running_var', 'encoder.layers.4.bn1.num_batches_tracked', 'encoder.layers.4.conv3x1_2.weight', 'encoder.layers.4.conv3x1_2.bias', 'encoder.layers.4.conv1x3_2.weight', 'encoder.layers.4.conv1x3_2.bias', 'encoder.layers.4.bn2.weight', 'encoder.layers.4.bn2.bias', 'encoder.layers.4.bn2.running_mean', 'encoder.layers.4.bn2.running_var', 'encoder.layers.4.bn2.num_batches_tracked', 'encoder.layers.5.conv3x1_1.weight', 'encoder.layers.5.conv3x1_1.bias', 'encoder.layers.5.conv1x3_1.weight', 'encoder.layers.5.conv1x3_1.bias', 'encoder.layers.5.bn1.weight', 'encoder.layers.5.bn1.bias', 'encoder.layers.5.bn1.running_mean', 'encoder.layers.5.bn1.running_var', 'encoder.layers.5.bn1.num_batches_tracked', 'encoder.layers.5.conv3x1_2.weight', 'encoder.layers.5.conv3x1_2.bias', 'encoder.layers.5.conv1x3_2.weight', 'encoder.layers.5.conv1x3_2.bias', 'encoder.layers.5.bn2.weight', 'encoder.layers.5.bn2.bias', 'encoder.layers.5.bn2.running_mean', 'encoder.layers.5.bn2.running_var', 'encoder.layers.5.bn2.num_batches_tracked', 'encoder.layers.6.conv.weight', 'encoder.layers.6.conv.bias', 'encoder.layers.6.bn.weight', 'encoder.layers.6.bn.bias', 'encoder.layers.6.bn.running_mean', 'encoder.layers.6.bn.running_var', 'encoder.layers.6.bn.num_batches_tracked', 'encoder.layers.7.conv3x1_1.weight', 'encoder.layers.7.conv3x1_1.bias', 'encoder.layers.7.conv1x3_1.weight', 'encoder.layers.7.conv1x3_1.bias', 'encoder.layers.7.bn1.weight', 'encoder.layers.7.bn1.bias', 'encoder.layers.7.bn1.running_mean', 'encoder.layers.7.bn1.running_var', 'encoder.layers.7.bn1.num_batches_tracked', 'encoder.layers.7.conv3x1_2.weight', 'encoder.layers.7.conv3x1_2.bias', 'encoder.layers.7.conv1x3_2.weight', 'encoder.layers.7.conv1x3_2.bias', 'encoder.layers.7.bn2.weight', 'encoder.layers.7.bn2.bias', 'encoder.layers.7.bn2.running_mean', 'encoder.layers.7.bn2.running_var', 'encoder.layers.7.bn2.num_batches_tracked', 'encoder.layers.8.conv3x1_1.weight', 'encoder.layers.8.conv3x1_1.bias', 'encoder.layers.8.conv1x3_1.weight', 'encoder.layers.8.conv1x3_1.bias', 'encoder.layers.8.bn1.weight', 'encoder.layers.8.bn1.bias', 'encoder.layers.8.bn1.running_mean', 'encoder.layers.8.bn1.running_var', 'encoder.layers.8.bn1.num_batches_tracked', 'encoder.layers.8.conv3x1_2.weight', 'encoder.layers.8.conv3x1_2.bias', 'encoder.layers.8.conv1x3_2.weight', 'encoder.layers.8.conv1x3_2.bias', 'encoder.layers.8.bn2.weight', 'encoder.layers.8.bn2.bias', 'encoder.layers.8.bn2.running_mean', 'encoder.layers.8.bn2.running_var', 'encoder.layers.8.bn2.num_batches_tracked', 'encoder.layers.9.conv3x1_1.weight', 'encoder.layers.9.conv3x1_1.bias', 'encoder.layers.9.conv1x3_1.weight', 'encoder.layers.9.conv1x3_1.bias', 'encoder.layers.9.bn1.weight', 'encoder.layers.9.bn1.bias', 'encoder.layers.9.bn1.running_mean', 'encoder.layers.9.bn1.running_var', 'encoder.layers.9.bn1.num_batches_tracked', 'encoder.layers.9.conv3x1_2.weight', 'encoder.layers.9.conv3x1_2.bias', 'encoder.layers.9.conv1x3_2.weight', 'encoder.layers.9.conv1x3_2.bias', 'encoder.layers.9.bn2.weight', 'encoder.layers.9.bn2.bias', 'encoder.layers.9.bn2.running_mean', 'encoder.layers.9.bn2.running_var', 'encoder.layers.9.bn2.num_batches_tracked', 'encoder.layers.10.conv3x1_1.weight', 'encoder.layers.10.conv3x1_1.bias', 'encoder.layers.10.conv1x3_1.weight', 'encoder.layers.10.conv1x3_1.bias', 'encoder.layers.10.bn1.weight', 'encoder.layers.10.bn1.bias', 'encoder.layers.10.bn1.running_mean', 'encoder.layers.10.bn1.running_var', 'encoder.layers.10.bn1.num_batches_tracked', 'encoder.layers.10.conv3x1_2.weight', 'encoder.layers.10.conv3x1_2.bias', 'encoder.layers.10.conv1x3_2.weight', 'encoder.layers.10.conv1x3_2.bias', 'encoder.layers.10.bn2.weight', 'encoder.layers.10.bn2.bias', 'encoder.layers.10.bn2.running_mean', 'encoder.layers.10.bn2.running_var', 'encoder.layers.10.bn2.num_batches_tracked', 'encoder.layers.11.conv3x1_1.weight', 'encoder.layers.11.conv3x1_1.bias', 'encoder.layers.11.conv1x3_1.weight', 'encoder.layers.11.conv1x3_1.bias', 'encoder.layers.11.bn1.weight', 'encoder.layers.11.bn1.bias', 'encoder.layers.11.bn1.running_mean', 'encoder.layers.11.bn1.running_var', 'encoder.layers.11.bn1.num_batches_tracked', 'encoder.layers.11.conv3x1_2.weight', 'encoder.layers.11.conv3x1_2.bias', 'encoder.layers.11.conv1x3_2.weight', 'encoder.layers.11.conv1x3_2.bias', 'encoder.layers.11.bn2.weight', 'encoder.layers.11.bn2.bias', 'encoder.layers.11.bn2.running_mean', 'encoder.layers.11.bn2.running_var', 'encoder.layers.11.bn2.num_batches_tracked', 'encoder.layers.12.conv3x1_1.weight', 'encoder.layers.12.conv3x1_1.bias', 'encoder.layers.12.conv1x3_1.weight', 'encoder.layers.12.conv1x3_1.bias', 'encoder.layers.12.bn1.weight', 'encoder.layers.12.bn1.bias', 'encoder.layers.12.bn1.running_mean', 'encoder.layers.12.bn1.running_var', 'encoder.layers.12.bn1.num_batches_tracked', 'encoder.layers.12.conv3x1_2.weight', 'encoder.layers.12.conv3x1_2.bias', 'encoder.layers.12.conv1x3_2.weight', 'encoder.layers.12.conv1x3_2.bias', 'encoder.layers.12.bn2.weight', 'encoder.layers.12.bn2.bias', 'encoder.layers.12.bn2.running_mean', 'encoder.layers.12.bn2.running_var', 'encoder.layers.12.bn2.num_batches_tracked', 'encoder.layers.13.conv3x1_1.weight', 'encoder.layers.13.conv3x1_1.bias', 'encoder.layers.13.conv1x3_1.weight', 'encoder.layers.13.conv1x3_1.bias', 'encoder.layers.13.bn1.weight', 'encoder.layers.13.bn1.bias', 'encoder.layers.13.bn1.running_mean', 'encoder.layers.13.bn1.running_var', 'encoder.layers.13.bn1.num_batches_tracked', 'encoder.layers.13.conv3x1_2.weight', 'encoder.layers.13.conv3x1_2.bias', 'encoder.layers.13.conv1x3_2.weight', 'encoder.layers.13.conv1x3_2.bias', 'encoder.layers.13.bn2.weight', 'encoder.layers.13.bn2.bias', 'encoder.layers.13.bn2.running_mean', 'encoder.layers.13.bn2.running_var', 'encoder.layers.13.bn2.num_batches_tracked', 'encoder.layers.14.conv3x1_1.weight', 'encoder.layers.14.conv3x1_1.bias', 'encoder.layers.14.conv1x3_1.weight', 'encoder.layers.14.conv1x3_1.bias', 'encoder.layers.14.bn1.weight', 'encoder.layers.14.bn1.bias', 'encoder.layers.14.bn1.running_mean', 'encoder.layers.14.bn1.running_var', 'encoder.layers.14.bn1.num_batches_tracked', 'encoder.layers.14.conv3x1_2.weight', 'encoder.layers.14.conv3x1_2.bias', 'encoder.layers.14.conv1x3_2.weight', 'encoder.layers.14.conv1x3_2.bias', 'encoder.layers.14.bn2.weight', 'encoder.layers.14.bn2.bias', 'encoder.layers.14.bn2.running_mean', 'encoder.layers.14.bn2.running_var', 'encoder.layers.14.bn2.num_batches_tracked', 'encoder.output_conv.weight', 'encoder.output_conv.bias', 'decoder.layers.0.conv.weight', 'decoder.layers.0.conv.bias', 'decoder.layers.0.bn.weight', 'decoder.layers.0.bn.bias', 'decoder.layers.0.bn.running_mean', 'decoder.layers.0.bn.running_var', 'decoder.layers.0.bn.num_batches_tracked', 'decoder.layers.1.conv3x1_1.weight', 'decoder.layers.1.conv3x1_1.bias', 'decoder.layers.1.conv1x3_1.weight', 'decoder.layers.1.conv1x3_1.bias', 'decoder.layers.1.bn1.weight', 'decoder.layers.1.bn1.bias', 'decoder.layers.1.bn1.running_mean', 'decoder.layers.1.bn1.running_var', 'decoder.layers.1.bn1.num_batches_tracked', 'decoder.layers.1.conv3x1_2.weight', 'decoder.layers.1.conv3x1_2.bias', 'decoder.layers.1.conv1x3_2.weight', 'decoder.layers.1.conv1x3_2.bias', 'decoder.layers.1.bn2.weight', 'decoder.layers.1.bn2.bias', 'decoder.layers.1.bn2.running_mean', 'decoder.layers.1.bn2.running_var', 'decoder.layers.1.bn2.num_batches_tracked', 'decoder.layers.2.conv3x1_1.weight', 'decoder.layers.2.conv3x1_1.bias', 'decoder.layers.2.conv1x3_1.weight', 'decoder.layers.2.conv1x3_1.bias', 'decoder.layers.2.bn1.weight', 'decoder.layers.2.bn1.bias', 'decoder.layers.2.bn1.running_mean', 'decoder.layers.2.bn1.running_var', 'decoder.layers.2.bn1.num_batches_tracked', 'decoder.layers.2.conv3x1_2.weight', 'decoder.layers.2.conv3x1_2.bias', 'decoder.layers.2.conv1x3_2.weight', 'decoder.layers.2.conv1x3_2.bias', 'decoder.layers.2.bn2.weight', 'decoder.layers.2.bn2.bias', 'decoder.layers.2.bn2.running_mean', 'decoder.layers.2.bn2.running_var', 'decoder.layers.2.bn2.num_batches_tracked', 'decoder.layers.3.conv.weight', 'decoder.layers.3.conv.bias', 'decoder.layers.3.bn.weight', 'decoder.layers.3.bn.bias', 'decoder.layers.3.bn.running_mean', 'decoder.layers.3.bn.running_var', 'decoder.layers.3.bn.num_batches_tracked', 'decoder.layers.4.conv3x1_1.weight', 'decoder.layers.4.conv3x1_1.bias', 'decoder.layers.4.conv1x3_1.weight', 'decoder.layers.4.conv1x3_1.bias', 'decoder.layers.4.bn1.weight', 'decoder.layers.4.bn1.bias', 'decoder.layers.4.bn1.running_mean', 'decoder.layers.4.bn1.running_var', 'decoder.layers.4.bn1.num_batches_tracked', 'decoder.layers.4.conv3x1_2.weight', 'decoder.layers.4.conv3x1_2.bias', 'decoder.layers.4.conv1x3_2.weight', 'decoder.layers.4.conv1x3_2.bias', 'decoder.layers.4.bn2.weight', 'decoder.layers.4.bn2.bias', 'decoder.layers.4.bn2.running_mean', 'decoder.layers.4.bn2.running_var', 'decoder.layers.4.bn2.num_batches_tracked', 'decoder.layers.5.conv3x1_1.weight', 'decoder.layers.5.conv3x1_1.bias', 'decoder.layers.5.conv1x3_1.weight', 'decoder.layers.5.conv1x3_1.bias', 'decoder.layers.5.bn1.weight', 'decoder.layers.5.bn1.bias', 'decoder.layers.5.bn1.running_mean', 'decoder.layers.5.bn1.running_var', 'decoder.layers.5.bn1.num_batches_tracked', 'decoder.layers.5.conv3x1_2.weight', 'decoder.layers.5.conv3x1_2.bias', 'decoder.layers.5.conv1x3_2.weight', 'decoder.layers.5.conv1x3_2.bias', 'decoder.layers.5.bn2.weight', 'decoder.layers.5.bn2.bias', 'decoder.layers.5.bn2.running_mean', 'decoder.layers.5.bn2.running_var', 'decoder.layers.5.bn2.num_batches_tracked', 'decoder.output_conv.weight', 'decoder.output_conv.bias'])\n",
            "Import Model erfnet with weights erfnet_pretrained.pth to FineTune\n",
            "========== TRAINING ===========\n",
            "/content/cityscapes/leftImg8bit/train\n",
            "/content/cityscapes/leftImg8bit/val\n",
            "----- TRAINING - EPOCH 1 -----\n",
            "LEARNING RATE:  5e-05\n",
            "loss: 2.943 (epoch: 1, step: 0) // Avg time/img: 0.5878 s\n",
            "loss: 2.941 (epoch: 1, step: 50) // Avg time/img: 0.0579 s\n",
            "loss: 2.941 (epoch: 1, step: 100) // Avg time/img: 0.0531 s\n",
            "loss: 2.94 (epoch: 1, step: 150) // Avg time/img: 0.0511 s\n",
            "loss: 2.94 (epoch: 1, step: 200) // Avg time/img: 0.0504 s\n",
            "loss: 2.94 (epoch: 1, step: 250) // Avg time/img: 0.0499 s\n",
            "loss: 2.94 (epoch: 1, step: 300) // Avg time/img: 0.0497 s\n",
            "loss: 2.94 (epoch: 1, step: 350) // Avg time/img: 0.0495 s\n",
            "loss: 2.94 (epoch: 1, step: 400) // Avg time/img: 0.0493 s\n",
            "loss: 2.94 (epoch: 1, step: 450) // Avg time/img: 0.0492 s\n",
            "----- VALIDATING - EPOCH 1 -----\n",
            "VAL loss: 2.942 (epoch: 1, step: 0) // Avg time/img: 0.0528 s\n",
            "VAL loss: 2.942 (epoch: 1, step: 50) // Avg time/img: 0.0350 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.89\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/erfnet_training_logitnorm_loss/model-001.pth (epoch: 1)\n",
            "save: ../save/erfnet_training_logitnorm_loss/model_best.pth (epoch: 1)\n",
            "----- TRAINING - EPOCH 2 -----\n",
            "LEARNING RATE:  4.774426908107499e-05\n",
            "loss: 2.939 (epoch: 2, step: 0) // Avg time/img: 0.0562 s\n",
            "loss: 2.939 (epoch: 2, step: 50) // Avg time/img: 0.0482 s\n",
            "loss: 2.939 (epoch: 2, step: 100) // Avg time/img: 0.0481 s\n",
            "loss: 2.939 (epoch: 2, step: 150) // Avg time/img: 0.0481 s\n",
            "loss: 2.939 (epoch: 2, step: 200) // Avg time/img: 0.0482 s\n",
            "loss: 2.939 (epoch: 2, step: 250) // Avg time/img: 0.0482 s\n",
            "loss: 2.939 (epoch: 2, step: 300) // Avg time/img: 0.0482 s\n",
            "loss: 2.939 (epoch: 2, step: 350) // Avg time/img: 0.0482 s\n",
            "loss: 2.939 (epoch: 2, step: 400) // Avg time/img: 0.0483 s\n",
            "loss: 2.939 (epoch: 2, step: 450) // Avg time/img: 0.0483 s\n",
            "----- VALIDATING - EPOCH 2 -----\n",
            "VAL loss: 2.941 (epoch: 2, step: 0) // Avg time/img: 0.0516 s\n",
            "VAL loss: 2.941 (epoch: 2, step: 50) // Avg time/img: 0.0347 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.45\u001b[0m %\n",
            "save: ../save/erfnet_training_logitnorm_loss/model-002.pth (epoch: 2)\n",
            "----- TRAINING - EPOCH 3 -----\n",
            "LEARNING RATE:  4.547662880414811e-05\n",
            "loss: 2.938 (epoch: 3, step: 0) // Avg time/img: 0.0529 s\n",
            "loss: 2.939 (epoch: 3, step: 50) // Avg time/img: 0.0479 s\n",
            "loss: 2.939 (epoch: 3, step: 100) // Avg time/img: 0.0482 s\n",
            "loss: 2.939 (epoch: 3, step: 150) // Avg time/img: 0.0483 s\n",
            "loss: 2.939 (epoch: 3, step: 200) // Avg time/img: 0.0482 s\n",
            "loss: 2.939 (epoch: 3, step: 250) // Avg time/img: 0.0482 s\n",
            "loss: 2.939 (epoch: 3, step: 300) // Avg time/img: 0.0484 s\n",
            "loss: 2.939 (epoch: 3, step: 350) // Avg time/img: 0.0484 s\n",
            "loss: 2.939 (epoch: 3, step: 400) // Avg time/img: 0.0484 s\n",
            "loss: 2.939 (epoch: 3, step: 450) // Avg time/img: 0.0484 s\n",
            "----- VALIDATING - EPOCH 3 -----\n",
            "VAL loss: 2.941 (epoch: 3, step: 0) // Avg time/img: 0.0498 s\n",
            "VAL loss: 2.941 (epoch: 3, step: 50) // Avg time/img: 0.0348 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.09\u001b[0m %\n",
            "save: ../save/erfnet_training_logitnorm_loss/model-003.pth (epoch: 3)\n",
            "----- TRAINING - EPOCH 4 -----\n",
            "LEARNING RATE:  4.319634861514096e-05\n",
            "loss: 2.937 (epoch: 4, step: 0) // Avg time/img: 0.0561 s\n",
            "loss: 2.939 (epoch: 4, step: 50) // Avg time/img: 0.0480 s\n",
            "loss: 2.939 (epoch: 4, step: 100) // Avg time/img: 0.0483 s\n",
            "loss: 2.938 (epoch: 4, step: 150) // Avg time/img: 0.0483 s\n",
            "loss: 2.938 (epoch: 4, step: 200) // Avg time/img: 0.0484 s\n",
            "loss: 2.938 (epoch: 4, step: 250) // Avg time/img: 0.0483 s\n",
            "loss: 2.938 (epoch: 4, step: 300) // Avg time/img: 0.0484 s\n",
            "loss: 2.938 (epoch: 4, step: 350) // Avg time/img: 0.0484 s\n",
            "loss: 2.938 (epoch: 4, step: 400) // Avg time/img: 0.0485 s\n",
            "loss: 2.938 (epoch: 4, step: 450) // Avg time/img: 0.0484 s\n",
            "----- VALIDATING - EPOCH 4 -----\n",
            "VAL loss: 2.94 (epoch: 4, step: 0) // Avg time/img: 0.0491 s\n",
            "VAL loss: 2.941 (epoch: 4, step: 50) // Avg time/img: 0.0353 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m70.79\u001b[0m %\n",
            "save: ../save/erfnet_training_logitnorm_loss/model-004.pth (epoch: 4)\n",
            "----- TRAINING - EPOCH 5 -----\n",
            "LEARNING RATE:  4.090260730254292e-05\n",
            "loss: 2.936 (epoch: 5, step: 0) // Avg time/img: 0.0573 s\n",
            "loss: 2.938 (epoch: 5, step: 50) // Avg time/img: 0.0487 s\n",
            "loss: 2.938 (epoch: 5, step: 100) // Avg time/img: 0.0486 s\n",
            "loss: 2.938 (epoch: 5, step: 150) // Avg time/img: 0.0483 s\n",
            "loss: 2.938 (epoch: 5, step: 200) // Avg time/img: 0.0484 s\n",
            "loss: 2.938 (epoch: 5, step: 250) // Avg time/img: 0.0483 s\n",
            "loss: 2.938 (epoch: 5, step: 300) // Avg time/img: 0.0483 s\n",
            "loss: 2.938 (epoch: 5, step: 350) // Avg time/img: 0.0482 s\n",
            "loss: 2.938 (epoch: 5, step: 400) // Avg time/img: 0.0483 s\n",
            "loss: 2.938 (epoch: 5, step: 450) // Avg time/img: 0.0483 s\n",
            "----- VALIDATING - EPOCH 5 -----\n",
            "VAL loss: 2.94 (epoch: 5, step: 0) // Avg time/img: 0.0586 s\n",
            "VAL loss: 2.94 (epoch: 5, step: 50) // Avg time/img: 0.0349 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m70.43\u001b[0m %\n",
            "save: ../save/erfnet_training_logitnorm_loss/model-005.pth (epoch: 5)\n",
            "----- TRAINING - EPOCH 6 -----\n",
            "LEARNING RATE:  3.859447533617852e-05\n",
            "loss: 2.936 (epoch: 6, step: 0) // Avg time/img: 0.0538 s\n",
            "loss: 2.937 (epoch: 6, step: 50) // Avg time/img: 0.0481 s\n",
            "loss: 2.937 (epoch: 6, step: 100) // Avg time/img: 0.0483 s\n",
            "loss: 2.937 (epoch: 6, step: 150) // Avg time/img: 0.0481 s\n",
            "loss: 2.937 (epoch: 6, step: 200) // Avg time/img: 0.0483 s\n",
            "loss: 2.937 (epoch: 6, step: 250) // Avg time/img: 0.0484 s\n",
            "loss: 2.937 (epoch: 6, step: 300) // Avg time/img: 0.0484 s\n",
            "loss: 2.937 (epoch: 6, step: 350) // Avg time/img: 0.0484 s\n",
            "loss: 2.937 (epoch: 6, step: 400) // Avg time/img: 0.0485 s\n",
            "loss: 2.937 (epoch: 6, step: 450) // Avg time/img: 0.0484 s\n",
            "----- VALIDATING - EPOCH 6 -----\n",
            "VAL loss: 2.938 (epoch: 6, step: 0) // Avg time/img: 0.0527 s\n",
            "VAL loss: 2.938 (epoch: 6, step: 50) // Avg time/img: 0.0349 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m69.99\u001b[0m %\n",
            "save: ../save/erfnet_training_logitnorm_loss/model-006.pth (epoch: 6)\n",
            "----- TRAINING - EPOCH 7 -----\n",
            "LEARNING RATE:  3.6270892346861e-05\n",
            "loss: 2.941 (epoch: 7, step: 0) // Avg time/img: 0.0559 s\n",
            "loss: 2.936 (epoch: 7, step: 50) // Avg time/img: 0.0482 s\n",
            "loss: 2.936 (epoch: 7, step: 100) // Avg time/img: 0.0483 s\n",
            "loss: 2.936 (epoch: 7, step: 150) // Avg time/img: 0.0481 s\n",
            "loss: 2.936 (epoch: 7, step: 200) // Avg time/img: 0.0482 s\n",
            "loss: 2.936 (epoch: 7, step: 250) // Avg time/img: 0.0482 s\n",
            "loss: 2.936 (epoch: 7, step: 300) // Avg time/img: 0.0482 s\n",
            "loss: 2.936 (epoch: 7, step: 350) // Avg time/img: 0.0483 s\n",
            "loss: 2.936 (epoch: 7, step: 400) // Avg time/img: 0.0483 s\n",
            "loss: 2.936 (epoch: 7, step: 450) // Avg time/img: 0.0483 s\n",
            "----- VALIDATING - EPOCH 7 -----\n",
            "VAL loss: 2.937 (epoch: 7, step: 0) // Avg time/img: 0.0477 s\n",
            "VAL loss: 2.937 (epoch: 7, step: 50) // Avg time/img: 0.0350 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m68.32\u001b[0m %\n",
            "save: ../save/erfnet_training_logitnorm_loss/model-007.pth (epoch: 7)\n",
            "----- TRAINING - EPOCH 8 -----\n",
            "LEARNING RATE:  3.393063796290625e-05\n",
            "loss: 2.934 (epoch: 8, step: 0) // Avg time/img: 0.0588 s\n",
            "loss: 2.935 (epoch: 8, step: 50) // Avg time/img: 0.0483 s\n",
            "loss: 2.935 (epoch: 8, step: 100) // Avg time/img: 0.0483 s\n",
            "loss: 2.935 (epoch: 8, step: 150) // Avg time/img: 0.0483 s\n",
            "loss: 2.935 (epoch: 8, step: 200) // Avg time/img: 0.0483 s\n",
            "loss: 2.935 (epoch: 8, step: 250) // Avg time/img: 0.0481 s\n",
            "loss: 2.935 (epoch: 8, step: 300) // Avg time/img: 0.0482 s\n",
            "loss: 2.935 (epoch: 8, step: 350) // Avg time/img: 0.0481 s\n",
            "loss: 2.935 (epoch: 8, step: 400) // Avg time/img: 0.0482 s\n",
            "loss: 2.935 (epoch: 8, step: 450) // Avg time/img: 0.0483 s\n",
            "----- VALIDATING - EPOCH 8 -----\n",
            "VAL loss: 2.936 (epoch: 8, step: 0) // Avg time/img: 0.0540 s\n",
            "VAL loss: 2.936 (epoch: 8, step: 50) // Avg time/img: 0.0348 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m66.61\u001b[0m %\n",
            "save: ../save/erfnet_training_logitnorm_loss/model-008.pth (epoch: 8)\n",
            "----- TRAINING - EPOCH 9 -----\n",
            "LEARNING RATE:  3.157229337446777e-05\n",
            "loss: 2.934 (epoch: 9, step: 0) // Avg time/img: 0.0593 s\n",
            "loss: 2.934 (epoch: 9, step: 50) // Avg time/img: 0.0481 s\n",
            "loss: 2.934 (epoch: 9, step: 100) // Avg time/img: 0.0477 s\n",
            "loss: 2.934 (epoch: 9, step: 150) // Avg time/img: 0.0479 s\n",
            "loss: 2.934 (epoch: 9, step: 200) // Avg time/img: 0.0479 s\n",
            "loss: 2.934 (epoch: 9, step: 250) // Avg time/img: 0.0480 s\n",
            "loss: 2.934 (epoch: 9, step: 300) // Avg time/img: 0.0480 s\n",
            "loss: 2.934 (epoch: 9, step: 350) // Avg time/img: 0.0481 s\n",
            "loss: 2.934 (epoch: 9, step: 400) // Avg time/img: 0.0481 s\n",
            "loss: 2.934 (epoch: 9, step: 450) // Avg time/img: 0.0482 s\n",
            "----- VALIDATING - EPOCH 9 -----\n",
            "VAL loss: 2.936 (epoch: 9, step: 0) // Avg time/img: 0.0495 s\n",
            "VAL loss: 2.935 (epoch: 9, step: 50) // Avg time/img: 0.0345 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m66.23\u001b[0m %\n",
            "save: ../save/erfnet_training_logitnorm_loss/model-009.pth (epoch: 9)\n",
            "----- TRAINING - EPOCH 10 -----\n",
            "LEARNING RATE:  2.9194189645999014e-05\n",
            "loss: 2.936 (epoch: 10, step: 0) // Avg time/img: 0.0536 s\n",
            "loss: 2.934 (epoch: 10, step: 50) // Avg time/img: 0.0483 s\n",
            "loss: 2.934 (epoch: 10, step: 100) // Avg time/img: 0.0482 s\n",
            "loss: 2.934 (epoch: 10, step: 150) // Avg time/img: 0.0481 s\n",
            "loss: 2.934 (epoch: 10, step: 200) // Avg time/img: 0.0480 s\n",
            "loss: 2.934 (epoch: 10, step: 250) // Avg time/img: 0.0480 s\n",
            "loss: 2.934 (epoch: 10, step: 300) // Avg time/img: 0.0480 s\n",
            "loss: 2.934 (epoch: 10, step: 350) // Avg time/img: 0.0480 s\n",
            "loss: 2.934 (epoch: 10, step: 400) // Avg time/img: 0.0481 s\n",
            "loss: 2.934 (epoch: 10, step: 450) // Avg time/img: 0.0481 s\n",
            "----- VALIDATING - EPOCH 10 -----\n",
            "VAL loss: 2.935 (epoch: 10, step: 0) // Avg time/img: 0.0509 s\n",
            "VAL loss: 2.935 (epoch: 10, step: 50) // Avg time/img: 0.0347 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m66.97\u001b[0m %\n",
            "save: ../save/erfnet_training_logitnorm_loss/model-010.pth (epoch: 10)\n",
            "----- TRAINING - EPOCH 11 -----\n",
            "LEARNING RATE:  2.679433656340733e-05\n",
            "loss: 2.932 (epoch: 11, step: 0) // Avg time/img: 0.0583 s\n",
            "loss: 2.934 (epoch: 11, step: 50) // Avg time/img: 0.0480 s\n",
            "loss: 2.934 (epoch: 11, step: 100) // Avg time/img: 0.0482 s\n",
            "loss: 2.934 (epoch: 11, step: 150) // Avg time/img: 0.0481 s\n",
            "loss: 2.934 (epoch: 11, step: 200) // Avg time/img: 0.0480 s\n",
            "loss: 2.934 (epoch: 11, step: 250) // Avg time/img: 0.0480 s\n",
            "loss: 2.933 (epoch: 11, step: 300) // Avg time/img: 0.0480 s\n",
            "loss: 2.933 (epoch: 11, step: 350) // Avg time/img: 0.0480 s\n",
            "loss: 2.933 (epoch: 11, step: 400) // Avg time/img: 0.0481 s\n",
            "loss: 2.933 (epoch: 11, step: 450) // Avg time/img: 0.0481 s\n",
            "----- VALIDATING - EPOCH 11 -----\n",
            "VAL loss: 2.935 (epoch: 11, step: 0) // Avg time/img: 0.0541 s\n",
            "VAL loss: 2.935 (epoch: 11, step: 50) // Avg time/img: 0.0351 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m67.43\u001b[0m %\n",
            "save: ../save/erfnet_training_logitnorm_loss/model-011.pth (epoch: 11)\n",
            "----- TRAINING - EPOCH 12 -----\n",
            "LEARNING RATE:  2.437032195894977e-05\n",
            "loss: 2.936 (epoch: 12, step: 0) // Avg time/img: 0.0631 s\n",
            "loss: 2.933 (epoch: 12, step: 50) // Avg time/img: 0.0482 s\n",
            "loss: 2.933 (epoch: 12, step: 100) // Avg time/img: 0.0478 s\n",
            "loss: 2.933 (epoch: 12, step: 150) // Avg time/img: 0.0480 s\n",
            "loss: 2.933 (epoch: 12, step: 200) // Avg time/img: 0.0479 s\n",
            "loss: 2.933 (epoch: 12, step: 250) // Avg time/img: 0.0482 s\n",
            "loss: 2.933 (epoch: 12, step: 300) // Avg time/img: 0.0482 s\n",
            "loss: 2.933 (epoch: 12, step: 350) // Avg time/img: 0.0482 s\n",
            "loss: 2.933 (epoch: 12, step: 400) // Avg time/img: 0.0482 s\n",
            "loss: 2.933 (epoch: 12, step: 450) // Avg time/img: 0.0483 s\n",
            "----- VALIDATING - EPOCH 12 -----\n",
            "VAL loss: 2.935 (epoch: 12, step: 0) // Avg time/img: 0.0455 s\n",
            "VAL loss: 2.934 (epoch: 12, step: 50) // Avg time/img: 0.0343 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m67.54\u001b[0m %\n",
            "save: ../save/erfnet_training_logitnorm_loss/model-012.pth (epoch: 12)\n",
            "----- TRAINING - EPOCH 13 -----\n",
            "LEARNING RATE:  2.191916452770435e-05\n",
            "loss: 2.934 (epoch: 13, step: 0) // Avg time/img: 0.0551 s\n",
            "loss: 2.933 (epoch: 13, step: 50) // Avg time/img: 0.0481 s\n",
            "loss: 2.933 (epoch: 13, step: 100) // Avg time/img: 0.0480 s\n",
            "loss: 2.933 (epoch: 13, step: 150) // Avg time/img: 0.0481 s\n",
            "loss: 2.933 (epoch: 13, step: 200) // Avg time/img: 0.0481 s\n",
            "loss: 2.933 (epoch: 13, step: 250) // Avg time/img: 0.0482 s\n",
            "loss: 2.933 (epoch: 13, step: 300) // Avg time/img: 0.0484 s\n",
            "loss: 2.933 (epoch: 13, step: 350) // Avg time/img: 0.0483 s\n",
            "loss: 2.933 (epoch: 13, step: 400) // Avg time/img: 0.0483 s\n",
            "loss: 2.933 (epoch: 13, step: 450) // Avg time/img: 0.0483 s\n",
            "----- VALIDATING - EPOCH 13 -----\n",
            "VAL loss: 2.935 (epoch: 13, step: 0) // Avg time/img: 0.0462 s\n",
            "VAL loss: 2.934 (epoch: 13, step: 50) // Avg time/img: 0.0349 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m67.58\u001b[0m %\n",
            "save: ../save/erfnet_training_logitnorm_loss/model-013.pth (epoch: 13)\n",
            "----- TRAINING - EPOCH 14 -----\n",
            "LEARNING RATE:  1.9437089939938174e-05\n",
            "loss: 2.934 (epoch: 14, step: 0) // Avg time/img: 0.0531 s\n",
            "loss: 2.933 (epoch: 14, step: 50) // Avg time/img: 0.0481 s\n",
            "loss: 2.933 (epoch: 14, step: 100) // Avg time/img: 0.0481 s\n",
            "loss: 2.933 (epoch: 14, step: 150) // Avg time/img: 0.0480 s\n",
            "loss: 2.933 (epoch: 14, step: 200) // Avg time/img: 0.0481 s\n",
            "loss: 2.933 (epoch: 14, step: 250) // Avg time/img: 0.0480 s\n",
            "loss: 2.933 (epoch: 14, step: 300) // Avg time/img: 0.0482 s\n",
            "loss: 2.933 (epoch: 14, step: 350) // Avg time/img: 0.0482 s\n",
            "loss: 2.933 (epoch: 14, step: 400) // Avg time/img: 0.0483 s\n",
            "loss: 2.933 (epoch: 14, step: 450) // Avg time/img: 0.0483 s\n",
            "----- VALIDATING - EPOCH 14 -----\n",
            "VAL loss: 2.935 (epoch: 14, step: 0) // Avg time/img: 0.0597 s\n",
            "VAL loss: 2.934 (epoch: 14, step: 50) // Avg time/img: 0.0353 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m67.08\u001b[0m %\n",
            "save: ../save/erfnet_training_logitnorm_loss/model-014.pth (epoch: 14)\n",
            "----- TRAINING - EPOCH 15 -----\n",
            "LEARNING RATE:  1.6919173095082493e-05\n",
            "loss: 2.932 (epoch: 15, step: 0) // Avg time/img: 0.0593 s\n",
            "loss: 2.933 (epoch: 15, step: 50) // Avg time/img: 0.0480 s\n",
            "loss: 2.933 (epoch: 15, step: 100) // Avg time/img: 0.0481 s\n",
            "loss: 2.933 (epoch: 15, step: 150) // Avg time/img: 0.0481 s\n",
            "loss: 2.933 (epoch: 15, step: 200) // Avg time/img: 0.0480 s\n",
            "loss: 2.933 (epoch: 15, step: 250) // Avg time/img: 0.0482 s\n",
            "loss: 2.933 (epoch: 15, step: 300) // Avg time/img: 0.0480 s\n",
            "loss: 2.933 (epoch: 15, step: 350) // Avg time/img: 0.0480 s\n",
            "loss: 2.933 (epoch: 15, step: 400) // Avg time/img: 0.0480 s\n",
            "loss: 2.933 (epoch: 15, step: 450) // Avg time/img: 0.0480 s\n",
            "----- VALIDATING - EPOCH 15 -----\n",
            "VAL loss: 2.935 (epoch: 15, step: 0) // Avg time/img: 0.0449 s\n",
            "VAL loss: 2.934 (epoch: 15, step: 50) // Avg time/img: 0.0348 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m67.39\u001b[0m %\n",
            "save: ../save/erfnet_training_logitnorm_loss/model-015.pth (epoch: 15)\n",
            "----- TRAINING - EPOCH 16 -----\n",
            "LEARNING RATE:  1.4358729437462937e-05\n",
            "loss: 2.936 (epoch: 16, step: 0) // Avg time/img: 0.0550 s\n",
            "loss: 2.933 (epoch: 16, step: 50) // Avg time/img: 0.0487 s\n",
            "loss: 2.933 (epoch: 16, step: 100) // Avg time/img: 0.0481 s\n",
            "loss: 2.933 (epoch: 16, step: 150) // Avg time/img: 0.0482 s\n",
            "loss: 2.933 (epoch: 16, step: 200) // Avg time/img: 0.0482 s\n",
            "loss: 2.933 (epoch: 16, step: 250) // Avg time/img: 0.0480 s\n",
            "loss: 2.933 (epoch: 16, step: 300) // Avg time/img: 0.0481 s\n",
            "loss: 2.933 (epoch: 16, step: 350) // Avg time/img: 0.0481 s\n",
            "loss: 2.933 (epoch: 16, step: 400) // Avg time/img: 0.0482 s\n",
            "loss: 2.933 (epoch: 16, step: 450) // Avg time/img: 0.0482 s\n",
            "----- VALIDATING - EPOCH 16 -----\n",
            "VAL loss: 2.935 (epoch: 16, step: 0) // Avg time/img: 0.0522 s\n",
            "VAL loss: 2.934 (epoch: 16, step: 50) // Avg time/img: 0.0349 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m67.19\u001b[0m %\n",
            "save: ../save/erfnet_training_logitnorm_loss/model-016.pth (epoch: 16)\n",
            "----- TRAINING - EPOCH 17 -----\n",
            "LEARNING RATE:  1.1746189430880188e-05\n",
            "loss: 2.933 (epoch: 17, step: 0) // Avg time/img: 0.0562 s\n",
            "loss: 2.933 (epoch: 17, step: 50) // Avg time/img: 0.0483 s\n",
            "loss: 2.933 (epoch: 17, step: 100) // Avg time/img: 0.0485 s\n",
            "loss: 2.933 (epoch: 17, step: 150) // Avg time/img: 0.0482 s\n",
            "loss: 2.933 (epoch: 17, step: 200) // Avg time/img: 0.0483 s\n",
            "loss: 2.933 (epoch: 17, step: 250) // Avg time/img: 0.0483 s\n",
            "loss: 2.933 (epoch: 17, step: 300) // Avg time/img: 0.0482 s\n",
            "loss: 2.933 (epoch: 17, step: 350) // Avg time/img: 0.0483 s\n",
            "loss: 2.933 (epoch: 17, step: 400) // Avg time/img: 0.0483 s\n",
            "loss: 2.933 (epoch: 17, step: 450) // Avg time/img: 0.0484 s\n",
            "----- VALIDATING - EPOCH 17 -----\n",
            "VAL loss: 2.935 (epoch: 17, step: 0) // Avg time/img: 0.0493 s\n",
            "VAL loss: 2.934 (epoch: 17, step: 50) // Avg time/img: 0.0351 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m67.09\u001b[0m %\n",
            "save: ../save/erfnet_training_logitnorm_loss/model-017.pth (epoch: 17)\n",
            "----- TRAINING - EPOCH 18 -----\n",
            "LEARNING RATE:  9.066760365683729e-06\n",
            "loss: 2.934 (epoch: 18, step: 0) // Avg time/img: 0.0539 s\n",
            "loss: 2.933 (epoch: 18, step: 50) // Avg time/img: 0.0484 s\n",
            "loss: 2.933 (epoch: 18, step: 100) // Avg time/img: 0.0481 s\n",
            "loss: 2.933 (epoch: 18, step: 150) // Avg time/img: 0.0480 s\n",
            "loss: 2.933 (epoch: 18, step: 200) // Avg time/img: 0.0480 s\n",
            "loss: 2.933 (epoch: 18, step: 250) // Avg time/img: 0.0479 s\n",
            "loss: 2.933 (epoch: 18, step: 300) // Avg time/img: 0.0479 s\n",
            "loss: 2.933 (epoch: 18, step: 350) // Avg time/img: 0.0479 s\n",
            "loss: 2.933 (epoch: 18, step: 400) // Avg time/img: 0.0480 s\n",
            "loss: 2.933 (epoch: 18, step: 450) // Avg time/img: 0.0480 s\n",
            "----- VALIDATING - EPOCH 18 -----\n",
            "VAL loss: 2.935 (epoch: 18, step: 0) // Avg time/img: 0.0581 s\n",
            "VAL loss: 2.934 (epoch: 18, step: 50) // Avg time/img: 0.0354 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m67.11\u001b[0m %\n",
            "save: ../save/erfnet_training_logitnorm_loss/model-018.pth (epoch: 18)\n",
            "----- TRAINING - EPOCH 19 -----\n",
            "LEARNING RATE:  6.294627058970836e-06\n",
            "loss: 2.933 (epoch: 19, step: 0) // Avg time/img: 0.0576 s\n",
            "loss: 2.933 (epoch: 19, step: 50) // Avg time/img: 0.0476 s\n",
            "loss: 2.933 (epoch: 19, step: 100) // Avg time/img: 0.0477 s\n",
            "loss: 2.933 (epoch: 19, step: 150) // Avg time/img: 0.0475 s\n",
            "loss: 2.933 (epoch: 19, step: 200) // Avg time/img: 0.0476 s\n",
            "loss: 2.933 (epoch: 19, step: 250) // Avg time/img: 0.0478 s\n",
            "loss: 2.933 (epoch: 19, step: 300) // Avg time/img: 0.0479 s\n",
            "loss: 2.933 (epoch: 19, step: 350) // Avg time/img: 0.0479 s\n",
            "loss: 2.933 (epoch: 19, step: 400) // Avg time/img: 0.0479 s\n",
            "loss: 2.933 (epoch: 19, step: 450) // Avg time/img: 0.0480 s\n",
            "----- VALIDATING - EPOCH 19 -----\n",
            "VAL loss: 2.935 (epoch: 19, step: 0) // Avg time/img: 0.0482 s\n",
            "VAL loss: 2.934 (epoch: 19, step: 50) // Avg time/img: 0.0350 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m67.07\u001b[0m %\n",
            "save: ../save/erfnet_training_logitnorm_loss/model-019.pth (epoch: 19)\n",
            "----- TRAINING - EPOCH 20 -----\n",
            "LEARNING RATE:  3.373207119183911e-06\n",
            "loss: 2.932 (epoch: 20, step: 0) // Avg time/img: 0.0556 s\n",
            "loss: 2.933 (epoch: 20, step: 50) // Avg time/img: 0.0485 s\n",
            "loss: 2.933 (epoch: 20, step: 100) // Avg time/img: 0.0483 s\n",
            "loss: 2.933 (epoch: 20, step: 150) // Avg time/img: 0.0483 s\n",
            "loss: 2.933 (epoch: 20, step: 200) // Avg time/img: 0.0483 s\n",
            "loss: 2.933 (epoch: 20, step: 250) // Avg time/img: 0.0484 s\n",
            "loss: 2.933 (epoch: 20, step: 300) // Avg time/img: 0.0485 s\n",
            "loss: 2.933 (epoch: 20, step: 350) // Avg time/img: 0.0485 s\n",
            "loss: 2.933 (epoch: 20, step: 400) // Avg time/img: 0.0485 s\n",
            "loss: 2.933 (epoch: 20, step: 450) // Avg time/img: 0.0485 s\n",
            "----- VALIDATING - EPOCH 20 -----\n",
            "VAL loss: 2.935 (epoch: 20, step: 0) // Avg time/img: 0.0530 s\n",
            "VAL loss: 2.934 (epoch: 20, step: 50) // Avg time/img: 0.0346 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m66.89\u001b[0m %\n",
            "save: ../save/erfnet_training_logitnorm_loss/model-020.pth (epoch: 20)\n",
            "========== TRAINING FINISHED ===========\n",
            "Model saved in /content/AnomalySegmentation/save/erfnet_training_logitnorm_loss\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_logitnorm_loss/ (stored 0%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_logitnorm_loss/model_best.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_logitnorm_loss/model-007.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_logitnorm_loss/model-015.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_logitnorm_loss/model-008.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_logitnorm_loss/model-004.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_logitnorm_loss/model-005.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_logitnorm_loss/model-002.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_logitnorm_loss/opts.txt (deflated 37%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_logitnorm_loss/model.txt (deflated 92%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_logitnorm_loss/model-014.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_logitnorm_loss/model-013.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_logitnorm_loss/model-012.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_logitnorm_loss/model-016.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_logitnorm_loss/model-011.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_logitnorm_loss/checkpoint.pth.tar (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_logitnorm_loss/model-010.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_logitnorm_loss/model-020.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_logitnorm_loss/model_best.pth.tar (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_logitnorm_loss/model-009.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_logitnorm_loss/model-003.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_logitnorm_loss/model-006.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_logitnorm_loss/automated_log.txt (deflated 69%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_logitnorm_loss/model-019.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_logitnorm_loss/model-017.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_logitnorm_loss/model-018.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_logitnorm_loss/best.txt (stored 0%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_logitnorm_loss/model-001.pth (deflated 10%)\n",
            "\n",
            "\n",
            "----- Fine-tuning with IsoMaxPlus loss -----\n",
            "odict_keys(['module.encoder.initial_block.conv.weight', 'module.encoder.initial_block.conv.bias', 'module.encoder.initial_block.bn.weight', 'module.encoder.initial_block.bn.bias', 'module.encoder.initial_block.bn.running_mean', 'module.encoder.initial_block.bn.running_var', 'module.encoder.layers.0.conv.weight', 'module.encoder.layers.0.conv.bias', 'module.encoder.layers.0.bn.weight', 'module.encoder.layers.0.bn.bias', 'module.encoder.layers.0.bn.running_mean', 'module.encoder.layers.0.bn.running_var', 'module.encoder.layers.1.conv3x1_1.weight', 'module.encoder.layers.1.conv3x1_1.bias', 'module.encoder.layers.1.conv1x3_1.weight', 'module.encoder.layers.1.conv1x3_1.bias', 'module.encoder.layers.1.conv3x1_2.weight', 'module.encoder.layers.1.conv3x1_2.bias', 'module.encoder.layers.1.conv1x3_2.weight', 'module.encoder.layers.1.conv1x3_2.bias', 'module.encoder.layers.1.bn1.weight', 'module.encoder.layers.1.bn1.bias', 'module.encoder.layers.1.bn1.running_mean', 'module.encoder.layers.1.bn1.running_var', 'module.encoder.layers.1.bn2.weight', 'module.encoder.layers.1.bn2.bias', 'module.encoder.layers.1.bn2.running_mean', 'module.encoder.layers.1.bn2.running_var', 'module.encoder.layers.2.conv3x1_1.weight', 'module.encoder.layers.2.conv3x1_1.bias', 'module.encoder.layers.2.conv1x3_1.weight', 'module.encoder.layers.2.conv1x3_1.bias', 'module.encoder.layers.2.conv3x1_2.weight', 'module.encoder.layers.2.conv3x1_2.bias', 'module.encoder.layers.2.conv1x3_2.weight', 'module.encoder.layers.2.conv1x3_2.bias', 'module.encoder.layers.2.bn1.weight', 'module.encoder.layers.2.bn1.bias', 'module.encoder.layers.2.bn1.running_mean', 'module.encoder.layers.2.bn1.running_var', 'module.encoder.layers.2.bn2.weight', 'module.encoder.layers.2.bn2.bias', 'module.encoder.layers.2.bn2.running_mean', 'module.encoder.layers.2.bn2.running_var', 'module.encoder.layers.3.conv3x1_1.weight', 'module.encoder.layers.3.conv3x1_1.bias', 'module.encoder.layers.3.conv1x3_1.weight', 'module.encoder.layers.3.conv1x3_1.bias', 'module.encoder.layers.3.conv3x1_2.weight', 'module.encoder.layers.3.conv3x1_2.bias', 'module.encoder.layers.3.conv1x3_2.weight', 'module.encoder.layers.3.conv1x3_2.bias', 'module.encoder.layers.3.bn1.weight', 'module.encoder.layers.3.bn1.bias', 'module.encoder.layers.3.bn1.running_mean', 'module.encoder.layers.3.bn1.running_var', 'module.encoder.layers.3.bn2.weight', 'module.encoder.layers.3.bn2.bias', 'module.encoder.layers.3.bn2.running_mean', 'module.encoder.layers.3.bn2.running_var', 'module.encoder.layers.4.conv3x1_1.weight', 'module.encoder.layers.4.conv3x1_1.bias', 'module.encoder.layers.4.conv1x3_1.weight', 'module.encoder.layers.4.conv1x3_1.bias', 'module.encoder.layers.4.conv3x1_2.weight', 'module.encoder.layers.4.conv3x1_2.bias', 'module.encoder.layers.4.conv1x3_2.weight', 'module.encoder.layers.4.conv1x3_2.bias', 'module.encoder.layers.4.bn1.weight', 'module.encoder.layers.4.bn1.bias', 'module.encoder.layers.4.bn1.running_mean', 'module.encoder.layers.4.bn1.running_var', 'module.encoder.layers.4.bn2.weight', 'module.encoder.layers.4.bn2.bias', 'module.encoder.layers.4.bn2.running_mean', 'module.encoder.layers.4.bn2.running_var', 'module.encoder.layers.5.conv3x1_1.weight', 'module.encoder.layers.5.conv3x1_1.bias', 'module.encoder.layers.5.conv1x3_1.weight', 'module.encoder.layers.5.conv1x3_1.bias', 'module.encoder.layers.5.conv3x1_2.weight', 'module.encoder.layers.5.conv3x1_2.bias', 'module.encoder.layers.5.conv1x3_2.weight', 'module.encoder.layers.5.conv1x3_2.bias', 'module.encoder.layers.5.bn1.weight', 'module.encoder.layers.5.bn1.bias', 'module.encoder.layers.5.bn1.running_mean', 'module.encoder.layers.5.bn1.running_var', 'module.encoder.layers.5.bn2.weight', 'module.encoder.layers.5.bn2.bias', 'module.encoder.layers.5.bn2.running_mean', 'module.encoder.layers.5.bn2.running_var', 'module.encoder.layers.6.conv.weight', 'module.encoder.layers.6.conv.bias', 'module.encoder.layers.6.bn.weight', 'module.encoder.layers.6.bn.bias', 'module.encoder.layers.6.bn.running_mean', 'module.encoder.layers.6.bn.running_var', 'module.encoder.layers.7.conv3x1_1.weight', 'module.encoder.layers.7.conv3x1_1.bias', 'module.encoder.layers.7.conv1x3_1.weight', 'module.encoder.layers.7.conv1x3_1.bias', 'module.encoder.layers.7.conv3x1_2.weight', 'module.encoder.layers.7.conv3x1_2.bias', 'module.encoder.layers.7.conv1x3_2.weight', 'module.encoder.layers.7.conv1x3_2.bias', 'module.encoder.layers.7.bn1.weight', 'module.encoder.layers.7.bn1.bias', 'module.encoder.layers.7.bn1.running_mean', 'module.encoder.layers.7.bn1.running_var', 'module.encoder.layers.7.bn2.weight', 'module.encoder.layers.7.bn2.bias', 'module.encoder.layers.7.bn2.running_mean', 'module.encoder.layers.7.bn2.running_var', 'module.encoder.layers.8.conv3x1_1.weight', 'module.encoder.layers.8.conv3x1_1.bias', 'module.encoder.layers.8.conv1x3_1.weight', 'module.encoder.layers.8.conv1x3_1.bias', 'module.encoder.layers.8.conv3x1_2.weight', 'module.encoder.layers.8.conv3x1_2.bias', 'module.encoder.layers.8.conv1x3_2.weight', 'module.encoder.layers.8.conv1x3_2.bias', 'module.encoder.layers.8.bn1.weight', 'module.encoder.layers.8.bn1.bias', 'module.encoder.layers.8.bn1.running_mean', 'module.encoder.layers.8.bn1.running_var', 'module.encoder.layers.8.bn2.weight', 'module.encoder.layers.8.bn2.bias', 'module.encoder.layers.8.bn2.running_mean', 'module.encoder.layers.8.bn2.running_var', 'module.encoder.layers.9.conv3x1_1.weight', 'module.encoder.layers.9.conv3x1_1.bias', 'module.encoder.layers.9.conv1x3_1.weight', 'module.encoder.layers.9.conv1x3_1.bias', 'module.encoder.layers.9.conv3x1_2.weight', 'module.encoder.layers.9.conv3x1_2.bias', 'module.encoder.layers.9.conv1x3_2.weight', 'module.encoder.layers.9.conv1x3_2.bias', 'module.encoder.layers.9.bn1.weight', 'module.encoder.layers.9.bn1.bias', 'module.encoder.layers.9.bn1.running_mean', 'module.encoder.layers.9.bn1.running_var', 'module.encoder.layers.9.bn2.weight', 'module.encoder.layers.9.bn2.bias', 'module.encoder.layers.9.bn2.running_mean', 'module.encoder.layers.9.bn2.running_var', 'module.encoder.layers.10.conv3x1_1.weight', 'module.encoder.layers.10.conv3x1_1.bias', 'module.encoder.layers.10.conv1x3_1.weight', 'module.encoder.layers.10.conv1x3_1.bias', 'module.encoder.layers.10.conv3x1_2.weight', 'module.encoder.layers.10.conv3x1_2.bias', 'module.encoder.layers.10.conv1x3_2.weight', 'module.encoder.layers.10.conv1x3_2.bias', 'module.encoder.layers.10.bn1.weight', 'module.encoder.layers.10.bn1.bias', 'module.encoder.layers.10.bn1.running_mean', 'module.encoder.layers.10.bn1.running_var', 'module.encoder.layers.10.bn2.weight', 'module.encoder.layers.10.bn2.bias', 'module.encoder.layers.10.bn2.running_mean', 'module.encoder.layers.10.bn2.running_var', 'module.encoder.layers.11.conv3x1_1.weight', 'module.encoder.layers.11.conv3x1_1.bias', 'module.encoder.layers.11.conv1x3_1.weight', 'module.encoder.layers.11.conv1x3_1.bias', 'module.encoder.layers.11.conv3x1_2.weight', 'module.encoder.layers.11.conv3x1_2.bias', 'module.encoder.layers.11.conv1x3_2.weight', 'module.encoder.layers.11.conv1x3_2.bias', 'module.encoder.layers.11.bn1.weight', 'module.encoder.layers.11.bn1.bias', 'module.encoder.layers.11.bn1.running_mean', 'module.encoder.layers.11.bn1.running_var', 'module.encoder.layers.11.bn2.weight', 'module.encoder.layers.11.bn2.bias', 'module.encoder.layers.11.bn2.running_mean', 'module.encoder.layers.11.bn2.running_var', 'module.encoder.layers.12.conv3x1_1.weight', 'module.encoder.layers.12.conv3x1_1.bias', 'module.encoder.layers.12.conv1x3_1.weight', 'module.encoder.layers.12.conv1x3_1.bias', 'module.encoder.layers.12.conv3x1_2.weight', 'module.encoder.layers.12.conv3x1_2.bias', 'module.encoder.layers.12.conv1x3_2.weight', 'module.encoder.layers.12.conv1x3_2.bias', 'module.encoder.layers.12.bn1.weight', 'module.encoder.layers.12.bn1.bias', 'module.encoder.layers.12.bn1.running_mean', 'module.encoder.layers.12.bn1.running_var', 'module.encoder.layers.12.bn2.weight', 'module.encoder.layers.12.bn2.bias', 'module.encoder.layers.12.bn2.running_mean', 'module.encoder.layers.12.bn2.running_var', 'module.encoder.layers.13.conv3x1_1.weight', 'module.encoder.layers.13.conv3x1_1.bias', 'module.encoder.layers.13.conv1x3_1.weight', 'module.encoder.layers.13.conv1x3_1.bias', 'module.encoder.layers.13.conv3x1_2.weight', 'module.encoder.layers.13.conv3x1_2.bias', 'module.encoder.layers.13.conv1x3_2.weight', 'module.encoder.layers.13.conv1x3_2.bias', 'module.encoder.layers.13.bn1.weight', 'module.encoder.layers.13.bn1.bias', 'module.encoder.layers.13.bn1.running_mean', 'module.encoder.layers.13.bn1.running_var', 'module.encoder.layers.13.bn2.weight', 'module.encoder.layers.13.bn2.bias', 'module.encoder.layers.13.bn2.running_mean', 'module.encoder.layers.13.bn2.running_var', 'module.encoder.layers.14.conv3x1_1.weight', 'module.encoder.layers.14.conv3x1_1.bias', 'module.encoder.layers.14.conv1x3_1.weight', 'module.encoder.layers.14.conv1x3_1.bias', 'module.encoder.layers.14.conv3x1_2.weight', 'module.encoder.layers.14.conv3x1_2.bias', 'module.encoder.layers.14.conv1x3_2.weight', 'module.encoder.layers.14.conv1x3_2.bias', 'module.encoder.layers.14.bn1.weight', 'module.encoder.layers.14.bn1.bias', 'module.encoder.layers.14.bn1.running_mean', 'module.encoder.layers.14.bn1.running_var', 'module.encoder.layers.14.bn2.weight', 'module.encoder.layers.14.bn2.bias', 'module.encoder.layers.14.bn2.running_mean', 'module.encoder.layers.14.bn2.running_var', 'module.decoder.layers.0.conv.weight', 'module.decoder.layers.0.conv.bias', 'module.decoder.layers.0.bn.weight', 'module.decoder.layers.0.bn.bias', 'module.decoder.layers.0.bn.running_mean', 'module.decoder.layers.0.bn.running_var', 'module.decoder.layers.1.conv3x1_1.weight', 'module.decoder.layers.1.conv3x1_1.bias', 'module.decoder.layers.1.conv1x3_1.weight', 'module.decoder.layers.1.conv1x3_1.bias', 'module.decoder.layers.1.bn1.weight', 'module.decoder.layers.1.bn1.bias', 'module.decoder.layers.1.bn1.running_mean', 'module.decoder.layers.1.bn1.running_var', 'module.decoder.layers.1.conv3x1_2.weight', 'module.decoder.layers.1.conv3x1_2.bias', 'module.decoder.layers.1.conv1x3_2.weight', 'module.decoder.layers.1.conv1x3_2.bias', 'module.decoder.layers.1.bn2.weight', 'module.decoder.layers.1.bn2.bias', 'module.decoder.layers.1.bn2.running_mean', 'module.decoder.layers.1.bn2.running_var', 'module.decoder.layers.2.conv3x1_1.weight', 'module.decoder.layers.2.conv3x1_1.bias', 'module.decoder.layers.2.conv1x3_1.weight', 'module.decoder.layers.2.conv1x3_1.bias', 'module.decoder.layers.2.bn1.weight', 'module.decoder.layers.2.bn1.bias', 'module.decoder.layers.2.bn1.running_mean', 'module.decoder.layers.2.bn1.running_var', 'module.decoder.layers.2.conv3x1_2.weight', 'module.decoder.layers.2.conv3x1_2.bias', 'module.decoder.layers.2.conv1x3_2.weight', 'module.decoder.layers.2.conv1x3_2.bias', 'module.decoder.layers.2.bn2.weight', 'module.decoder.layers.2.bn2.bias', 'module.decoder.layers.2.bn2.running_mean', 'module.decoder.layers.2.bn2.running_var', 'module.decoder.layers.3.conv.weight', 'module.decoder.layers.3.conv.bias', 'module.decoder.layers.3.bn.weight', 'module.decoder.layers.3.bn.bias', 'module.decoder.layers.3.bn.running_mean', 'module.decoder.layers.3.bn.running_var', 'module.decoder.layers.4.conv3x1_1.weight', 'module.decoder.layers.4.conv3x1_1.bias', 'module.decoder.layers.4.conv1x3_1.weight', 'module.decoder.layers.4.conv1x3_1.bias', 'module.decoder.layers.4.bn1.weight', 'module.decoder.layers.4.bn1.bias', 'module.decoder.layers.4.bn1.running_mean', 'module.decoder.layers.4.bn1.running_var', 'module.decoder.layers.4.conv3x1_2.weight', 'module.decoder.layers.4.conv3x1_2.bias', 'module.decoder.layers.4.conv1x3_2.weight', 'module.decoder.layers.4.conv1x3_2.bias', 'module.decoder.layers.4.bn2.weight', 'module.decoder.layers.4.bn2.bias', 'module.decoder.layers.4.bn2.running_mean', 'module.decoder.layers.4.bn2.running_var', 'module.decoder.layers.5.conv3x1_1.weight', 'module.decoder.layers.5.conv3x1_1.bias', 'module.decoder.layers.5.conv1x3_1.weight', 'module.decoder.layers.5.conv1x3_1.bias', 'module.decoder.layers.5.bn1.weight', 'module.decoder.layers.5.bn1.bias', 'module.decoder.layers.5.bn1.running_mean', 'module.decoder.layers.5.bn1.running_var', 'module.decoder.layers.5.conv3x1_2.weight', 'module.decoder.layers.5.conv3x1_2.bias', 'module.decoder.layers.5.conv1x3_2.weight', 'module.decoder.layers.5.conv1x3_2.bias', 'module.decoder.layers.5.bn2.weight', 'module.decoder.layers.5.bn2.bias', 'module.decoder.layers.5.bn2.running_mean', 'module.decoder.layers.5.bn2.running_var', 'module.decoder.output_conv.weight', 'module.decoder.output_conv.bias'])\n",
            "odict_keys(['encoder.initial_block.conv.weight', 'encoder.initial_block.conv.bias', 'encoder.initial_block.bn.weight', 'encoder.initial_block.bn.bias', 'encoder.initial_block.bn.running_mean', 'encoder.initial_block.bn.running_var', 'encoder.initial_block.bn.num_batches_tracked', 'encoder.layers.0.conv.weight', 'encoder.layers.0.conv.bias', 'encoder.layers.0.bn.weight', 'encoder.layers.0.bn.bias', 'encoder.layers.0.bn.running_mean', 'encoder.layers.0.bn.running_var', 'encoder.layers.0.bn.num_batches_tracked', 'encoder.layers.1.conv3x1_1.weight', 'encoder.layers.1.conv3x1_1.bias', 'encoder.layers.1.conv1x3_1.weight', 'encoder.layers.1.conv1x3_1.bias', 'encoder.layers.1.bn1.weight', 'encoder.layers.1.bn1.bias', 'encoder.layers.1.bn1.running_mean', 'encoder.layers.1.bn1.running_var', 'encoder.layers.1.bn1.num_batches_tracked', 'encoder.layers.1.conv3x1_2.weight', 'encoder.layers.1.conv3x1_2.bias', 'encoder.layers.1.conv1x3_2.weight', 'encoder.layers.1.conv1x3_2.bias', 'encoder.layers.1.bn2.weight', 'encoder.layers.1.bn2.bias', 'encoder.layers.1.bn2.running_mean', 'encoder.layers.1.bn2.running_var', 'encoder.layers.1.bn2.num_batches_tracked', 'encoder.layers.2.conv3x1_1.weight', 'encoder.layers.2.conv3x1_1.bias', 'encoder.layers.2.conv1x3_1.weight', 'encoder.layers.2.conv1x3_1.bias', 'encoder.layers.2.bn1.weight', 'encoder.layers.2.bn1.bias', 'encoder.layers.2.bn1.running_mean', 'encoder.layers.2.bn1.running_var', 'encoder.layers.2.bn1.num_batches_tracked', 'encoder.layers.2.conv3x1_2.weight', 'encoder.layers.2.conv3x1_2.bias', 'encoder.layers.2.conv1x3_2.weight', 'encoder.layers.2.conv1x3_2.bias', 'encoder.layers.2.bn2.weight', 'encoder.layers.2.bn2.bias', 'encoder.layers.2.bn2.running_mean', 'encoder.layers.2.bn2.running_var', 'encoder.layers.2.bn2.num_batches_tracked', 'encoder.layers.3.conv3x1_1.weight', 'encoder.layers.3.conv3x1_1.bias', 'encoder.layers.3.conv1x3_1.weight', 'encoder.layers.3.conv1x3_1.bias', 'encoder.layers.3.bn1.weight', 'encoder.layers.3.bn1.bias', 'encoder.layers.3.bn1.running_mean', 'encoder.layers.3.bn1.running_var', 'encoder.layers.3.bn1.num_batches_tracked', 'encoder.layers.3.conv3x1_2.weight', 'encoder.layers.3.conv3x1_2.bias', 'encoder.layers.3.conv1x3_2.weight', 'encoder.layers.3.conv1x3_2.bias', 'encoder.layers.3.bn2.weight', 'encoder.layers.3.bn2.bias', 'encoder.layers.3.bn2.running_mean', 'encoder.layers.3.bn2.running_var', 'encoder.layers.3.bn2.num_batches_tracked', 'encoder.layers.4.conv3x1_1.weight', 'encoder.layers.4.conv3x1_1.bias', 'encoder.layers.4.conv1x3_1.weight', 'encoder.layers.4.conv1x3_1.bias', 'encoder.layers.4.bn1.weight', 'encoder.layers.4.bn1.bias', 'encoder.layers.4.bn1.running_mean', 'encoder.layers.4.bn1.running_var', 'encoder.layers.4.bn1.num_batches_tracked', 'encoder.layers.4.conv3x1_2.weight', 'encoder.layers.4.conv3x1_2.bias', 'encoder.layers.4.conv1x3_2.weight', 'encoder.layers.4.conv1x3_2.bias', 'encoder.layers.4.bn2.weight', 'encoder.layers.4.bn2.bias', 'encoder.layers.4.bn2.running_mean', 'encoder.layers.4.bn2.running_var', 'encoder.layers.4.bn2.num_batches_tracked', 'encoder.layers.5.conv3x1_1.weight', 'encoder.layers.5.conv3x1_1.bias', 'encoder.layers.5.conv1x3_1.weight', 'encoder.layers.5.conv1x3_1.bias', 'encoder.layers.5.bn1.weight', 'encoder.layers.5.bn1.bias', 'encoder.layers.5.bn1.running_mean', 'encoder.layers.5.bn1.running_var', 'encoder.layers.5.bn1.num_batches_tracked', 'encoder.layers.5.conv3x1_2.weight', 'encoder.layers.5.conv3x1_2.bias', 'encoder.layers.5.conv1x3_2.weight', 'encoder.layers.5.conv1x3_2.bias', 'encoder.layers.5.bn2.weight', 'encoder.layers.5.bn2.bias', 'encoder.layers.5.bn2.running_mean', 'encoder.layers.5.bn2.running_var', 'encoder.layers.5.bn2.num_batches_tracked', 'encoder.layers.6.conv.weight', 'encoder.layers.6.conv.bias', 'encoder.layers.6.bn.weight', 'encoder.layers.6.bn.bias', 'encoder.layers.6.bn.running_mean', 'encoder.layers.6.bn.running_var', 'encoder.layers.6.bn.num_batches_tracked', 'encoder.layers.7.conv3x1_1.weight', 'encoder.layers.7.conv3x1_1.bias', 'encoder.layers.7.conv1x3_1.weight', 'encoder.layers.7.conv1x3_1.bias', 'encoder.layers.7.bn1.weight', 'encoder.layers.7.bn1.bias', 'encoder.layers.7.bn1.running_mean', 'encoder.layers.7.bn1.running_var', 'encoder.layers.7.bn1.num_batches_tracked', 'encoder.layers.7.conv3x1_2.weight', 'encoder.layers.7.conv3x1_2.bias', 'encoder.layers.7.conv1x3_2.weight', 'encoder.layers.7.conv1x3_2.bias', 'encoder.layers.7.bn2.weight', 'encoder.layers.7.bn2.bias', 'encoder.layers.7.bn2.running_mean', 'encoder.layers.7.bn2.running_var', 'encoder.layers.7.bn2.num_batches_tracked', 'encoder.layers.8.conv3x1_1.weight', 'encoder.layers.8.conv3x1_1.bias', 'encoder.layers.8.conv1x3_1.weight', 'encoder.layers.8.conv1x3_1.bias', 'encoder.layers.8.bn1.weight', 'encoder.layers.8.bn1.bias', 'encoder.layers.8.bn1.running_mean', 'encoder.layers.8.bn1.running_var', 'encoder.layers.8.bn1.num_batches_tracked', 'encoder.layers.8.conv3x1_2.weight', 'encoder.layers.8.conv3x1_2.bias', 'encoder.layers.8.conv1x3_2.weight', 'encoder.layers.8.conv1x3_2.bias', 'encoder.layers.8.bn2.weight', 'encoder.layers.8.bn2.bias', 'encoder.layers.8.bn2.running_mean', 'encoder.layers.8.bn2.running_var', 'encoder.layers.8.bn2.num_batches_tracked', 'encoder.layers.9.conv3x1_1.weight', 'encoder.layers.9.conv3x1_1.bias', 'encoder.layers.9.conv1x3_1.weight', 'encoder.layers.9.conv1x3_1.bias', 'encoder.layers.9.bn1.weight', 'encoder.layers.9.bn1.bias', 'encoder.layers.9.bn1.running_mean', 'encoder.layers.9.bn1.running_var', 'encoder.layers.9.bn1.num_batches_tracked', 'encoder.layers.9.conv3x1_2.weight', 'encoder.layers.9.conv3x1_2.bias', 'encoder.layers.9.conv1x3_2.weight', 'encoder.layers.9.conv1x3_2.bias', 'encoder.layers.9.bn2.weight', 'encoder.layers.9.bn2.bias', 'encoder.layers.9.bn2.running_mean', 'encoder.layers.9.bn2.running_var', 'encoder.layers.9.bn2.num_batches_tracked', 'encoder.layers.10.conv3x1_1.weight', 'encoder.layers.10.conv3x1_1.bias', 'encoder.layers.10.conv1x3_1.weight', 'encoder.layers.10.conv1x3_1.bias', 'encoder.layers.10.bn1.weight', 'encoder.layers.10.bn1.bias', 'encoder.layers.10.bn1.running_mean', 'encoder.layers.10.bn1.running_var', 'encoder.layers.10.bn1.num_batches_tracked', 'encoder.layers.10.conv3x1_2.weight', 'encoder.layers.10.conv3x1_2.bias', 'encoder.layers.10.conv1x3_2.weight', 'encoder.layers.10.conv1x3_2.bias', 'encoder.layers.10.bn2.weight', 'encoder.layers.10.bn2.bias', 'encoder.layers.10.bn2.running_mean', 'encoder.layers.10.bn2.running_var', 'encoder.layers.10.bn2.num_batches_tracked', 'encoder.layers.11.conv3x1_1.weight', 'encoder.layers.11.conv3x1_1.bias', 'encoder.layers.11.conv1x3_1.weight', 'encoder.layers.11.conv1x3_1.bias', 'encoder.layers.11.bn1.weight', 'encoder.layers.11.bn1.bias', 'encoder.layers.11.bn1.running_mean', 'encoder.layers.11.bn1.running_var', 'encoder.layers.11.bn1.num_batches_tracked', 'encoder.layers.11.conv3x1_2.weight', 'encoder.layers.11.conv3x1_2.bias', 'encoder.layers.11.conv1x3_2.weight', 'encoder.layers.11.conv1x3_2.bias', 'encoder.layers.11.bn2.weight', 'encoder.layers.11.bn2.bias', 'encoder.layers.11.bn2.running_mean', 'encoder.layers.11.bn2.running_var', 'encoder.layers.11.bn2.num_batches_tracked', 'encoder.layers.12.conv3x1_1.weight', 'encoder.layers.12.conv3x1_1.bias', 'encoder.layers.12.conv1x3_1.weight', 'encoder.layers.12.conv1x3_1.bias', 'encoder.layers.12.bn1.weight', 'encoder.layers.12.bn1.bias', 'encoder.layers.12.bn1.running_mean', 'encoder.layers.12.bn1.running_var', 'encoder.layers.12.bn1.num_batches_tracked', 'encoder.layers.12.conv3x1_2.weight', 'encoder.layers.12.conv3x1_2.bias', 'encoder.layers.12.conv1x3_2.weight', 'encoder.layers.12.conv1x3_2.bias', 'encoder.layers.12.bn2.weight', 'encoder.layers.12.bn2.bias', 'encoder.layers.12.bn2.running_mean', 'encoder.layers.12.bn2.running_var', 'encoder.layers.12.bn2.num_batches_tracked', 'encoder.layers.13.conv3x1_1.weight', 'encoder.layers.13.conv3x1_1.bias', 'encoder.layers.13.conv1x3_1.weight', 'encoder.layers.13.conv1x3_1.bias', 'encoder.layers.13.bn1.weight', 'encoder.layers.13.bn1.bias', 'encoder.layers.13.bn1.running_mean', 'encoder.layers.13.bn1.running_var', 'encoder.layers.13.bn1.num_batches_tracked', 'encoder.layers.13.conv3x1_2.weight', 'encoder.layers.13.conv3x1_2.bias', 'encoder.layers.13.conv1x3_2.weight', 'encoder.layers.13.conv1x3_2.bias', 'encoder.layers.13.bn2.weight', 'encoder.layers.13.bn2.bias', 'encoder.layers.13.bn2.running_mean', 'encoder.layers.13.bn2.running_var', 'encoder.layers.13.bn2.num_batches_tracked', 'encoder.layers.14.conv3x1_1.weight', 'encoder.layers.14.conv3x1_1.bias', 'encoder.layers.14.conv1x3_1.weight', 'encoder.layers.14.conv1x3_1.bias', 'encoder.layers.14.bn1.weight', 'encoder.layers.14.bn1.bias', 'encoder.layers.14.bn1.running_mean', 'encoder.layers.14.bn1.running_var', 'encoder.layers.14.bn1.num_batches_tracked', 'encoder.layers.14.conv3x1_2.weight', 'encoder.layers.14.conv3x1_2.bias', 'encoder.layers.14.conv1x3_2.weight', 'encoder.layers.14.conv1x3_2.bias', 'encoder.layers.14.bn2.weight', 'encoder.layers.14.bn2.bias', 'encoder.layers.14.bn2.running_mean', 'encoder.layers.14.bn2.running_var', 'encoder.layers.14.bn2.num_batches_tracked', 'encoder.output_conv.weight', 'encoder.output_conv.bias', 'decoder.loss_first_part.prototypes', 'decoder.loss_first_part.distance_scale', 'decoder.layers.0.conv.weight', 'decoder.layers.0.conv.bias', 'decoder.layers.0.bn.weight', 'decoder.layers.0.bn.bias', 'decoder.layers.0.bn.running_mean', 'decoder.layers.0.bn.running_var', 'decoder.layers.0.bn.num_batches_tracked', 'decoder.layers.1.conv3x1_1.weight', 'decoder.layers.1.conv3x1_1.bias', 'decoder.layers.1.conv1x3_1.weight', 'decoder.layers.1.conv1x3_1.bias', 'decoder.layers.1.bn1.weight', 'decoder.layers.1.bn1.bias', 'decoder.layers.1.bn1.running_mean', 'decoder.layers.1.bn1.running_var', 'decoder.layers.1.bn1.num_batches_tracked', 'decoder.layers.1.conv3x1_2.weight', 'decoder.layers.1.conv3x1_2.bias', 'decoder.layers.1.conv1x3_2.weight', 'decoder.layers.1.conv1x3_2.bias', 'decoder.layers.1.bn2.weight', 'decoder.layers.1.bn2.bias', 'decoder.layers.1.bn2.running_mean', 'decoder.layers.1.bn2.running_var', 'decoder.layers.1.bn2.num_batches_tracked', 'decoder.layers.2.conv3x1_1.weight', 'decoder.layers.2.conv3x1_1.bias', 'decoder.layers.2.conv1x3_1.weight', 'decoder.layers.2.conv1x3_1.bias', 'decoder.layers.2.bn1.weight', 'decoder.layers.2.bn1.bias', 'decoder.layers.2.bn1.running_mean', 'decoder.layers.2.bn1.running_var', 'decoder.layers.2.bn1.num_batches_tracked', 'decoder.layers.2.conv3x1_2.weight', 'decoder.layers.2.conv3x1_2.bias', 'decoder.layers.2.conv1x3_2.weight', 'decoder.layers.2.conv1x3_2.bias', 'decoder.layers.2.bn2.weight', 'decoder.layers.2.bn2.bias', 'decoder.layers.2.bn2.running_mean', 'decoder.layers.2.bn2.running_var', 'decoder.layers.2.bn2.num_batches_tracked', 'decoder.layers.3.conv.weight', 'decoder.layers.3.conv.bias', 'decoder.layers.3.bn.weight', 'decoder.layers.3.bn.bias', 'decoder.layers.3.bn.running_mean', 'decoder.layers.3.bn.running_var', 'decoder.layers.3.bn.num_batches_tracked', 'decoder.layers.4.conv3x1_1.weight', 'decoder.layers.4.conv3x1_1.bias', 'decoder.layers.4.conv1x3_1.weight', 'decoder.layers.4.conv1x3_1.bias', 'decoder.layers.4.bn1.weight', 'decoder.layers.4.bn1.bias', 'decoder.layers.4.bn1.running_mean', 'decoder.layers.4.bn1.running_var', 'decoder.layers.4.bn1.num_batches_tracked', 'decoder.layers.4.conv3x1_2.weight', 'decoder.layers.4.conv3x1_2.bias', 'decoder.layers.4.conv1x3_2.weight', 'decoder.layers.4.conv1x3_2.bias', 'decoder.layers.4.bn2.weight', 'decoder.layers.4.bn2.bias', 'decoder.layers.4.bn2.running_mean', 'decoder.layers.4.bn2.running_var', 'decoder.layers.4.bn2.num_batches_tracked', 'decoder.layers.5.conv3x1_1.weight', 'decoder.layers.5.conv3x1_1.bias', 'decoder.layers.5.conv1x3_1.weight', 'decoder.layers.5.conv1x3_1.bias', 'decoder.layers.5.bn1.weight', 'decoder.layers.5.bn1.bias', 'decoder.layers.5.bn1.running_mean', 'decoder.layers.5.bn1.running_var', 'decoder.layers.5.bn1.num_batches_tracked', 'decoder.layers.5.conv3x1_2.weight', 'decoder.layers.5.conv3x1_2.bias', 'decoder.layers.5.conv1x3_2.weight', 'decoder.layers.5.conv1x3_2.bias', 'decoder.layers.5.bn2.weight', 'decoder.layers.5.bn2.bias', 'decoder.layers.5.bn2.running_mean', 'decoder.layers.5.bn2.running_var', 'decoder.layers.5.bn2.num_batches_tracked', 'decoder.output_conv.weight', 'decoder.output_conv.bias'])\n",
            "Size mismatch for decoder.output_conv.weight: torch.Size([16, 16, 1, 1]) vs torch.Size([16, 20, 2, 2])\n",
            "Size mismatch for decoder.output_conv.bias: torch.Size([16]) vs torch.Size([20])\n",
            "Import Model erfnet_isomaxplus with weights erfnet_pretrained.pth to FineTune\n",
            "========== TRAINING ===========\n",
            "/content/cityscapes/leftImg8bit/train\n",
            "/content/cityscapes/leftImg8bit/val\n",
            "----- TRAINING - EPOCH 1 -----\n",
            "LEARNING RATE:  5e-05\n",
            "loss: 5.576 (epoch: 1, step: 0) // Avg time/img: 0.9179 s\n",
            "loss: 5.586 (epoch: 1, step: 50) // Avg time/img: 0.1128 s\n",
            "loss: 5.534 (epoch: 1, step: 100) // Avg time/img: 0.1064 s\n",
            "loss: 5.48 (epoch: 1, step: 150) // Avg time/img: 0.1047 s\n",
            "loss: 5.419 (epoch: 1, step: 200) // Avg time/img: 0.1034 s\n",
            "loss: 5.359 (epoch: 1, step: 250) // Avg time/img: 0.1024 s\n",
            "loss: 5.297 (epoch: 1, step: 300) // Avg time/img: 0.1016 s\n",
            "loss: 5.23 (epoch: 1, step: 350) // Avg time/img: 0.1013 s\n",
            "loss: 5.163 (epoch: 1, step: 400) // Avg time/img: 0.1010 s\n",
            "loss: 5.099 (epoch: 1, step: 450) // Avg time/img: 0.1011 s\n",
            "----- VALIDATING - EPOCH 1 -----\n",
            "VAL loss: 4.168 (epoch: 1, step: 0) // Avg time/img: 0.0568 s\n",
            "VAL loss: 4.233 (epoch: 1, step: 50) // Avg time/img: 0.0409 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m0.96\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model-001.pth (epoch: 1)\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model_best.pth (epoch: 1)\n",
            "----- TRAINING - EPOCH 2 -----\n",
            "LEARNING RATE:  4.774426908107499e-05\n",
            "loss: 4.423 (epoch: 2, step: 0) // Avg time/img: 0.1619 s\n",
            "loss: 4.21 (epoch: 2, step: 50) // Avg time/img: 0.1019 s\n",
            "loss: 4.136 (epoch: 2, step: 100) // Avg time/img: 0.1017 s\n",
            "loss: 4.054 (epoch: 2, step: 150) // Avg time/img: 0.1008 s\n",
            "loss: 3.969 (epoch: 2, step: 200) // Avg time/img: 0.1006 s\n",
            "loss: 3.89 (epoch: 2, step: 250) // Avg time/img: 0.1004 s\n",
            "loss: 3.802 (epoch: 2, step: 300) // Avg time/img: 0.1003 s\n",
            "loss: 3.711 (epoch: 2, step: 350) // Avg time/img: 0.1002 s\n",
            "loss: 3.626 (epoch: 2, step: 400) // Avg time/img: 0.1002 s\n",
            "loss: 3.542 (epoch: 2, step: 450) // Avg time/img: 0.1001 s\n",
            "----- VALIDATING - EPOCH 2 -----\n",
            "VAL loss: 2.387 (epoch: 2, step: 0) // Avg time/img: 0.0572 s\n",
            "VAL loss: 2.624 (epoch: 2, step: 50) // Avg time/img: 0.0408 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m10.39\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model-002.pth (epoch: 2)\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model_best.pth (epoch: 2)\n",
            "----- TRAINING - EPOCH 3 -----\n",
            "LEARNING RATE:  4.547662880414811e-05\n",
            "loss: 2.496 (epoch: 3, step: 0) // Avg time/img: 0.1282 s\n",
            "loss: 2.549 (epoch: 3, step: 50) // Avg time/img: 0.1002 s\n",
            "loss: 2.489 (epoch: 3, step: 100) // Avg time/img: 0.1003 s\n",
            "loss: 2.427 (epoch: 3, step: 150) // Avg time/img: 0.1003 s\n",
            "loss: 2.375 (epoch: 3, step: 200) // Avg time/img: 0.1007 s\n",
            "loss: 2.322 (epoch: 3, step: 250) // Avg time/img: 0.1005 s\n",
            "loss: 2.276 (epoch: 3, step: 300) // Avg time/img: 0.1004 s\n",
            "loss: 2.227 (epoch: 3, step: 350) // Avg time/img: 0.1004 s\n",
            "loss: 2.183 (epoch: 3, step: 400) // Avg time/img: 0.1003 s\n",
            "loss: 2.141 (epoch: 3, step: 450) // Avg time/img: 0.1006 s\n",
            "----- VALIDATING - EPOCH 3 -----\n",
            "VAL loss: 1.487 (epoch: 3, step: 0) // Avg time/img: 0.0634 s\n",
            "VAL loss: 1.772 (epoch: 3, step: 50) // Avg time/img: 0.0410 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m17.12\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model-003.pth (epoch: 3)\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model_best.pth (epoch: 3)\n",
            "----- TRAINING - EPOCH 4 -----\n",
            "LEARNING RATE:  4.319634861514096e-05\n",
            "loss: 1.764 (epoch: 4, step: 0) // Avg time/img: 0.1183 s\n",
            "loss: 1.673 (epoch: 4, step: 50) // Avg time/img: 0.1016 s\n",
            "loss: 1.66 (epoch: 4, step: 100) // Avg time/img: 0.1000 s\n",
            "loss: 1.638 (epoch: 4, step: 150) // Avg time/img: 0.1008 s\n",
            "loss: 1.616 (epoch: 4, step: 200) // Avg time/img: 0.1008 s\n",
            "loss: 1.595 (epoch: 4, step: 250) // Avg time/img: 0.1004 s\n",
            "loss: 1.576 (epoch: 4, step: 300) // Avg time/img: 0.1003 s\n",
            "loss: 1.56 (epoch: 4, step: 350) // Avg time/img: 0.1005 s\n",
            "loss: 1.544 (epoch: 4, step: 400) // Avg time/img: 0.1004 s\n",
            "loss: 1.526 (epoch: 4, step: 450) // Avg time/img: 0.1004 s\n",
            "----- VALIDATING - EPOCH 4 -----\n",
            "VAL loss: 1.168 (epoch: 4, step: 0) // Avg time/img: 0.0532 s\n",
            "VAL loss: 1.435 (epoch: 4, step: 50) // Avg time/img: 0.0418 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m21.19\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model-004.pth (epoch: 4)\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model_best.pth (epoch: 4)\n",
            "----- TRAINING - EPOCH 5 -----\n",
            "LEARNING RATE:  4.090260730254292e-05\n",
            "loss: 1.727 (epoch: 5, step: 0) // Avg time/img: 0.1210 s\n",
            "loss: 1.336 (epoch: 5, step: 50) // Avg time/img: 0.0972 s\n",
            "loss: 1.317 (epoch: 5, step: 100) // Avg time/img: 0.0992 s\n",
            "loss: 1.31 (epoch: 5, step: 150) // Avg time/img: 0.0991 s\n",
            "loss: 1.308 (epoch: 5, step: 200) // Avg time/img: 0.0986 s\n",
            "loss: 1.299 (epoch: 5, step: 250) // Avg time/img: 0.0988 s\n",
            "loss: 1.292 (epoch: 5, step: 300) // Avg time/img: 0.0992 s\n",
            "loss: 1.278 (epoch: 5, step: 350) // Avg time/img: 0.0989 s\n",
            "loss: 1.267 (epoch: 5, step: 400) // Avg time/img: 0.0990 s\n",
            "loss: 1.259 (epoch: 5, step: 450) // Avg time/img: 0.0991 s\n",
            "----- VALIDATING - EPOCH 5 -----\n",
            "VAL loss: 1.028 (epoch: 5, step: 0) // Avg time/img: 0.0566 s\n",
            "VAL loss: 1.289 (epoch: 5, step: 50) // Avg time/img: 0.0408 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m25.61\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model-005.pth (epoch: 5)\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model_best.pth (epoch: 5)\n",
            "----- TRAINING - EPOCH 6 -----\n",
            "LEARNING RATE:  3.859447533617852e-05\n",
            "loss: 1.374 (epoch: 6, step: 0) // Avg time/img: 0.1099 s\n",
            "loss: 1.158 (epoch: 6, step: 50) // Avg time/img: 0.1007 s\n",
            "loss: 1.155 (epoch: 6, step: 100) // Avg time/img: 0.1004 s\n",
            "loss: 1.14 (epoch: 6, step: 150) // Avg time/img: 0.0999 s\n",
            "loss: 1.137 (epoch: 6, step: 200) // Avg time/img: 0.1002 s\n",
            "loss: 1.129 (epoch: 6, step: 250) // Avg time/img: 0.1006 s\n",
            "loss: 1.123 (epoch: 6, step: 300) // Avg time/img: 0.1000 s\n",
            "loss: 1.118 (epoch: 6, step: 350) // Avg time/img: 0.1001 s\n",
            "loss: 1.115 (epoch: 6, step: 400) // Avg time/img: 0.0999 s\n",
            "loss: 1.116 (epoch: 6, step: 450) // Avg time/img: 0.1001 s\n",
            "----- VALIDATING - EPOCH 6 -----\n",
            "VAL loss: 0.9108 (epoch: 6, step: 0) // Avg time/img: 0.0563 s\n",
            "VAL loss: 1.166 (epoch: 6, step: 50) // Avg time/img: 0.0408 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m26.34\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model-006.pth (epoch: 6)\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model_best.pth (epoch: 6)\n",
            "----- TRAINING - EPOCH 7 -----\n",
            "LEARNING RATE:  3.6270892346861e-05\n",
            "loss: 0.9215 (epoch: 7, step: 0) // Avg time/img: 0.1302 s\n",
            "loss: 1.05 (epoch: 7, step: 50) // Avg time/img: 0.1024 s\n",
            "loss: 1.031 (epoch: 7, step: 100) // Avg time/img: 0.1012 s\n",
            "loss: 1.051 (epoch: 7, step: 150) // Avg time/img: 0.1014 s\n",
            "loss: 1.044 (epoch: 7, step: 200) // Avg time/img: 0.1004 s\n",
            "loss: 1.041 (epoch: 7, step: 250) // Avg time/img: 0.0997 s\n",
            "loss: 1.038 (epoch: 7, step: 300) // Avg time/img: 0.1000 s\n",
            "loss: 1.033 (epoch: 7, step: 350) // Avg time/img: 0.1001 s\n",
            "loss: 1.03 (epoch: 7, step: 400) // Avg time/img: 0.1001 s\n",
            "loss: 1.025 (epoch: 7, step: 450) // Avg time/img: 0.1001 s\n",
            "----- VALIDATING - EPOCH 7 -----\n",
            "VAL loss: 0.8476 (epoch: 7, step: 0) // Avg time/img: 0.0535 s\n",
            "VAL loss: 1.097 (epoch: 7, step: 50) // Avg time/img: 0.0409 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m26.80\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model-007.pth (epoch: 7)\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model_best.pth (epoch: 7)\n",
            "----- TRAINING - EPOCH 8 -----\n",
            "LEARNING RATE:  3.393063796290625e-05\n",
            "loss: 1.027 (epoch: 8, step: 0) // Avg time/img: 0.1106 s\n",
            "loss: 0.9933 (epoch: 8, step: 50) // Avg time/img: 0.0983 s\n",
            "loss: 0.9976 (epoch: 8, step: 100) // Avg time/img: 0.0988 s\n",
            "loss: 0.9948 (epoch: 8, step: 150) // Avg time/img: 0.0992 s\n",
            "loss: 0.9849 (epoch: 8, step: 200) // Avg time/img: 0.0994 s\n",
            "loss: 0.9822 (epoch: 8, step: 250) // Avg time/img: 0.0996 s\n",
            "loss: 0.9702 (epoch: 8, step: 300) // Avg time/img: 0.0998 s\n",
            "loss: 0.9649 (epoch: 8, step: 350) // Avg time/img: 0.1012 s\n",
            "loss: 0.9641 (epoch: 8, step: 400) // Avg time/img: 0.1033 s\n",
            "loss: 0.964 (epoch: 8, step: 450) // Avg time/img: 0.1041 s\n",
            "----- VALIDATING - EPOCH 8 -----\n",
            "VAL loss: 0.8218 (epoch: 8, step: 0) // Avg time/img: 0.0525 s\n",
            "VAL loss: 1.061 (epoch: 8, step: 50) // Avg time/img: 0.0407 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m27.13\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model-008.pth (epoch: 8)\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model_best.pth (epoch: 8)\n",
            "----- TRAINING - EPOCH 9 -----\n",
            "LEARNING RATE:  3.157229337446777e-05\n",
            "loss: 0.7967 (epoch: 9, step: 0) // Avg time/img: 0.1161 s\n",
            "loss: 0.9008 (epoch: 9, step: 50) // Avg time/img: 0.1029 s\n",
            "loss: 0.9177 (epoch: 9, step: 100) // Avg time/img: 0.1011 s\n",
            "loss: 0.9207 (epoch: 9, step: 150) // Avg time/img: 0.1009 s\n",
            "loss: 0.9166 (epoch: 9, step: 200) // Avg time/img: 0.1007 s\n",
            "loss: 0.9207 (epoch: 9, step: 250) // Avg time/img: 0.1000 s\n",
            "loss: 0.9152 (epoch: 9, step: 300) // Avg time/img: 0.0998 s\n",
            "loss: 0.912 (epoch: 9, step: 350) // Avg time/img: 0.0997 s\n",
            "loss: 0.9099 (epoch: 9, step: 400) // Avg time/img: 0.0994 s\n",
            "loss: 0.9102 (epoch: 9, step: 450) // Avg time/img: 0.0997 s\n",
            "----- VALIDATING - EPOCH 9 -----\n",
            "VAL loss: 0.7639 (epoch: 9, step: 0) // Avg time/img: 0.0544 s\n",
            "VAL loss: 0.9982 (epoch: 9, step: 50) // Avg time/img: 0.0410 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m27.40\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model-009.pth (epoch: 9)\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model_best.pth (epoch: 9)\n",
            "----- TRAINING - EPOCH 10 -----\n",
            "LEARNING RATE:  2.9194189645999014e-05\n",
            "loss: 1.666 (epoch: 10, step: 0) // Avg time/img: 0.1341 s\n",
            "loss: 0.9275 (epoch: 10, step: 50) // Avg time/img: 0.0993 s\n",
            "loss: 0.8985 (epoch: 10, step: 100) // Avg time/img: 0.1009 s\n",
            "loss: 0.8856 (epoch: 10, step: 150) // Avg time/img: 0.1010 s\n",
            "loss: 0.8876 (epoch: 10, step: 200) // Avg time/img: 0.1005 s\n",
            "loss: 0.8839 (epoch: 10, step: 250) // Avg time/img: 0.1010 s\n",
            "loss: 0.8842 (epoch: 10, step: 300) // Avg time/img: 0.1004 s\n",
            "loss: 0.8797 (epoch: 10, step: 350) // Avg time/img: 0.1004 s\n",
            "loss: 0.8749 (epoch: 10, step: 400) // Avg time/img: 0.1006 s\n",
            "loss: 0.871 (epoch: 10, step: 450) // Avg time/img: 0.1005 s\n",
            "----- VALIDATING - EPOCH 10 -----\n",
            "VAL loss: 0.7415 (epoch: 10, step: 0) // Avg time/img: 0.0573 s\n",
            "VAL loss: 0.974 (epoch: 10, step: 50) // Avg time/img: 0.0408 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m28.07\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model-010.pth (epoch: 10)\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model_best.pth (epoch: 10)\n",
            "----- TRAINING - EPOCH 11 -----\n",
            "LEARNING RATE:  2.679433656340733e-05\n",
            "loss: 0.7937 (epoch: 11, step: 0) // Avg time/img: 0.1178 s\n",
            "loss: 0.8408 (epoch: 11, step: 50) // Avg time/img: 0.1008 s\n",
            "loss: 0.8463 (epoch: 11, step: 100) // Avg time/img: 0.0996 s\n",
            "loss: 0.8484 (epoch: 11, step: 150) // Avg time/img: 0.0997 s\n",
            "loss: 0.8498 (epoch: 11, step: 200) // Avg time/img: 0.1001 s\n",
            "loss: 0.8522 (epoch: 11, step: 250) // Avg time/img: 0.0999 s\n",
            "loss: 0.8483 (epoch: 11, step: 300) // Avg time/img: 0.0999 s\n",
            "loss: 0.842 (epoch: 11, step: 350) // Avg time/img: 0.1000 s\n",
            "loss: 0.8376 (epoch: 11, step: 400) // Avg time/img: 0.0998 s\n",
            "loss: 0.8368 (epoch: 11, step: 450) // Avg time/img: 0.0999 s\n",
            "----- VALIDATING - EPOCH 11 -----\n",
            "VAL loss: 0.7212 (epoch: 11, step: 0) // Avg time/img: 0.0587 s\n",
            "VAL loss: 0.9473 (epoch: 11, step: 50) // Avg time/img: 0.0415 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m28.53\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model-011.pth (epoch: 11)\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model_best.pth (epoch: 11)\n",
            "----- TRAINING - EPOCH 12 -----\n",
            "LEARNING RATE:  2.437032195894977e-05\n",
            "loss: 0.5979 (epoch: 12, step: 0) // Avg time/img: 0.1156 s\n",
            "loss: 0.8109 (epoch: 12, step: 50) // Avg time/img: 0.0997 s\n",
            "loss: 0.8085 (epoch: 12, step: 100) // Avg time/img: 0.1003 s\n",
            "loss: 0.821 (epoch: 12, step: 150) // Avg time/img: 0.1005 s\n",
            "loss: 0.8137 (epoch: 12, step: 200) // Avg time/img: 0.1000 s\n",
            "loss: 0.8134 (epoch: 12, step: 250) // Avg time/img: 0.1003 s\n",
            "loss: 0.8116 (epoch: 12, step: 300) // Avg time/img: 0.1001 s\n",
            "loss: 0.8096 (epoch: 12, step: 350) // Avg time/img: 0.0999 s\n",
            "loss: 0.8098 (epoch: 12, step: 400) // Avg time/img: 0.1001 s\n",
            "loss: 0.8084 (epoch: 12, step: 450) // Avg time/img: 0.1001 s\n",
            "----- VALIDATING - EPOCH 12 -----\n",
            "VAL loss: 0.6999 (epoch: 12, step: 0) // Avg time/img: 0.0562 s\n",
            "VAL loss: 0.9212 (epoch: 12, step: 50) // Avg time/img: 0.0416 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m28.96\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model-012.pth (epoch: 12)\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model_best.pth (epoch: 12)\n",
            "----- TRAINING - EPOCH 13 -----\n",
            "LEARNING RATE:  2.191916452770435e-05\n",
            "loss: 0.6637 (epoch: 13, step: 0) // Avg time/img: 0.1687 s\n",
            "loss: 0.7712 (epoch: 13, step: 50) // Avg time/img: 0.1013 s\n",
            "loss: 0.7714 (epoch: 13, step: 100) // Avg time/img: 0.1004 s\n",
            "loss: 0.7768 (epoch: 13, step: 150) // Avg time/img: 0.0998 s\n",
            "loss: 0.7749 (epoch: 13, step: 200) // Avg time/img: 0.1004 s\n",
            "loss: 0.7807 (epoch: 13, step: 250) // Avg time/img: 0.1004 s\n",
            "loss: 0.7808 (epoch: 13, step: 300) // Avg time/img: 0.1001 s\n",
            "loss: 0.7807 (epoch: 13, step: 350) // Avg time/img: 0.1004 s\n",
            "loss: 0.7838 (epoch: 13, step: 400) // Avg time/img: 0.1007 s\n",
            "loss: 0.7835 (epoch: 13, step: 450) // Avg time/img: 0.1003 s\n",
            "----- VALIDATING - EPOCH 13 -----\n",
            "VAL loss: 0.6575 (epoch: 13, step: 0) // Avg time/img: 0.0507 s\n",
            "VAL loss: 0.8829 (epoch: 13, step: 50) // Avg time/img: 0.0405 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m29.49\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model-013.pth (epoch: 13)\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model_best.pth (epoch: 13)\n",
            "----- TRAINING - EPOCH 14 -----\n",
            "LEARNING RATE:  1.9437089939938174e-05\n",
            "loss: 0.7319 (epoch: 14, step: 0) // Avg time/img: 0.1149 s\n",
            "loss: 0.7985 (epoch: 14, step: 50) // Avg time/img: 0.1012 s\n",
            "loss: 0.7976 (epoch: 14, step: 100) // Avg time/img: 0.0995 s\n",
            "loss: 0.7778 (epoch: 14, step: 150) // Avg time/img: 0.0994 s\n",
            "loss: 0.7706 (epoch: 14, step: 200) // Avg time/img: 0.0996 s\n",
            "loss: 0.7722 (epoch: 14, step: 250) // Avg time/img: 0.0993 s\n",
            "loss: 0.7691 (epoch: 14, step: 300) // Avg time/img: 0.0997 s\n",
            "loss: 0.7662 (epoch: 14, step: 350) // Avg time/img: 0.0996 s\n",
            "loss: 0.7652 (epoch: 14, step: 400) // Avg time/img: 0.0992 s\n",
            "loss: 0.7667 (epoch: 14, step: 450) // Avg time/img: 0.0994 s\n",
            "----- VALIDATING - EPOCH 14 -----\n",
            "VAL loss: 0.6366 (epoch: 14, step: 0) // Avg time/img: 0.0607 s\n",
            "VAL loss: 0.8532 (epoch: 14, step: 50) // Avg time/img: 0.0406 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m29.97\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model-014.pth (epoch: 14)\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model_best.pth (epoch: 14)\n",
            "----- TRAINING - EPOCH 15 -----\n",
            "LEARNING RATE:  1.6919173095082493e-05\n",
            "loss: 0.7383 (epoch: 15, step: 0) // Avg time/img: 0.1103 s\n",
            "loss: 0.7497 (epoch: 15, step: 50) // Avg time/img: 0.0998 s\n",
            "loss: 0.7387 (epoch: 15, step: 100) // Avg time/img: 0.0993 s\n",
            "loss: 0.742 (epoch: 15, step: 150) // Avg time/img: 0.0994 s\n",
            "loss: 0.7542 (epoch: 15, step: 200) // Avg time/img: 0.0988 s\n",
            "loss: 0.7496 (epoch: 15, step: 250) // Avg time/img: 0.0993 s\n",
            "loss: 0.7487 (epoch: 15, step: 300) // Avg time/img: 0.0997 s\n",
            "loss: 0.7498 (epoch: 15, step: 350) // Avg time/img: 0.0995 s\n",
            "loss: 0.752 (epoch: 15, step: 400) // Avg time/img: 0.0997 s\n",
            "loss: 0.7531 (epoch: 15, step: 450) // Avg time/img: 0.0997 s\n",
            "----- VALIDATING - EPOCH 15 -----\n",
            "VAL loss: 0.6428 (epoch: 15, step: 0) // Avg time/img: 0.0533 s\n",
            "VAL loss: 0.8524 (epoch: 15, step: 50) // Avg time/img: 0.0410 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m30.35\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model-015.pth (epoch: 15)\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model_best.pth (epoch: 15)\n",
            "----- TRAINING - EPOCH 16 -----\n",
            "LEARNING RATE:  1.4358729437462937e-05\n",
            "loss: 0.7049 (epoch: 16, step: 0) // Avg time/img: 0.1315 s\n",
            "loss: 0.7369 (epoch: 16, step: 50) // Avg time/img: 0.1003 s\n",
            "loss: 0.7455 (epoch: 16, step: 100) // Avg time/img: 0.0971 s\n",
            "loss: 0.7326 (epoch: 16, step: 150) // Avg time/img: 0.0974 s\n",
            "loss: 0.7375 (epoch: 16, step: 200) // Avg time/img: 0.0983 s\n",
            "loss: 0.7427 (epoch: 16, step: 250) // Avg time/img: 0.0980 s\n",
            "loss: 0.7392 (epoch: 16, step: 300) // Avg time/img: 0.0982 s\n",
            "loss: 0.7392 (epoch: 16, step: 350) // Avg time/img: 0.0986 s\n",
            "loss: 0.7409 (epoch: 16, step: 400) // Avg time/img: 0.0987 s\n",
            "loss: 0.7408 (epoch: 16, step: 450) // Avg time/img: 0.0992 s\n",
            "----- VALIDATING - EPOCH 16 -----\n",
            "VAL loss: 0.629 (epoch: 16, step: 0) // Avg time/img: 0.0535 s\n",
            "VAL loss: 0.8345 (epoch: 16, step: 50) // Avg time/img: 0.0408 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m30.52\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model-016.pth (epoch: 16)\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model_best.pth (epoch: 16)\n",
            "----- TRAINING - EPOCH 17 -----\n",
            "LEARNING RATE:  1.1746189430880188e-05\n",
            "loss: 0.7328 (epoch: 17, step: 0) // Avg time/img: 0.1164 s\n",
            "loss: 0.7492 (epoch: 17, step: 50) // Avg time/img: 0.1020 s\n",
            "loss: 0.7384 (epoch: 17, step: 100) // Avg time/img: 0.0997 s\n",
            "loss: 0.7329 (epoch: 17, step: 150) // Avg time/img: 0.1005 s\n",
            "loss: 0.7334 (epoch: 17, step: 200) // Avg time/img: 0.1004 s\n",
            "loss: 0.737 (epoch: 17, step: 250) // Avg time/img: 0.1002 s\n",
            "loss: 0.7322 (epoch: 17, step: 300) // Avg time/img: 0.1002 s\n",
            "loss: 0.7369 (epoch: 17, step: 350) // Avg time/img: 0.1001 s\n",
            "loss: 0.7341 (epoch: 17, step: 400) // Avg time/img: 0.1003 s\n",
            "loss: 0.7301 (epoch: 17, step: 450) // Avg time/img: 0.1004 s\n",
            "----- VALIDATING - EPOCH 17 -----\n",
            "VAL loss: 0.6156 (epoch: 17, step: 0) // Avg time/img: 0.0560 s\n",
            "VAL loss: 0.8224 (epoch: 17, step: 50) // Avg time/img: 0.0417 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m30.85\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model-017.pth (epoch: 17)\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model_best.pth (epoch: 17)\n",
            "----- TRAINING - EPOCH 18 -----\n",
            "LEARNING RATE:  9.066760365683729e-06\n",
            "loss: 0.84 (epoch: 18, step: 0) // Avg time/img: 0.1167 s\n",
            "loss: 0.7256 (epoch: 18, step: 50) // Avg time/img: 0.0985 s\n",
            "loss: 0.7218 (epoch: 18, step: 100) // Avg time/img: 0.1005 s\n",
            "loss: 0.7245 (epoch: 18, step: 150) // Avg time/img: 0.1008 s\n",
            "loss: 0.7244 (epoch: 18, step: 200) // Avg time/img: 0.1011 s\n",
            "loss: 0.7235 (epoch: 18, step: 250) // Avg time/img: 0.1013 s\n",
            "loss: 0.7226 (epoch: 18, step: 300) // Avg time/img: 0.1007 s\n",
            "loss: 0.7242 (epoch: 18, step: 350) // Avg time/img: 0.1005 s\n",
            "loss: 0.7252 (epoch: 18, step: 400) // Avg time/img: 0.1005 s\n",
            "loss: 0.7204 (epoch: 18, step: 450) // Avg time/img: 0.1002 s\n",
            "----- VALIDATING - EPOCH 18 -----\n",
            "VAL loss: 0.6384 (epoch: 18, step: 0) // Avg time/img: 0.0602 s\n",
            "VAL loss: 0.8405 (epoch: 18, step: 50) // Avg time/img: 0.0417 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m31.23\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model-018.pth (epoch: 18)\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model_best.pth (epoch: 18)\n",
            "----- TRAINING - EPOCH 19 -----\n",
            "LEARNING RATE:  6.294627058970836e-06\n",
            "loss: 0.6308 (epoch: 19, step: 0) // Avg time/img: 0.1128 s\n",
            "loss: 0.7237 (epoch: 19, step: 50) // Avg time/img: 0.1001 s\n",
            "loss: 0.7241 (epoch: 19, step: 100) // Avg time/img: 0.1001 s\n",
            "loss: 0.7278 (epoch: 19, step: 150) // Avg time/img: 0.1007 s\n",
            "loss: 0.723 (epoch: 19, step: 200) // Avg time/img: 0.1005 s\n",
            "loss: 0.7205 (epoch: 19, step: 250) // Avg time/img: 0.1002 s\n",
            "loss: 0.7196 (epoch: 19, step: 300) // Avg time/img: 0.1000 s\n",
            "loss: 0.7187 (epoch: 19, step: 350) // Avg time/img: 0.0998 s\n",
            "loss: 0.7167 (epoch: 19, step: 400) // Avg time/img: 0.0995 s\n",
            "loss: 0.7187 (epoch: 19, step: 450) // Avg time/img: 0.0994 s\n",
            "----- VALIDATING - EPOCH 19 -----\n",
            "VAL loss: 0.6233 (epoch: 19, step: 0) // Avg time/img: 0.0624 s\n",
            "VAL loss: 0.8262 (epoch: 19, step: 50) // Avg time/img: 0.0413 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m31.34\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model-019.pth (epoch: 19)\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model_best.pth (epoch: 19)\n",
            "----- TRAINING - EPOCH 20 -----\n",
            "LEARNING RATE:  3.373207119183911e-06\n",
            "loss: 0.6882 (epoch: 20, step: 0) // Avg time/img: 0.1174 s\n",
            "loss: 0.7242 (epoch: 20, step: 50) // Avg time/img: 0.1009 s\n",
            "loss: 0.7215 (epoch: 20, step: 100) // Avg time/img: 0.0983 s\n",
            "loss: 0.7149 (epoch: 20, step: 150) // Avg time/img: 0.0985 s\n",
            "loss: 0.7152 (epoch: 20, step: 200) // Avg time/img: 0.0986 s\n",
            "loss: 0.7144 (epoch: 20, step: 250) // Avg time/img: 0.0988 s\n",
            "loss: 0.7153 (epoch: 20, step: 300) // Avg time/img: 0.0988 s\n",
            "loss: 0.7155 (epoch: 20, step: 350) // Avg time/img: 0.0991 s\n",
            "loss: 0.7154 (epoch: 20, step: 400) // Avg time/img: 0.0993 s\n",
            "loss: 0.7133 (epoch: 20, step: 450) // Avg time/img: 0.0993 s\n",
            "----- VALIDATING - EPOCH 20 -----\n",
            "VAL loss: 0.6154 (epoch: 20, step: 0) // Avg time/img: 0.0518 s\n",
            "VAL loss: 0.8187 (epoch: 20, step: 50) // Avg time/img: 0.0415 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m31.25\u001b[0m %\n",
            "save: ../save/erfnet_training_isomaxplus_loss/model-020.pth (epoch: 20)\n",
            "========== TRAINING FINISHED ===========\n",
            "Model saved in /content/AnomalySegmentation/save/erfnet_training_isomaxplus_loss\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_isomaxplus_loss/ (stored 0%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_isomaxplus_loss/model_best.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_isomaxplus_loss/model-007.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_isomaxplus_loss/model-015.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_isomaxplus_loss/model-008.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_isomaxplus_loss/model-004.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_isomaxplus_loss/model-005.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_isomaxplus_loss/model-002.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_isomaxplus_loss/opts.txt (deflated 38%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_isomaxplus_loss/model.txt (deflated 92%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_isomaxplus_loss/model-014.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_isomaxplus_loss/model-013.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_isomaxplus_loss/model-012.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_isomaxplus_loss/model-016.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_isomaxplus_loss/model-011.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_isomaxplus_loss/checkpoint.pth.tar (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_isomaxplus_loss/model-010.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_isomaxplus_loss/model-020.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_isomaxplus_loss/model_best.pth.tar (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_isomaxplus_loss/model-009.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_isomaxplus_loss/model-003.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_isomaxplus_loss/model-006.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_isomaxplus_loss/automated_log.txt (deflated 63%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_isomaxplus_loss/model-019.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_isomaxplus_loss/model-017.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_isomaxplus_loss/model-018.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_isomaxplus_loss/best.txt (stored 0%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training_isomaxplus_loss/model-001.pth (deflated 10%)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}