{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypo8OBRZ-1p3"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/RonPlusSign/AnomalySegmentation/blob/giorgia/Project6.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUjRrYEW8-uz"
      },
      "source": [
        "# **Anomaly Segmentation Project 6**\n",
        "##*Andrea Delli, Christian Dellisanti, Giorgia Modi*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4x3MajLNeXhX"
      },
      "source": [
        "##**Dataset Preparation**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/AnomalySegmentation"
      ],
      "metadata": {
        "id": "yorO9_xVX2bJ"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_tj8W3BeVPo",
        "outputId": "5e001fc8-778e-408b-9006-33b89f3d4df8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.4/1.4 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m473.6/473.6 kB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for visdom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for typing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=12YJq48XkCxQHjN3CmLc-zM5dThSak4Ta\n",
            "From (redirected): https://drive.google.com/uc?id=12YJq48XkCxQHjN3CmLc-zM5dThSak4Ta&confirm=t&uuid=e82a7219-addf-4e8b-86ef-b2245313afea\n",
            "To: /content/Validation_Dataset.zip\n",
            "100% 329M/329M [00:01<00:00, 196MB/s]\n",
            "Cloning into 'AnomalySegmentation'...\n",
            "remote: Enumerating objects: 693, done.\u001b[K\n",
            "remote: Counting objects: 100% (130/130), done.\u001b[K\n",
            "remote: Compressing objects: 100% (63/63), done.\u001b[K\n",
            "remote: Total 693 (delta 92), reused 100 (delta 67), pack-reused 563 (from 2)\u001b[K\n",
            "Receiving objects: 100% (693/693), 120.70 MiB | 38.19 MiB/s, done.\n",
            "Resolving deltas: 100% (463/463), done.\n",
            "Already up to date.\n"
          ]
        }
      ],
      "source": [
        "!pip  install -q numpy matplotlib Pillow torchvision visdom ood_metrics icecream cityscapesscripts\n",
        "\n",
        "import sys, os\n",
        "if not os.path.isfile('/content/Validation_Dataset.zip'):\n",
        "  !gdown 12YJq48XkCxQHjN3CmLc-zM5dThSak4Ta\n",
        "if not os.path.isdir('/content/Validation_Dataset'):\n",
        "  !unzip -q Validation_Dataset.zip\n",
        "if not os.path.isdir('/content/AnomalySegmentation'):\n",
        "  #!git clone https://github.com/shyam671/AnomalySegmentation_CourseProjectBaseCode.git\n",
        "  #token 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\n",
        "  !git clone -b giorgia https://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@github.com/RonPlusSign/AnomalySegmentation.git\n",
        "!cd /content/AnomalySegmentation && git pull"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAmA1igJhHhR"
      },
      "source": [
        "##**mIoU**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3w0jewGkphY",
        "outputId": "30be5f71-48c4-4145-98f3-6b3720c28697"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cityscapes username or email address: s306027@studenti.polito.it\n",
            "Cityscapes password: \n",
            "Store credentials unencrypted in '/root/.local/share/cityscapesscripts/credentials.json' [y/N]: y\n",
            "Downloading cityscapes package 'gtFine_trainvaltest.zip' to '/content/cityscapes/gtFine_trainvaltest.zip'\n",
            "Download progress: 100% 241M/241M [00:12<00:00, 19.5MB/s]\n",
            "rm: cannot remove '/content/cityscapes/README': No such file or directory\n",
            "rm: cannot remove '/content/cityscapes/license.txt': No such file or directory\n",
            "Downloading cityscapes package 'leftImg8bit_trainvaltest.zip' to '/content/cityscapes/leftImg8bit_trainvaltest.zip'\n",
            "Download progress:  98% 10.8G/11.0G [15:11<00:17, 12.7MB/s]\n",
            "rm: cannot remove '/content/cityscapes/README': No such file or directory\n",
            "rm: cannot remove '/content/cityscapes/license.txt': No such file or directory\n",
            "replace /content/cityscapes/README? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace /content/cityscapes/license.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "Processing 5000 annotation files\n",
            "Progress: 100.0 % "
          ]
        }
      ],
      "source": [
        "import  os\n",
        "# s306027@studenti.polito.it\n",
        "# %mR+g$L\\~5U03O9)IZ-_\n",
        "# Per Eseguire tutto ci mette 23 min sia CPU che GPU\n",
        "createLabel = True\n",
        "if not os.path.isdir('/content/cityscapes'):\n",
        "  !mkdir /content/cityscapes\n",
        "\n",
        "if not os.path.isfile('/content/cityscapes/gtFine_trainvaltest.zip'):\n",
        "  !csDownload gtFine_trainvaltest.zip -d /content/cityscapes/\n",
        "  !rm /content/cityscapes/README\n",
        "  !rm /content/cityscapes/license.txt\n",
        "\n",
        "if not os.path.isfile('/content/cityscapes/leftImg8bit_trainvaltest.zip'):\n",
        "  !csDownload leftImg8bit_trainvaltest.zip -d /content/cityscapes/\n",
        "  !rm /content/cityscapes/README\n",
        "  !rm /content/cityscapes/license.txt\n",
        "\n",
        "if not os.path.isdir('/content/cityscapes/gtFine'):\n",
        "  !unzip -q /content/cityscapes/gtFine_trainvaltest.zip -d /content/cityscapes/\n",
        "  createLabel = True\n",
        "\n",
        "\n",
        "if not os.path.isdir('/content/cityscapes/leftImg8bit'):\n",
        "  !unzip -q /content/cityscapes/leftImg8bit_trainvaltest.zip -d /content/cityscapes/\n",
        "  createLabel = True\n",
        "\n",
        "if createLabel:\n",
        "  os.environ['CITYSCAPES_DATASET'] = '/content/cityscapes/'\n",
        "  !csCreateTrainIdLabelImgs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAeuewkbhM3p",
        "outputId": "91464ffe-a919-48c1-cf3e-c3a79a673dcb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "498 val/munster/munster_000172_000019_leftImg8bit.png\n",
            "499 val/munster/munster_000173_000019_leftImg8bit.png\n",
            "-------------MSP-------------------\n",
            "---------------------------------------\n",
            "Took  80.77754092216492 seconds\n",
            "=======================================\n",
            "Per-Class IoU:\n",
            "\u001b[0m97.62\u001b[0m Road\n",
            "\u001b[0m81.37\u001b[0m sidewalk\n",
            "\u001b[0m90.77\u001b[0m building\n",
            "\u001b[0m49.43\u001b[0m wall\n",
            "\u001b[0m54.93\u001b[0m fence\n",
            "\u001b[0m60.81\u001b[0m pole\n",
            "\u001b[0m62.60\u001b[0m traffic light\n",
            "\u001b[0m72.32\u001b[0m traffic sign\n",
            "\u001b[0m91.35\u001b[0m vegetation\n",
            "\u001b[0m60.97\u001b[0m terrain\n",
            "\u001b[0m93.38\u001b[0m sky\n",
            "\u001b[0m76.11\u001b[0m person\n",
            "\u001b[0m53.45\u001b[0m rider\n",
            "\u001b[0m92.91\u001b[0m car\n",
            "\u001b[0m72.78\u001b[0m truck\n",
            "\u001b[0m78.87\u001b[0m bus\n",
            "\u001b[0m63.86\u001b[0m train\n",
            "\u001b[0m46.41\u001b[0m motorcycle\n",
            "\u001b[0m71.89\u001b[0m bicycle\n",
            "=======================================\n",
            "MEAN IoU:  \u001b[0m72.20\u001b[0m %\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "# ci mette 7 min con la GPU\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  !python -W ignore /content/AnomalySegmentation/eval/eval_iou.py --loadDir /content/AnomalySegmentation/trained_models/ --datadir /content/cityscapes/  | tail -n 28\n",
        "else:\n",
        "  !python -W ignore /content/AnomalySegmentation/eval/eval_iou.py  --loadDir  /content/AnomalySegmentation/trained_models/ --datadir /content/cityscapes/  --cpu | tail -n 28"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RZTrDS4Mysu"
      },
      "source": [
        "##**Anomaly Inference**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9afwM8zdM7_l",
        "outputId": "80d0aaaa-35f3-4bb8-a9fb-b46181f31866"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------\n",
            "\n",
            "Dataset: RoadAnomaly21 method: MSP\n",
            "AUPRC score: 29.100168300581203\n",
            "FPR@TPR95: 62.51075321069286\n",
            "\n",
            "Dataset: RoadAnomaly21 method: MaxLogit\n",
            "AUPRC score: 38.31957797222208\n",
            "FPR@TPR95: 59.3370558914899\n",
            "\n",
            "Dataset: RoadAnomaly21 method: MaxEntropy\n",
            "AUPRC score: 31.005102648344756\n",
            "FPR@TPR95: 62.593151130093226\n",
            "----------------------------\n",
            "\n",
            "Dataset: RoadObsticle21 method: MSP\n",
            "AUPRC score: 2.7116243119338366\n",
            "FPR@TPR95: 64.9739786894368\n",
            "\n",
            "Dataset: RoadObsticle21 method: MaxLogit\n",
            "AUPRC score: 4.626567617520253\n",
            "FPR@TPR95: 48.443439151949555\n",
            "\n",
            "Dataset: RoadObsticle21 method: MaxEntropy\n",
            "AUPRC score: 3.051560023478638\n",
            "FPR@TPR95: 65.59968252759046\n",
            "----------------------------\n",
            "\n",
            "Dataset: FS_LostFound_full method: MSP\n",
            "AUPRC score: 1.747872547607269\n",
            "FPR@TPR95: 50.76348570192957\n",
            "\n",
            "Dataset: FS_LostFound_full method: MaxLogit\n",
            "AUPRC score: 3.3014401015087245\n",
            "FPR@TPR95: 45.494876929038305\n",
            "\n",
            "Dataset: FS_LostFound_full method: MaxEntropy\n",
            "AUPRC score: 2.581709137723009\n",
            "FPR@TPR95: 50.368099783135676\n",
            "----------------------------\n",
            "\n",
            "Dataset: fs_static method: MSP\n",
            "AUPRC score: 7.4700433549050915\n",
            "FPR@TPR95: 41.82346831776172\n",
            "\n",
            "Dataset: fs_static method: MaxLogit\n",
            "AUPRC score: 9.498677970785756\n",
            "FPR@TPR95: 40.3000747567442\n",
            "\n",
            "Dataset: fs_static method: MaxEntropy\n",
            "AUPRC score: 8.82636607633996\n",
            "FPR@TPR95: 41.52332673090571\n",
            "----------------------------\n",
            "\n",
            "Dataset: RoadAnomaly method: MSP\n",
            "AUPRC score: 12.426265849563665\n",
            "FPR@TPR95: 82.49244029880458\n",
            "\n",
            "Dataset: RoadAnomaly method: MaxLogit\n",
            "AUPRC score: 15.581983301641019\n",
            "FPR@TPR95: 73.24766535735604\n",
            "\n",
            "Dataset: RoadAnomaly method: MaxEntropy\n",
            "AUPRC score: 12.678035094227063\n",
            "FPR@TPR95: 82.63192451735861\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "no_execute = False\n",
        "just_once = False\n",
        "\n",
        "for dataset_dir in ['RoadAnomaly21', 'RoadObsticle21', 'FS_LostFound_full', 'fs_static', 'RoadAnomaly']:\n",
        "  print(\"----------------------------\")\n",
        "  for method in [\"MSP\", \"MaxLogit\", \"MaxEntropy\"]:\n",
        "\n",
        "    if no_execute:\n",
        "      break\n",
        "\n",
        "    format_file = os.listdir(f'/content/Validation_Dataset/{dataset_dir}/images')[0].split(\".\")[1]\n",
        "    input =f'/content/Validation_Dataset/{dataset_dir}/images/\\*.{format_file}'\n",
        "    print(f\"\\nDataset: {dataset_dir} method: {method}\")\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "      !python -W ignore /content/AnomalySegmentation/eval/evalAnomaly.py --input {input} --method  {method}  | tail -n 2\n",
        "    else:\n",
        "      !python -W ignore /content/AnomalySegmentation/eval/evalAnomaly.py --input {input} --method {method}  --cpu | tail -n 2\n",
        "\n",
        "    if just_once:\n",
        "      no_execute = True\n",
        "      just_once = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhQWIx8rklfO"
      },
      "source": [
        "##**Temperature Scaling**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7DsE7oO1n9G"
      },
      "source": [
        "**Anomaly Inference with temperature**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Zu-dIEeqFLq3",
        "outputId": "47825080-3d12-479f-c2c2-51ca15ecab1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------\n",
            "\n",
            "Dataset: RoadAnomaly21 method : MSP Temperature: 0.5\n",
            "AUPRC score: 27.060833635879618\n",
            "FPR@TPR95: 62.730810427606734\n",
            "\n",
            "Dataset: RoadAnomaly21 method : MSP Temperature: 0.75\n",
            "AUPRC score: 28.156063054348103\n",
            "FPR@TPR95: 62.478737323984326\n",
            "\n",
            "Dataset: RoadAnomaly21 method : MSP Temperature: 1.1\n",
            "AUPRC score: 29.40955379121979\n",
            "FPR@TPR95: 62.58986549662704\n",
            "----------------------------\n",
            "\n",
            "Dataset: RoadObsticle21 method : MSP Temperature: 0.5\n",
            "AUPRC score: 2.4195519558429823\n",
            "FPR@TPR95: 63.22544524787239\n",
            "\n",
            "Dataset: RoadObsticle21 method : MSP Temperature: 0.75\n",
            "AUPRC score: 2.5668802249367677\n",
            "FPR@TPR95: 64.05285534718263\n",
            "\n",
            "Dataset: RoadObsticle21 method : MSP Temperature: 1.1\n",
            "AUPRC score: 2.7658075767433776\n",
            "FPR@TPR95: 65.52358106228223\n",
            "----------------------------\n",
            "\n",
            "Dataset: FS_LostFound_full method : MSP Temperature: 0.5\n",
            "AUPRC score: 1.2802500246431052\n",
            "FPR@TPR95: 66.73710676943257\n",
            "\n",
            "Dataset: FS_LostFound_full method : MSP Temperature: 0.75\n",
            "AUPRC score: 1.4927065686510383\n",
            "FPR@TPR95: 51.848262648332636\n",
            "\n",
            "Dataset: FS_LostFound_full method : MSP Temperature: 1.1\n",
            "AUPRC score: 1.8596703140506141\n",
            "FPR@TPR95: 50.38650128754133\n",
            "----------------------------\n",
            "\n",
            "Dataset: fs_static method : MSP Temperature: 0.5\n",
            "AUPRC score: 6.6011970066164665\n",
            "FPR@TPR95: 43.47565874225287\n",
            "\n",
            "Dataset: fs_static method : MSP Temperature: 0.75\n",
            "AUPRC score: 6.99079114995491\n",
            "FPR@TPR95: 42.49329123307483\n",
            "\n",
            "Dataset: fs_static method : MSP Temperature: 1.1\n",
            "AUPRC score: 7.686696846804934\n",
            "FPR@TPR95: 41.586844199987\n",
            "----------------------------\n",
            "\n",
            "Dataset: RoadAnomaly method : MSP Temperature: 0.5\n",
            "AUPRC score: 12.187681345765725\n",
            "FPR@TPR95: 82.02224728951396\n",
            "\n",
            "Dataset: RoadAnomaly method : MSP Temperature: 0.75\n",
            "AUPRC score: 12.319186617225913\n",
            "FPR@TPR95: 82.28451947325927\n",
            "\n",
            "Dataset: RoadAnomaly method : MSP Temperature: 1.1\n",
            "AUPRC score: 12.465779148190585\n",
            "FPR@TPR95: 82.62125003163526\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "no_execute = False\n",
        "just_once = False\n",
        "for dataset_dir in ['RoadAnomaly21', 'RoadObsticle21', 'FS_LostFound_full', 'fs_static', 'RoadAnomaly']:\n",
        "  print(\"----------------------------\")\n",
        "  for t in [0.5, 0.75, 1.1]:\n",
        "    if no_execute:\n",
        "        break\n",
        "\n",
        "    format_file = os.listdir(f'/content/Validation_Dataset/{dataset_dir}/images')[0].split(\".\")[1]\n",
        "    input =f'/content/Validation_Dataset/{dataset_dir}/images/\\*.{format_file}'\n",
        "    print(f\"\\nDataset: {dataset_dir}, method: MSP, Temperature: {t}\")\n",
        "    if torch.cuda.is_available():\n",
        "      !python -W ignore /content/AnomalySegmentation/eval/evalAnomaly.py --input {input} --method 'MSP' --temperature {t} | tail -n 2\n",
        "    else:\n",
        "      !python -W ignore /content/AnomalySegmentation/eval/evalAnomaly.py --input {input} --method 'MSP' --cpu --temperature {t} | tail -n 2\n",
        "\n",
        "    if just_once:\n",
        "      no_execute = True\n",
        "      just_once = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nY-OAYlIjGaG"
      },
      "source": [
        "## **Void Classifier**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine-Tuning"
      ],
      "metadata": {
        "id": "vNdTJZh4IP7G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQhmfT3zjJcG",
        "outputId": "d070d1dd-4336-4de7-a121-516f91e3e516"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Import Model erfnet with weights erfnet_pretrained.pth to FineTune\n",
            "========== DECODER TRAINING ===========\n",
            "/content/cityscapes/leftImg8bit/train\n",
            "/content/cityscapes/leftImg8bit/val\n",
            "<class '__main__.CrossEntropyLoss2d'>\n",
            "----- TRAINING - EPOCH 1 -----\n",
            "LEARNING RATE:  0.0005\n",
            "loss: 0.09732 (epoch: 1, step: 0) // Avg time/img: 0.5937 s\n",
            "loss: 0.1106 (epoch: 1, step: 50) // Avg time/img: 0.0508 s\n",
            "loss: 0.1094 (epoch: 1, step: 100) // Avg time/img: 0.0458 s\n",
            "loss: 0.1096 (epoch: 1, step: 150) // Avg time/img: 0.0441 s\n",
            "loss: 0.1088 (epoch: 1, step: 200) // Avg time/img: 0.0432 s\n",
            "loss: 0.1096 (epoch: 1, step: 250) // Avg time/img: 0.0428 s\n",
            "loss: 0.1095 (epoch: 1, step: 300) // Avg time/img: 0.0426 s\n",
            "loss: 0.1085 (epoch: 1, step: 350) // Avg time/img: 0.0424 s\n",
            "loss: 0.1086 (epoch: 1, step: 400) // Avg time/img: 0.0425 s\n",
            "loss: 0.1089 (epoch: 1, step: 450) // Avg time/img: 0.0424 s\n",
            "----- VALIDATING - EPOCH 1 -----\n",
            "VAL loss: 0.1809 (epoch: 1, step: 0) // Avg time/img: 0.0544 s\n",
            "VAL loss: 0.2874 (epoch: 1, step: 50) // Avg time/img: 0.0334 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.88\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/erfnet_training1/model-001.pth (epoch: 1)\n",
            "save: ../save/erfnet_training1/model_best.pth (epoch: 1)\n",
            "----- TRAINING - EPOCH 2 -----\n",
            "LEARNING RATE:  0.00045476628804148113\n",
            "loss: 0.1015 (epoch: 2, step: 0) // Avg time/img: 0.0606 s\n",
            "loss: 0.1068 (epoch: 2, step: 50) // Avg time/img: 0.0422 s\n",
            "loss: 0.1083 (epoch: 2, step: 100) // Avg time/img: 0.0420 s\n",
            "loss: 0.1106 (epoch: 2, step: 150) // Avg time/img: 0.0421 s\n",
            "loss: 0.1099 (epoch: 2, step: 200) // Avg time/img: 0.0422 s\n",
            "loss: 0.1101 (epoch: 2, step: 250) // Avg time/img: 0.0421 s\n",
            "loss: 0.1099 (epoch: 2, step: 300) // Avg time/img: 0.0421 s\n",
            "loss: 0.1096 (epoch: 2, step: 350) // Avg time/img: 0.0420 s\n",
            "loss: 0.1094 (epoch: 2, step: 400) // Avg time/img: 0.0420 s\n",
            "loss: 0.1096 (epoch: 2, step: 450) // Avg time/img: 0.0420 s\n",
            "----- VALIDATING - EPOCH 2 -----\n",
            "VAL loss: 0.216 (epoch: 2, step: 0) // Avg time/img: 0.0470 s\n",
            "VAL loss: 0.2825 (epoch: 2, step: 50) // Avg time/img: 0.0332 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m72.10\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/erfnet_training1/model-002.pth (epoch: 2)\n",
            "save: ../save/erfnet_training1/model_best.pth (epoch: 2)\n",
            "----- TRAINING - EPOCH 3 -----\n",
            "LEARNING RATE:  0.00040902607302542923\n",
            "loss: 0.096 (epoch: 3, step: 0) // Avg time/img: 0.0542 s\n",
            "loss: 0.1066 (epoch: 3, step: 50) // Avg time/img: 0.0428 s\n",
            "loss: 0.1082 (epoch: 3, step: 100) // Avg time/img: 0.0421 s\n",
            "loss: 0.1082 (epoch: 3, step: 150) // Avg time/img: 0.0423 s\n",
            "loss: 0.1094 (epoch: 3, step: 200) // Avg time/img: 0.0422 s\n",
            "loss: 0.1094 (epoch: 3, step: 250) // Avg time/img: 0.0421 s\n",
            "loss: 0.1094 (epoch: 3, step: 300) // Avg time/img: 0.0421 s\n",
            "loss: 0.1093 (epoch: 3, step: 350) // Avg time/img: 0.0421 s\n",
            "loss: 0.1089 (epoch: 3, step: 400) // Avg time/img: 0.0422 s\n",
            "loss: 0.1089 (epoch: 3, step: 450) // Avg time/img: 0.0422 s\n",
            "----- VALIDATING - EPOCH 3 -----\n",
            "VAL loss: 0.2056 (epoch: 3, step: 0) // Avg time/img: 0.0459 s\n",
            "VAL loss: 0.2846 (epoch: 3, step: 50) // Avg time/img: 0.0334 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.90\u001b[0m %\n",
            "save: ../save/erfnet_training1/model-003.pth (epoch: 3)\n",
            "----- TRAINING - EPOCH 4 -----\n",
            "LEARNING RATE:  0.00036270892346860996\n",
            "loss: 0.1216 (epoch: 4, step: 0) // Avg time/img: 0.0479 s\n",
            "loss: 0.1069 (epoch: 4, step: 50) // Avg time/img: 0.0426 s\n",
            "loss: 0.1087 (epoch: 4, step: 100) // Avg time/img: 0.0420 s\n",
            "loss: 0.1086 (epoch: 4, step: 150) // Avg time/img: 0.0418 s\n",
            "loss: 0.1077 (epoch: 4, step: 200) // Avg time/img: 0.0419 s\n",
            "loss: 0.1081 (epoch: 4, step: 250) // Avg time/img: 0.0419 s\n",
            "loss: 0.1079 (epoch: 4, step: 300) // Avg time/img: 0.0419 s\n",
            "loss: 0.108 (epoch: 4, step: 350) // Avg time/img: 0.0418 s\n",
            "loss: 0.108 (epoch: 4, step: 400) // Avg time/img: 0.0419 s\n",
            "loss: 0.1086 (epoch: 4, step: 450) // Avg time/img: 0.0419 s\n",
            "----- VALIDATING - EPOCH 4 -----\n",
            "VAL loss: 0.1886 (epoch: 4, step: 0) // Avg time/img: 0.0483 s\n",
            "VAL loss: 0.284 (epoch: 4, step: 50) // Avg time/img: 0.0332 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.98\u001b[0m %\n",
            "save: ../save/erfnet_training1/model-004.pth (epoch: 4)\n",
            "----- TRAINING - EPOCH 5 -----\n",
            "LEARNING RATE:  0.00031572293374467766\n",
            "loss: 0.1298 (epoch: 5, step: 0) // Avg time/img: 0.0510 s\n",
            "loss: 0.1181 (epoch: 5, step: 50) // Avg time/img: 0.0424 s\n",
            "loss: 0.1139 (epoch: 5, step: 100) // Avg time/img: 0.0420 s\n",
            "loss: 0.1108 (epoch: 5, step: 150) // Avg time/img: 0.0419 s\n",
            "loss: 0.1099 (epoch: 5, step: 200) // Avg time/img: 0.0421 s\n",
            "loss: 0.1101 (epoch: 5, step: 250) // Avg time/img: 0.0421 s\n",
            "loss: 0.1098 (epoch: 5, step: 300) // Avg time/img: 0.0421 s\n",
            "loss: 0.1089 (epoch: 5, step: 350) // Avg time/img: 0.0421 s\n",
            "loss: 0.109 (epoch: 5, step: 400) // Avg time/img: 0.0421 s\n",
            "loss: 0.1091 (epoch: 5, step: 450) // Avg time/img: 0.0421 s\n",
            "----- VALIDATING - EPOCH 5 -----\n",
            "VAL loss: 0.208 (epoch: 5, step: 0) // Avg time/img: 0.0453 s\n",
            "VAL loss: 0.2827 (epoch: 5, step: 50) // Avg time/img: 0.0334 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.68\u001b[0m %\n",
            "save: ../save/erfnet_training1/model-005.pth (epoch: 5)\n",
            "----- TRAINING - EPOCH 6 -----\n",
            "LEARNING RATE:  0.0002679433656340733\n",
            "loss: 0.1002 (epoch: 6, step: 0) // Avg time/img: 0.0491 s\n",
            "loss: 0.1073 (epoch: 6, step: 50) // Avg time/img: 0.0418 s\n",
            "loss: 0.1065 (epoch: 6, step: 100) // Avg time/img: 0.0419 s\n",
            "loss: 0.1089 (epoch: 6, step: 150) // Avg time/img: 0.0420 s\n",
            "loss: 0.1093 (epoch: 6, step: 200) // Avg time/img: 0.0421 s\n",
            "loss: 0.1088 (epoch: 6, step: 250) // Avg time/img: 0.0421 s\n",
            "loss: 0.1084 (epoch: 6, step: 300) // Avg time/img: 0.0422 s\n",
            "loss: 0.1086 (epoch: 6, step: 350) // Avg time/img: 0.0422 s\n",
            "loss: 0.1087 (epoch: 6, step: 400) // Avg time/img: 0.0422 s\n",
            "loss: 0.109 (epoch: 6, step: 450) // Avg time/img: 0.0422 s\n",
            "----- VALIDATING - EPOCH 6 -----\n",
            "VAL loss: 0.2052 (epoch: 6, step: 0) // Avg time/img: 0.0457 s\n",
            "VAL loss: 0.2846 (epoch: 6, step: 50) // Avg time/img: 0.0334 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.81\u001b[0m %\n",
            "save: ../save/erfnet_training1/model-006.pth (epoch: 6)\n",
            "----- TRAINING - EPOCH 7 -----\n",
            "LEARNING RATE:  0.00021919164527704348\n",
            "loss: 0.09946 (epoch: 7, step: 0) // Avg time/img: 0.0509 s\n",
            "loss: 0.1093 (epoch: 7, step: 50) // Avg time/img: 0.0421 s\n",
            "loss: 0.1091 (epoch: 7, step: 100) // Avg time/img: 0.0420 s\n",
            "loss: 0.1087 (epoch: 7, step: 150) // Avg time/img: 0.0421 s\n",
            "loss: 0.1093 (epoch: 7, step: 200) // Avg time/img: 0.0421 s\n",
            "loss: 0.1092 (epoch: 7, step: 250) // Avg time/img: 0.0422 s\n",
            "loss: 0.1092 (epoch: 7, step: 300) // Avg time/img: 0.0421 s\n",
            "loss: 0.1091 (epoch: 7, step: 350) // Avg time/img: 0.0421 s\n",
            "loss: 0.1088 (epoch: 7, step: 400) // Avg time/img: 0.0422 s\n",
            "loss: 0.1085 (epoch: 7, step: 450) // Avg time/img: 0.0421 s\n",
            "----- VALIDATING - EPOCH 7 -----\n",
            "VAL loss: 0.1982 (epoch: 7, step: 0) // Avg time/img: 0.0474 s\n",
            "VAL loss: 0.2848 (epoch: 7, step: 50) // Avg time/img: 0.0334 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.97\u001b[0m %\n",
            "save: ../save/erfnet_training1/model-007.pth (epoch: 7)\n",
            "----- TRAINING - EPOCH 8 -----\n",
            "LEARNING RATE:  0.00016919173095082495\n",
            "loss: 0.1204 (epoch: 8, step: 0) // Avg time/img: 0.0488 s\n",
            "loss: 0.1128 (epoch: 8, step: 50) // Avg time/img: 0.0422 s\n",
            "loss: 0.1104 (epoch: 8, step: 100) // Avg time/img: 0.0420 s\n",
            "loss: 0.1101 (epoch: 8, step: 150) // Avg time/img: 0.0418 s\n",
            "loss: 0.1104 (epoch: 8, step: 200) // Avg time/img: 0.0420 s\n",
            "loss: 0.11 (epoch: 8, step: 250) // Avg time/img: 0.0421 s\n",
            "loss: 0.1096 (epoch: 8, step: 300) // Avg time/img: 0.0421 s\n",
            "loss: 0.1098 (epoch: 8, step: 350) // Avg time/img: 0.0420 s\n",
            "loss: 0.1093 (epoch: 8, step: 400) // Avg time/img: 0.0421 s\n",
            "loss: 0.1087 (epoch: 8, step: 450) // Avg time/img: 0.0421 s\n",
            "----- VALIDATING - EPOCH 8 -----\n",
            "VAL loss: 0.217 (epoch: 8, step: 0) // Avg time/img: 0.0470 s\n",
            "VAL loss: 0.2807 (epoch: 8, step: 50) // Avg time/img: 0.0339 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m72.22\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/erfnet_training1/model-008.pth (epoch: 8)\n",
            "save: ../save/erfnet_training1/model_best.pth (epoch: 8)\n",
            "----- TRAINING - EPOCH 9 -----\n",
            "LEARNING RATE:  0.00011746189430880188\n",
            "loss: 0.09498 (epoch: 9, step: 0) // Avg time/img: 0.0525 s\n",
            "loss: 0.1082 (epoch: 9, step: 50) // Avg time/img: 0.0432 s\n",
            "loss: 0.1094 (epoch: 9, step: 100) // Avg time/img: 0.0425 s\n",
            "loss: 0.1095 (epoch: 9, step: 150) // Avg time/img: 0.0426 s\n",
            "loss: 0.1092 (epoch: 9, step: 200) // Avg time/img: 0.0424 s\n",
            "loss: 0.1083 (epoch: 9, step: 250) // Avg time/img: 0.0422 s\n",
            "loss: 0.1088 (epoch: 9, step: 300) // Avg time/img: 0.0423 s\n",
            "loss: 0.1091 (epoch: 9, step: 350) // Avg time/img: 0.0422 s\n",
            "loss: 0.1087 (epoch: 9, step: 400) // Avg time/img: 0.0423 s\n",
            "loss: 0.1089 (epoch: 9, step: 450) // Avg time/img: 0.0423 s\n",
            "----- VALIDATING - EPOCH 9 -----\n",
            "VAL loss: 0.2032 (epoch: 9, step: 0) // Avg time/img: 0.0442 s\n",
            "VAL loss: 0.2821 (epoch: 9, step: 50) // Avg time/img: 0.0348 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m72.01\u001b[0m %\n",
            "save: ../save/erfnet_training1/model-009.pth (epoch: 9)\n",
            "----- TRAINING - EPOCH 10 -----\n",
            "LEARNING RATE:  6.294627058970836e-05\n",
            "loss: 0.09977 (epoch: 10, step: 0) // Avg time/img: 0.0509 s\n",
            "loss: 0.1109 (epoch: 10, step: 50) // Avg time/img: 0.0427 s\n",
            "loss: 0.1096 (epoch: 10, step: 100) // Avg time/img: 0.0422 s\n",
            "loss: 0.1103 (epoch: 10, step: 150) // Avg time/img: 0.0423 s\n",
            "loss: 0.1096 (epoch: 10, step: 200) // Avg time/img: 0.0423 s\n",
            "loss: 0.1096 (epoch: 10, step: 250) // Avg time/img: 0.0422 s\n",
            "loss: 0.1097 (epoch: 10, step: 300) // Avg time/img: 0.0422 s\n",
            "loss: 0.109 (epoch: 10, step: 350) // Avg time/img: 0.0422 s\n",
            "loss: 0.1092 (epoch: 10, step: 400) // Avg time/img: 0.0422 s\n",
            "loss: 0.1091 (epoch: 10, step: 450) // Avg time/img: 0.0421 s\n",
            "----- VALIDATING - EPOCH 10 -----\n",
            "VAL loss: 0.1996 (epoch: 10, step: 0) // Avg time/img: 0.0470 s\n",
            "VAL loss: 0.2855 (epoch: 10, step: 50) // Avg time/img: 0.0334 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.88\u001b[0m %\n",
            "save: ../save/erfnet_training1/model-010.pth (epoch: 10)\n",
            "========== TRAINING FINISHED ===========\n"
          ]
        }
      ],
      "source": [
        "# Fine tune ERFNet (10 epochs)\n",
        "!cd /content/AnomalySegmentation/train; python -W ignore main.py --savedir erfnet_training1 --datadir /content/cityscapes --model erfnet --cuda --num-epochs=10 --epochs-save=1 --FineTune --decoder --state=/content/AnomalySegmentation/trained_models/erfnet_pretrained.pth --loadWeights=erfnet_pretrained.pth"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r save_erfnet_training1.zip /content/AnomalySegmentation/save/erfnet_training1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1UvkfWcM1ob",
        "outputId": "ba6f76b8-ba99-4a75-bfde-63692cc68dcc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/AnomalySegmentation/save/erfnet_training1/ (stored 0%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training1/model_best.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training1/model-002.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training1/automated_log.txt (deflated 63%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training1/model-001.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training1/model-003.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training1/model-006.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training1/model-010.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training1/model.txt (deflated 92%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training1/model-005.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training1/model-007.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training1/best.txt (stored 0%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training1/model_best.pth.tar (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training1/opts.txt (deflated 40%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training1/model-008.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training1/model-009.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training1/checkpoint.pth.tar (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training1/erfnet.py (deflated 78%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training1/model-004.pth (deflated 10%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine tune ERFNet (20 epochs)\n",
        "!cd /content/AnomalySegmentation/train; python -W ignore main.py --savedir erfnet_training2 --datadir /content/cityscapes --model erfnet --cuda --num-epochs=20 --epochs-save=1 --FineTune --decoder --state=/content/AnomalySegmentation/trained_models/erfnet_pretrained.pth --loadWeights=erfnet_pretrained.pth\n",
        "!zip -r save_erfnet_training2.zip /content/AnomalySegmentation/save/erfnet_training2"
      ],
      "metadata": {
        "id": "wSJMvmnGLtsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FineTune ENet (20 epochs)\n",
        "!cd /content/AnomalySegmentation/train; python -W ignore main.py --savedir enet_training1 --datadir /content/cityscapes --model enet --cuda --num-epochs=20 --epochs-save=1 --FineTune --loadWeights=enet_pretrained\n",
        "!zip -r save_enet_training1.zip /content/AnomalySegmentation/save/enet_training1"
      ],
      "metadata": {
        "id": "9_1Uo-kgXFGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finetune BiSeNet (20 epochs)\n",
        "!cd /content/AnomalySegmentation/train; python -W ignore main.py --savedir bisenet_training1 --datadir /content/cityscapes --model bisenet --cuda --num-epochs=20 --epochs-save=1 --FineTune --loadWeights=bisenetv1_pretrained.pth\n",
        "!zip -r save_bisenet_training1.zip /content/AnomalySegmentation/save/bisenet_training1"
      ],
      "metadata": {
        "id": "h-ftZ-p1Yn9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation"
      ],
      "metadata": {
        "id": "mMrPCZ56IShf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "no_execute = False\n",
        "just_once = False\n",
        "\n",
        "for net in [\"erfnet\", \"enet\", \"bisenet\"]:\n",
        "  print(\"----------------------------\")\n",
        "  for dataset_dir in ['RoadAnomaly21', 'RoadObsticle21', 'FS_LostFound_full', 'fs_static', 'RoadAnomaly']:\n",
        "\n",
        "    if no_execute:\n",
        "      break\n",
        "\n",
        "    load_dir = f'/content/AnomalySegmentation/save/{net}_training1'\n",
        "    weights = f'{load_dir}/model_best.pth'\n",
        "    format_file = os.listdir(f'/content/Validation_Dataset/{dataset_dir}/images')[0].split(\".\")[1]\n",
        "    input =f'/content/Validation_Dataset/{dataset_dir}/images/\\*.{format_file}'\n",
        "    print(f\"\\nDataset: {dataset_dir} net: {net}\")\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "      !python -W ignore /content/AnomalySegmentation/eval/evalAnomaly.py --input {input} --void --model {net} --loadDir {load_dir} --loadWeights {weights} | tail -n 2\n",
        "    else:\n",
        "      !python -W ignore /content/AnomalySegmentation/eval/evalAnomaly.py --input {input} --void --model {net} --loadDir {load_dir} --loadWeights {weights} --cpu | tail -n 2\n",
        "\n",
        "    if just_once:\n",
        "      no_execute = True\n",
        "      just_once = False"
      ],
      "metadata": {
        "id": "cchB40LlIT9a"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}