{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypo8OBRZ-1p3"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/RonPlusSign/AnomalySegmentation/blob/mahalanobis_2/Project6.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUjRrYEW8-uz"
      },
      "source": [
        "# **Anomaly Segmentation Project 6**\n",
        "##*Andrea Delli, Christian Dellisanti, Giorgia Modi*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4x3MajLNeXhX"
      },
      "source": [
        "##**Dataset Preparation**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/AnomalySegmentation"
      ],
      "metadata": {
        "id": "yorO9_xVX2bJ"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_tj8W3BeVPo",
        "outputId": "76688668-8b03-4510-e417-749b4375fd20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'AnomalySegmentation'...\n",
            "remote: Enumerating objects: 1260, done.\u001b[K\n",
            "remote: Counting objects: 100% (80/80), done.\u001b[K\n",
            "remote: Compressing objects: 100% (55/55), done.\u001b[K\n",
            "remote: Total 1260 (delta 48), reused 51 (delta 25), pack-reused 1180 (from 2)\u001b[K\n",
            "Receiving objects: 100% (1260/1260), 285.33 MiB | 34.75 MiB/s, done.\n",
            "Resolving deltas: 100% (855/855), done.\n",
            "Updating files: 100% (69/69), done.\n",
            "Already up to date.\n"
          ]
        }
      ],
      "source": [
        "!pip  install -q numpy matplotlib Pillow torchvision visdom ood_metrics icecream cityscapesscripts tqdm #triton\n",
        "\n",
        "import sys, os\n",
        "if not os.path.isfile('/content/Validation_Dataset.zip'):\n",
        "  !gdown 12YJq48XkCxQHjN3CmLc-zM5dThSak4Ta\n",
        "if not os.path.isdir('/content/Validation_Dataset'):\n",
        "  !unzip -q Validation_Dataset.zip\n",
        "if not os.path.isdir('/content/AnomalySegmentation'):\n",
        "  #!git clone https://github.com/shyam671/AnomalySegmentation_CourseProjectBaseCode.git\n",
        "  #token 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\n",
        "  !git clone -b mahalanobis_2 https://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@github.com/RonPlusSign/AnomalySegmentation.git\n",
        "!cd /content/AnomalySegmentation && git pull\n",
        "#!cd /content/AnomalySegmentation && git checkout main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAmA1igJhHhR"
      },
      "source": [
        "##**mIoU**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3w0jewGkphY",
        "outputId": "a68de0ef-d187-435f-c7ca-d63355657ebd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 5000 annotation files\n",
            "Progress: 13.86 % Failed to convert: /content/cityscapes/gtFine/test/bielefeld/bielefeld_000000_056493_gtFine_polygons.json\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/PIL/ImageFile.py\", line 554, in _save\n",
            "    fh = fp.fileno()\n",
            "AttributeError: '_idat' object has no attribute 'fileno'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/csCreateTrainIdLabelImgs\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/cityscapesscripts/preparation/createTrainIdLabelImgs.py\", line 67, in main\n",
            "    json2labelImg( f , dst , \"trainIds\" )\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/cityscapesscripts/preparation/json2labelImg.py\", line 124, in json2labelImg\n",
            "    labelImg.save( outImg )\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/PIL/Image.py\", line 2605, in save\n",
            "    save_handler(self, fp, filename)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/PIL/PngImagePlugin.py\", line 1488, in _save\n",
            "    ImageFile._save(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/PIL/ImageFile.py\", line 558, in _save\n",
            "    _encode_tile(im, fp, tile, bufsize, None, exc)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/PIL/ImageFile.py\", line 584, in _encode_tile\n",
            "    errcode, data = encoder.encode(bufsize)[1:]\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "import  os\n",
        "# s306027@studenti.polito.it\n",
        "# %mR+g$L\\~5U03O9)IZ-_\n",
        "# Per Eseguire tutto ci mette 23 min sia CPU che GPU\n",
        "createLabel = True\n",
        "fast_download = True\n",
        "super_fast_download = False\n",
        "if super_fast_download:\n",
        "  !gdown 1-fjLAk4_-GkixW1-GP_cYDBUmhnVbApL\n",
        "  !unzip -q cityscapes.zip\n",
        "  !mv  ./content/cityscapes /content/cityscapes\n",
        "  !rm -rf ./content\n",
        "else:\n",
        "  if not os.path.isdir('/content/cityscapes'):\n",
        "    !mkdir /content/cityscapes\n",
        "\n",
        "  if not os.path.isfile('/content/cityscapes/gtFine_trainvaltest.zip'):\n",
        "    if fast_download:\n",
        "      !gdown 1J31rnVd33GBt-IYGYqC9mv73q7vc55pw -O /content/cityscapes/\n",
        "    else:\n",
        "      !csDownload gtFine_trainvaltest.zip -d /content/cityscapes/\n",
        "\n",
        "  if not os.path.isdir('/content/cityscapes/gtFine'):\n",
        "    !unzip -q /content/cityscapes/gtFine_trainvaltest.zip -d /content/cityscapes/\n",
        "    createLabel = True\n",
        "    !rm /content/cityscapes/README\n",
        "    !rm /content/cityscapes/license.txt\n",
        "\n",
        "\n",
        "  if not os.path.isfile('/content/cityscapes/leftImg8bit_trainvaltest.zip'):\n",
        "    if fast_download:\n",
        "      #https://drive.google.com/file/d/1m8Y3Zc6vG11Q9SxW7Be5EGXTDq4s4RlJ/view?usp=sharing\n",
        "      !gdown 1m8Y3Zc6vG11Q9SxW7Be5EGXTDq4s4RlJ -O /content/cityscapes/\n",
        "    else:\n",
        "      !csDownload leftImg8bit_trainvaltest.zip -d /content/cityscapes/\n",
        "\n",
        "\n",
        "  if not os.path.isdir('/content/cityscapes/leftImg8bit'):\n",
        "    !unzip -q /content/cityscapes/leftImg8bit_trainvaltest.zip -d /content/cityscapes/\n",
        "    createLabel = True\n",
        "    !rm /content/cityscapes/README\n",
        "    !rm /content/cityscapes/license.txt\n",
        "\n",
        "  if createLabel:\n",
        "    os.environ['CITYSCAPES_DATASET'] = '/content/cityscapes/'\n",
        "    !csCreateTrainIdLabelImgs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAeuewkbhM3p",
        "outputId": "91464ffe-a919-48c1-cf3e-c3a79a673dcb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "498 val/munster/munster_000172_000019_leftImg8bit.png\n",
            "499 val/munster/munster_000173_000019_leftImg8bit.png\n",
            "-------------MSP-------------------\n",
            "---------------------------------------\n",
            "Took  80.77754092216492 seconds\n",
            "=======================================\n",
            "Per-Class IoU:\n",
            "\u001b[0m97.62\u001b[0m Road\n",
            "\u001b[0m81.37\u001b[0m sidewalk\n",
            "\u001b[0m90.77\u001b[0m building\n",
            "\u001b[0m49.43\u001b[0m wall\n",
            "\u001b[0m54.93\u001b[0m fence\n",
            "\u001b[0m60.81\u001b[0m pole\n",
            "\u001b[0m62.60\u001b[0m traffic light\n",
            "\u001b[0m72.32\u001b[0m traffic sign\n",
            "\u001b[0m91.35\u001b[0m vegetation\n",
            "\u001b[0m60.97\u001b[0m terrain\n",
            "\u001b[0m93.38\u001b[0m sky\n",
            "\u001b[0m76.11\u001b[0m person\n",
            "\u001b[0m53.45\u001b[0m rider\n",
            "\u001b[0m92.91\u001b[0m car\n",
            "\u001b[0m72.78\u001b[0m truck\n",
            "\u001b[0m78.87\u001b[0m bus\n",
            "\u001b[0m63.86\u001b[0m train\n",
            "\u001b[0m46.41\u001b[0m motorcycle\n",
            "\u001b[0m71.89\u001b[0m bicycle\n",
            "=======================================\n",
            "MEAN IoU:  \u001b[0m72.20\u001b[0m %\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "# ci mette 7 min con la GPU\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  !python -W ignore /content/AnomalySegmentation/eval/eval_iou.py --loadDir /content/AnomalySegmentation/trained_models/ --datadir /content/cityscapes/  | tail -n 28\n",
        "else:\n",
        "  !python -W ignore /content/AnomalySegmentation/eval/eval_iou.py  --loadDir  /content/AnomalySegmentation/trained_models/ --datadir /content/cityscapes/  --cpu | tail -n 28"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RZTrDS4Mysu"
      },
      "source": [
        "##**Anomaly Inference**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9afwM8zdM7_l",
        "outputId": "80d0aaaa-35f3-4bb8-a9fb-b46181f31866"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------\n",
            "\n",
            "Dataset: RoadAnomaly21 method: MSP\n",
            "AUPRC score: 29.100168300581203\n",
            "FPR@TPR95: 62.51075321069286\n",
            "\n",
            "Dataset: RoadAnomaly21 method: MaxLogit\n",
            "AUPRC score: 38.31957797222208\n",
            "FPR@TPR95: 59.3370558914899\n",
            "\n",
            "Dataset: RoadAnomaly21 method: MaxEntropy\n",
            "AUPRC score: 31.005102648344756\n",
            "FPR@TPR95: 62.593151130093226\n",
            "----------------------------\n",
            "\n",
            "Dataset: RoadObsticle21 method: MSP\n",
            "AUPRC score: 2.7116243119338366\n",
            "FPR@TPR95: 64.9739786894368\n",
            "\n",
            "Dataset: RoadObsticle21 method: MaxLogit\n",
            "AUPRC score: 4.626567617520253\n",
            "FPR@TPR95: 48.443439151949555\n",
            "\n",
            "Dataset: RoadObsticle21 method: MaxEntropy\n",
            "AUPRC score: 3.051560023478638\n",
            "FPR@TPR95: 65.59968252759046\n",
            "----------------------------\n",
            "\n",
            "Dataset: FS_LostFound_full method: MSP\n",
            "AUPRC score: 1.747872547607269\n",
            "FPR@TPR95: 50.76348570192957\n",
            "\n",
            "Dataset: FS_LostFound_full method: MaxLogit\n",
            "AUPRC score: 3.3014401015087245\n",
            "FPR@TPR95: 45.494876929038305\n",
            "\n",
            "Dataset: FS_LostFound_full method: MaxEntropy\n",
            "AUPRC score: 2.581709137723009\n",
            "FPR@TPR95: 50.368099783135676\n",
            "----------------------------\n",
            "\n",
            "Dataset: fs_static method: MSP\n",
            "AUPRC score: 7.4700433549050915\n",
            "FPR@TPR95: 41.82346831776172\n",
            "\n",
            "Dataset: fs_static method: MaxLogit\n",
            "AUPRC score: 9.498677970785756\n",
            "FPR@TPR95: 40.3000747567442\n",
            "\n",
            "Dataset: fs_static method: MaxEntropy\n",
            "AUPRC score: 8.82636607633996\n",
            "FPR@TPR95: 41.52332673090571\n",
            "----------------------------\n",
            "\n",
            "Dataset: RoadAnomaly method: MSP\n",
            "AUPRC score: 12.426265849563665\n",
            "FPR@TPR95: 82.49244029880458\n",
            "\n",
            "Dataset: RoadAnomaly method: MaxLogit\n",
            "AUPRC score: 15.581983301641019\n",
            "FPR@TPR95: 73.24766535735604\n",
            "\n",
            "Dataset: RoadAnomaly method: MaxEntropy\n",
            "AUPRC score: 12.678035094227063\n",
            "FPR@TPR95: 82.63192451735861\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "no_execute = False\n",
        "just_once = False\n",
        "\n",
        "for dataset_dir in ['RoadAnomaly21', 'RoadObsticle21', 'FS_LostFound_full', 'fs_static', 'RoadAnomaly']:\n",
        "  print(\"----------------------------\")\n",
        "  for method in [\"MSP\", \"MaxLogit\", \"MaxEntropy\"]:\n",
        "\n",
        "    if no_execute:\n",
        "      break\n",
        "\n",
        "    format_file = os.listdir(f'/content/Validation_Dataset/{dataset_dir}/images')[0].split(\".\")[1]\n",
        "    input =f'/content/Validation_Dataset/{dataset_dir}/images/\\*.{format_file}'\n",
        "    print(f\"\\nDataset: {dataset_dir} method: {method}\")\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "      !python -W ignore /content/AnomalySegmentation/eval/evalAnomaly.py --input {input} --method  {method}  | tail -n 2\n",
        "    else:\n",
        "      !python -W ignore /content/AnomalySegmentation/eval/evalAnomaly.py --input {input} --method {method}  --cpu | tail -n 2\n",
        "\n",
        "    if just_once:\n",
        "      no_execute = True\n",
        "      just_once = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhQWIx8rklfO"
      },
      "source": [
        "##**Temperature Scaling**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7DsE7oO1n9G"
      },
      "source": [
        "**Anomaly Inference with temperature**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Zu-dIEeqFLq3",
        "outputId": "47825080-3d12-479f-c2c2-51ca15ecab1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------\n",
            "\n",
            "Dataset: RoadAnomaly21 method : MSP Temperature: 0.5\n",
            "AUPRC score: 27.060833635879618\n",
            "FPR@TPR95: 62.730810427606734\n",
            "\n",
            "Dataset: RoadAnomaly21 method : MSP Temperature: 0.75\n",
            "AUPRC score: 28.156063054348103\n",
            "FPR@TPR95: 62.478737323984326\n",
            "\n",
            "Dataset: RoadAnomaly21 method : MSP Temperature: 1.1\n",
            "AUPRC score: 29.40955379121979\n",
            "FPR@TPR95: 62.58986549662704\n",
            "----------------------------\n",
            "\n",
            "Dataset: RoadObsticle21 method : MSP Temperature: 0.5\n",
            "AUPRC score: 2.4195519558429823\n",
            "FPR@TPR95: 63.22544524787239\n",
            "\n",
            "Dataset: RoadObsticle21 method : MSP Temperature: 0.75\n",
            "AUPRC score: 2.5668802249367677\n",
            "FPR@TPR95: 64.05285534718263\n",
            "\n",
            "Dataset: RoadObsticle21 method : MSP Temperature: 1.1\n",
            "AUPRC score: 2.7658075767433776\n",
            "FPR@TPR95: 65.52358106228223\n",
            "----------------------------\n",
            "\n",
            "Dataset: FS_LostFound_full method : MSP Temperature: 0.5\n",
            "AUPRC score: 1.2802500246431052\n",
            "FPR@TPR95: 66.73710676943257\n",
            "\n",
            "Dataset: FS_LostFound_full method : MSP Temperature: 0.75\n",
            "AUPRC score: 1.4927065686510383\n",
            "FPR@TPR95: 51.848262648332636\n",
            "\n",
            "Dataset: FS_LostFound_full method : MSP Temperature: 1.1\n",
            "AUPRC score: 1.8596703140506141\n",
            "FPR@TPR95: 50.38650128754133\n",
            "----------------------------\n",
            "\n",
            "Dataset: fs_static method : MSP Temperature: 0.5\n",
            "AUPRC score: 6.6011970066164665\n",
            "FPR@TPR95: 43.47565874225287\n",
            "\n",
            "Dataset: fs_static method : MSP Temperature: 0.75\n",
            "AUPRC score: 6.99079114995491\n",
            "FPR@TPR95: 42.49329123307483\n",
            "\n",
            "Dataset: fs_static method : MSP Temperature: 1.1\n",
            "AUPRC score: 7.686696846804934\n",
            "FPR@TPR95: 41.586844199987\n",
            "----------------------------\n",
            "\n",
            "Dataset: RoadAnomaly method : MSP Temperature: 0.5\n",
            "AUPRC score: 12.187681345765725\n",
            "FPR@TPR95: 82.02224728951396\n",
            "\n",
            "Dataset: RoadAnomaly method : MSP Temperature: 0.75\n",
            "AUPRC score: 12.319186617225913\n",
            "FPR@TPR95: 82.28451947325927\n",
            "\n",
            "Dataset: RoadAnomaly method : MSP Temperature: 1.1\n",
            "AUPRC score: 12.465779148190585\n",
            "FPR@TPR95: 82.62125003163526\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "no_execute = False\n",
        "just_once = False\n",
        "for dataset_dir in ['RoadAnomaly21', 'RoadObsticle21', 'FS_LostFound_full', 'fs_static', 'RoadAnomaly']:\n",
        "  print(\"----------------------------\")\n",
        "  for t in [0.5, 0.75, 1.1]:\n",
        "    if no_execute:\n",
        "        break\n",
        "\n",
        "    format_file = os.listdir(f'/content/Validation_Dataset/{dataset_dir}/images')[0].split(\".\")[1]\n",
        "    input =f'/content/Validation_Dataset/{dataset_dir}/images/\\*.{format_file}'\n",
        "    print(f\"\\nDataset: {dataset_dir}, method: MSP, Temperature: {t}\")\n",
        "    if torch.cuda.is_available():\n",
        "      !python -W ignore /content/AnomalySegmentation/eval/evalAnomaly.py --input {input} --method 'MSP' --temperature {t} | tail -n 2\n",
        "    else:\n",
        "      !python -W ignore /content/AnomalySegmentation/eval/evalAnomaly.py --input {input} --method 'MSP' --cpu --temperature {t} | tail -n 2\n",
        "\n",
        "    if just_once:\n",
        "      no_execute = True\n",
        "      just_once = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nY-OAYlIjGaG"
      },
      "source": [
        "## **Void Classifier**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine-Tuning"
      ],
      "metadata": {
        "id": "vNdTJZh4IP7G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####ERFNET"
      ],
      "metadata": {
        "id": "LYpg0U39MrDL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQhmfT3zjJcG",
        "outputId": "7a42935a-0014-4f51-acca-c8057510b8a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Import Model erfnet with weights erfnet_pretrained.pth to FineTune\n",
            "========== DECODER TRAINING ===========\n",
            "/content/cityscapes/leftImg8bit/train\n",
            "/content/cityscapes/leftImg8bit/val\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/AnomalySegmentation/train/main.py\", line 604, in <module>\n",
            "    main(parser.parse_args())\n",
            "  File \"/content/AnomalySegmentation/train/main.py\", line 566, in main\n",
            "    model = train(args, model, False)   #Train decoder\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py\", line 465, in _fn\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/content/AnomalySegmentation/train/main.py\", line 150, in train\n",
            "    assert os.path.exists(args.datadir), \"Error: datadir (dataset directory) could not be loaded\"\n",
            "  File \"/content/AnomalySegmentation/train/main.py\", line 157, in torch_dynamo_resume_in_train_at_150\n",
            "    dataset_train = cityscapes(args.datadir, co_transform, 'train')\n",
            "  File \"/content/AnomalySegmentation/train/main.py\", line 158, in torch_dynamo_resume_in_train_at_157\n",
            "    dataset_val = cityscapes(args.datadir, co_transform_val, 'val')\n",
            "  File \"/content/AnomalySegmentation/train/main.py\", line 164, in torch_dynamo_resume_in_train_at_158\n",
            "    loader = DataLoader(dataset_train, num_workers=args.num_workers, batch_size=args.batch_size, shuffle=True)\n",
            "  File \"/content/AnomalySegmentation/train/main.py\", line 165, in torch_dynamo_resume_in_train_at_164\n",
            "    loader_val = DataLoader(dataset_val, num_workers=args.num_workers, batch_size=args.batch_size, shuffle=False)\n",
            "  File \"/content/AnomalySegmentation/train/main.py\", line 169, in torch_dynamo_resume_in_train_at_165\n",
            "    criterion = CrossEntropyLoss2d(weight)\n",
            "  File \"/content/AnomalySegmentation/train/main.py\", line 78, in __init__\n",
            "    self.loss = torch.nn.NLLLoss2d(weight)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/typing_extensions.py\", line 2811, in __new__\n",
            "    warnings.warn(msg, category=category, stacklevel=stacklevel + 1)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/typing_extensions.py\", line 2813, in torch_dynamo_resume_in___new___at_2811\n",
            "    return original_new(cls, *args, **kwargs)\n",
            "TypeError: object.__new__() takes exactly one argument (the type to instantiate)\n"
          ]
        }
      ],
      "source": [
        "# Fine tune ERFNet (10 epochs)\n",
        "!cd /content/AnomalySegmentation/train; python -W ignore main.py --savedir erfnet_training1 --datadir /content/cityscapes --model erfnet --cuda --num-epochs=10 --epochs-save=1 --FineTune --decoder --state=/content/AnomalySegmentation/trained_models/erfnet_pretrained.pth --loadWeights=erfnet_pretrained.pth"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r save_erfnet_training1.zip /content/AnomalySegmentation/save/erfnet_training1"
      ],
      "metadata": {
        "id": "N1UvkfWcM1ob",
        "outputId": "ba6f76b8-ba99-4a75-bfde-63692cc68dcc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/AnomalySegmentation/save/erfnet_training1/ (stored 0%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training1/model_best.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training1/model-002.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training1/automated_log.txt (deflated 63%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training1/model-001.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training1/model-003.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training1/model-006.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training1/model-010.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training1/model.txt (deflated 92%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training1/model-005.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training1/model-007.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training1/best.txt (stored 0%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training1/model_best.pth.tar (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training1/opts.txt (deflated 40%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training1/model-008.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training1/model-009.pth (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training1/checkpoint.pth.tar (deflated 10%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training1/erfnet.py (deflated 78%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training1/model-004.pth (deflated 10%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine tune ERFNet (20 epochs)\n",
        "!cd /content/AnomalySegmentation/train; python -W ignore main.py --savedir erfnet_training2 --datadir /content/cityscapes --model erfnet --cuda --num-epochs=20 --epochs-save=1 --FineTune --decoder --state=/content/AnomalySegmentation/trained_models/erfnet_pretrained.pth --loadWeights=erfnet_pretrained.pth\n",
        "!zip -r save_erfnet_training2.zip /content/AnomalySegmentation/save/erfnet_training2"
      ],
      "metadata": {
        "id": "wSJMvmnGLtsU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d51917f5-33dc-4e45-fd58-6f49c9fdbbe2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Import Model erfnet with weights erfnet_pretrained.pth to FineTune\n",
            "========== DECODER TRAINING ===========\n",
            "/content/cityscapes/leftImg8bit/train\n",
            "/content/cityscapes/leftImg8bit/val\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/AnomalySegmentation/train/main.py\", line 604, in <module>\n",
            "    main(parser.parse_args())\n",
            "  File \"/content/AnomalySegmentation/train/main.py\", line 566, in main\n",
            "    model = train(args, model, False)   #Train decoder\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py\", line 465, in _fn\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/content/AnomalySegmentation/train/main.py\", line 150, in train\n",
            "    assert os.path.exists(args.datadir), \"Error: datadir (dataset directory) could not be loaded\"\n",
            "  File \"/content/AnomalySegmentation/train/main.py\", line 157, in torch_dynamo_resume_in_train_at_150\n",
            "    dataset_train = cityscapes(args.datadir, co_transform, 'train')\n",
            "  File \"/content/AnomalySegmentation/train/main.py\", line 158, in torch_dynamo_resume_in_train_at_157\n",
            "    dataset_val = cityscapes(args.datadir, co_transform_val, 'val')\n",
            "  File \"/content/AnomalySegmentation/train/main.py\", line 164, in torch_dynamo_resume_in_train_at_158\n",
            "    loader = DataLoader(dataset_train, num_workers=args.num_workers, batch_size=args.batch_size, shuffle=True)\n",
            "  File \"/content/AnomalySegmentation/train/main.py\", line 165, in torch_dynamo_resume_in_train_at_164\n",
            "    loader_val = DataLoader(dataset_val, num_workers=args.num_workers, batch_size=args.batch_size, shuffle=False)\n",
            "  File \"/content/AnomalySegmentation/train/main.py\", line 169, in torch_dynamo_resume_in_train_at_165\n",
            "    criterion = CrossEntropyLoss2d(weight)\n",
            "  File \"/content/AnomalySegmentation/train/main.py\", line 78, in __init__\n",
            "    self.loss = torch.nn.NLLLoss2d(weight)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/typing_extensions.py\", line 2811, in __new__\n",
            "    warnings.warn(msg, category=category, stacklevel=stacklevel + 1)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/typing_extensions.py\", line 2813, in torch_dynamo_resume_in___new___at_2811\n",
            "    return original_new(cls, *args, **kwargs)\n",
            "TypeError: object.__new__() takes exactly one argument (the type to instantiate)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training2/ (stored 0%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training2/opts.txt (deflated 40%)\n",
            "  adding: content/AnomalySegmentation/save/erfnet_training2/erfnet.py (deflated 78%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####ENET"
      ],
      "metadata": {
        "id": "G3aC1UuCMz9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FineTune ENet (20 epochs)\n",
        "!cd /content/AnomalySegmentation/train; python -W ignore main.py --savedir enet_training1 --datadir /content/cityscapes --model enet  --num-epochs=20 --epochs-save=1 --cuda --FineTune --loadWeights=enet_pretrained\n",
        "!zip -r save_enet_training1.zip /content/AnomalySegmentation/save/enet_training1"
      ],
      "metadata": {
        "id": "9_1Uo-kgXFGK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cda0a369-7b7c-42be-9211-0f1ee2ad9036"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "odict_keys(['initial_block.main_branch.weight', 'initial_block.batch_norm.weight', 'initial_block.batch_norm.bias', 'initial_block.batch_norm.running_mean', 'initial_block.batch_norm.running_var', 'initial_block.batch_norm.num_batches_tracked', 'initial_block.out_activation.weight', 'downsample1_0.ext_conv1.0.weight', 'downsample1_0.ext_conv1.1.weight', 'downsample1_0.ext_conv1.1.bias', 'downsample1_0.ext_conv1.1.running_mean', 'downsample1_0.ext_conv1.1.running_var', 'downsample1_0.ext_conv1.1.num_batches_tracked', 'downsample1_0.ext_conv1.2.weight', 'downsample1_0.ext_conv2.0.weight', 'downsample1_0.ext_conv2.1.weight', 'downsample1_0.ext_conv2.1.bias', 'downsample1_0.ext_conv2.1.running_mean', 'downsample1_0.ext_conv2.1.running_var', 'downsample1_0.ext_conv2.1.num_batches_tracked', 'downsample1_0.ext_conv2.2.weight', 'downsample1_0.ext_conv3.0.weight', 'downsample1_0.ext_conv3.1.weight', 'downsample1_0.ext_conv3.1.bias', 'downsample1_0.ext_conv3.1.running_mean', 'downsample1_0.ext_conv3.1.running_var', 'downsample1_0.ext_conv3.1.num_batches_tracked', 'downsample1_0.ext_conv3.2.weight', 'downsample1_0.out_activation.weight', 'regular1_1.ext_conv1.0.weight', 'regular1_1.ext_conv1.1.weight', 'regular1_1.ext_conv1.1.bias', 'regular1_1.ext_conv1.1.running_mean', 'regular1_1.ext_conv1.1.running_var', 'regular1_1.ext_conv1.1.num_batches_tracked', 'regular1_1.ext_conv1.2.weight', 'regular1_1.ext_conv2.0.weight', 'regular1_1.ext_conv2.1.weight', 'regular1_1.ext_conv2.1.bias', 'regular1_1.ext_conv2.1.running_mean', 'regular1_1.ext_conv2.1.running_var', 'regular1_1.ext_conv2.1.num_batches_tracked', 'regular1_1.ext_conv2.2.weight', 'regular1_1.ext_conv3.0.weight', 'regular1_1.ext_conv3.1.weight', 'regular1_1.ext_conv3.1.bias', 'regular1_1.ext_conv3.1.running_mean', 'regular1_1.ext_conv3.1.running_var', 'regular1_1.ext_conv3.1.num_batches_tracked', 'regular1_1.ext_conv3.2.weight', 'regular1_1.out_activation.weight', 'regular1_2.ext_conv1.0.weight', 'regular1_2.ext_conv1.1.weight', 'regular1_2.ext_conv1.1.bias', 'regular1_2.ext_conv1.1.running_mean', 'regular1_2.ext_conv1.1.running_var', 'regular1_2.ext_conv1.1.num_batches_tracked', 'regular1_2.ext_conv1.2.weight', 'regular1_2.ext_conv2.0.weight', 'regular1_2.ext_conv2.1.weight', 'regular1_2.ext_conv2.1.bias', 'regular1_2.ext_conv2.1.running_mean', 'regular1_2.ext_conv2.1.running_var', 'regular1_2.ext_conv2.1.num_batches_tracked', 'regular1_2.ext_conv2.2.weight', 'regular1_2.ext_conv3.0.weight', 'regular1_2.ext_conv3.1.weight', 'regular1_2.ext_conv3.1.bias', 'regular1_2.ext_conv3.1.running_mean', 'regular1_2.ext_conv3.1.running_var', 'regular1_2.ext_conv3.1.num_batches_tracked', 'regular1_2.ext_conv3.2.weight', 'regular1_2.out_activation.weight', 'regular1_3.ext_conv1.0.weight', 'regular1_3.ext_conv1.1.weight', 'regular1_3.ext_conv1.1.bias', 'regular1_3.ext_conv1.1.running_mean', 'regular1_3.ext_conv1.1.running_var', 'regular1_3.ext_conv1.1.num_batches_tracked', 'regular1_3.ext_conv1.2.weight', 'regular1_3.ext_conv2.0.weight', 'regular1_3.ext_conv2.1.weight', 'regular1_3.ext_conv2.1.bias', 'regular1_3.ext_conv2.1.running_mean', 'regular1_3.ext_conv2.1.running_var', 'regular1_3.ext_conv2.1.num_batches_tracked', 'regular1_3.ext_conv2.2.weight', 'regular1_3.ext_conv3.0.weight', 'regular1_3.ext_conv3.1.weight', 'regular1_3.ext_conv3.1.bias', 'regular1_3.ext_conv3.1.running_mean', 'regular1_3.ext_conv3.1.running_var', 'regular1_3.ext_conv3.1.num_batches_tracked', 'regular1_3.ext_conv3.2.weight', 'regular1_3.out_activation.weight', 'regular1_4.ext_conv1.0.weight', 'regular1_4.ext_conv1.1.weight', 'regular1_4.ext_conv1.1.bias', 'regular1_4.ext_conv1.1.running_mean', 'regular1_4.ext_conv1.1.running_var', 'regular1_4.ext_conv1.1.num_batches_tracked', 'regular1_4.ext_conv1.2.weight', 'regular1_4.ext_conv2.0.weight', 'regular1_4.ext_conv2.1.weight', 'regular1_4.ext_conv2.1.bias', 'regular1_4.ext_conv2.1.running_mean', 'regular1_4.ext_conv2.1.running_var', 'regular1_4.ext_conv2.1.num_batches_tracked', 'regular1_4.ext_conv2.2.weight', 'regular1_4.ext_conv3.0.weight', 'regular1_4.ext_conv3.1.weight', 'regular1_4.ext_conv3.1.bias', 'regular1_4.ext_conv3.1.running_mean', 'regular1_4.ext_conv3.1.running_var', 'regular1_4.ext_conv3.1.num_batches_tracked', 'regular1_4.ext_conv3.2.weight', 'regular1_4.out_activation.weight', 'downsample2_0.ext_conv1.0.weight', 'downsample2_0.ext_conv1.1.weight', 'downsample2_0.ext_conv1.1.bias', 'downsample2_0.ext_conv1.1.running_mean', 'downsample2_0.ext_conv1.1.running_var', 'downsample2_0.ext_conv1.1.num_batches_tracked', 'downsample2_0.ext_conv1.2.weight', 'downsample2_0.ext_conv2.0.weight', 'downsample2_0.ext_conv2.1.weight', 'downsample2_0.ext_conv2.1.bias', 'downsample2_0.ext_conv2.1.running_mean', 'downsample2_0.ext_conv2.1.running_var', 'downsample2_0.ext_conv2.1.num_batches_tracked', 'downsample2_0.ext_conv2.2.weight', 'downsample2_0.ext_conv3.0.weight', 'downsample2_0.ext_conv3.1.weight', 'downsample2_0.ext_conv3.1.bias', 'downsample2_0.ext_conv3.1.running_mean', 'downsample2_0.ext_conv3.1.running_var', 'downsample2_0.ext_conv3.1.num_batches_tracked', 'downsample2_0.ext_conv3.2.weight', 'downsample2_0.out_activation.weight', 'regular2_1.ext_conv1.0.weight', 'regular2_1.ext_conv1.1.weight', 'regular2_1.ext_conv1.1.bias', 'regular2_1.ext_conv1.1.running_mean', 'regular2_1.ext_conv1.1.running_var', 'regular2_1.ext_conv1.1.num_batches_tracked', 'regular2_1.ext_conv1.2.weight', 'regular2_1.ext_conv2.0.weight', 'regular2_1.ext_conv2.1.weight', 'regular2_1.ext_conv2.1.bias', 'regular2_1.ext_conv2.1.running_mean', 'regular2_1.ext_conv2.1.running_var', 'regular2_1.ext_conv2.1.num_batches_tracked', 'regular2_1.ext_conv2.2.weight', 'regular2_1.ext_conv3.0.weight', 'regular2_1.ext_conv3.1.weight', 'regular2_1.ext_conv3.1.bias', 'regular2_1.ext_conv3.1.running_mean', 'regular2_1.ext_conv3.1.running_var', 'regular2_1.ext_conv3.1.num_batches_tracked', 'regular2_1.ext_conv3.2.weight', 'regular2_1.out_activation.weight', 'dilated2_2.ext_conv1.0.weight', 'dilated2_2.ext_conv1.1.weight', 'dilated2_2.ext_conv1.1.bias', 'dilated2_2.ext_conv1.1.running_mean', 'dilated2_2.ext_conv1.1.running_var', 'dilated2_2.ext_conv1.1.num_batches_tracked', 'dilated2_2.ext_conv1.2.weight', 'dilated2_2.ext_conv2.0.weight', 'dilated2_2.ext_conv2.1.weight', 'dilated2_2.ext_conv2.1.bias', 'dilated2_2.ext_conv2.1.running_mean', 'dilated2_2.ext_conv2.1.running_var', 'dilated2_2.ext_conv2.1.num_batches_tracked', 'dilated2_2.ext_conv2.2.weight', 'dilated2_2.ext_conv3.0.weight', 'dilated2_2.ext_conv3.1.weight', 'dilated2_2.ext_conv3.1.bias', 'dilated2_2.ext_conv3.1.running_mean', 'dilated2_2.ext_conv3.1.running_var', 'dilated2_2.ext_conv3.1.num_batches_tracked', 'dilated2_2.ext_conv3.2.weight', 'dilated2_2.out_activation.weight', 'asymmetric2_3.ext_conv1.0.weight', 'asymmetric2_3.ext_conv1.1.weight', 'asymmetric2_3.ext_conv1.1.bias', 'asymmetric2_3.ext_conv1.1.running_mean', 'asymmetric2_3.ext_conv1.1.running_var', 'asymmetric2_3.ext_conv1.1.num_batches_tracked', 'asymmetric2_3.ext_conv1.2.weight', 'asymmetric2_3.ext_conv2.0.weight', 'asymmetric2_3.ext_conv2.1.weight', 'asymmetric2_3.ext_conv2.1.bias', 'asymmetric2_3.ext_conv2.1.running_mean', 'asymmetric2_3.ext_conv2.1.running_var', 'asymmetric2_3.ext_conv2.1.num_batches_tracked', 'asymmetric2_3.ext_conv2.2.weight', 'asymmetric2_3.ext_conv2.3.weight', 'asymmetric2_3.ext_conv2.4.weight', 'asymmetric2_3.ext_conv2.4.bias', 'asymmetric2_3.ext_conv2.4.running_mean', 'asymmetric2_3.ext_conv2.4.running_var', 'asymmetric2_3.ext_conv2.4.num_batches_tracked', 'asymmetric2_3.ext_conv2.5.weight', 'asymmetric2_3.ext_conv3.0.weight', 'asymmetric2_3.ext_conv3.1.weight', 'asymmetric2_3.ext_conv3.1.bias', 'asymmetric2_3.ext_conv3.1.running_mean', 'asymmetric2_3.ext_conv3.1.running_var', 'asymmetric2_3.ext_conv3.1.num_batches_tracked', 'asymmetric2_3.ext_conv3.2.weight', 'asymmetric2_3.out_activation.weight', 'dilated2_4.ext_conv1.0.weight', 'dilated2_4.ext_conv1.1.weight', 'dilated2_4.ext_conv1.1.bias', 'dilated2_4.ext_conv1.1.running_mean', 'dilated2_4.ext_conv1.1.running_var', 'dilated2_4.ext_conv1.1.num_batches_tracked', 'dilated2_4.ext_conv1.2.weight', 'dilated2_4.ext_conv2.0.weight', 'dilated2_4.ext_conv2.1.weight', 'dilated2_4.ext_conv2.1.bias', 'dilated2_4.ext_conv2.1.running_mean', 'dilated2_4.ext_conv2.1.running_var', 'dilated2_4.ext_conv2.1.num_batches_tracked', 'dilated2_4.ext_conv2.2.weight', 'dilated2_4.ext_conv3.0.weight', 'dilated2_4.ext_conv3.1.weight', 'dilated2_4.ext_conv3.1.bias', 'dilated2_4.ext_conv3.1.running_mean', 'dilated2_4.ext_conv3.1.running_var', 'dilated2_4.ext_conv3.1.num_batches_tracked', 'dilated2_4.ext_conv3.2.weight', 'dilated2_4.out_activation.weight', 'regular2_5.ext_conv1.0.weight', 'regular2_5.ext_conv1.1.weight', 'regular2_5.ext_conv1.1.bias', 'regular2_5.ext_conv1.1.running_mean', 'regular2_5.ext_conv1.1.running_var', 'regular2_5.ext_conv1.1.num_batches_tracked', 'regular2_5.ext_conv1.2.weight', 'regular2_5.ext_conv2.0.weight', 'regular2_5.ext_conv2.1.weight', 'regular2_5.ext_conv2.1.bias', 'regular2_5.ext_conv2.1.running_mean', 'regular2_5.ext_conv2.1.running_var', 'regular2_5.ext_conv2.1.num_batches_tracked', 'regular2_5.ext_conv2.2.weight', 'regular2_5.ext_conv3.0.weight', 'regular2_5.ext_conv3.1.weight', 'regular2_5.ext_conv3.1.bias', 'regular2_5.ext_conv3.1.running_mean', 'regular2_5.ext_conv3.1.running_var', 'regular2_5.ext_conv3.1.num_batches_tracked', 'regular2_5.ext_conv3.2.weight', 'regular2_5.out_activation.weight', 'dilated2_6.ext_conv1.0.weight', 'dilated2_6.ext_conv1.1.weight', 'dilated2_6.ext_conv1.1.bias', 'dilated2_6.ext_conv1.1.running_mean', 'dilated2_6.ext_conv1.1.running_var', 'dilated2_6.ext_conv1.1.num_batches_tracked', 'dilated2_6.ext_conv1.2.weight', 'dilated2_6.ext_conv2.0.weight', 'dilated2_6.ext_conv2.1.weight', 'dilated2_6.ext_conv2.1.bias', 'dilated2_6.ext_conv2.1.running_mean', 'dilated2_6.ext_conv2.1.running_var', 'dilated2_6.ext_conv2.1.num_batches_tracked', 'dilated2_6.ext_conv2.2.weight', 'dilated2_6.ext_conv3.0.weight', 'dilated2_6.ext_conv3.1.weight', 'dilated2_6.ext_conv3.1.bias', 'dilated2_6.ext_conv3.1.running_mean', 'dilated2_6.ext_conv3.1.running_var', 'dilated2_6.ext_conv3.1.num_batches_tracked', 'dilated2_6.ext_conv3.2.weight', 'dilated2_6.out_activation.weight', 'asymmetric2_7.ext_conv1.0.weight', 'asymmetric2_7.ext_conv1.1.weight', 'asymmetric2_7.ext_conv1.1.bias', 'asymmetric2_7.ext_conv1.1.running_mean', 'asymmetric2_7.ext_conv1.1.running_var', 'asymmetric2_7.ext_conv1.1.num_batches_tracked', 'asymmetric2_7.ext_conv1.2.weight', 'asymmetric2_7.ext_conv2.0.weight', 'asymmetric2_7.ext_conv2.1.weight', 'asymmetric2_7.ext_conv2.1.bias', 'asymmetric2_7.ext_conv2.1.running_mean', 'asymmetric2_7.ext_conv2.1.running_var', 'asymmetric2_7.ext_conv2.1.num_batches_tracked', 'asymmetric2_7.ext_conv2.2.weight', 'asymmetric2_7.ext_conv2.3.weight', 'asymmetric2_7.ext_conv2.4.weight', 'asymmetric2_7.ext_conv2.4.bias', 'asymmetric2_7.ext_conv2.4.running_mean', 'asymmetric2_7.ext_conv2.4.running_var', 'asymmetric2_7.ext_conv2.4.num_batches_tracked', 'asymmetric2_7.ext_conv2.5.weight', 'asymmetric2_7.ext_conv3.0.weight', 'asymmetric2_7.ext_conv3.1.weight', 'asymmetric2_7.ext_conv3.1.bias', 'asymmetric2_7.ext_conv3.1.running_mean', 'asymmetric2_7.ext_conv3.1.running_var', 'asymmetric2_7.ext_conv3.1.num_batches_tracked', 'asymmetric2_7.ext_conv3.2.weight', 'asymmetric2_7.out_activation.weight', 'dilated2_8.ext_conv1.0.weight', 'dilated2_8.ext_conv1.1.weight', 'dilated2_8.ext_conv1.1.bias', 'dilated2_8.ext_conv1.1.running_mean', 'dilated2_8.ext_conv1.1.running_var', 'dilated2_8.ext_conv1.1.num_batches_tracked', 'dilated2_8.ext_conv1.2.weight', 'dilated2_8.ext_conv2.0.weight', 'dilated2_8.ext_conv2.1.weight', 'dilated2_8.ext_conv2.1.bias', 'dilated2_8.ext_conv2.1.running_mean', 'dilated2_8.ext_conv2.1.running_var', 'dilated2_8.ext_conv2.1.num_batches_tracked', 'dilated2_8.ext_conv2.2.weight', 'dilated2_8.ext_conv3.0.weight', 'dilated2_8.ext_conv3.1.weight', 'dilated2_8.ext_conv3.1.bias', 'dilated2_8.ext_conv3.1.running_mean', 'dilated2_8.ext_conv3.1.running_var', 'dilated2_8.ext_conv3.1.num_batches_tracked', 'dilated2_8.ext_conv3.2.weight', 'dilated2_8.out_activation.weight', 'regular3_0.ext_conv1.0.weight', 'regular3_0.ext_conv1.1.weight', 'regular3_0.ext_conv1.1.bias', 'regular3_0.ext_conv1.1.running_mean', 'regular3_0.ext_conv1.1.running_var', 'regular3_0.ext_conv1.1.num_batches_tracked', 'regular3_0.ext_conv1.2.weight', 'regular3_0.ext_conv2.0.weight', 'regular3_0.ext_conv2.1.weight', 'regular3_0.ext_conv2.1.bias', 'regular3_0.ext_conv2.1.running_mean', 'regular3_0.ext_conv2.1.running_var', 'regular3_0.ext_conv2.1.num_batches_tracked', 'regular3_0.ext_conv2.2.weight', 'regular3_0.ext_conv3.0.weight', 'regular3_0.ext_conv3.1.weight', 'regular3_0.ext_conv3.1.bias', 'regular3_0.ext_conv3.1.running_mean', 'regular3_0.ext_conv3.1.running_var', 'regular3_0.ext_conv3.1.num_batches_tracked', 'regular3_0.ext_conv3.2.weight', 'regular3_0.out_activation.weight', 'dilated3_1.ext_conv1.0.weight', 'dilated3_1.ext_conv1.1.weight', 'dilated3_1.ext_conv1.1.bias', 'dilated3_1.ext_conv1.1.running_mean', 'dilated3_1.ext_conv1.1.running_var', 'dilated3_1.ext_conv1.1.num_batches_tracked', 'dilated3_1.ext_conv1.2.weight', 'dilated3_1.ext_conv2.0.weight', 'dilated3_1.ext_conv2.1.weight', 'dilated3_1.ext_conv2.1.bias', 'dilated3_1.ext_conv2.1.running_mean', 'dilated3_1.ext_conv2.1.running_var', 'dilated3_1.ext_conv2.1.num_batches_tracked', 'dilated3_1.ext_conv2.2.weight', 'dilated3_1.ext_conv3.0.weight', 'dilated3_1.ext_conv3.1.weight', 'dilated3_1.ext_conv3.1.bias', 'dilated3_1.ext_conv3.1.running_mean', 'dilated3_1.ext_conv3.1.running_var', 'dilated3_1.ext_conv3.1.num_batches_tracked', 'dilated3_1.ext_conv3.2.weight', 'dilated3_1.out_activation.weight', 'asymmetric3_2.ext_conv1.0.weight', 'asymmetric3_2.ext_conv1.1.weight', 'asymmetric3_2.ext_conv1.1.bias', 'asymmetric3_2.ext_conv1.1.running_mean', 'asymmetric3_2.ext_conv1.1.running_var', 'asymmetric3_2.ext_conv1.1.num_batches_tracked', 'asymmetric3_2.ext_conv1.2.weight', 'asymmetric3_2.ext_conv2.0.weight', 'asymmetric3_2.ext_conv2.1.weight', 'asymmetric3_2.ext_conv2.1.bias', 'asymmetric3_2.ext_conv2.1.running_mean', 'asymmetric3_2.ext_conv2.1.running_var', 'asymmetric3_2.ext_conv2.1.num_batches_tracked', 'asymmetric3_2.ext_conv2.2.weight', 'asymmetric3_2.ext_conv2.3.weight', 'asymmetric3_2.ext_conv2.4.weight', 'asymmetric3_2.ext_conv2.4.bias', 'asymmetric3_2.ext_conv2.4.running_mean', 'asymmetric3_2.ext_conv2.4.running_var', 'asymmetric3_2.ext_conv2.4.num_batches_tracked', 'asymmetric3_2.ext_conv2.5.weight', 'asymmetric3_2.ext_conv3.0.weight', 'asymmetric3_2.ext_conv3.1.weight', 'asymmetric3_2.ext_conv3.1.bias', 'asymmetric3_2.ext_conv3.1.running_mean', 'asymmetric3_2.ext_conv3.1.running_var', 'asymmetric3_2.ext_conv3.1.num_batches_tracked', 'asymmetric3_2.ext_conv3.2.weight', 'asymmetric3_2.out_activation.weight', 'dilated3_3.ext_conv1.0.weight', 'dilated3_3.ext_conv1.1.weight', 'dilated3_3.ext_conv1.1.bias', 'dilated3_3.ext_conv1.1.running_mean', 'dilated3_3.ext_conv1.1.running_var', 'dilated3_3.ext_conv1.1.num_batches_tracked', 'dilated3_3.ext_conv1.2.weight', 'dilated3_3.ext_conv2.0.weight', 'dilated3_3.ext_conv2.1.weight', 'dilated3_3.ext_conv2.1.bias', 'dilated3_3.ext_conv2.1.running_mean', 'dilated3_3.ext_conv2.1.running_var', 'dilated3_3.ext_conv2.1.num_batches_tracked', 'dilated3_3.ext_conv2.2.weight', 'dilated3_3.ext_conv3.0.weight', 'dilated3_3.ext_conv3.1.weight', 'dilated3_3.ext_conv3.1.bias', 'dilated3_3.ext_conv3.1.running_mean', 'dilated3_3.ext_conv3.1.running_var', 'dilated3_3.ext_conv3.1.num_batches_tracked', 'dilated3_3.ext_conv3.2.weight', 'dilated3_3.out_activation.weight', 'regular3_4.ext_conv1.0.weight', 'regular3_4.ext_conv1.1.weight', 'regular3_4.ext_conv1.1.bias', 'regular3_4.ext_conv1.1.running_mean', 'regular3_4.ext_conv1.1.running_var', 'regular3_4.ext_conv1.1.num_batches_tracked', 'regular3_4.ext_conv1.2.weight', 'regular3_4.ext_conv2.0.weight', 'regular3_4.ext_conv2.1.weight', 'regular3_4.ext_conv2.1.bias', 'regular3_4.ext_conv2.1.running_mean', 'regular3_4.ext_conv2.1.running_var', 'regular3_4.ext_conv2.1.num_batches_tracked', 'regular3_4.ext_conv2.2.weight', 'regular3_4.ext_conv3.0.weight', 'regular3_4.ext_conv3.1.weight', 'regular3_4.ext_conv3.1.bias', 'regular3_4.ext_conv3.1.running_mean', 'regular3_4.ext_conv3.1.running_var', 'regular3_4.ext_conv3.1.num_batches_tracked', 'regular3_4.ext_conv3.2.weight', 'regular3_4.out_activation.weight', 'dilated3_5.ext_conv1.0.weight', 'dilated3_5.ext_conv1.1.weight', 'dilated3_5.ext_conv1.1.bias', 'dilated3_5.ext_conv1.1.running_mean', 'dilated3_5.ext_conv1.1.running_var', 'dilated3_5.ext_conv1.1.num_batches_tracked', 'dilated3_5.ext_conv1.2.weight', 'dilated3_5.ext_conv2.0.weight', 'dilated3_5.ext_conv2.1.weight', 'dilated3_5.ext_conv2.1.bias', 'dilated3_5.ext_conv2.1.running_mean', 'dilated3_5.ext_conv2.1.running_var', 'dilated3_5.ext_conv2.1.num_batches_tracked', 'dilated3_5.ext_conv2.2.weight', 'dilated3_5.ext_conv3.0.weight', 'dilated3_5.ext_conv3.1.weight', 'dilated3_5.ext_conv3.1.bias', 'dilated3_5.ext_conv3.1.running_mean', 'dilated3_5.ext_conv3.1.running_var', 'dilated3_5.ext_conv3.1.num_batches_tracked', 'dilated3_5.ext_conv3.2.weight', 'dilated3_5.out_activation.weight', 'asymmetric3_6.ext_conv1.0.weight', 'asymmetric3_6.ext_conv1.1.weight', 'asymmetric3_6.ext_conv1.1.bias', 'asymmetric3_6.ext_conv1.1.running_mean', 'asymmetric3_6.ext_conv1.1.running_var', 'asymmetric3_6.ext_conv1.1.num_batches_tracked', 'asymmetric3_6.ext_conv1.2.weight', 'asymmetric3_6.ext_conv2.0.weight', 'asymmetric3_6.ext_conv2.1.weight', 'asymmetric3_6.ext_conv2.1.bias', 'asymmetric3_6.ext_conv2.1.running_mean', 'asymmetric3_6.ext_conv2.1.running_var', 'asymmetric3_6.ext_conv2.1.num_batches_tracked', 'asymmetric3_6.ext_conv2.2.weight', 'asymmetric3_6.ext_conv2.3.weight', 'asymmetric3_6.ext_conv2.4.weight', 'asymmetric3_6.ext_conv2.4.bias', 'asymmetric3_6.ext_conv2.4.running_mean', 'asymmetric3_6.ext_conv2.4.running_var', 'asymmetric3_6.ext_conv2.4.num_batches_tracked', 'asymmetric3_6.ext_conv2.5.weight', 'asymmetric3_6.ext_conv3.0.weight', 'asymmetric3_6.ext_conv3.1.weight', 'asymmetric3_6.ext_conv3.1.bias', 'asymmetric3_6.ext_conv3.1.running_mean', 'asymmetric3_6.ext_conv3.1.running_var', 'asymmetric3_6.ext_conv3.1.num_batches_tracked', 'asymmetric3_6.ext_conv3.2.weight', 'asymmetric3_6.out_activation.weight', 'dilated3_7.ext_conv1.0.weight', 'dilated3_7.ext_conv1.1.weight', 'dilated3_7.ext_conv1.1.bias', 'dilated3_7.ext_conv1.1.running_mean', 'dilated3_7.ext_conv1.1.running_var', 'dilated3_7.ext_conv1.1.num_batches_tracked', 'dilated3_7.ext_conv1.2.weight', 'dilated3_7.ext_conv2.0.weight', 'dilated3_7.ext_conv2.1.weight', 'dilated3_7.ext_conv2.1.bias', 'dilated3_7.ext_conv2.1.running_mean', 'dilated3_7.ext_conv2.1.running_var', 'dilated3_7.ext_conv2.1.num_batches_tracked', 'dilated3_7.ext_conv2.2.weight', 'dilated3_7.ext_conv3.0.weight', 'dilated3_7.ext_conv3.1.weight', 'dilated3_7.ext_conv3.1.bias', 'dilated3_7.ext_conv3.1.running_mean', 'dilated3_7.ext_conv3.1.running_var', 'dilated3_7.ext_conv3.1.num_batches_tracked', 'dilated3_7.ext_conv3.2.weight', 'dilated3_7.out_activation.weight', 'upsample4_0.main_conv1.0.weight', 'upsample4_0.main_conv1.1.weight', 'upsample4_0.main_conv1.1.bias', 'upsample4_0.main_conv1.1.running_mean', 'upsample4_0.main_conv1.1.running_var', 'upsample4_0.main_conv1.1.num_batches_tracked', 'upsample4_0.ext_conv1.0.weight', 'upsample4_0.ext_conv1.1.weight', 'upsample4_0.ext_conv1.1.bias', 'upsample4_0.ext_conv1.1.running_mean', 'upsample4_0.ext_conv1.1.running_var', 'upsample4_0.ext_conv1.1.num_batches_tracked', 'upsample4_0.ext_tconv1.weight', 'upsample4_0.ext_tconv1_bnorm.weight', 'upsample4_0.ext_tconv1_bnorm.bias', 'upsample4_0.ext_tconv1_bnorm.running_mean', 'upsample4_0.ext_tconv1_bnorm.running_var', 'upsample4_0.ext_tconv1_bnorm.num_batches_tracked', 'upsample4_0.ext_conv2.0.weight', 'upsample4_0.ext_conv2.1.weight', 'upsample4_0.ext_conv2.1.bias', 'upsample4_0.ext_conv2.1.running_mean', 'upsample4_0.ext_conv2.1.running_var', 'upsample4_0.ext_conv2.1.num_batches_tracked', 'regular4_1.ext_conv1.0.weight', 'regular4_1.ext_conv1.1.weight', 'regular4_1.ext_conv1.1.bias', 'regular4_1.ext_conv1.1.running_mean', 'regular4_1.ext_conv1.1.running_var', 'regular4_1.ext_conv1.1.num_batches_tracked', 'regular4_1.ext_conv2.0.weight', 'regular4_1.ext_conv2.1.weight', 'regular4_1.ext_conv2.1.bias', 'regular4_1.ext_conv2.1.running_mean', 'regular4_1.ext_conv2.1.running_var', 'regular4_1.ext_conv2.1.num_batches_tracked', 'regular4_1.ext_conv3.0.weight', 'regular4_1.ext_conv3.1.weight', 'regular4_1.ext_conv3.1.bias', 'regular4_1.ext_conv3.1.running_mean', 'regular4_1.ext_conv3.1.running_var', 'regular4_1.ext_conv3.1.num_batches_tracked', 'regular4_2.ext_conv1.0.weight', 'regular4_2.ext_conv1.1.weight', 'regular4_2.ext_conv1.1.bias', 'regular4_2.ext_conv1.1.running_mean', 'regular4_2.ext_conv1.1.running_var', 'regular4_2.ext_conv1.1.num_batches_tracked', 'regular4_2.ext_conv2.0.weight', 'regular4_2.ext_conv2.1.weight', 'regular4_2.ext_conv2.1.bias', 'regular4_2.ext_conv2.1.running_mean', 'regular4_2.ext_conv2.1.running_var', 'regular4_2.ext_conv2.1.num_batches_tracked', 'regular4_2.ext_conv3.0.weight', 'regular4_2.ext_conv3.1.weight', 'regular4_2.ext_conv3.1.bias', 'regular4_2.ext_conv3.1.running_mean', 'regular4_2.ext_conv3.1.running_var', 'regular4_2.ext_conv3.1.num_batches_tracked', 'upsample5_0.main_conv1.0.weight', 'upsample5_0.main_conv1.1.weight', 'upsample5_0.main_conv1.1.bias', 'upsample5_0.main_conv1.1.running_mean', 'upsample5_0.main_conv1.1.running_var', 'upsample5_0.main_conv1.1.num_batches_tracked', 'upsample5_0.ext_conv1.0.weight', 'upsample5_0.ext_conv1.1.weight', 'upsample5_0.ext_conv1.1.bias', 'upsample5_0.ext_conv1.1.running_mean', 'upsample5_0.ext_conv1.1.running_var', 'upsample5_0.ext_conv1.1.num_batches_tracked', 'upsample5_0.ext_tconv1.weight', 'upsample5_0.ext_tconv1_bnorm.weight', 'upsample5_0.ext_tconv1_bnorm.bias', 'upsample5_0.ext_tconv1_bnorm.running_mean', 'upsample5_0.ext_tconv1_bnorm.running_var', 'upsample5_0.ext_tconv1_bnorm.num_batches_tracked', 'upsample5_0.ext_conv2.0.weight', 'upsample5_0.ext_conv2.1.weight', 'upsample5_0.ext_conv2.1.bias', 'upsample5_0.ext_conv2.1.running_mean', 'upsample5_0.ext_conv2.1.running_var', 'upsample5_0.ext_conv2.1.num_batches_tracked', 'regular5_1.ext_conv1.0.weight', 'regular5_1.ext_conv1.1.weight', 'regular5_1.ext_conv1.1.bias', 'regular5_1.ext_conv1.1.running_mean', 'regular5_1.ext_conv1.1.running_var', 'regular5_1.ext_conv1.1.num_batches_tracked', 'regular5_1.ext_conv2.0.weight', 'regular5_1.ext_conv2.1.weight', 'regular5_1.ext_conv2.1.bias', 'regular5_1.ext_conv2.1.running_mean', 'regular5_1.ext_conv2.1.running_var', 'regular5_1.ext_conv2.1.num_batches_tracked', 'regular5_1.ext_conv3.0.weight', 'regular5_1.ext_conv3.1.weight', 'regular5_1.ext_conv3.1.bias', 'regular5_1.ext_conv3.1.running_mean', 'regular5_1.ext_conv3.1.running_var', 'regular5_1.ext_conv3.1.num_batches_tracked', 'transposed_conv.weight'])\n",
            "odict_keys(['initial_block.main_branch.weight', 'initial_block.batch_norm.weight', 'initial_block.batch_norm.bias', 'initial_block.batch_norm.running_mean', 'initial_block.batch_norm.running_var', 'initial_block.batch_norm.num_batches_tracked', 'initial_block.out_activation.weight', 'downsample1_0.ext_conv1.0.weight', 'downsample1_0.ext_conv1.1.weight', 'downsample1_0.ext_conv1.1.bias', 'downsample1_0.ext_conv1.1.running_mean', 'downsample1_0.ext_conv1.1.running_var', 'downsample1_0.ext_conv1.1.num_batches_tracked', 'downsample1_0.ext_conv1.2.weight', 'downsample1_0.ext_conv2.0.weight', 'downsample1_0.ext_conv2.1.weight', 'downsample1_0.ext_conv2.1.bias', 'downsample1_0.ext_conv2.1.running_mean', 'downsample1_0.ext_conv2.1.running_var', 'downsample1_0.ext_conv2.1.num_batches_tracked', 'downsample1_0.ext_conv2.2.weight', 'downsample1_0.ext_conv3.0.weight', 'downsample1_0.ext_conv3.1.weight', 'downsample1_0.ext_conv3.1.bias', 'downsample1_0.ext_conv3.1.running_mean', 'downsample1_0.ext_conv3.1.running_var', 'downsample1_0.ext_conv3.1.num_batches_tracked', 'downsample1_0.ext_conv3.2.weight', 'downsample1_0.out_activation.weight', 'regular1_1.ext_conv1.0.weight', 'regular1_1.ext_conv1.1.weight', 'regular1_1.ext_conv1.1.bias', 'regular1_1.ext_conv1.1.running_mean', 'regular1_1.ext_conv1.1.running_var', 'regular1_1.ext_conv1.1.num_batches_tracked', 'regular1_1.ext_conv1.2.weight', 'regular1_1.ext_conv2.0.weight', 'regular1_1.ext_conv2.1.weight', 'regular1_1.ext_conv2.1.bias', 'regular1_1.ext_conv2.1.running_mean', 'regular1_1.ext_conv2.1.running_var', 'regular1_1.ext_conv2.1.num_batches_tracked', 'regular1_1.ext_conv2.2.weight', 'regular1_1.ext_conv3.0.weight', 'regular1_1.ext_conv3.1.weight', 'regular1_1.ext_conv3.1.bias', 'regular1_1.ext_conv3.1.running_mean', 'regular1_1.ext_conv3.1.running_var', 'regular1_1.ext_conv3.1.num_batches_tracked', 'regular1_1.ext_conv3.2.weight', 'regular1_1.out_activation.weight', 'regular1_2.ext_conv1.0.weight', 'regular1_2.ext_conv1.1.weight', 'regular1_2.ext_conv1.1.bias', 'regular1_2.ext_conv1.1.running_mean', 'regular1_2.ext_conv1.1.running_var', 'regular1_2.ext_conv1.1.num_batches_tracked', 'regular1_2.ext_conv1.2.weight', 'regular1_2.ext_conv2.0.weight', 'regular1_2.ext_conv2.1.weight', 'regular1_2.ext_conv2.1.bias', 'regular1_2.ext_conv2.1.running_mean', 'regular1_2.ext_conv2.1.running_var', 'regular1_2.ext_conv2.1.num_batches_tracked', 'regular1_2.ext_conv2.2.weight', 'regular1_2.ext_conv3.0.weight', 'regular1_2.ext_conv3.1.weight', 'regular1_2.ext_conv3.1.bias', 'regular1_2.ext_conv3.1.running_mean', 'regular1_2.ext_conv3.1.running_var', 'regular1_2.ext_conv3.1.num_batches_tracked', 'regular1_2.ext_conv3.2.weight', 'regular1_2.out_activation.weight', 'regular1_3.ext_conv1.0.weight', 'regular1_3.ext_conv1.1.weight', 'regular1_3.ext_conv1.1.bias', 'regular1_3.ext_conv1.1.running_mean', 'regular1_3.ext_conv1.1.running_var', 'regular1_3.ext_conv1.1.num_batches_tracked', 'regular1_3.ext_conv1.2.weight', 'regular1_3.ext_conv2.0.weight', 'regular1_3.ext_conv2.1.weight', 'regular1_3.ext_conv2.1.bias', 'regular1_3.ext_conv2.1.running_mean', 'regular1_3.ext_conv2.1.running_var', 'regular1_3.ext_conv2.1.num_batches_tracked', 'regular1_3.ext_conv2.2.weight', 'regular1_3.ext_conv3.0.weight', 'regular1_3.ext_conv3.1.weight', 'regular1_3.ext_conv3.1.bias', 'regular1_3.ext_conv3.1.running_mean', 'regular1_3.ext_conv3.1.running_var', 'regular1_3.ext_conv3.1.num_batches_tracked', 'regular1_3.ext_conv3.2.weight', 'regular1_3.out_activation.weight', 'regular1_4.ext_conv1.0.weight', 'regular1_4.ext_conv1.1.weight', 'regular1_4.ext_conv1.1.bias', 'regular1_4.ext_conv1.1.running_mean', 'regular1_4.ext_conv1.1.running_var', 'regular1_4.ext_conv1.1.num_batches_tracked', 'regular1_4.ext_conv1.2.weight', 'regular1_4.ext_conv2.0.weight', 'regular1_4.ext_conv2.1.weight', 'regular1_4.ext_conv2.1.bias', 'regular1_4.ext_conv2.1.running_mean', 'regular1_4.ext_conv2.1.running_var', 'regular1_4.ext_conv2.1.num_batches_tracked', 'regular1_4.ext_conv2.2.weight', 'regular1_4.ext_conv3.0.weight', 'regular1_4.ext_conv3.1.weight', 'regular1_4.ext_conv3.1.bias', 'regular1_4.ext_conv3.1.running_mean', 'regular1_4.ext_conv3.1.running_var', 'regular1_4.ext_conv3.1.num_batches_tracked', 'regular1_4.ext_conv3.2.weight', 'regular1_4.out_activation.weight', 'downsample2_0.ext_conv1.0.weight', 'downsample2_0.ext_conv1.1.weight', 'downsample2_0.ext_conv1.1.bias', 'downsample2_0.ext_conv1.1.running_mean', 'downsample2_0.ext_conv1.1.running_var', 'downsample2_0.ext_conv1.1.num_batches_tracked', 'downsample2_0.ext_conv1.2.weight', 'downsample2_0.ext_conv2.0.weight', 'downsample2_0.ext_conv2.1.weight', 'downsample2_0.ext_conv2.1.bias', 'downsample2_0.ext_conv2.1.running_mean', 'downsample2_0.ext_conv2.1.running_var', 'downsample2_0.ext_conv2.1.num_batches_tracked', 'downsample2_0.ext_conv2.2.weight', 'downsample2_0.ext_conv3.0.weight', 'downsample2_0.ext_conv3.1.weight', 'downsample2_0.ext_conv3.1.bias', 'downsample2_0.ext_conv3.1.running_mean', 'downsample2_0.ext_conv3.1.running_var', 'downsample2_0.ext_conv3.1.num_batches_tracked', 'downsample2_0.ext_conv3.2.weight', 'downsample2_0.out_activation.weight', 'regular2_1.ext_conv1.0.weight', 'regular2_1.ext_conv1.1.weight', 'regular2_1.ext_conv1.1.bias', 'regular2_1.ext_conv1.1.running_mean', 'regular2_1.ext_conv1.1.running_var', 'regular2_1.ext_conv1.1.num_batches_tracked', 'regular2_1.ext_conv1.2.weight', 'regular2_1.ext_conv2.0.weight', 'regular2_1.ext_conv2.1.weight', 'regular2_1.ext_conv2.1.bias', 'regular2_1.ext_conv2.1.running_mean', 'regular2_1.ext_conv2.1.running_var', 'regular2_1.ext_conv2.1.num_batches_tracked', 'regular2_1.ext_conv2.2.weight', 'regular2_1.ext_conv3.0.weight', 'regular2_1.ext_conv3.1.weight', 'regular2_1.ext_conv3.1.bias', 'regular2_1.ext_conv3.1.running_mean', 'regular2_1.ext_conv3.1.running_var', 'regular2_1.ext_conv3.1.num_batches_tracked', 'regular2_1.ext_conv3.2.weight', 'regular2_1.out_activation.weight', 'dilated2_2.ext_conv1.0.weight', 'dilated2_2.ext_conv1.1.weight', 'dilated2_2.ext_conv1.1.bias', 'dilated2_2.ext_conv1.1.running_mean', 'dilated2_2.ext_conv1.1.running_var', 'dilated2_2.ext_conv1.1.num_batches_tracked', 'dilated2_2.ext_conv1.2.weight', 'dilated2_2.ext_conv2.0.weight', 'dilated2_2.ext_conv2.1.weight', 'dilated2_2.ext_conv2.1.bias', 'dilated2_2.ext_conv2.1.running_mean', 'dilated2_2.ext_conv2.1.running_var', 'dilated2_2.ext_conv2.1.num_batches_tracked', 'dilated2_2.ext_conv2.2.weight', 'dilated2_2.ext_conv3.0.weight', 'dilated2_2.ext_conv3.1.weight', 'dilated2_2.ext_conv3.1.bias', 'dilated2_2.ext_conv3.1.running_mean', 'dilated2_2.ext_conv3.1.running_var', 'dilated2_2.ext_conv3.1.num_batches_tracked', 'dilated2_2.ext_conv3.2.weight', 'dilated2_2.out_activation.weight', 'asymmetric2_3.ext_conv1.0.weight', 'asymmetric2_3.ext_conv1.1.weight', 'asymmetric2_3.ext_conv1.1.bias', 'asymmetric2_3.ext_conv1.1.running_mean', 'asymmetric2_3.ext_conv1.1.running_var', 'asymmetric2_3.ext_conv1.1.num_batches_tracked', 'asymmetric2_3.ext_conv1.2.weight', 'asymmetric2_3.ext_conv2.0.weight', 'asymmetric2_3.ext_conv2.1.weight', 'asymmetric2_3.ext_conv2.1.bias', 'asymmetric2_3.ext_conv2.1.running_mean', 'asymmetric2_3.ext_conv2.1.running_var', 'asymmetric2_3.ext_conv2.1.num_batches_tracked', 'asymmetric2_3.ext_conv2.2.weight', 'asymmetric2_3.ext_conv2.3.weight', 'asymmetric2_3.ext_conv2.4.weight', 'asymmetric2_3.ext_conv2.4.bias', 'asymmetric2_3.ext_conv2.4.running_mean', 'asymmetric2_3.ext_conv2.4.running_var', 'asymmetric2_3.ext_conv2.4.num_batches_tracked', 'asymmetric2_3.ext_conv2.5.weight', 'asymmetric2_3.ext_conv3.0.weight', 'asymmetric2_3.ext_conv3.1.weight', 'asymmetric2_3.ext_conv3.1.bias', 'asymmetric2_3.ext_conv3.1.running_mean', 'asymmetric2_3.ext_conv3.1.running_var', 'asymmetric2_3.ext_conv3.1.num_batches_tracked', 'asymmetric2_3.ext_conv3.2.weight', 'asymmetric2_3.out_activation.weight', 'dilated2_4.ext_conv1.0.weight', 'dilated2_4.ext_conv1.1.weight', 'dilated2_4.ext_conv1.1.bias', 'dilated2_4.ext_conv1.1.running_mean', 'dilated2_4.ext_conv1.1.running_var', 'dilated2_4.ext_conv1.1.num_batches_tracked', 'dilated2_4.ext_conv1.2.weight', 'dilated2_4.ext_conv2.0.weight', 'dilated2_4.ext_conv2.1.weight', 'dilated2_4.ext_conv2.1.bias', 'dilated2_4.ext_conv2.1.running_mean', 'dilated2_4.ext_conv2.1.running_var', 'dilated2_4.ext_conv2.1.num_batches_tracked', 'dilated2_4.ext_conv2.2.weight', 'dilated2_4.ext_conv3.0.weight', 'dilated2_4.ext_conv3.1.weight', 'dilated2_4.ext_conv3.1.bias', 'dilated2_4.ext_conv3.1.running_mean', 'dilated2_4.ext_conv3.1.running_var', 'dilated2_4.ext_conv3.1.num_batches_tracked', 'dilated2_4.ext_conv3.2.weight', 'dilated2_4.out_activation.weight', 'regular2_5.ext_conv1.0.weight', 'regular2_5.ext_conv1.1.weight', 'regular2_5.ext_conv1.1.bias', 'regular2_5.ext_conv1.1.running_mean', 'regular2_5.ext_conv1.1.running_var', 'regular2_5.ext_conv1.1.num_batches_tracked', 'regular2_5.ext_conv1.2.weight', 'regular2_5.ext_conv2.0.weight', 'regular2_5.ext_conv2.1.weight', 'regular2_5.ext_conv2.1.bias', 'regular2_5.ext_conv2.1.running_mean', 'regular2_5.ext_conv2.1.running_var', 'regular2_5.ext_conv2.1.num_batches_tracked', 'regular2_5.ext_conv2.2.weight', 'regular2_5.ext_conv3.0.weight', 'regular2_5.ext_conv3.1.weight', 'regular2_5.ext_conv3.1.bias', 'regular2_5.ext_conv3.1.running_mean', 'regular2_5.ext_conv3.1.running_var', 'regular2_5.ext_conv3.1.num_batches_tracked', 'regular2_5.ext_conv3.2.weight', 'regular2_5.out_activation.weight', 'dilated2_6.ext_conv1.0.weight', 'dilated2_6.ext_conv1.1.weight', 'dilated2_6.ext_conv1.1.bias', 'dilated2_6.ext_conv1.1.running_mean', 'dilated2_6.ext_conv1.1.running_var', 'dilated2_6.ext_conv1.1.num_batches_tracked', 'dilated2_6.ext_conv1.2.weight', 'dilated2_6.ext_conv2.0.weight', 'dilated2_6.ext_conv2.1.weight', 'dilated2_6.ext_conv2.1.bias', 'dilated2_6.ext_conv2.1.running_mean', 'dilated2_6.ext_conv2.1.running_var', 'dilated2_6.ext_conv2.1.num_batches_tracked', 'dilated2_6.ext_conv2.2.weight', 'dilated2_6.ext_conv3.0.weight', 'dilated2_6.ext_conv3.1.weight', 'dilated2_6.ext_conv3.1.bias', 'dilated2_6.ext_conv3.1.running_mean', 'dilated2_6.ext_conv3.1.running_var', 'dilated2_6.ext_conv3.1.num_batches_tracked', 'dilated2_6.ext_conv3.2.weight', 'dilated2_6.out_activation.weight', 'asymmetric2_7.ext_conv1.0.weight', 'asymmetric2_7.ext_conv1.1.weight', 'asymmetric2_7.ext_conv1.1.bias', 'asymmetric2_7.ext_conv1.1.running_mean', 'asymmetric2_7.ext_conv1.1.running_var', 'asymmetric2_7.ext_conv1.1.num_batches_tracked', 'asymmetric2_7.ext_conv1.2.weight', 'asymmetric2_7.ext_conv2.0.weight', 'asymmetric2_7.ext_conv2.1.weight', 'asymmetric2_7.ext_conv2.1.bias', 'asymmetric2_7.ext_conv2.1.running_mean', 'asymmetric2_7.ext_conv2.1.running_var', 'asymmetric2_7.ext_conv2.1.num_batches_tracked', 'asymmetric2_7.ext_conv2.2.weight', 'asymmetric2_7.ext_conv2.3.weight', 'asymmetric2_7.ext_conv2.4.weight', 'asymmetric2_7.ext_conv2.4.bias', 'asymmetric2_7.ext_conv2.4.running_mean', 'asymmetric2_7.ext_conv2.4.running_var', 'asymmetric2_7.ext_conv2.4.num_batches_tracked', 'asymmetric2_7.ext_conv2.5.weight', 'asymmetric2_7.ext_conv3.0.weight', 'asymmetric2_7.ext_conv3.1.weight', 'asymmetric2_7.ext_conv3.1.bias', 'asymmetric2_7.ext_conv3.1.running_mean', 'asymmetric2_7.ext_conv3.1.running_var', 'asymmetric2_7.ext_conv3.1.num_batches_tracked', 'asymmetric2_7.ext_conv3.2.weight', 'asymmetric2_7.out_activation.weight', 'dilated2_8.ext_conv1.0.weight', 'dilated2_8.ext_conv1.1.weight', 'dilated2_8.ext_conv1.1.bias', 'dilated2_8.ext_conv1.1.running_mean', 'dilated2_8.ext_conv1.1.running_var', 'dilated2_8.ext_conv1.1.num_batches_tracked', 'dilated2_8.ext_conv1.2.weight', 'dilated2_8.ext_conv2.0.weight', 'dilated2_8.ext_conv2.1.weight', 'dilated2_8.ext_conv2.1.bias', 'dilated2_8.ext_conv2.1.running_mean', 'dilated2_8.ext_conv2.1.running_var', 'dilated2_8.ext_conv2.1.num_batches_tracked', 'dilated2_8.ext_conv2.2.weight', 'dilated2_8.ext_conv3.0.weight', 'dilated2_8.ext_conv3.1.weight', 'dilated2_8.ext_conv3.1.bias', 'dilated2_8.ext_conv3.1.running_mean', 'dilated2_8.ext_conv3.1.running_var', 'dilated2_8.ext_conv3.1.num_batches_tracked', 'dilated2_8.ext_conv3.2.weight', 'dilated2_8.out_activation.weight', 'regular3_0.ext_conv1.0.weight', 'regular3_0.ext_conv1.1.weight', 'regular3_0.ext_conv1.1.bias', 'regular3_0.ext_conv1.1.running_mean', 'regular3_0.ext_conv1.1.running_var', 'regular3_0.ext_conv1.1.num_batches_tracked', 'regular3_0.ext_conv1.2.weight', 'regular3_0.ext_conv2.0.weight', 'regular3_0.ext_conv2.1.weight', 'regular3_0.ext_conv2.1.bias', 'regular3_0.ext_conv2.1.running_mean', 'regular3_0.ext_conv2.1.running_var', 'regular3_0.ext_conv2.1.num_batches_tracked', 'regular3_0.ext_conv2.2.weight', 'regular3_0.ext_conv3.0.weight', 'regular3_0.ext_conv3.1.weight', 'regular3_0.ext_conv3.1.bias', 'regular3_0.ext_conv3.1.running_mean', 'regular3_0.ext_conv3.1.running_var', 'regular3_0.ext_conv3.1.num_batches_tracked', 'regular3_0.ext_conv3.2.weight', 'regular3_0.out_activation.weight', 'dilated3_1.ext_conv1.0.weight', 'dilated3_1.ext_conv1.1.weight', 'dilated3_1.ext_conv1.1.bias', 'dilated3_1.ext_conv1.1.running_mean', 'dilated3_1.ext_conv1.1.running_var', 'dilated3_1.ext_conv1.1.num_batches_tracked', 'dilated3_1.ext_conv1.2.weight', 'dilated3_1.ext_conv2.0.weight', 'dilated3_1.ext_conv2.1.weight', 'dilated3_1.ext_conv2.1.bias', 'dilated3_1.ext_conv2.1.running_mean', 'dilated3_1.ext_conv2.1.running_var', 'dilated3_1.ext_conv2.1.num_batches_tracked', 'dilated3_1.ext_conv2.2.weight', 'dilated3_1.ext_conv3.0.weight', 'dilated3_1.ext_conv3.1.weight', 'dilated3_1.ext_conv3.1.bias', 'dilated3_1.ext_conv3.1.running_mean', 'dilated3_1.ext_conv3.1.running_var', 'dilated3_1.ext_conv3.1.num_batches_tracked', 'dilated3_1.ext_conv3.2.weight', 'dilated3_1.out_activation.weight', 'asymmetric3_2.ext_conv1.0.weight', 'asymmetric3_2.ext_conv1.1.weight', 'asymmetric3_2.ext_conv1.1.bias', 'asymmetric3_2.ext_conv1.1.running_mean', 'asymmetric3_2.ext_conv1.1.running_var', 'asymmetric3_2.ext_conv1.1.num_batches_tracked', 'asymmetric3_2.ext_conv1.2.weight', 'asymmetric3_2.ext_conv2.0.weight', 'asymmetric3_2.ext_conv2.1.weight', 'asymmetric3_2.ext_conv2.1.bias', 'asymmetric3_2.ext_conv2.1.running_mean', 'asymmetric3_2.ext_conv2.1.running_var', 'asymmetric3_2.ext_conv2.1.num_batches_tracked', 'asymmetric3_2.ext_conv2.2.weight', 'asymmetric3_2.ext_conv2.3.weight', 'asymmetric3_2.ext_conv2.4.weight', 'asymmetric3_2.ext_conv2.4.bias', 'asymmetric3_2.ext_conv2.4.running_mean', 'asymmetric3_2.ext_conv2.4.running_var', 'asymmetric3_2.ext_conv2.4.num_batches_tracked', 'asymmetric3_2.ext_conv2.5.weight', 'asymmetric3_2.ext_conv3.0.weight', 'asymmetric3_2.ext_conv3.1.weight', 'asymmetric3_2.ext_conv3.1.bias', 'asymmetric3_2.ext_conv3.1.running_mean', 'asymmetric3_2.ext_conv3.1.running_var', 'asymmetric3_2.ext_conv3.1.num_batches_tracked', 'asymmetric3_2.ext_conv3.2.weight', 'asymmetric3_2.out_activation.weight', 'dilated3_3.ext_conv1.0.weight', 'dilated3_3.ext_conv1.1.weight', 'dilated3_3.ext_conv1.1.bias', 'dilated3_3.ext_conv1.1.running_mean', 'dilated3_3.ext_conv1.1.running_var', 'dilated3_3.ext_conv1.1.num_batches_tracked', 'dilated3_3.ext_conv1.2.weight', 'dilated3_3.ext_conv2.0.weight', 'dilated3_3.ext_conv2.1.weight', 'dilated3_3.ext_conv2.1.bias', 'dilated3_3.ext_conv2.1.running_mean', 'dilated3_3.ext_conv2.1.running_var', 'dilated3_3.ext_conv2.1.num_batches_tracked', 'dilated3_3.ext_conv2.2.weight', 'dilated3_3.ext_conv3.0.weight', 'dilated3_3.ext_conv3.1.weight', 'dilated3_3.ext_conv3.1.bias', 'dilated3_3.ext_conv3.1.running_mean', 'dilated3_3.ext_conv3.1.running_var', 'dilated3_3.ext_conv3.1.num_batches_tracked', 'dilated3_3.ext_conv3.2.weight', 'dilated3_3.out_activation.weight', 'regular3_4.ext_conv1.0.weight', 'regular3_4.ext_conv1.1.weight', 'regular3_4.ext_conv1.1.bias', 'regular3_4.ext_conv1.1.running_mean', 'regular3_4.ext_conv1.1.running_var', 'regular3_4.ext_conv1.1.num_batches_tracked', 'regular3_4.ext_conv1.2.weight', 'regular3_4.ext_conv2.0.weight', 'regular3_4.ext_conv2.1.weight', 'regular3_4.ext_conv2.1.bias', 'regular3_4.ext_conv2.1.running_mean', 'regular3_4.ext_conv2.1.running_var', 'regular3_4.ext_conv2.1.num_batches_tracked', 'regular3_4.ext_conv2.2.weight', 'regular3_4.ext_conv3.0.weight', 'regular3_4.ext_conv3.1.weight', 'regular3_4.ext_conv3.1.bias', 'regular3_4.ext_conv3.1.running_mean', 'regular3_4.ext_conv3.1.running_var', 'regular3_4.ext_conv3.1.num_batches_tracked', 'regular3_4.ext_conv3.2.weight', 'regular3_4.out_activation.weight', 'dilated3_5.ext_conv1.0.weight', 'dilated3_5.ext_conv1.1.weight', 'dilated3_5.ext_conv1.1.bias', 'dilated3_5.ext_conv1.1.running_mean', 'dilated3_5.ext_conv1.1.running_var', 'dilated3_5.ext_conv1.1.num_batches_tracked', 'dilated3_5.ext_conv1.2.weight', 'dilated3_5.ext_conv2.0.weight', 'dilated3_5.ext_conv2.1.weight', 'dilated3_5.ext_conv2.1.bias', 'dilated3_5.ext_conv2.1.running_mean', 'dilated3_5.ext_conv2.1.running_var', 'dilated3_5.ext_conv2.1.num_batches_tracked', 'dilated3_5.ext_conv2.2.weight', 'dilated3_5.ext_conv3.0.weight', 'dilated3_5.ext_conv3.1.weight', 'dilated3_5.ext_conv3.1.bias', 'dilated3_5.ext_conv3.1.running_mean', 'dilated3_5.ext_conv3.1.running_var', 'dilated3_5.ext_conv3.1.num_batches_tracked', 'dilated3_5.ext_conv3.2.weight', 'dilated3_5.out_activation.weight', 'asymmetric3_6.ext_conv1.0.weight', 'asymmetric3_6.ext_conv1.1.weight', 'asymmetric3_6.ext_conv1.1.bias', 'asymmetric3_6.ext_conv1.1.running_mean', 'asymmetric3_6.ext_conv1.1.running_var', 'asymmetric3_6.ext_conv1.1.num_batches_tracked', 'asymmetric3_6.ext_conv1.2.weight', 'asymmetric3_6.ext_conv2.0.weight', 'asymmetric3_6.ext_conv2.1.weight', 'asymmetric3_6.ext_conv2.1.bias', 'asymmetric3_6.ext_conv2.1.running_mean', 'asymmetric3_6.ext_conv2.1.running_var', 'asymmetric3_6.ext_conv2.1.num_batches_tracked', 'asymmetric3_6.ext_conv2.2.weight', 'asymmetric3_6.ext_conv2.3.weight', 'asymmetric3_6.ext_conv2.4.weight', 'asymmetric3_6.ext_conv2.4.bias', 'asymmetric3_6.ext_conv2.4.running_mean', 'asymmetric3_6.ext_conv2.4.running_var', 'asymmetric3_6.ext_conv2.4.num_batches_tracked', 'asymmetric3_6.ext_conv2.5.weight', 'asymmetric3_6.ext_conv3.0.weight', 'asymmetric3_6.ext_conv3.1.weight', 'asymmetric3_6.ext_conv3.1.bias', 'asymmetric3_6.ext_conv3.1.running_mean', 'asymmetric3_6.ext_conv3.1.running_var', 'asymmetric3_6.ext_conv3.1.num_batches_tracked', 'asymmetric3_6.ext_conv3.2.weight', 'asymmetric3_6.out_activation.weight', 'dilated3_7.ext_conv1.0.weight', 'dilated3_7.ext_conv1.1.weight', 'dilated3_7.ext_conv1.1.bias', 'dilated3_7.ext_conv1.1.running_mean', 'dilated3_7.ext_conv1.1.running_var', 'dilated3_7.ext_conv1.1.num_batches_tracked', 'dilated3_7.ext_conv1.2.weight', 'dilated3_7.ext_conv2.0.weight', 'dilated3_7.ext_conv2.1.weight', 'dilated3_7.ext_conv2.1.bias', 'dilated3_7.ext_conv2.1.running_mean', 'dilated3_7.ext_conv2.1.running_var', 'dilated3_7.ext_conv2.1.num_batches_tracked', 'dilated3_7.ext_conv2.2.weight', 'dilated3_7.ext_conv3.0.weight', 'dilated3_7.ext_conv3.1.weight', 'dilated3_7.ext_conv3.1.bias', 'dilated3_7.ext_conv3.1.running_mean', 'dilated3_7.ext_conv3.1.running_var', 'dilated3_7.ext_conv3.1.num_batches_tracked', 'dilated3_7.ext_conv3.2.weight', 'dilated3_7.out_activation.weight', 'upsample4_0.main_conv1.0.weight', 'upsample4_0.main_conv1.1.weight', 'upsample4_0.main_conv1.1.bias', 'upsample4_0.main_conv1.1.running_mean', 'upsample4_0.main_conv1.1.running_var', 'upsample4_0.main_conv1.1.num_batches_tracked', 'upsample4_0.ext_conv1.0.weight', 'upsample4_0.ext_conv1.1.weight', 'upsample4_0.ext_conv1.1.bias', 'upsample4_0.ext_conv1.1.running_mean', 'upsample4_0.ext_conv1.1.running_var', 'upsample4_0.ext_conv1.1.num_batches_tracked', 'upsample4_0.ext_tconv1.weight', 'upsample4_0.ext_tconv1_bnorm.weight', 'upsample4_0.ext_tconv1_bnorm.bias', 'upsample4_0.ext_tconv1_bnorm.running_mean', 'upsample4_0.ext_tconv1_bnorm.running_var', 'upsample4_0.ext_tconv1_bnorm.num_batches_tracked', 'upsample4_0.ext_conv2.0.weight', 'upsample4_0.ext_conv2.1.weight', 'upsample4_0.ext_conv2.1.bias', 'upsample4_0.ext_conv2.1.running_mean', 'upsample4_0.ext_conv2.1.running_var', 'upsample4_0.ext_conv2.1.num_batches_tracked', 'regular4_1.ext_conv1.0.weight', 'regular4_1.ext_conv1.1.weight', 'regular4_1.ext_conv1.1.bias', 'regular4_1.ext_conv1.1.running_mean', 'regular4_1.ext_conv1.1.running_var', 'regular4_1.ext_conv1.1.num_batches_tracked', 'regular4_1.ext_conv2.0.weight', 'regular4_1.ext_conv2.1.weight', 'regular4_1.ext_conv2.1.bias', 'regular4_1.ext_conv2.1.running_mean', 'regular4_1.ext_conv2.1.running_var', 'regular4_1.ext_conv2.1.num_batches_tracked', 'regular4_1.ext_conv3.0.weight', 'regular4_1.ext_conv3.1.weight', 'regular4_1.ext_conv3.1.bias', 'regular4_1.ext_conv3.1.running_mean', 'regular4_1.ext_conv3.1.running_var', 'regular4_1.ext_conv3.1.num_batches_tracked', 'regular4_2.ext_conv1.0.weight', 'regular4_2.ext_conv1.1.weight', 'regular4_2.ext_conv1.1.bias', 'regular4_2.ext_conv1.1.running_mean', 'regular4_2.ext_conv1.1.running_var', 'regular4_2.ext_conv1.1.num_batches_tracked', 'regular4_2.ext_conv2.0.weight', 'regular4_2.ext_conv2.1.weight', 'regular4_2.ext_conv2.1.bias', 'regular4_2.ext_conv2.1.running_mean', 'regular4_2.ext_conv2.1.running_var', 'regular4_2.ext_conv2.1.num_batches_tracked', 'regular4_2.ext_conv3.0.weight', 'regular4_2.ext_conv3.1.weight', 'regular4_2.ext_conv3.1.bias', 'regular4_2.ext_conv3.1.running_mean', 'regular4_2.ext_conv3.1.running_var', 'regular4_2.ext_conv3.1.num_batches_tracked', 'upsample5_0.main_conv1.0.weight', 'upsample5_0.main_conv1.1.weight', 'upsample5_0.main_conv1.1.bias', 'upsample5_0.main_conv1.1.running_mean', 'upsample5_0.main_conv1.1.running_var', 'upsample5_0.main_conv1.1.num_batches_tracked', 'upsample5_0.ext_conv1.0.weight', 'upsample5_0.ext_conv1.1.weight', 'upsample5_0.ext_conv1.1.bias', 'upsample5_0.ext_conv1.1.running_mean', 'upsample5_0.ext_conv1.1.running_var', 'upsample5_0.ext_conv1.1.num_batches_tracked', 'upsample5_0.ext_tconv1.weight', 'upsample5_0.ext_tconv1_bnorm.weight', 'upsample5_0.ext_tconv1_bnorm.bias', 'upsample5_0.ext_tconv1_bnorm.running_mean', 'upsample5_0.ext_tconv1_bnorm.running_var', 'upsample5_0.ext_tconv1_bnorm.num_batches_tracked', 'upsample5_0.ext_conv2.0.weight', 'upsample5_0.ext_conv2.1.weight', 'upsample5_0.ext_conv2.1.bias', 'upsample5_0.ext_conv2.1.running_mean', 'upsample5_0.ext_conv2.1.running_var', 'upsample5_0.ext_conv2.1.num_batches_tracked', 'regular5_1.ext_conv1.0.weight', 'regular5_1.ext_conv1.1.weight', 'regular5_1.ext_conv1.1.bias', 'regular5_1.ext_conv1.1.running_mean', 'regular5_1.ext_conv1.1.running_var', 'regular5_1.ext_conv1.1.num_batches_tracked', 'regular5_1.ext_conv2.0.weight', 'regular5_1.ext_conv2.1.weight', 'regular5_1.ext_conv2.1.bias', 'regular5_1.ext_conv2.1.running_mean', 'regular5_1.ext_conv2.1.running_var', 'regular5_1.ext_conv2.1.num_batches_tracked', 'regular5_1.ext_conv3.0.weight', 'regular5_1.ext_conv3.1.weight', 'regular5_1.ext_conv3.1.bias', 'regular5_1.ext_conv3.1.running_mean', 'regular5_1.ext_conv3.1.running_var', 'regular5_1.ext_conv3.1.num_batches_tracked', 'transposed_conv.weight'])\n",
            "Import Model enet with weights enet_pretrained to FineTune\n",
            "========== TRAINING ===========\n",
            "/content/cityscapes/leftImg8bit/train\n",
            "/content/cityscapes/leftImg8bit/val\n",
            "<class '__main__.CrossEntropyLoss2d'>\n",
            "----- TRAINING - EPOCH 1 -----\n",
            "LEARNING RATE:  5e-05\n",
            "loss: 10.41 (epoch: 1, step: 0) // Avg time/img: 0.6229 s\n",
            "loss: 10.08 (epoch: 1, step: 50) // Avg time/img: 0.0636 s\n",
            "loss: 9.986 (epoch: 1, step: 100) // Avg time/img: 0.0579 s\n",
            "loss: 9.887 (epoch: 1, step: 150) // Avg time/img: 0.0568 s\n",
            "loss: 9.761 (epoch: 1, step: 200) // Avg time/img: 0.0562 s\n",
            "loss: 9.651 (epoch: 1, step: 250) // Avg time/img: 0.0558 s\n",
            "loss: 9.534 (epoch: 1, step: 300) // Avg time/img: 0.0552 s\n",
            "loss: 9.426 (epoch: 1, step: 350) // Avg time/img: 0.0550 s\n",
            "loss: 9.327 (epoch: 1, step: 400) // Avg time/img: 0.0548 s\n",
            "loss: 9.227 (epoch: 1, step: 450) // Avg time/img: 0.0548 s\n",
            "----- VALIDATING - EPOCH 1 -----\n",
            "VAL loss: 8.321 (epoch: 1, step: 0) // Avg time/img: 0.0417 s\n",
            "VAL loss: 7.994 (epoch: 1, step: 50) // Avg time/img: 0.0318 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m1.24\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training1/model-001.pth (epoch: 1)\n",
            "save: ../save/enet_training1/model_best.pth (epoch: 1)\n",
            "----- TRAINING - EPOCH 2 -----\n",
            "LEARNING RATE:  4.774426908107499e-05\n",
            "loss: 7.93 (epoch: 2, step: 0) // Avg time/img: 0.0601 s\n",
            "loss: 7.96 (epoch: 2, step: 50) // Avg time/img: 0.0531 s\n",
            "loss: 7.928 (epoch: 2, step: 100) // Avg time/img: 0.0541 s\n",
            "loss: 7.81 (epoch: 2, step: 150) // Avg time/img: 0.0538 s\n",
            "loss: 7.702 (epoch: 2, step: 200) // Avg time/img: 0.0540 s\n",
            "loss: 7.6 (epoch: 2, step: 250) // Avg time/img: 0.0535 s\n",
            "loss: 7.512 (epoch: 2, step: 300) // Avg time/img: 0.0537 s\n",
            "loss: 7.419 (epoch: 2, step: 350) // Avg time/img: 0.0535 s\n",
            "loss: 7.327 (epoch: 2, step: 400) // Avg time/img: 0.0537 s\n",
            "loss: 7.242 (epoch: 2, step: 450) // Avg time/img: 0.0537 s\n",
            "----- VALIDATING - EPOCH 2 -----\n",
            "VAL loss: 6.321 (epoch: 2, step: 0) // Avg time/img: 0.0525 s\n",
            "VAL loss: 6.296 (epoch: 2, step: 50) // Avg time/img: 0.0338 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m2.38\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training1/model-002.pth (epoch: 2)\n",
            "save: ../save/enet_training1/model_best.pth (epoch: 2)\n",
            "----- TRAINING - EPOCH 3 -----\n",
            "LEARNING RATE:  4.547662880414811e-05\n",
            "loss: 6.217 (epoch: 3, step: 0) // Avg time/img: 0.0586 s\n",
            "loss: 6.255 (epoch: 3, step: 50) // Avg time/img: 0.0535 s\n",
            "loss: 6.177 (epoch: 3, step: 100) // Avg time/img: 0.0529 s\n",
            "loss: 6.106 (epoch: 3, step: 150) // Avg time/img: 0.0526 s\n",
            "loss: 6.04 (epoch: 3, step: 200) // Avg time/img: 0.0529 s\n",
            "loss: 5.954 (epoch: 3, step: 250) // Avg time/img: 0.0528 s\n",
            "loss: 5.891 (epoch: 3, step: 300) // Avg time/img: 0.0533 s\n",
            "loss: 5.815 (epoch: 3, step: 350) // Avg time/img: 0.0533 s\n",
            "loss: 5.741 (epoch: 3, step: 400) // Avg time/img: 0.0532 s\n",
            "loss: 5.675 (epoch: 3, step: 450) // Avg time/img: 0.0534 s\n",
            "----- VALIDATING - EPOCH 3 -----\n",
            "VAL loss: 4.804 (epoch: 3, step: 0) // Avg time/img: 0.0549 s\n",
            "VAL loss: 4.963 (epoch: 3, step: 50) // Avg time/img: 0.0338 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m5.06\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training1/model-003.pth (epoch: 3)\n",
            "save: ../save/enet_training1/model_best.pth (epoch: 3)\n",
            "----- TRAINING - EPOCH 4 -----\n",
            "LEARNING RATE:  4.319634861514096e-05\n",
            "loss: 5.254 (epoch: 4, step: 0) // Avg time/img: 0.0636 s\n",
            "loss: 4.952 (epoch: 4, step: 50) // Avg time/img: 0.0542 s\n",
            "loss: 4.894 (epoch: 4, step: 100) // Avg time/img: 0.0541 s\n",
            "loss: 4.842 (epoch: 4, step: 150) // Avg time/img: 0.0537 s\n",
            "loss: 4.747 (epoch: 4, step: 200) // Avg time/img: 0.0537 s\n",
            "loss: 4.703 (epoch: 4, step: 250) // Avg time/img: 0.0537 s\n",
            "loss: 4.639 (epoch: 4, step: 300) // Avg time/img: 0.0536 s\n",
            "loss: 4.591 (epoch: 4, step: 350) // Avg time/img: 0.0539 s\n",
            "loss: 4.537 (epoch: 4, step: 400) // Avg time/img: 0.0540 s\n",
            "loss: 4.491 (epoch: 4, step: 450) // Avg time/img: 0.0541 s\n",
            "----- VALIDATING - EPOCH 4 -----\n",
            "VAL loss: 3.702 (epoch: 4, step: 0) // Avg time/img: 0.0559 s\n",
            "VAL loss: 3.96 (epoch: 4, step: 50) // Avg time/img: 0.0340 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m8.24\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training1/model-004.pth (epoch: 4)\n",
            "save: ../save/enet_training1/model_best.pth (epoch: 4)\n",
            "----- TRAINING - EPOCH 5 -----\n",
            "LEARNING RATE:  4.090260730254292e-05\n",
            "loss: 4.192 (epoch: 5, step: 0) // Avg time/img: 0.0414 s\n",
            "loss: 4.0 (epoch: 5, step: 50) // Avg time/img: 0.0526 s\n",
            "loss: 3.942 (epoch: 5, step: 100) // Avg time/img: 0.0534 s\n",
            "loss: 3.891 (epoch: 5, step: 150) // Avg time/img: 0.0529 s\n",
            "loss: 3.831 (epoch: 5, step: 200) // Avg time/img: 0.0532 s\n",
            "loss: 3.792 (epoch: 5, step: 250) // Avg time/img: 0.0530 s\n",
            "loss: 3.762 (epoch: 5, step: 300) // Avg time/img: 0.0531 s\n",
            "loss: 3.716 (epoch: 5, step: 350) // Avg time/img: 0.0533 s\n",
            "loss: 3.684 (epoch: 5, step: 400) // Avg time/img: 0.0533 s\n",
            "loss: 3.657 (epoch: 5, step: 450) // Avg time/img: 0.0534 s\n",
            "----- VALIDATING - EPOCH 5 -----\n",
            "VAL loss: 3.008 (epoch: 5, step: 0) // Avg time/img: 0.0495 s\n",
            "VAL loss: 3.35 (epoch: 5, step: 50) // Avg time/img: 0.0348 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m12.11\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training1/model-005.pth (epoch: 5)\n",
            "save: ../save/enet_training1/model_best.pth (epoch: 5)\n",
            "----- TRAINING - EPOCH 6 -----\n",
            "LEARNING RATE:  3.859447533617852e-05\n",
            "loss: 2.847 (epoch: 6, step: 0) // Avg time/img: 0.0525 s\n",
            "loss: 3.266 (epoch: 6, step: 50) // Avg time/img: 0.0549 s\n",
            "loss: 3.236 (epoch: 6, step: 100) // Avg time/img: 0.0541 s\n",
            "loss: 3.186 (epoch: 6, step: 150) // Avg time/img: 0.0544 s\n",
            "loss: 3.159 (epoch: 6, step: 200) // Avg time/img: 0.0542 s\n",
            "loss: 3.147 (epoch: 6, step: 250) // Avg time/img: 0.0537 s\n",
            "loss: 3.121 (epoch: 6, step: 300) // Avg time/img: 0.0541 s\n",
            "loss: 3.082 (epoch: 6, step: 350) // Avg time/img: 0.0539 s\n",
            "loss: 3.056 (epoch: 6, step: 400) // Avg time/img: 0.0541 s\n",
            "loss: 3.027 (epoch: 6, step: 450) // Avg time/img: 0.0540 s\n",
            "----- VALIDATING - EPOCH 6 -----\n",
            "VAL loss: 2.454 (epoch: 6, step: 0) // Avg time/img: 0.0584 s\n",
            "VAL loss: 2.802 (epoch: 6, step: 50) // Avg time/img: 0.0339 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m14.85\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training1/model-006.pth (epoch: 6)\n",
            "save: ../save/enet_training1/model_best.pth (epoch: 6)\n",
            "----- TRAINING - EPOCH 7 -----\n",
            "LEARNING RATE:  3.6270892346861e-05\n",
            "loss: 2.981 (epoch: 7, step: 0) // Avg time/img: 0.0772 s\n",
            "loss: 2.752 (epoch: 7, step: 50) // Avg time/img: 0.0566 s\n",
            "loss: 2.704 (epoch: 7, step: 100) // Avg time/img: 0.0551 s\n",
            "loss: 2.685 (epoch: 7, step: 150) // Avg time/img: 0.0554 s\n",
            "loss: 2.645 (epoch: 7, step: 200) // Avg time/img: 0.0550 s\n",
            "loss: 2.626 (epoch: 7, step: 250) // Avg time/img: 0.0552 s\n",
            "loss: 2.619 (epoch: 7, step: 300) // Avg time/img: 0.0549 s\n",
            "loss: 2.582 (epoch: 7, step: 350) // Avg time/img: 0.0548 s\n",
            "loss: 2.56 (epoch: 7, step: 400) // Avg time/img: 0.0547 s\n",
            "loss: 2.537 (epoch: 7, step: 450) // Avg time/img: 0.0545 s\n",
            "----- VALIDATING - EPOCH 7 -----\n",
            "VAL loss: 2.024 (epoch: 7, step: 0) // Avg time/img: 0.0571 s\n",
            "VAL loss: 2.394 (epoch: 7, step: 50) // Avg time/img: 0.0331 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m17.25\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training1/model-007.pth (epoch: 7)\n",
            "save: ../save/enet_training1/model_best.pth (epoch: 7)\n",
            "----- TRAINING - EPOCH 8 -----\n",
            "LEARNING RATE:  3.393063796290625e-05\n",
            "loss: 2.452 (epoch: 8, step: 0) // Avg time/img: 0.0613 s\n",
            "loss: 2.323 (epoch: 8, step: 50) // Avg time/img: 0.0529 s\n",
            "loss: 2.302 (epoch: 8, step: 100) // Avg time/img: 0.0542 s\n",
            "loss: 2.277 (epoch: 8, step: 150) // Avg time/img: 0.0543 s\n",
            "loss: 2.265 (epoch: 8, step: 200) // Avg time/img: 0.0544 s\n",
            "loss: 2.252 (epoch: 8, step: 250) // Avg time/img: 0.0544 s\n",
            "loss: 2.225 (epoch: 8, step: 300) // Avg time/img: 0.0539 s\n",
            "loss: 2.208 (epoch: 8, step: 350) // Avg time/img: 0.0541 s\n",
            "loss: 2.203 (epoch: 8, step: 400) // Avg time/img: 0.0540 s\n",
            "loss: 2.172 (epoch: 8, step: 450) // Avg time/img: 0.0541 s\n",
            "----- VALIDATING - EPOCH 8 -----\n",
            "VAL loss: 1.754 (epoch: 8, step: 0) // Avg time/img: 0.0584 s\n",
            "VAL loss: 2.102 (epoch: 8, step: 50) // Avg time/img: 0.0345 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m18.76\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training1/model-008.pth (epoch: 8)\n",
            "save: ../save/enet_training1/model_best.pth (epoch: 8)\n",
            "----- TRAINING - EPOCH 9 -----\n",
            "LEARNING RATE:  3.157229337446777e-05\n",
            "loss: 1.984 (epoch: 9, step: 0) // Avg time/img: 0.0516 s\n",
            "loss: 2.026 (epoch: 9, step: 50) // Avg time/img: 0.0556 s\n",
            "loss: 2.006 (epoch: 9, step: 100) // Avg time/img: 0.0545 s\n",
            "loss: 1.997 (epoch: 9, step: 150) // Avg time/img: 0.0543 s\n",
            "loss: 1.965 (epoch: 9, step: 200) // Avg time/img: 0.0542 s\n",
            "loss: 1.955 (epoch: 9, step: 250) // Avg time/img: 0.0540 s\n",
            "loss: 1.933 (epoch: 9, step: 300) // Avg time/img: 0.0545 s\n",
            "loss: 1.912 (epoch: 9, step: 350) // Avg time/img: 0.0544 s\n",
            "loss: 1.903 (epoch: 9, step: 400) // Avg time/img: 0.0545 s\n",
            "loss: 1.881 (epoch: 9, step: 450) // Avg time/img: 0.0544 s\n",
            "----- VALIDATING - EPOCH 9 -----\n",
            "VAL loss: 1.539 (epoch: 9, step: 0) // Avg time/img: 0.0760 s\n",
            "VAL loss: 1.862 (epoch: 9, step: 50) // Avg time/img: 0.0329 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m22.78\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training1/model-009.pth (epoch: 9)\n",
            "save: ../save/enet_training1/model_best.pth (epoch: 9)\n",
            "----- TRAINING - EPOCH 10 -----\n",
            "LEARNING RATE:  2.9194189645999014e-05\n",
            "loss: 1.598 (epoch: 10, step: 0) // Avg time/img: 0.0831 s\n",
            "loss: 1.736 (epoch: 10, step: 50) // Avg time/img: 0.0527 s\n",
            "loss: 1.701 (epoch: 10, step: 100) // Avg time/img: 0.0534 s\n",
            "loss: 1.726 (epoch: 10, step: 150) // Avg time/img: 0.0538 s\n",
            "loss: 1.726 (epoch: 10, step: 200) // Avg time/img: 0.0539 s\n",
            "loss: 1.721 (epoch: 10, step: 250) // Avg time/img: 0.0539 s\n",
            "loss: 1.712 (epoch: 10, step: 300) // Avg time/img: 0.0539 s\n",
            "loss: 1.7 (epoch: 10, step: 350) // Avg time/img: 0.0542 s\n",
            "loss: 1.694 (epoch: 10, step: 400) // Avg time/img: 0.0540 s\n",
            "loss: 1.685 (epoch: 10, step: 450) // Avg time/img: 0.0540 s\n",
            "----- VALIDATING - EPOCH 10 -----\n",
            "VAL loss: 1.355 (epoch: 10, step: 0) // Avg time/img: 0.0579 s\n",
            "VAL loss: 1.684 (epoch: 10, step: 50) // Avg time/img: 0.0350 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m25.10\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training1/model-010.pth (epoch: 10)\n",
            "save: ../save/enet_training1/model_best.pth (epoch: 10)\n",
            "----- TRAINING - EPOCH 11 -----\n",
            "LEARNING RATE:  2.679433656340733e-05\n",
            "loss: 1.355 (epoch: 11, step: 0) // Avg time/img: 0.0565 s\n",
            "loss: 1.636 (epoch: 11, step: 50) // Avg time/img: 0.0520 s\n",
            "loss: 1.587 (epoch: 11, step: 100) // Avg time/img: 0.0540 s\n",
            "loss: 1.562 (epoch: 11, step: 150) // Avg time/img: 0.0538 s\n",
            "loss: 1.558 (epoch: 11, step: 200) // Avg time/img: 0.0541 s\n",
            "loss: 1.541 (epoch: 11, step: 250) // Avg time/img: 0.0539 s\n",
            "loss: 1.538 (epoch: 11, step: 300) // Avg time/img: 0.0539 s\n",
            "loss: 1.527 (epoch: 11, step: 350) // Avg time/img: 0.0542 s\n",
            "loss: 1.52 (epoch: 11, step: 400) // Avg time/img: 0.0540 s\n",
            "loss: 1.512 (epoch: 11, step: 450) // Avg time/img: 0.0540 s\n",
            "----- VALIDATING - EPOCH 11 -----\n",
            "VAL loss: 1.227 (epoch: 11, step: 0) // Avg time/img: 0.0578 s\n",
            "VAL loss: 1.548 (epoch: 11, step: 50) // Avg time/img: 0.0354 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m26.59\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training1/model-011.pth (epoch: 11)\n",
            "save: ../save/enet_training1/model_best.pth (epoch: 11)\n",
            "----- TRAINING - EPOCH 12 -----\n",
            "LEARNING RATE:  2.437032195894977e-05\n",
            "loss: 1.219 (epoch: 12, step: 0) // Avg time/img: 0.0555 s\n",
            "loss: 1.434 (epoch: 12, step: 50) // Avg time/img: 0.0517 s\n",
            "loss: 1.442 (epoch: 12, step: 100) // Avg time/img: 0.0524 s\n",
            "loss: 1.45 (epoch: 12, step: 150) // Avg time/img: 0.0526 s\n",
            "loss: 1.437 (epoch: 12, step: 200) // Avg time/img: 0.0532 s\n",
            "loss: 1.415 (epoch: 12, step: 250) // Avg time/img: 0.0531 s\n",
            "loss: 1.416 (epoch: 12, step: 300) // Avg time/img: 0.0534 s\n",
            "loss: 1.4 (epoch: 12, step: 350) // Avg time/img: 0.0533 s\n",
            "loss: 1.389 (epoch: 12, step: 400) // Avg time/img: 0.0532 s\n",
            "loss: 1.387 (epoch: 12, step: 450) // Avg time/img: 0.0532 s\n",
            "----- VALIDATING - EPOCH 12 -----\n",
            "VAL loss: 1.138 (epoch: 12, step: 0) // Avg time/img: 0.0574 s\n",
            "VAL loss: 1.443 (epoch: 12, step: 50) // Avg time/img: 0.0327 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m28.11\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training1/model-012.pth (epoch: 12)\n",
            "save: ../save/enet_training1/model_best.pth (epoch: 12)\n",
            "----- TRAINING - EPOCH 13 -----\n",
            "LEARNING RATE:  2.191916452770435e-05\n",
            "loss: 1.304 (epoch: 13, step: 0) // Avg time/img: 0.0546 s\n",
            "loss: 1.334 (epoch: 13, step: 50) // Avg time/img: 0.0547 s\n",
            "loss: 1.306 (epoch: 13, step: 100) // Avg time/img: 0.0540 s\n",
            "loss: 1.319 (epoch: 13, step: 150) // Avg time/img: 0.0541 s\n",
            "loss: 1.319 (epoch: 13, step: 200) // Avg time/img: 0.0540 s\n",
            "loss: 1.299 (epoch: 13, step: 250) // Avg time/img: 0.0541 s\n",
            "loss: 1.295 (epoch: 13, step: 300) // Avg time/img: 0.0541 s\n",
            "loss: 1.287 (epoch: 13, step: 350) // Avg time/img: 0.0538 s\n",
            "loss: 1.279 (epoch: 13, step: 400) // Avg time/img: 0.0541 s\n",
            "loss: 1.278 (epoch: 13, step: 450) // Avg time/img: 0.0541 s\n",
            "----- VALIDATING - EPOCH 13 -----\n",
            "VAL loss: 1.03 (epoch: 13, step: 0) // Avg time/img: 0.0693 s\n",
            "VAL loss: 1.342 (epoch: 13, step: 50) // Avg time/img: 0.0359 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m29.28\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training1/model-013.pth (epoch: 13)\n",
            "save: ../save/enet_training1/model_best.pth (epoch: 13)\n",
            "----- TRAINING - EPOCH 14 -----\n",
            "LEARNING RATE:  1.9437089939938174e-05\n",
            "loss: 1.462 (epoch: 14, step: 0) // Avg time/img: 0.0675 s\n",
            "loss: 1.215 (epoch: 14, step: 50) // Avg time/img: 0.0552 s\n",
            "loss: 1.192 (epoch: 14, step: 100) // Avg time/img: 0.0544 s\n",
            "loss: 1.201 (epoch: 14, step: 150) // Avg time/img: 0.0545 s\n",
            "loss: 1.193 (epoch: 14, step: 200) // Avg time/img: 0.0546 s\n",
            "loss: 1.196 (epoch: 14, step: 250) // Avg time/img: 0.0551 s\n",
            "loss: 1.206 (epoch: 14, step: 300) // Avg time/img: 0.0545 s\n",
            "loss: 1.2 (epoch: 14, step: 350) // Avg time/img: 0.0548 s\n",
            "loss: 1.199 (epoch: 14, step: 400) // Avg time/img: 0.0546 s\n",
            "loss: 1.201 (epoch: 14, step: 450) // Avg time/img: 0.0546 s\n",
            "----- VALIDATING - EPOCH 14 -----\n",
            "VAL loss: 0.9944 (epoch: 14, step: 0) // Avg time/img: 0.0537 s\n",
            "VAL loss: 1.267 (epoch: 14, step: 50) // Avg time/img: 0.0343 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m30.69\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training1/model-014.pth (epoch: 14)\n",
            "save: ../save/enet_training1/model_best.pth (epoch: 14)\n",
            "----- TRAINING - EPOCH 15 -----\n",
            "LEARNING RATE:  1.6919173095082493e-05\n",
            "loss: 1.076 (epoch: 15, step: 0) // Avg time/img: 0.0529 s\n",
            "loss: 1.125 (epoch: 15, step: 50) // Avg time/img: 0.0529 s\n",
            "loss: 1.137 (epoch: 15, step: 100) // Avg time/img: 0.0548 s\n",
            "loss: 1.152 (epoch: 15, step: 150) // Avg time/img: 0.0543 s\n",
            "loss: 1.156 (epoch: 15, step: 200) // Avg time/img: 0.0548 s\n",
            "loss: 1.148 (epoch: 15, step: 250) // Avg time/img: 0.0547 s\n",
            "loss: 1.145 (epoch: 15, step: 300) // Avg time/img: 0.0549 s\n",
            "loss: 1.136 (epoch: 15, step: 350) // Avg time/img: 0.0547 s\n",
            "loss: 1.133 (epoch: 15, step: 400) // Avg time/img: 0.0547 s\n",
            "loss: 1.127 (epoch: 15, step: 450) // Avg time/img: 0.0548 s\n",
            "----- VALIDATING - EPOCH 15 -----\n",
            "VAL loss: 0.9062 (epoch: 15, step: 0) // Avg time/img: 0.0501 s\n",
            "VAL loss: 1.202 (epoch: 15, step: 50) // Avg time/img: 0.0352 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m31.87\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training1/model-015.pth (epoch: 15)\n",
            "save: ../save/enet_training1/model_best.pth (epoch: 15)\n",
            "----- TRAINING - EPOCH 16 -----\n",
            "LEARNING RATE:  1.4358729437462937e-05\n",
            "loss: 1.037 (epoch: 16, step: 0) // Avg time/img: 0.0593 s\n",
            "loss: 1.129 (epoch: 16, step: 50) // Avg time/img: 0.0555 s\n",
            "loss: 1.119 (epoch: 16, step: 100) // Avg time/img: 0.0546 s\n",
            "loss: 1.089 (epoch: 16, step: 150) // Avg time/img: 0.0549 s\n",
            "loss: 1.086 (epoch: 16, step: 200) // Avg time/img: 0.0547 s\n",
            "loss: 1.072 (epoch: 16, step: 250) // Avg time/img: 0.0548 s\n",
            "loss: 1.063 (epoch: 16, step: 300) // Avg time/img: 0.0547 s\n",
            "loss: 1.069 (epoch: 16, step: 350) // Avg time/img: 0.0549 s\n",
            "loss: 1.069 (epoch: 16, step: 400) // Avg time/img: 0.0547 s\n",
            "loss: 1.067 (epoch: 16, step: 450) // Avg time/img: 0.0550 s\n",
            "----- VALIDATING - EPOCH 16 -----\n",
            "VAL loss: 0.8613 (epoch: 16, step: 0) // Avg time/img: 0.0559 s\n",
            "VAL loss: 1.158 (epoch: 16, step: 50) // Avg time/img: 0.0348 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m32.83\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training1/model-016.pth (epoch: 16)\n",
            "save: ../save/enet_training1/model_best.pth (epoch: 16)\n",
            "----- TRAINING - EPOCH 17 -----\n",
            "LEARNING RATE:  1.1746189430880188e-05\n",
            "loss: 0.8075 (epoch: 17, step: 0) // Avg time/img: 0.0529 s\n",
            "loss: 1.077 (epoch: 17, step: 50) // Avg time/img: 0.0544 s\n",
            "loss: 1.07 (epoch: 17, step: 100) // Avg time/img: 0.0550 s\n",
            "loss: 1.053 (epoch: 17, step: 150) // Avg time/img: 0.0553 s\n",
            "loss: 1.039 (epoch: 17, step: 200) // Avg time/img: 0.0560 s\n",
            "loss: 1.025 (epoch: 17, step: 250) // Avg time/img: 0.0555 s\n",
            "loss: 1.023 (epoch: 17, step: 300) // Avg time/img: 0.0559 s\n",
            "loss: 1.023 (epoch: 17, step: 350) // Avg time/img: 0.0556 s\n",
            "loss: 1.025 (epoch: 17, step: 400) // Avg time/img: 0.0555 s\n",
            "loss: 1.019 (epoch: 17, step: 450) // Avg time/img: 0.0552 s\n",
            "----- VALIDATING - EPOCH 17 -----\n",
            "VAL loss: 0.8445 (epoch: 17, step: 0) // Avg time/img: 0.0481 s\n",
            "VAL loss: 1.118 (epoch: 17, step: 50) // Avg time/img: 0.0333 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m33.44\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training1/model-017.pth (epoch: 17)\n",
            "save: ../save/enet_training1/model_best.pth (epoch: 17)\n",
            "----- TRAINING - EPOCH 18 -----\n",
            "LEARNING RATE:  9.066760365683729e-06\n",
            "loss: 1.003 (epoch: 18, step: 0) // Avg time/img: 0.0772 s\n",
            "loss: 0.9728 (epoch: 18, step: 50) // Avg time/img: 0.0537 s\n",
            "loss: 0.9814 (epoch: 18, step: 100) // Avg time/img: 0.0543 s\n",
            "loss: 0.9928 (epoch: 18, step: 150) // Avg time/img: 0.0548 s\n",
            "loss: 0.9875 (epoch: 18, step: 200) // Avg time/img: 0.0546 s\n",
            "loss: 0.9903 (epoch: 18, step: 250) // Avg time/img: 0.0547 s\n",
            "loss: 0.987 (epoch: 18, step: 300) // Avg time/img: 0.0546 s\n",
            "loss: 0.9849 (epoch: 18, step: 350) // Avg time/img: 0.0546 s\n",
            "loss: 0.9821 (epoch: 18, step: 400) // Avg time/img: 0.0545 s\n",
            "loss: 0.9828 (epoch: 18, step: 450) // Avg time/img: 0.0548 s\n",
            "----- VALIDATING - EPOCH 18 -----\n",
            "VAL loss: 0.8267 (epoch: 18, step: 0) // Avg time/img: 0.0498 s\n",
            "VAL loss: 1.095 (epoch: 18, step: 50) // Avg time/img: 0.0350 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m33.71\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training1/model-018.pth (epoch: 18)\n",
            "save: ../save/enet_training1/model_best.pth (epoch: 18)\n",
            "----- TRAINING - EPOCH 19 -----\n",
            "LEARNING RATE:  6.294627058970836e-06\n",
            "loss: 0.8197 (epoch: 19, step: 0) // Avg time/img: 0.0633 s\n",
            "loss: 0.9671 (epoch: 19, step: 50) // Avg time/img: 0.0529 s\n",
            "loss: 0.984 (epoch: 19, step: 100) // Avg time/img: 0.0538 s\n",
            "loss: 0.9722 (epoch: 19, step: 150) // Avg time/img: 0.0541 s\n",
            "loss: 0.9649 (epoch: 19, step: 200) // Avg time/img: 0.0544 s\n",
            "loss: 0.9679 (epoch: 19, step: 250) // Avg time/img: 0.0540 s\n",
            "loss: 0.9711 (epoch: 19, step: 300) // Avg time/img: 0.0540 s\n",
            "loss: 0.9675 (epoch: 19, step: 350) // Avg time/img: 0.0540 s\n",
            "loss: 0.9665 (epoch: 19, step: 400) // Avg time/img: 0.0544 s\n",
            "loss: 0.9619 (epoch: 19, step: 450) // Avg time/img: 0.0546 s\n",
            "----- VALIDATING - EPOCH 19 -----\n",
            "VAL loss: 0.792 (epoch: 19, step: 0) // Avg time/img: 0.0560 s\n",
            "VAL loss: 1.067 (epoch: 19, step: 50) // Avg time/img: 0.0350 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m34.07\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training1/model-019.pth (epoch: 19)\n",
            "save: ../save/enet_training1/model_best.pth (epoch: 19)\n",
            "----- TRAINING - EPOCH 20 -----\n",
            "LEARNING RATE:  3.373207119183911e-06\n",
            "loss: 0.8169 (epoch: 20, step: 0) // Avg time/img: 0.0620 s\n",
            "loss: 0.9404 (epoch: 20, step: 50) // Avg time/img: 0.0537 s\n",
            "loss: 0.9419 (epoch: 20, step: 100) // Avg time/img: 0.0538 s\n",
            "loss: 0.9288 (epoch: 20, step: 150) // Avg time/img: 0.0547 s\n",
            "loss: 0.9415 (epoch: 20, step: 200) // Avg time/img: 0.0550 s\n",
            "loss: 0.952 (epoch: 20, step: 250) // Avg time/img: 0.0551 s\n",
            "loss: 0.9574 (epoch: 20, step: 300) // Avg time/img: 0.0550 s\n",
            "loss: 0.9569 (epoch: 20, step: 350) // Avg time/img: 0.0552 s\n",
            "loss: 0.9585 (epoch: 20, step: 400) // Avg time/img: 0.0552 s\n",
            "loss: 0.9473 (epoch: 20, step: 450) // Avg time/img: 0.0552 s\n",
            "----- VALIDATING - EPOCH 20 -----\n",
            "VAL loss: 0.7734 (epoch: 20, step: 0) // Avg time/img: 0.0648 s\n",
            "VAL loss: 1.056 (epoch: 20, step: 50) // Avg time/img: 0.0374 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m34.19\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training1/model-020.pth (epoch: 20)\n",
            "save: ../save/enet_training1/model_best.pth (epoch: 20)\n",
            "========== TRAINING FINISHED ===========\n",
            "updating: content/AnomalySegmentation/save/enet_training1/ (stored 0%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/automated_log.txt (deflated 66%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model.txt (deflated 96%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/enet.py (deflated 84%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/opts.txt (deflated 37%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model_best.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-002.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-001.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-003.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-006.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-010.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-005.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-007.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/best.txt (stored 0%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model_best.pth.tar (deflated 19%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-008.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-009.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/checkpoint.pth.tar (deflated 19%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-004.pth (deflated 18%)\n",
            "  adding: content/AnomalySegmentation/save/enet_training1/model-014.pth (deflated 18%)\n",
            "  adding: content/AnomalySegmentation/save/enet_training1/model-011.pth (deflated 18%)\n",
            "  adding: content/AnomalySegmentation/save/enet_training1/model-020.pth (deflated 18%)\n",
            "  adding: content/AnomalySegmentation/save/enet_training1/model-015.pth (deflated 18%)\n",
            "  adding: content/AnomalySegmentation/save/enet_training1/model-012.pth (deflated 18%)\n",
            "  adding: content/AnomalySegmentation/save/enet_training1/model-018.pth (deflated 18%)\n",
            "  adding: content/AnomalySegmentation/save/enet_training1/model-016.pth (deflated 18%)\n",
            "  adding: content/AnomalySegmentation/save/enet_training1/model-019.pth (deflated 18%)\n",
            "  adding: content/AnomalySegmentation/save/enet_training1/model-013.pth (deflated 18%)\n",
            "  adding: content/AnomalySegmentation/save/enet_training1/model-017.pth (deflated 18%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r save_enet_training1.zip /content/AnomalySegmentation/save/enet_training1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYLM7axZRkRx",
        "outputId": "5ffbe005-ee11-4208-aad9-d94a56bcf9d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updating: content/AnomalySegmentation/save/enet_training1/ (stored 0%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/automated_log.txt (deflated 66%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model.txt (deflated 96%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/enet.py (deflated 84%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/opts.txt (deflated 37%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model_best.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-002.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-001.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-003.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-006.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-010.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-005.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-007.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/best.txt (stored 0%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model_best.pth.tar (deflated 19%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-008.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-009.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/checkpoint.pth.tar (deflated 19%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-004.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-014.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-011.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-020.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-015.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-012.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-018.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-016.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-019.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-013.pth (deflated 18%)\n",
            "updating: content/AnomalySegmentation/save/enet_training1/model-017.pth (deflated 18%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####BISENET"
      ],
      "metadata": {
        "id": "Cm1ITQbYM9n7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/AnomalySegmentation && git pull"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f80b9df-737a-492a-f51b-2daa5e46cb65",
        "collapsed": true,
        "id": "wyZLcBENT_Hy"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "remote: Enumerating objects: 7, done.\u001b[K\n",
            "remote: Counting objects:  14% (1/7)\u001b[K\rremote: Counting objects:  28% (2/7)\u001b[K\rremote: Counting objects:  42% (3/7)\u001b[K\rremote: Counting objects:  57% (4/7)\u001b[K\rremote: Counting objects:  71% (5/7)\u001b[K\rremote: Counting objects:  85% (6/7)\u001b[K\rremote: Counting objects: 100% (7/7)\u001b[K\rremote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1/1)\u001b[K\rremote: Compressing objects: 100% (1/1), done.\u001b[K\n",
            "remote: Total 4 (delta 3), reused 4 (delta 3), pack-reused 0 (from 0)\u001b[K\n",
            "Unpacking objects:  25% (1/4)\rUnpacking objects:  50% (2/4)\rUnpacking objects:  75% (3/4)\rUnpacking objects: 100% (4/4)\rUnpacking objects: 100% (4/4), 358 bytes | 179.00 KiB/s, done.\n",
            "From https://github.com/RonPlusSign/AnomalySegmentation\n",
            "   07c6057..bf241fd  main       -> origin/main\n",
            "Updating 07c6057..bf241fd\n",
            "Fast-forward\n",
            " train/main.py | 2 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " 1 file changed, 1 insertion(+), 1 deletion(-)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Finetune BiSeNet (20 epochs)\n",
        "!cd /content/AnomalySegmentation/train; python -W ignore main.py --savedir bisenet_training1 --datadir /content/cityscapes --model bisenet --cuda --num-epochs=20 --epochs-save=1 --FineTune --loadWeights=bisenetv1_pretrained.pth\n",
        "!zip -r save_bisenet_training1.zip /content/AnomalySegmentation/save/bisenet_training1"
      ],
      "metadata": {
        "id": "h-ftZ-p1Yn9s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f13ccc48-93d2-4527-e695-a693a2af691d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "odict_keys(['cp.resnet.conv1.weight', 'cp.resnet.bn1.weight', 'cp.resnet.bn1.bias', 'cp.resnet.bn1.running_mean', 'cp.resnet.bn1.running_var', 'cp.resnet.bn1.num_batches_tracked', 'cp.resnet.layer1.0.conv1.weight', 'cp.resnet.layer1.0.bn1.weight', 'cp.resnet.layer1.0.bn1.bias', 'cp.resnet.layer1.0.bn1.running_mean', 'cp.resnet.layer1.0.bn1.running_var', 'cp.resnet.layer1.0.bn1.num_batches_tracked', 'cp.resnet.layer1.0.conv2.weight', 'cp.resnet.layer1.0.bn2.weight', 'cp.resnet.layer1.0.bn2.bias', 'cp.resnet.layer1.0.bn2.running_mean', 'cp.resnet.layer1.0.bn2.running_var', 'cp.resnet.layer1.0.bn2.num_batches_tracked', 'cp.resnet.layer1.1.conv1.weight', 'cp.resnet.layer1.1.bn1.weight', 'cp.resnet.layer1.1.bn1.bias', 'cp.resnet.layer1.1.bn1.running_mean', 'cp.resnet.layer1.1.bn1.running_var', 'cp.resnet.layer1.1.bn1.num_batches_tracked', 'cp.resnet.layer1.1.conv2.weight', 'cp.resnet.layer1.1.bn2.weight', 'cp.resnet.layer1.1.bn2.bias', 'cp.resnet.layer1.1.bn2.running_mean', 'cp.resnet.layer1.1.bn2.running_var', 'cp.resnet.layer1.1.bn2.num_batches_tracked', 'cp.resnet.layer2.0.conv1.weight', 'cp.resnet.layer2.0.bn1.weight', 'cp.resnet.layer2.0.bn1.bias', 'cp.resnet.layer2.0.bn1.running_mean', 'cp.resnet.layer2.0.bn1.running_var', 'cp.resnet.layer2.0.bn1.num_batches_tracked', 'cp.resnet.layer2.0.conv2.weight', 'cp.resnet.layer2.0.bn2.weight', 'cp.resnet.layer2.0.bn2.bias', 'cp.resnet.layer2.0.bn2.running_mean', 'cp.resnet.layer2.0.bn2.running_var', 'cp.resnet.layer2.0.bn2.num_batches_tracked', 'cp.resnet.layer2.0.downsample.0.weight', 'cp.resnet.layer2.0.downsample.1.weight', 'cp.resnet.layer2.0.downsample.1.bias', 'cp.resnet.layer2.0.downsample.1.running_mean', 'cp.resnet.layer2.0.downsample.1.running_var', 'cp.resnet.layer2.0.downsample.1.num_batches_tracked', 'cp.resnet.layer2.1.conv1.weight', 'cp.resnet.layer2.1.bn1.weight', 'cp.resnet.layer2.1.bn1.bias', 'cp.resnet.layer2.1.bn1.running_mean', 'cp.resnet.layer2.1.bn1.running_var', 'cp.resnet.layer2.1.bn1.num_batches_tracked', 'cp.resnet.layer2.1.conv2.weight', 'cp.resnet.layer2.1.bn2.weight', 'cp.resnet.layer2.1.bn2.bias', 'cp.resnet.layer2.1.bn2.running_mean', 'cp.resnet.layer2.1.bn2.running_var', 'cp.resnet.layer2.1.bn2.num_batches_tracked', 'cp.resnet.layer3.0.conv1.weight', 'cp.resnet.layer3.0.bn1.weight', 'cp.resnet.layer3.0.bn1.bias', 'cp.resnet.layer3.0.bn1.running_mean', 'cp.resnet.layer3.0.bn1.running_var', 'cp.resnet.layer3.0.bn1.num_batches_tracked', 'cp.resnet.layer3.0.conv2.weight', 'cp.resnet.layer3.0.bn2.weight', 'cp.resnet.layer3.0.bn2.bias', 'cp.resnet.layer3.0.bn2.running_mean', 'cp.resnet.layer3.0.bn2.running_var', 'cp.resnet.layer3.0.bn2.num_batches_tracked', 'cp.resnet.layer3.0.downsample.0.weight', 'cp.resnet.layer3.0.downsample.1.weight', 'cp.resnet.layer3.0.downsample.1.bias', 'cp.resnet.layer3.0.downsample.1.running_mean', 'cp.resnet.layer3.0.downsample.1.running_var', 'cp.resnet.layer3.0.downsample.1.num_batches_tracked', 'cp.resnet.layer3.1.conv1.weight', 'cp.resnet.layer3.1.bn1.weight', 'cp.resnet.layer3.1.bn1.bias', 'cp.resnet.layer3.1.bn1.running_mean', 'cp.resnet.layer3.1.bn1.running_var', 'cp.resnet.layer3.1.bn1.num_batches_tracked', 'cp.resnet.layer3.1.conv2.weight', 'cp.resnet.layer3.1.bn2.weight', 'cp.resnet.layer3.1.bn2.bias', 'cp.resnet.layer3.1.bn2.running_mean', 'cp.resnet.layer3.1.bn2.running_var', 'cp.resnet.layer3.1.bn2.num_batches_tracked', 'cp.resnet.layer4.0.conv1.weight', 'cp.resnet.layer4.0.bn1.weight', 'cp.resnet.layer4.0.bn1.bias', 'cp.resnet.layer4.0.bn1.running_mean', 'cp.resnet.layer4.0.bn1.running_var', 'cp.resnet.layer4.0.bn1.num_batches_tracked', 'cp.resnet.layer4.0.conv2.weight', 'cp.resnet.layer4.0.bn2.weight', 'cp.resnet.layer4.0.bn2.bias', 'cp.resnet.layer4.0.bn2.running_mean', 'cp.resnet.layer4.0.bn2.running_var', 'cp.resnet.layer4.0.bn2.num_batches_tracked', 'cp.resnet.layer4.0.downsample.0.weight', 'cp.resnet.layer4.0.downsample.1.weight', 'cp.resnet.layer4.0.downsample.1.bias', 'cp.resnet.layer4.0.downsample.1.running_mean', 'cp.resnet.layer4.0.downsample.1.running_var', 'cp.resnet.layer4.0.downsample.1.num_batches_tracked', 'cp.resnet.layer4.1.conv1.weight', 'cp.resnet.layer4.1.bn1.weight', 'cp.resnet.layer4.1.bn1.bias', 'cp.resnet.layer4.1.bn1.running_mean', 'cp.resnet.layer4.1.bn1.running_var', 'cp.resnet.layer4.1.bn1.num_batches_tracked', 'cp.resnet.layer4.1.conv2.weight', 'cp.resnet.layer4.1.bn2.weight', 'cp.resnet.layer4.1.bn2.bias', 'cp.resnet.layer4.1.bn2.running_mean', 'cp.resnet.layer4.1.bn2.running_var', 'cp.resnet.layer4.1.bn2.num_batches_tracked', 'cp.arm16.conv.conv.weight', 'cp.arm16.conv.bn.weight', 'cp.arm16.conv.bn.bias', 'cp.arm16.conv.bn.running_mean', 'cp.arm16.conv.bn.running_var', 'cp.arm16.conv.bn.num_batches_tracked', 'cp.arm16.conv_atten.weight', 'cp.arm16.bn_atten.weight', 'cp.arm16.bn_atten.bias', 'cp.arm16.bn_atten.running_mean', 'cp.arm16.bn_atten.running_var', 'cp.arm16.bn_atten.num_batches_tracked', 'cp.arm32.conv.conv.weight', 'cp.arm32.conv.bn.weight', 'cp.arm32.conv.bn.bias', 'cp.arm32.conv.bn.running_mean', 'cp.arm32.conv.bn.running_var', 'cp.arm32.conv.bn.num_batches_tracked', 'cp.arm32.conv_atten.weight', 'cp.arm32.bn_atten.weight', 'cp.arm32.bn_atten.bias', 'cp.arm32.bn_atten.running_mean', 'cp.arm32.bn_atten.running_var', 'cp.arm32.bn_atten.num_batches_tracked', 'cp.conv_head32.conv.weight', 'cp.conv_head32.bn.weight', 'cp.conv_head32.bn.bias', 'cp.conv_head32.bn.running_mean', 'cp.conv_head32.bn.running_var', 'cp.conv_head32.bn.num_batches_tracked', 'cp.conv_head16.conv.weight', 'cp.conv_head16.bn.weight', 'cp.conv_head16.bn.bias', 'cp.conv_head16.bn.running_mean', 'cp.conv_head16.bn.running_var', 'cp.conv_head16.bn.num_batches_tracked', 'cp.conv_avg.conv.weight', 'cp.conv_avg.bn.weight', 'cp.conv_avg.bn.bias', 'cp.conv_avg.bn.running_mean', 'cp.conv_avg.bn.running_var', 'cp.conv_avg.bn.num_batches_tracked', 'sp.conv1.conv.weight', 'sp.conv1.bn.weight', 'sp.conv1.bn.bias', 'sp.conv1.bn.running_mean', 'sp.conv1.bn.running_var', 'sp.conv1.bn.num_batches_tracked', 'sp.conv2.conv.weight', 'sp.conv2.bn.weight', 'sp.conv2.bn.bias', 'sp.conv2.bn.running_mean', 'sp.conv2.bn.running_var', 'sp.conv2.bn.num_batches_tracked', 'sp.conv3.conv.weight', 'sp.conv3.bn.weight', 'sp.conv3.bn.bias', 'sp.conv3.bn.running_mean', 'sp.conv3.bn.running_var', 'sp.conv3.bn.num_batches_tracked', 'sp.conv_out.conv.weight', 'sp.conv_out.bn.weight', 'sp.conv_out.bn.bias', 'sp.conv_out.bn.running_mean', 'sp.conv_out.bn.running_var', 'sp.conv_out.bn.num_batches_tracked', 'ffm.convblk.conv.weight', 'ffm.convblk.bn.weight', 'ffm.convblk.bn.bias', 'ffm.convblk.bn.running_mean', 'ffm.convblk.bn.running_var', 'ffm.convblk.bn.num_batches_tracked', 'ffm.conv.weight', 'ffm.bn.weight', 'ffm.bn.bias', 'ffm.bn.running_mean', 'ffm.bn.running_var', 'ffm.bn.num_batches_tracked', 'conv_out.conv.conv.weight', 'conv_out.conv.bn.weight', 'conv_out.conv.bn.bias', 'conv_out.conv.bn.running_mean', 'conv_out.conv.bn.running_var', 'conv_out.conv.bn.num_batches_tracked', 'conv_out.conv_out.weight', 'conv_out.conv_out.bias', 'conv_out16.conv.conv.weight', 'conv_out16.conv.bn.weight', 'conv_out16.conv.bn.bias', 'conv_out16.conv.bn.running_mean', 'conv_out16.conv.bn.running_var', 'conv_out16.conv.bn.num_batches_tracked', 'conv_out16.conv_out.weight', 'conv_out16.conv_out.bias', 'conv_out32.conv.conv.weight', 'conv_out32.conv.bn.weight', 'conv_out32.conv.bn.bias', 'conv_out32.conv.bn.running_mean', 'conv_out32.conv.bn.running_var', 'conv_out32.conv.bn.num_batches_tracked', 'conv_out32.conv_out.weight', 'conv_out32.conv_out.bias'])\n",
            "odict_keys(['cp.resnet.conv1.weight', 'cp.resnet.bn1.weight', 'cp.resnet.bn1.bias', 'cp.resnet.bn1.running_mean', 'cp.resnet.bn1.running_var', 'cp.resnet.bn1.num_batches_tracked', 'cp.resnet.layer1.0.conv1.weight', 'cp.resnet.layer1.0.bn1.weight', 'cp.resnet.layer1.0.bn1.bias', 'cp.resnet.layer1.0.bn1.running_mean', 'cp.resnet.layer1.0.bn1.running_var', 'cp.resnet.layer1.0.bn1.num_batches_tracked', 'cp.resnet.layer1.0.conv2.weight', 'cp.resnet.layer1.0.bn2.weight', 'cp.resnet.layer1.0.bn2.bias', 'cp.resnet.layer1.0.bn2.running_mean', 'cp.resnet.layer1.0.bn2.running_var', 'cp.resnet.layer1.0.bn2.num_batches_tracked', 'cp.resnet.layer1.1.conv1.weight', 'cp.resnet.layer1.1.bn1.weight', 'cp.resnet.layer1.1.bn1.bias', 'cp.resnet.layer1.1.bn1.running_mean', 'cp.resnet.layer1.1.bn1.running_var', 'cp.resnet.layer1.1.bn1.num_batches_tracked', 'cp.resnet.layer1.1.conv2.weight', 'cp.resnet.layer1.1.bn2.weight', 'cp.resnet.layer1.1.bn2.bias', 'cp.resnet.layer1.1.bn2.running_mean', 'cp.resnet.layer1.1.bn2.running_var', 'cp.resnet.layer1.1.bn2.num_batches_tracked', 'cp.resnet.layer2.0.conv1.weight', 'cp.resnet.layer2.0.bn1.weight', 'cp.resnet.layer2.0.bn1.bias', 'cp.resnet.layer2.0.bn1.running_mean', 'cp.resnet.layer2.0.bn1.running_var', 'cp.resnet.layer2.0.bn1.num_batches_tracked', 'cp.resnet.layer2.0.conv2.weight', 'cp.resnet.layer2.0.bn2.weight', 'cp.resnet.layer2.0.bn2.bias', 'cp.resnet.layer2.0.bn2.running_mean', 'cp.resnet.layer2.0.bn2.running_var', 'cp.resnet.layer2.0.bn2.num_batches_tracked', 'cp.resnet.layer2.0.downsample.0.weight', 'cp.resnet.layer2.0.downsample.1.weight', 'cp.resnet.layer2.0.downsample.1.bias', 'cp.resnet.layer2.0.downsample.1.running_mean', 'cp.resnet.layer2.0.downsample.1.running_var', 'cp.resnet.layer2.0.downsample.1.num_batches_tracked', 'cp.resnet.layer2.1.conv1.weight', 'cp.resnet.layer2.1.bn1.weight', 'cp.resnet.layer2.1.bn1.bias', 'cp.resnet.layer2.1.bn1.running_mean', 'cp.resnet.layer2.1.bn1.running_var', 'cp.resnet.layer2.1.bn1.num_batches_tracked', 'cp.resnet.layer2.1.conv2.weight', 'cp.resnet.layer2.1.bn2.weight', 'cp.resnet.layer2.1.bn2.bias', 'cp.resnet.layer2.1.bn2.running_mean', 'cp.resnet.layer2.1.bn2.running_var', 'cp.resnet.layer2.1.bn2.num_batches_tracked', 'cp.resnet.layer3.0.conv1.weight', 'cp.resnet.layer3.0.bn1.weight', 'cp.resnet.layer3.0.bn1.bias', 'cp.resnet.layer3.0.bn1.running_mean', 'cp.resnet.layer3.0.bn1.running_var', 'cp.resnet.layer3.0.bn1.num_batches_tracked', 'cp.resnet.layer3.0.conv2.weight', 'cp.resnet.layer3.0.bn2.weight', 'cp.resnet.layer3.0.bn2.bias', 'cp.resnet.layer3.0.bn2.running_mean', 'cp.resnet.layer3.0.bn2.running_var', 'cp.resnet.layer3.0.bn2.num_batches_tracked', 'cp.resnet.layer3.0.downsample.0.weight', 'cp.resnet.layer3.0.downsample.1.weight', 'cp.resnet.layer3.0.downsample.1.bias', 'cp.resnet.layer3.0.downsample.1.running_mean', 'cp.resnet.layer3.0.downsample.1.running_var', 'cp.resnet.layer3.0.downsample.1.num_batches_tracked', 'cp.resnet.layer3.1.conv1.weight', 'cp.resnet.layer3.1.bn1.weight', 'cp.resnet.layer3.1.bn1.bias', 'cp.resnet.layer3.1.bn1.running_mean', 'cp.resnet.layer3.1.bn1.running_var', 'cp.resnet.layer3.1.bn1.num_batches_tracked', 'cp.resnet.layer3.1.conv2.weight', 'cp.resnet.layer3.1.bn2.weight', 'cp.resnet.layer3.1.bn2.bias', 'cp.resnet.layer3.1.bn2.running_mean', 'cp.resnet.layer3.1.bn2.running_var', 'cp.resnet.layer3.1.bn2.num_batches_tracked', 'cp.resnet.layer4.0.conv1.weight', 'cp.resnet.layer4.0.bn1.weight', 'cp.resnet.layer4.0.bn1.bias', 'cp.resnet.layer4.0.bn1.running_mean', 'cp.resnet.layer4.0.bn1.running_var', 'cp.resnet.layer4.0.bn1.num_batches_tracked', 'cp.resnet.layer4.0.conv2.weight', 'cp.resnet.layer4.0.bn2.weight', 'cp.resnet.layer4.0.bn2.bias', 'cp.resnet.layer4.0.bn2.running_mean', 'cp.resnet.layer4.0.bn2.running_var', 'cp.resnet.layer4.0.bn2.num_batches_tracked', 'cp.resnet.layer4.0.downsample.0.weight', 'cp.resnet.layer4.0.downsample.1.weight', 'cp.resnet.layer4.0.downsample.1.bias', 'cp.resnet.layer4.0.downsample.1.running_mean', 'cp.resnet.layer4.0.downsample.1.running_var', 'cp.resnet.layer4.0.downsample.1.num_batches_tracked', 'cp.resnet.layer4.1.conv1.weight', 'cp.resnet.layer4.1.bn1.weight', 'cp.resnet.layer4.1.bn1.bias', 'cp.resnet.layer4.1.bn1.running_mean', 'cp.resnet.layer4.1.bn1.running_var', 'cp.resnet.layer4.1.bn1.num_batches_tracked', 'cp.resnet.layer4.1.conv2.weight', 'cp.resnet.layer4.1.bn2.weight', 'cp.resnet.layer4.1.bn2.bias', 'cp.resnet.layer4.1.bn2.running_mean', 'cp.resnet.layer4.1.bn2.running_var', 'cp.resnet.layer4.1.bn2.num_batches_tracked', 'cp.arm16.conv.conv.weight', 'cp.arm16.conv.bn.weight', 'cp.arm16.conv.bn.bias', 'cp.arm16.conv.bn.running_mean', 'cp.arm16.conv.bn.running_var', 'cp.arm16.conv.bn.num_batches_tracked', 'cp.arm16.conv_atten.weight', 'cp.arm16.bn_atten.weight', 'cp.arm16.bn_atten.bias', 'cp.arm16.bn_atten.running_mean', 'cp.arm16.bn_atten.running_var', 'cp.arm16.bn_atten.num_batches_tracked', 'cp.arm32.conv.conv.weight', 'cp.arm32.conv.bn.weight', 'cp.arm32.conv.bn.bias', 'cp.arm32.conv.bn.running_mean', 'cp.arm32.conv.bn.running_var', 'cp.arm32.conv.bn.num_batches_tracked', 'cp.arm32.conv_atten.weight', 'cp.arm32.bn_atten.weight', 'cp.arm32.bn_atten.bias', 'cp.arm32.bn_atten.running_mean', 'cp.arm32.bn_atten.running_var', 'cp.arm32.bn_atten.num_batches_tracked', 'cp.conv_head32.conv.weight', 'cp.conv_head32.bn.weight', 'cp.conv_head32.bn.bias', 'cp.conv_head32.bn.running_mean', 'cp.conv_head32.bn.running_var', 'cp.conv_head32.bn.num_batches_tracked', 'cp.conv_head16.conv.weight', 'cp.conv_head16.bn.weight', 'cp.conv_head16.bn.bias', 'cp.conv_head16.bn.running_mean', 'cp.conv_head16.bn.running_var', 'cp.conv_head16.bn.num_batches_tracked', 'cp.conv_avg.conv.weight', 'cp.conv_avg.bn.weight', 'cp.conv_avg.bn.bias', 'cp.conv_avg.bn.running_mean', 'cp.conv_avg.bn.running_var', 'cp.conv_avg.bn.num_batches_tracked', 'sp.conv1.conv.weight', 'sp.conv1.bn.weight', 'sp.conv1.bn.bias', 'sp.conv1.bn.running_mean', 'sp.conv1.bn.running_var', 'sp.conv1.bn.num_batches_tracked', 'sp.conv2.conv.weight', 'sp.conv2.bn.weight', 'sp.conv2.bn.bias', 'sp.conv2.bn.running_mean', 'sp.conv2.bn.running_var', 'sp.conv2.bn.num_batches_tracked', 'sp.conv3.conv.weight', 'sp.conv3.bn.weight', 'sp.conv3.bn.bias', 'sp.conv3.bn.running_mean', 'sp.conv3.bn.running_var', 'sp.conv3.bn.num_batches_tracked', 'sp.conv_out.conv.weight', 'sp.conv_out.bn.weight', 'sp.conv_out.bn.bias', 'sp.conv_out.bn.running_mean', 'sp.conv_out.bn.running_var', 'sp.conv_out.bn.num_batches_tracked', 'ffm.convblk.conv.weight', 'ffm.convblk.bn.weight', 'ffm.convblk.bn.bias', 'ffm.convblk.bn.running_mean', 'ffm.convblk.bn.running_var', 'ffm.convblk.bn.num_batches_tracked', 'ffm.conv.weight', 'ffm.bn.weight', 'ffm.bn.bias', 'ffm.bn.running_mean', 'ffm.bn.running_var', 'ffm.bn.num_batches_tracked', 'conv_out.conv.conv.weight', 'conv_out.conv.bn.weight', 'conv_out.conv.bn.bias', 'conv_out.conv.bn.running_mean', 'conv_out.conv.bn.running_var', 'conv_out.conv.bn.num_batches_tracked', 'conv_out.conv_out.weight', 'conv_out.conv_out.bias', 'conv_out16.conv.conv.weight', 'conv_out16.conv.bn.weight', 'conv_out16.conv.bn.bias', 'conv_out16.conv.bn.running_mean', 'conv_out16.conv.bn.running_var', 'conv_out16.conv.bn.num_batches_tracked', 'conv_out16.conv_out.weight', 'conv_out16.conv_out.bias', 'conv_out32.conv.conv.weight', 'conv_out32.conv.bn.weight', 'conv_out32.conv.bn.bias', 'conv_out32.conv.bn.running_mean', 'conv_out32.conv.bn.running_var', 'conv_out32.conv.bn.num_batches_tracked', 'conv_out32.conv_out.weight', 'conv_out32.conv_out.bias'])\n",
            "conv_out.conv_out.weight in own_state\n",
            "Size mismatch for conv_out.conv_out.weight: torch.Size([20, 256, 1, 1]) vs torch.Size([19, 256, 1, 1])\n",
            "conv_out.conv_out.bias in own_state\n",
            "Size mismatch for conv_out.conv_out.bias: torch.Size([20]) vs torch.Size([19])\n",
            "conv_out16.conv_out.weight in own_state\n",
            "Size mismatch for conv_out16.conv_out.weight: torch.Size([20, 64, 1, 1]) vs torch.Size([19, 64, 1, 1])\n",
            "conv_out16.conv_out.bias in own_state\n",
            "Size mismatch for conv_out16.conv_out.bias: torch.Size([20]) vs torch.Size([19])\n",
            "conv_out32.conv_out.weight in own_state\n",
            "Size mismatch for conv_out32.conv_out.weight: torch.Size([20, 64, 1, 1]) vs torch.Size([19, 64, 1, 1])\n",
            "conv_out32.conv_out.bias in own_state\n",
            "Size mismatch for conv_out32.conv_out.bias: torch.Size([20]) vs torch.Size([19])\n",
            "Import Model bisenet with weights bisenetv1_pretrained.pth to FineTune\n",
            "========== TRAINING ===========\n",
            "/content/cityscapes/leftImg8bit/train\n",
            "/content/cityscapes/leftImg8bit/val\n",
            "<class '__main__.CrossEntropyLoss2d'>\n",
            "----- TRAINING - EPOCH 1 -----\n",
            "LEARNING RATE:  0.0025\n",
            "loss: 0.4135 (epoch: 1, step: 0) // Avg time/img: 0.5223 s\n",
            "loss: 0.3298 (epoch: 1, step: 50) // Avg time/img: 0.0438 s\n",
            "loss: 0.3058 (epoch: 1, step: 100) // Avg time/img: 0.0390 s\n",
            "loss: 0.2876 (epoch: 1, step: 150) // Avg time/img: 0.0375 s\n",
            "loss: 0.2787 (epoch: 1, step: 200) // Avg time/img: 0.0369 s\n",
            "loss: 0.2708 (epoch: 1, step: 250) // Avg time/img: 0.0365 s\n",
            "loss: 0.2649 (epoch: 1, step: 300) // Avg time/img: 0.0364 s\n",
            "loss: 0.2594 (epoch: 1, step: 350) // Avg time/img: 0.0362 s\n",
            "loss: 0.256 (epoch: 1, step: 400) // Avg time/img: 0.0362 s\n",
            "loss: 0.2529 (epoch: 1, step: 450) // Avg time/img: 0.0361 s\n",
            "----- VALIDATING - EPOCH 1 -----\n",
            "VAL loss: 0.1632 (epoch: 1, step: 0) // Avg time/img: 0.0359 s\n",
            "VAL loss: 0.3407 (epoch: 1, step: 50) // Avg time/img: 0.0223 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m65.55\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/bisenet_training1/model-001.pth (epoch: 1)\n",
            "save: ../save/bisenet_training1/model_best.pth (epoch: 1)\n",
            "----- TRAINING - EPOCH 2 -----\n",
            "LEARNING RATE:  0.0023872134540537495\n",
            "loss: 0.2414 (epoch: 2, step: 0) // Avg time/img: 0.0425 s\n",
            "loss: 0.2249 (epoch: 2, step: 50) // Avg time/img: 0.0361 s\n",
            "loss: 0.2301 (epoch: 2, step: 100) // Avg time/img: 0.0355 s\n",
            "loss: 0.2275 (epoch: 2, step: 150) // Avg time/img: 0.0354 s\n",
            "loss: 0.2262 (epoch: 2, step: 200) // Avg time/img: 0.0355 s\n",
            "loss: 0.2259 (epoch: 2, step: 250) // Avg time/img: 0.0356 s\n",
            "loss: 0.2256 (epoch: 2, step: 300) // Avg time/img: 0.0355 s\n",
            "loss: 0.2255 (epoch: 2, step: 350) // Avg time/img: 0.0355 s\n",
            "loss: 0.2256 (epoch: 2, step: 400) // Avg time/img: 0.0355 s\n",
            "loss: 0.2261 (epoch: 2, step: 450) // Avg time/img: 0.0355 s\n",
            "----- VALIDATING - EPOCH 2 -----\n",
            "VAL loss: 0.1606 (epoch: 2, step: 0) // Avg time/img: 0.0290 s\n",
            "VAL loss: 0.3336 (epoch: 2, step: 50) // Avg time/img: 0.0217 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m65.41\u001b[0m %\n",
            "save: ../save/bisenet_training1/model-002.pth (epoch: 2)\n",
            "----- TRAINING - EPOCH 3 -----\n",
            "LEARNING RATE:  0.0022738314402074057\n",
            "loss: 0.1743 (epoch: 3, step: 0) // Avg time/img: 0.0431 s\n",
            "loss: 0.2214 (epoch: 3, step: 50) // Avg time/img: 0.0356 s\n",
            "loss: 0.221 (epoch: 3, step: 100) // Avg time/img: 0.0356 s\n",
            "loss: 0.2208 (epoch: 3, step: 150) // Avg time/img: 0.0357 s\n",
            "loss: 0.2177 (epoch: 3, step: 200) // Avg time/img: 0.0357 s\n",
            "loss: 0.2175 (epoch: 3, step: 250) // Avg time/img: 0.0357 s\n",
            "loss: 0.2194 (epoch: 3, step: 300) // Avg time/img: 0.0358 s\n",
            "loss: 0.2205 (epoch: 3, step: 350) // Avg time/img: 0.0358 s\n",
            "loss: 0.2193 (epoch: 3, step: 400) // Avg time/img: 0.0358 s\n",
            "loss: 0.2197 (epoch: 3, step: 450) // Avg time/img: 0.0356 s\n",
            "----- VALIDATING - EPOCH 3 -----\n",
            "VAL loss: 0.1585 (epoch: 3, step: 0) // Avg time/img: 0.0334 s\n",
            "VAL loss: 0.329 (epoch: 3, step: 50) // Avg time/img: 0.0221 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m65.52\u001b[0m %\n",
            "save: ../save/bisenet_training1/model-003.pth (epoch: 3)\n",
            "----- TRAINING - EPOCH 4 -----\n",
            "LEARNING RATE:  0.0021598174307570477\n",
            "loss: 0.1685 (epoch: 4, step: 0) // Avg time/img: 0.0411 s\n",
            "loss: 0.2234 (epoch: 4, step: 50) // Avg time/img: 0.0359 s\n",
            "loss: 0.219 (epoch: 4, step: 100) // Avg time/img: 0.0354 s\n",
            "loss: 0.2168 (epoch: 4, step: 150) // Avg time/img: 0.0355 s\n",
            "loss: 0.2165 (epoch: 4, step: 200) // Avg time/img: 0.0356 s\n",
            "loss: 0.2163 (epoch: 4, step: 250) // Avg time/img: 0.0355 s\n",
            "loss: 0.2171 (epoch: 4, step: 300) // Avg time/img: 0.0356 s\n",
            "loss: 0.2179 (epoch: 4, step: 350) // Avg time/img: 0.0355 s\n",
            "loss: 0.2172 (epoch: 4, step: 400) // Avg time/img: 0.0354 s\n",
            "loss: 0.2177 (epoch: 4, step: 450) // Avg time/img: 0.0355 s\n",
            "----- VALIDATING - EPOCH 4 -----\n",
            "VAL loss: 0.1574 (epoch: 4, step: 0) // Avg time/img: 0.0295 s\n",
            "VAL loss: 0.3274 (epoch: 4, step: 50) // Avg time/img: 0.0215 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m65.54\u001b[0m %\n",
            "save: ../save/bisenet_training1/model-004.pth (epoch: 4)\n",
            "----- TRAINING - EPOCH 5 -----\n",
            "LEARNING RATE:  0.002045130365127146\n",
            "loss: 0.2186 (epoch: 5, step: 0) // Avg time/img: 0.0397 s\n",
            "loss: 0.2124 (epoch: 5, step: 50) // Avg time/img: 0.0352 s\n",
            "loss: 0.214 (epoch: 5, step: 100) // Avg time/img: 0.0353 s\n",
            "loss: 0.2166 (epoch: 5, step: 150) // Avg time/img: 0.0352 s\n",
            "loss: 0.217 (epoch: 5, step: 200) // Avg time/img: 0.0353 s\n",
            "loss: 0.2162 (epoch: 5, step: 250) // Avg time/img: 0.0353 s\n",
            "loss: 0.2159 (epoch: 5, step: 300) // Avg time/img: 0.0354 s\n",
            "loss: 0.2166 (epoch: 5, step: 350) // Avg time/img: 0.0355 s\n",
            "loss: 0.2163 (epoch: 5, step: 400) // Avg time/img: 0.0356 s\n",
            "loss: 0.2159 (epoch: 5, step: 450) // Avg time/img: 0.0356 s\n",
            "----- VALIDATING - EPOCH 5 -----\n",
            "VAL loss: 0.156 (epoch: 5, step: 0) // Avg time/img: 0.0352 s\n",
            "VAL loss: 0.3222 (epoch: 5, step: 50) // Avg time/img: 0.0222 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m65.76\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/bisenet_training1/model-005.pth (epoch: 5)\n",
            "save: ../save/bisenet_training1/model_best.pth (epoch: 5)\n",
            "----- TRAINING - EPOCH 6 -----\n",
            "LEARNING RATE:  0.0019297237668089262\n",
            "loss: 0.2863 (epoch: 6, step: 0) // Avg time/img: 0.0442 s\n",
            "loss: 0.2075 (epoch: 6, step: 50) // Avg time/img: 0.0361 s\n",
            "loss: 0.2091 (epoch: 6, step: 100) // Avg time/img: 0.0356 s\n",
            "loss: 0.2093 (epoch: 6, step: 150) // Avg time/img: 0.0354 s\n",
            "loss: 0.2104 (epoch: 6, step: 200) // Avg time/img: 0.0356 s\n",
            "loss: 0.2115 (epoch: 6, step: 250) // Avg time/img: 0.0355 s\n",
            "loss: 0.2119 (epoch: 6, step: 300) // Avg time/img: 0.0355 s\n",
            "loss: 0.2116 (epoch: 6, step: 350) // Avg time/img: 0.0355 s\n",
            "loss: 0.2126 (epoch: 6, step: 400) // Avg time/img: 0.0355 s\n",
            "loss: 0.2138 (epoch: 6, step: 450) // Avg time/img: 0.0354 s\n",
            "----- VALIDATING - EPOCH 6 -----\n",
            "VAL loss: 0.1564 (epoch: 6, step: 0) // Avg time/img: 0.0370 s\n",
            "VAL loss: 0.3225 (epoch: 6, step: 50) // Avg time/img: 0.0215 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m65.89\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/bisenet_training1/model-006.pth (epoch: 6)\n",
            "save: ../save/bisenet_training1/model_best.pth (epoch: 6)\n",
            "----- TRAINING - EPOCH 7 -----\n",
            "LEARNING RATE:  0.0018135446173430498\n",
            "loss: 0.2585 (epoch: 7, step: 0) // Avg time/img: 0.0420 s\n",
            "loss: 0.2074 (epoch: 7, step: 50) // Avg time/img: 0.0358 s\n",
            "loss: 0.2106 (epoch: 7, step: 100) // Avg time/img: 0.0356 s\n",
            "loss: 0.2117 (epoch: 7, step: 150) // Avg time/img: 0.0356 s\n",
            "loss: 0.2117 (epoch: 7, step: 200) // Avg time/img: 0.0356 s\n",
            "loss: 0.2128 (epoch: 7, step: 250) // Avg time/img: 0.0357 s\n",
            "loss: 0.2109 (epoch: 7, step: 300) // Avg time/img: 0.0355 s\n",
            "loss: 0.2104 (epoch: 7, step: 350) // Avg time/img: 0.0357 s\n",
            "loss: 0.2113 (epoch: 7, step: 400) // Avg time/img: 0.0357 s\n",
            "loss: 0.2118 (epoch: 7, step: 450) // Avg time/img: 0.0358 s\n",
            "----- VALIDATING - EPOCH 7 -----\n",
            "VAL loss: 0.1558 (epoch: 7, step: 0) // Avg time/img: 0.0326 s\n",
            "VAL loss: 0.3235 (epoch: 7, step: 50) // Avg time/img: 0.0222 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m65.54\u001b[0m %\n",
            "save: ../save/bisenet_training1/model-007.pth (epoch: 7)\n",
            "----- TRAINING - EPOCH 8 -----\n",
            "LEARNING RATE:  0.0016965318981453127\n",
            "loss: 0.1711 (epoch: 8, step: 0) // Avg time/img: 0.0424 s\n",
            "loss: 0.2119 (epoch: 8, step: 50) // Avg time/img: 0.0361 s\n",
            "loss: 0.2098 (epoch: 8, step: 100) // Avg time/img: 0.0359 s\n",
            "loss: 0.2081 (epoch: 8, step: 150) // Avg time/img: 0.0356 s\n",
            "loss: 0.2071 (epoch: 8, step: 200) // Avg time/img: 0.0355 s\n",
            "loss: 0.2081 (epoch: 8, step: 250) // Avg time/img: 0.0356 s\n",
            "loss: 0.2087 (epoch: 8, step: 300) // Avg time/img: 0.0354 s\n",
            "loss: 0.2094 (epoch: 8, step: 350) // Avg time/img: 0.0355 s\n",
            "loss: 0.21 (epoch: 8, step: 400) // Avg time/img: 0.0356 s\n",
            "loss: 0.211 (epoch: 8, step: 450) // Avg time/img: 0.0356 s\n",
            "----- VALIDATING - EPOCH 8 -----\n",
            "VAL loss: 0.1525 (epoch: 8, step: 0) // Avg time/img: 0.0369 s\n",
            "VAL loss: 0.3211 (epoch: 8, step: 50) // Avg time/img: 0.0216 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m65.88\u001b[0m %\n",
            "save: ../save/bisenet_training1/model-008.pth (epoch: 8)\n",
            "----- TRAINING - EPOCH 9 -----\n",
            "LEARNING RATE:  0.0015786146687233882\n",
            "loss: 0.2422 (epoch: 9, step: 0) // Avg time/img: 0.0475 s\n",
            "loss: 0.2092 (epoch: 9, step: 50) // Avg time/img: 0.0353 s\n",
            "loss: 0.2116 (epoch: 9, step: 100) // Avg time/img: 0.0354 s\n",
            "loss: 0.2135 (epoch: 9, step: 150) // Avg time/img: 0.0357 s\n",
            "loss: 0.2141 (epoch: 9, step: 200) // Avg time/img: 0.0356 s\n",
            "loss: 0.2124 (epoch: 9, step: 250) // Avg time/img: 0.0356 s\n",
            "loss: 0.2117 (epoch: 9, step: 300) // Avg time/img: 0.0356 s\n",
            "loss: 0.2122 (epoch: 9, step: 350) // Avg time/img: 0.0355 s\n",
            "loss: 0.2128 (epoch: 9, step: 400) // Avg time/img: 0.0356 s\n",
            "loss: 0.2121 (epoch: 9, step: 450) // Avg time/img: 0.0355 s\n",
            "----- VALIDATING - EPOCH 9 -----\n",
            "VAL loss: 0.1554 (epoch: 9, step: 0) // Avg time/img: 0.0423 s\n",
            "VAL loss: 0.325 (epoch: 9, step: 50) // Avg time/img: 0.0227 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m65.64\u001b[0m %\n",
            "save: ../save/bisenet_training1/model-009.pth (epoch: 9)\n",
            "----- TRAINING - EPOCH 10 -----\n",
            "LEARNING RATE:  0.0014597094822999506\n",
            "loss: 0.1769 (epoch: 10, step: 0) // Avg time/img: 0.0488 s\n",
            "loss: 0.2227 (epoch: 10, step: 50) // Avg time/img: 0.0361 s\n",
            "loss: 0.2171 (epoch: 10, step: 100) // Avg time/img: 0.0358 s\n",
            "loss: 0.216 (epoch: 10, step: 150) // Avg time/img: 0.0357 s\n",
            "loss: 0.215 (epoch: 10, step: 200) // Avg time/img: 0.0356 s\n",
            "loss: 0.214 (epoch: 10, step: 250) // Avg time/img: 0.0358 s\n",
            "loss: 0.2138 (epoch: 10, step: 300) // Avg time/img: 0.0357 s\n",
            "loss: 0.2132 (epoch: 10, step: 350) // Avg time/img: 0.0356 s\n",
            "loss: 0.2114 (epoch: 10, step: 400) // Avg time/img: 0.0357 s\n",
            "loss: 0.211 (epoch: 10, step: 450) // Avg time/img: 0.0356 s\n",
            "----- VALIDATING - EPOCH 10 -----\n",
            "VAL loss: 0.1543 (epoch: 10, step: 0) // Avg time/img: 0.0315 s\n",
            "VAL loss: 0.3228 (epoch: 10, step: 50) // Avg time/img: 0.0233 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m65.71\u001b[0m %\n",
            "save: ../save/bisenet_training1/model-010.pth (epoch: 10)\n",
            "----- TRAINING - EPOCH 11 -----\n",
            "LEARNING RATE:  0.0013397168281703664\n",
            "loss: 0.1687 (epoch: 11, step: 0) // Avg time/img: 0.0443 s\n",
            "loss: 0.2149 (epoch: 11, step: 50) // Avg time/img: 0.0371 s\n",
            "loss: 0.2125 (epoch: 11, step: 100) // Avg time/img: 0.0362 s\n",
            "loss: 0.2104 (epoch: 11, step: 150) // Avg time/img: 0.0359 s\n",
            "loss: 0.2095 (epoch: 11, step: 200) // Avg time/img: 0.0360 s\n",
            "loss: 0.2107 (epoch: 11, step: 250) // Avg time/img: 0.0359 s\n",
            "loss: 0.2111 (epoch: 11, step: 300) // Avg time/img: 0.0360 s\n",
            "loss: 0.2103 (epoch: 11, step: 350) // Avg time/img: 0.0360 s\n",
            "loss: 0.2094 (epoch: 11, step: 400) // Avg time/img: 0.0359 s\n",
            "loss: 0.2096 (epoch: 11, step: 450) // Avg time/img: 0.0360 s\n",
            "----- VALIDATING - EPOCH 11 -----\n",
            "VAL loss: 0.1542 (epoch: 11, step: 0) // Avg time/img: 0.0316 s\n",
            "VAL loss: 0.32 (epoch: 11, step: 50) // Avg time/img: 0.0223 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m65.99\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/bisenet_training1/model-011.pth (epoch: 11)\n",
            "save: ../save/bisenet_training1/model_best.pth (epoch: 11)\n",
            "----- TRAINING - EPOCH 12 -----\n",
            "LEARNING RATE:  0.0012185160979474884\n",
            "loss: 0.1901 (epoch: 12, step: 0) // Avg time/img: 0.0421 s\n",
            "loss: 0.2048 (epoch: 12, step: 50) // Avg time/img: 0.0360 s\n",
            "loss: 0.2075 (epoch: 12, step: 100) // Avg time/img: 0.0355 s\n",
            "loss: 0.2087 (epoch: 12, step: 150) // Avg time/img: 0.0352 s\n",
            "loss: 0.2099 (epoch: 12, step: 200) // Avg time/img: 0.0354 s\n",
            "loss: 0.2087 (epoch: 12, step: 250) // Avg time/img: 0.0354 s\n",
            "loss: 0.2089 (epoch: 12, step: 300) // Avg time/img: 0.0355 s\n",
            "loss: 0.2104 (epoch: 12, step: 350) // Avg time/img: 0.0355 s\n",
            "loss: 0.2107 (epoch: 12, step: 400) // Avg time/img: 0.0354 s\n",
            "loss: 0.2108 (epoch: 12, step: 450) // Avg time/img: 0.0355 s\n",
            "----- VALIDATING - EPOCH 12 -----\n",
            "VAL loss: 0.1537 (epoch: 12, step: 0) // Avg time/img: 0.0315 s\n",
            "VAL loss: 0.3193 (epoch: 12, step: 50) // Avg time/img: 0.0224 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m66.09\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/bisenet_training1/model-012.pth (epoch: 12)\n",
            "save: ../save/bisenet_training1/model_best.pth (epoch: 12)\n",
            "----- TRAINING - EPOCH 13 -----\n",
            "LEARNING RATE:  0.0010959582263852174\n",
            "loss: 0.1624 (epoch: 13, step: 0) // Avg time/img: 0.0457 s\n",
            "loss: 0.2108 (epoch: 13, step: 50) // Avg time/img: 0.0357 s\n",
            "loss: 0.2099 (epoch: 13, step: 100) // Avg time/img: 0.0353 s\n",
            "loss: 0.2107 (epoch: 13, step: 150) // Avg time/img: 0.0350 s\n",
            "loss: 0.2084 (epoch: 13, step: 200) // Avg time/img: 0.0351 s\n",
            "loss: 0.2089 (epoch: 13, step: 250) // Avg time/img: 0.0353 s\n",
            "loss: 0.2091 (epoch: 13, step: 300) // Avg time/img: 0.0354 s\n",
            "loss: 0.2094 (epoch: 13, step: 350) // Avg time/img: 0.0354 s\n",
            "loss: 0.2092 (epoch: 13, step: 400) // Avg time/img: 0.0354 s\n",
            "loss: 0.2093 (epoch: 13, step: 450) // Avg time/img: 0.0355 s\n",
            "----- VALIDATING - EPOCH 13 -----\n",
            "VAL loss: 0.1536 (epoch: 13, step: 0) // Avg time/img: 0.0320 s\n",
            "VAL loss: 0.3193 (epoch: 13, step: 50) // Avg time/img: 0.0221 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m65.97\u001b[0m %\n",
            "save: ../save/bisenet_training1/model-013.pth (epoch: 13)\n",
            "----- TRAINING - EPOCH 14 -----\n",
            "LEARNING RATE:  0.0009718544969969087\n",
            "loss: 0.2078 (epoch: 14, step: 0) // Avg time/img: 0.0454 s\n",
            "loss: 0.2052 (epoch: 14, step: 50) // Avg time/img: 0.0349 s\n",
            "loss: 0.2044 (epoch: 14, step: 100) // Avg time/img: 0.0350 s\n",
            "loss: 0.2077 (epoch: 14, step: 150) // Avg time/img: 0.0350 s\n",
            "loss: 0.206 (epoch: 14, step: 200) // Avg time/img: 0.0351 s\n",
            "loss: 0.2077 (epoch: 14, step: 250) // Avg time/img: 0.0354 s\n",
            "loss: 0.2067 (epoch: 14, step: 300) // Avg time/img: 0.0354 s\n",
            "loss: 0.2076 (epoch: 14, step: 350) // Avg time/img: 0.0355 s\n",
            "loss: 0.2078 (epoch: 14, step: 400) // Avg time/img: 0.0354 s\n",
            "loss: 0.2074 (epoch: 14, step: 450) // Avg time/img: 0.0355 s\n",
            "----- VALIDATING - EPOCH 14 -----\n",
            "VAL loss: 0.153 (epoch: 14, step: 0) // Avg time/img: 0.0332 s\n",
            "VAL loss: 0.3172 (epoch: 14, step: 50) // Avg time/img: 0.0218 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m66.11\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/bisenet_training1/model-014.pth (epoch: 14)\n",
            "save: ../save/bisenet_training1/model_best.pth (epoch: 14)\n",
            "----- TRAINING - EPOCH 15 -----\n",
            "LEARNING RATE:  0.0008459586547541247\n",
            "loss: 0.2014 (epoch: 15, step: 0) // Avg time/img: 0.0463 s\n",
            "loss: 0.2115 (epoch: 15, step: 50) // Avg time/img: 0.0356 s\n",
            "loss: 0.2117 (epoch: 15, step: 100) // Avg time/img: 0.0354 s\n",
            "loss: 0.2107 (epoch: 15, step: 150) // Avg time/img: 0.0353 s\n",
            "loss: 0.2101 (epoch: 15, step: 200) // Avg time/img: 0.0355 s\n",
            "loss: 0.2095 (epoch: 15, step: 250) // Avg time/img: 0.0355 s\n",
            "loss: 0.2104 (epoch: 15, step: 300) // Avg time/img: 0.0355 s\n",
            "loss: 0.209 (epoch: 15, step: 350) // Avg time/img: 0.0356 s\n",
            "loss: 0.2081 (epoch: 15, step: 400) // Avg time/img: 0.0356 s\n",
            "loss: 0.2083 (epoch: 15, step: 450) // Avg time/img: 0.0357 s\n",
            "----- VALIDATING - EPOCH 15 -----\n",
            "VAL loss: 0.1524 (epoch: 15, step: 0) // Avg time/img: 0.0346 s\n",
            "VAL loss: 0.3183 (epoch: 15, step: 50) // Avg time/img: 0.0228 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m65.96\u001b[0m %\n",
            "save: ../save/bisenet_training1/model-015.pth (epoch: 15)\n",
            "----- TRAINING - EPOCH 16 -----\n",
            "LEARNING RATE:  0.0007179364718731469\n",
            "loss: 0.3048 (epoch: 16, step: 0) // Avg time/img: 0.0425 s\n",
            "loss: 0.2035 (epoch: 16, step: 50) // Avg time/img: 0.0361 s\n",
            "loss: 0.2108 (epoch: 16, step: 100) // Avg time/img: 0.0363 s\n",
            "loss: 0.211 (epoch: 16, step: 150) // Avg time/img: 0.0361 s\n",
            "loss: 0.2099 (epoch: 16, step: 200) // Avg time/img: 0.0359 s\n",
            "loss: 0.2112 (epoch: 16, step: 250) // Avg time/img: 0.0358 s\n",
            "loss: 0.2081 (epoch: 16, step: 300) // Avg time/img: 0.0357 s\n",
            "loss: 0.2086 (epoch: 16, step: 350) // Avg time/img: 0.0357 s\n",
            "loss: 0.2085 (epoch: 16, step: 400) // Avg time/img: 0.0359 s\n",
            "loss: 0.2085 (epoch: 16, step: 450) // Avg time/img: 0.0358 s\n",
            "----- VALIDATING - EPOCH 16 -----\n",
            "VAL loss: 0.1535 (epoch: 16, step: 0) // Avg time/img: 0.0299 s\n",
            "VAL loss: 0.3203 (epoch: 16, step: 50) // Avg time/img: 0.0224 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m66.11\u001b[0m %\n",
            "save: ../save/bisenet_training1/model-016.pth (epoch: 16)\n",
            "----- TRAINING - EPOCH 17 -----\n",
            "LEARNING RATE:  0.0005873094715440094\n",
            "loss: 0.2015 (epoch: 17, step: 0) // Avg time/img: 0.0442 s\n",
            "loss: 0.2058 (epoch: 17, step: 50) // Avg time/img: 0.0352 s\n",
            "loss: 0.2023 (epoch: 17, step: 100) // Avg time/img: 0.0354 s\n",
            "loss: 0.2049 (epoch: 17, step: 150) // Avg time/img: 0.0355 s\n",
            "loss: 0.209 (epoch: 17, step: 200) // Avg time/img: 0.0355 s\n",
            "loss: 0.2062 (epoch: 17, step: 250) // Avg time/img: 0.0357 s\n",
            "loss: 0.2065 (epoch: 17, step: 300) // Avg time/img: 0.0357 s\n",
            "loss: 0.2067 (epoch: 17, step: 350) // Avg time/img: 0.0356 s\n",
            "loss: 0.208 (epoch: 17, step: 400) // Avg time/img: 0.0356 s\n",
            "loss: 0.2091 (epoch: 17, step: 450) // Avg time/img: 0.0356 s\n",
            "----- VALIDATING - EPOCH 17 -----\n",
            "VAL loss: 0.1522 (epoch: 17, step: 0) // Avg time/img: 0.0317 s\n",
            "VAL loss: 0.3166 (epoch: 17, step: 50) // Avg time/img: 0.0230 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m66.01\u001b[0m %\n",
            "save: ../save/bisenet_training1/model-017.pth (epoch: 17)\n",
            "----- TRAINING - EPOCH 18 -----\n",
            "LEARNING RATE:  0.0004533380182841864\n",
            "loss: 0.3121 (epoch: 18, step: 0) // Avg time/img: 0.0388 s\n",
            "loss: 0.2069 (epoch: 18, step: 50) // Avg time/img: 0.0361 s\n",
            "loss: 0.2094 (epoch: 18, step: 100) // Avg time/img: 0.0356 s\n",
            "loss: 0.2082 (epoch: 18, step: 150) // Avg time/img: 0.0358 s\n",
            "loss: 0.2103 (epoch: 18, step: 200) // Avg time/img: 0.0356 s\n",
            "loss: 0.2088 (epoch: 18, step: 250) // Avg time/img: 0.0357 s\n",
            "loss: 0.2087 (epoch: 18, step: 300) // Avg time/img: 0.0359 s\n",
            "loss: 0.2089 (epoch: 18, step: 350) // Avg time/img: 0.0357 s\n",
            "loss: 0.2083 (epoch: 18, step: 400) // Avg time/img: 0.0358 s\n",
            "loss: 0.2086 (epoch: 18, step: 450) // Avg time/img: 0.0357 s\n",
            "----- VALIDATING - EPOCH 18 -----\n",
            "VAL loss: 0.1513 (epoch: 18, step: 0) // Avg time/img: 0.0351 s\n",
            "VAL loss: 0.3182 (epoch: 18, step: 50) // Avg time/img: 0.0224 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m66.04\u001b[0m %\n",
            "save: ../save/bisenet_training1/model-018.pth (epoch: 18)\n",
            "----- TRAINING - EPOCH 19 -----\n",
            "LEARNING RATE:  0.00031473135294854176\n",
            "loss: 0.1791 (epoch: 19, step: 0) // Avg time/img: 0.0426 s\n",
            "loss: 0.2055 (epoch: 19, step: 50) // Avg time/img: 0.0357 s\n",
            "loss: 0.2094 (epoch: 19, step: 100) // Avg time/img: 0.0355 s\n",
            "loss: 0.2082 (epoch: 19, step: 150) // Avg time/img: 0.0355 s\n",
            "loss: 0.2088 (epoch: 19, step: 200) // Avg time/img: 0.0358 s\n",
            "loss: 0.2079 (epoch: 19, step: 250) // Avg time/img: 0.0357 s\n",
            "loss: 0.2084 (epoch: 19, step: 300) // Avg time/img: 0.0357 s\n",
            "loss: 0.2084 (epoch: 19, step: 350) // Avg time/img: 0.0357 s\n",
            "loss: 0.2074 (epoch: 19, step: 400) // Avg time/img: 0.0357 s\n",
            "loss: 0.2067 (epoch: 19, step: 450) // Avg time/img: 0.0357 s\n",
            "----- VALIDATING - EPOCH 19 -----\n",
            "VAL loss: 0.1514 (epoch: 19, step: 0) // Avg time/img: 0.0315 s\n",
            "VAL loss: 0.3149 (epoch: 19, step: 50) // Avg time/img: 0.0218 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m66.26\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/bisenet_training1/model-019.pth (epoch: 19)\n",
            "save: ../save/bisenet_training1/model_best.pth (epoch: 19)\n",
            "----- TRAINING - EPOCH 20 -----\n",
            "LEARNING RATE:  0.00016866035595919555\n",
            "loss: 0.188 (epoch: 20, step: 0) // Avg time/img: 0.0450 s\n",
            "loss: 0.2009 (epoch: 20, step: 50) // Avg time/img: 0.0351 s\n",
            "loss: 0.2046 (epoch: 20, step: 100) // Avg time/img: 0.0353 s\n",
            "loss: 0.2019 (epoch: 20, step: 150) // Avg time/img: 0.0354 s\n",
            "loss: 0.2026 (epoch: 20, step: 200) // Avg time/img: 0.0353 s\n",
            "loss: 0.2035 (epoch: 20, step: 250) // Avg time/img: 0.0355 s\n",
            "loss: 0.2047 (epoch: 20, step: 300) // Avg time/img: 0.0354 s\n",
            "loss: 0.2058 (epoch: 20, step: 350) // Avg time/img: 0.0353 s\n",
            "loss: 0.2065 (epoch: 20, step: 400) // Avg time/img: 0.0355 s\n",
            "loss: 0.2072 (epoch: 20, step: 450) // Avg time/img: 0.0355 s\n",
            "----- VALIDATING - EPOCH 20 -----\n",
            "VAL loss: 0.1508 (epoch: 20, step: 0) // Avg time/img: 0.0293 s\n",
            "VAL loss: 0.3149 (epoch: 20, step: 50) // Avg time/img: 0.0229 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m66.37\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/bisenet_training1/model-020.pth (epoch: 20)\n",
            "save: ../save/bisenet_training1/model_best.pth (epoch: 20)\n",
            "========== TRAINING FINISHED ===========\n",
            "updating: content/AnomalySegmentation/save/bisenet_training1/ (stored 0%)\n",
            "updating: content/AnomalySegmentation/save/bisenet_training1/automated_log.txt (deflated 66%)\n",
            "updating: content/AnomalySegmentation/save/bisenet_training1/bisenet.py (deflated 82%)\n",
            "updating: content/AnomalySegmentation/save/bisenet_training1/model-010.pth (deflated 7%)\n",
            "updating: content/AnomalySegmentation/save/bisenet_training1/model.txt (deflated 91%)\n",
            "updating: content/AnomalySegmentation/save/bisenet_training1/opts.txt (deflated 37%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training1/model-014.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training1/model_best.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training1/model-002.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training1/model-011.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training1/model-001.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training1/model-003.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training1/model-020.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training1/model-006.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training1/model-015.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training1/model-005.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training1/model-007.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training1/model-012.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training1/model-018.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training1/best.txt (stored 0%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training1/model_best.pth.tar (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training1/model-016.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training1/model-019.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training1/model-013.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training1/model-008.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training1/model-017.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training1/model-009.pth (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training1/checkpoint.pth.tar (deflated 7%)\n",
            "  adding: content/AnomalySegmentation/save/bisenet_training1/model-004.pth (deflated 7%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation"
      ],
      "metadata": {
        "id": "mMrPCZ56IShf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "no_execute = False\n",
        "just_once = False\n",
        "\n",
        "for net in [\"erfnet\", \"enet\", \"bisenet\"]:\n",
        "  print(\"----------------------------\")\n",
        "  for dataset_dir in ['RoadAnomaly21', 'RoadObsticle21', 'FS_LostFound_full', 'fs_static', 'RoadAnomaly']:\n",
        "\n",
        "    if no_execute:\n",
        "      break\n",
        "\n",
        "    load_dir = f'/content/AnomalySegmentation/save/{net}_training1'\n",
        "    weights = f'/model_best.pth'\n",
        "    format_file = os.listdir(f'/content/Validation_Dataset/{dataset_dir}/images')[0].split(\".\")[1]\n",
        "    input =f'/content/Validation_Dataset/{dataset_dir}/images/\\*.{format_file}'\n",
        "    print(f\"\\nDataset: {dataset_dir} net: {net}\")\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "      !python -W ignore /content/AnomalySegmentation/eval/evalAnomaly.py --input {input} --void --model {net} --loadDir {load_dir} --loadWeights {weights} | tail -n 2\n",
        "    else:\n",
        "      !python -W ignore /content/AnomalySegmentation/eval/evalAnomaly.py --input {input} --void --model {net} --loadDir {load_dir} --loadWeights {weights} --cpu | tail -n 2\n",
        "\n",
        "    if just_once:\n",
        "      no_execute = True\n",
        "      just_once = False"
      ],
      "metadata": {
        "id": "cchB40LlIT9a",
        "outputId": "30674a33-f5d0-47a4-a215-15cf17f372ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------\n",
            "\n",
            "Dataset: RoadAnomaly21 net: erfnet\n",
            "AUPRC score: 23.066538132595678\n",
            "FPR@TPR95: 83.00218872131218\n",
            "\n",
            "Dataset: RoadObsticle21 net: erfnet\n",
            "AUPRC score: 1.2479619783498568\n",
            "FPR@TPR95: 98.77720541475112\n",
            "\n",
            "Dataset: FS_LostFound_full net: erfnet\n",
            "AUPRC score: 3.9670465669000987\n",
            "FPR@TPR95: 37.15986952996774\n",
            "\n",
            "Dataset: fs_static net: erfnet\n",
            "AUPRC score: 12.255619101525266\n",
            "FPR@TPR95: 82.25043369836908\n",
            "\n",
            "Dataset: RoadAnomaly net: erfnet\n",
            "AUPRC score: 10.108372222814976\n",
            "FPR@TPR95: 97.9176876261683\n",
            "----------------------------\n",
            "\n",
            "Dataset: RoadAnomaly21 net: enet\n",
            "AUPRC score: 17.710540775846635\n",
            "FPR@TPR95: 93.9867224686995\n",
            "\n",
            "Dataset: RoadObsticle21 net: enet\n",
            "AUPRC score: 1.3713287968833945\n",
            "FPR@TPR95: 91.93726483525218\n",
            "\n",
            "Dataset: FS_LostFound_full net: enet\n",
            "AUPRC score: 0.9859109956888955\n",
            "FPR@TPR95: 60.28533261005652\n",
            "\n",
            "Dataset: fs_static net: enet\n",
            "AUPRC score: 7.72351282202996\n",
            "FPR@TPR95: 78.34549796336687\n",
            "\n",
            "Dataset: RoadAnomaly net: enet\n",
            "AUPRC score: 12.788489618794117\n",
            "FPR@TPR95: 86.93573427839908\n",
            "----------------------------\n",
            "\n",
            "Dataset: RoadAnomaly21 net: bisenet\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n",
            "100% 44.7M/44.7M [00:00<00:00, 352MB/s]\n",
            "AUPRC score: 21.81055194591118\n",
            "FPR@TPR95: 91.87575225697388\n",
            "\n",
            "Dataset: RoadObsticle21 net: bisenet\n",
            "AUPRC score: 10.298027408646206\n",
            "FPR@TPR95: 44.90818638296744\n",
            "\n",
            "Dataset: FS_LostFound_full net: bisenet\n",
            "AUPRC score: 3.5811069764660606\n",
            "FPR@TPR95: 44.204070137081445\n",
            "\n",
            "Dataset: fs_static net: bisenet\n",
            "AUPRC score: 9.766178585596988\n",
            "FPR@TPR95: 50.448052314843984\n",
            "\n",
            "Dataset: RoadAnomaly net: bisenet\n",
            "AUPRC score: 12.153410762011895\n",
            "FPR@TPR95: 91.2516594411978\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**MAHALANOBIS**"
      ],
      "metadata": {
        "id": "uVugUIVbxMSq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/AnomalySegmentation && git pull -q"
      ],
      "metadata": {
        "id": "vZiXsIPdxnUp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datadir = '/content/cityscapes'\n",
        "loadWeights = '/save/erfnet_training1/model_best.pth'\n",
        "loadDir = '/content/AnomalySegmentation'\n",
        "\n",
        "!python /content/AnomalySegmentation/eval/mahalanobis.py --datadir {datadir} --model erfnet --loadDir {loadDir} --loadWeights {loadWeights}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S70ucntWxy4F",
        "outputId": "b5412ed0-f07a-4ccc-e86e-c6499e4956cf"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model: erfnet\n",
            "Loading weights: /content/AnomalySegmentation/save/erfnet_training1/model_best.pth\n",
            "/content/cityscapes/leftImg8bit/train\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/content/AnomalySegmentation/eval/mahalanobis.py:133: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model = load_my_state_dict(model, torch.load(weightspath, map_location=lambda storage, loc: storage))\n",
            "odict_keys(['module.encoder.initial_block.conv.weight', 'module.encoder.initial_block.conv.bias', 'module.encoder.initial_block.bn.weight', 'module.encoder.initial_block.bn.bias', 'module.encoder.initial_block.bn.running_mean', 'module.encoder.initial_block.bn.running_var', 'module.encoder.initial_block.bn.num_batches_tracked', 'module.encoder.layers.0.conv.weight', 'module.encoder.layers.0.conv.bias', 'module.encoder.layers.0.bn.weight', 'module.encoder.layers.0.bn.bias', 'module.encoder.layers.0.bn.running_mean', 'module.encoder.layers.0.bn.running_var', 'module.encoder.layers.0.bn.num_batches_tracked', 'module.encoder.layers.1.conv3x1_1.weight', 'module.encoder.layers.1.conv3x1_1.bias', 'module.encoder.layers.1.conv1x3_1.weight', 'module.encoder.layers.1.conv1x3_1.bias', 'module.encoder.layers.1.bn1.weight', 'module.encoder.layers.1.bn1.bias', 'module.encoder.layers.1.bn1.running_mean', 'module.encoder.layers.1.bn1.running_var', 'module.encoder.layers.1.bn1.num_batches_tracked', 'module.encoder.layers.1.conv3x1_2.weight', 'module.encoder.layers.1.conv3x1_2.bias', 'module.encoder.layers.1.conv1x3_2.weight', 'module.encoder.layers.1.conv1x3_2.bias', 'module.encoder.layers.1.bn2.weight', 'module.encoder.layers.1.bn2.bias', 'module.encoder.layers.1.bn2.running_mean', 'module.encoder.layers.1.bn2.running_var', 'module.encoder.layers.1.bn2.num_batches_tracked', 'module.encoder.layers.2.conv3x1_1.weight', 'module.encoder.layers.2.conv3x1_1.bias', 'module.encoder.layers.2.conv1x3_1.weight', 'module.encoder.layers.2.conv1x3_1.bias', 'module.encoder.layers.2.bn1.weight', 'module.encoder.layers.2.bn1.bias', 'module.encoder.layers.2.bn1.running_mean', 'module.encoder.layers.2.bn1.running_var', 'module.encoder.layers.2.bn1.num_batches_tracked', 'module.encoder.layers.2.conv3x1_2.weight', 'module.encoder.layers.2.conv3x1_2.bias', 'module.encoder.layers.2.conv1x3_2.weight', 'module.encoder.layers.2.conv1x3_2.bias', 'module.encoder.layers.2.bn2.weight', 'module.encoder.layers.2.bn2.bias', 'module.encoder.layers.2.bn2.running_mean', 'module.encoder.layers.2.bn2.running_var', 'module.encoder.layers.2.bn2.num_batches_tracked', 'module.encoder.layers.3.conv3x1_1.weight', 'module.encoder.layers.3.conv3x1_1.bias', 'module.encoder.layers.3.conv1x3_1.weight', 'module.encoder.layers.3.conv1x3_1.bias', 'module.encoder.layers.3.bn1.weight', 'module.encoder.layers.3.bn1.bias', 'module.encoder.layers.3.bn1.running_mean', 'module.encoder.layers.3.bn1.running_var', 'module.encoder.layers.3.bn1.num_batches_tracked', 'module.encoder.layers.3.conv3x1_2.weight', 'module.encoder.layers.3.conv3x1_2.bias', 'module.encoder.layers.3.conv1x3_2.weight', 'module.encoder.layers.3.conv1x3_2.bias', 'module.encoder.layers.3.bn2.weight', 'module.encoder.layers.3.bn2.bias', 'module.encoder.layers.3.bn2.running_mean', 'module.encoder.layers.3.bn2.running_var', 'module.encoder.layers.3.bn2.num_batches_tracked', 'module.encoder.layers.4.conv3x1_1.weight', 'module.encoder.layers.4.conv3x1_1.bias', 'module.encoder.layers.4.conv1x3_1.weight', 'module.encoder.layers.4.conv1x3_1.bias', 'module.encoder.layers.4.bn1.weight', 'module.encoder.layers.4.bn1.bias', 'module.encoder.layers.4.bn1.running_mean', 'module.encoder.layers.4.bn1.running_var', 'module.encoder.layers.4.bn1.num_batches_tracked', 'module.encoder.layers.4.conv3x1_2.weight', 'module.encoder.layers.4.conv3x1_2.bias', 'module.encoder.layers.4.conv1x3_2.weight', 'module.encoder.layers.4.conv1x3_2.bias', 'module.encoder.layers.4.bn2.weight', 'module.encoder.layers.4.bn2.bias', 'module.encoder.layers.4.bn2.running_mean', 'module.encoder.layers.4.bn2.running_var', 'module.encoder.layers.4.bn2.num_batches_tracked', 'module.encoder.layers.5.conv3x1_1.weight', 'module.encoder.layers.5.conv3x1_1.bias', 'module.encoder.layers.5.conv1x3_1.weight', 'module.encoder.layers.5.conv1x3_1.bias', 'module.encoder.layers.5.bn1.weight', 'module.encoder.layers.5.bn1.bias', 'module.encoder.layers.5.bn1.running_mean', 'module.encoder.layers.5.bn1.running_var', 'module.encoder.layers.5.bn1.num_batches_tracked', 'module.encoder.layers.5.conv3x1_2.weight', 'module.encoder.layers.5.conv3x1_2.bias', 'module.encoder.layers.5.conv1x3_2.weight', 'module.encoder.layers.5.conv1x3_2.bias', 'module.encoder.layers.5.bn2.weight', 'module.encoder.layers.5.bn2.bias', 'module.encoder.layers.5.bn2.running_mean', 'module.encoder.layers.5.bn2.running_var', 'module.encoder.layers.5.bn2.num_batches_tracked', 'module.encoder.layers.6.conv.weight', 'module.encoder.layers.6.conv.bias', 'module.encoder.layers.6.bn.weight', 'module.encoder.layers.6.bn.bias', 'module.encoder.layers.6.bn.running_mean', 'module.encoder.layers.6.bn.running_var', 'module.encoder.layers.6.bn.num_batches_tracked', 'module.encoder.layers.7.conv3x1_1.weight', 'module.encoder.layers.7.conv3x1_1.bias', 'module.encoder.layers.7.conv1x3_1.weight', 'module.encoder.layers.7.conv1x3_1.bias', 'module.encoder.layers.7.bn1.weight', 'module.encoder.layers.7.bn1.bias', 'module.encoder.layers.7.bn1.running_mean', 'module.encoder.layers.7.bn1.running_var', 'module.encoder.layers.7.bn1.num_batches_tracked', 'module.encoder.layers.7.conv3x1_2.weight', 'module.encoder.layers.7.conv3x1_2.bias', 'module.encoder.layers.7.conv1x3_2.weight', 'module.encoder.layers.7.conv1x3_2.bias', 'module.encoder.layers.7.bn2.weight', 'module.encoder.layers.7.bn2.bias', 'module.encoder.layers.7.bn2.running_mean', 'module.encoder.layers.7.bn2.running_var', 'module.encoder.layers.7.bn2.num_batches_tracked', 'module.encoder.layers.8.conv3x1_1.weight', 'module.encoder.layers.8.conv3x1_1.bias', 'module.encoder.layers.8.conv1x3_1.weight', 'module.encoder.layers.8.conv1x3_1.bias', 'module.encoder.layers.8.bn1.weight', 'module.encoder.layers.8.bn1.bias', 'module.encoder.layers.8.bn1.running_mean', 'module.encoder.layers.8.bn1.running_var', 'module.encoder.layers.8.bn1.num_batches_tracked', 'module.encoder.layers.8.conv3x1_2.weight', 'module.encoder.layers.8.conv3x1_2.bias', 'module.encoder.layers.8.conv1x3_2.weight', 'module.encoder.layers.8.conv1x3_2.bias', 'module.encoder.layers.8.bn2.weight', 'module.encoder.layers.8.bn2.bias', 'module.encoder.layers.8.bn2.running_mean', 'module.encoder.layers.8.bn2.running_var', 'module.encoder.layers.8.bn2.num_batches_tracked', 'module.encoder.layers.9.conv3x1_1.weight', 'module.encoder.layers.9.conv3x1_1.bias', 'module.encoder.layers.9.conv1x3_1.weight', 'module.encoder.layers.9.conv1x3_1.bias', 'module.encoder.layers.9.bn1.weight', 'module.encoder.layers.9.bn1.bias', 'module.encoder.layers.9.bn1.running_mean', 'module.encoder.layers.9.bn1.running_var', 'module.encoder.layers.9.bn1.num_batches_tracked', 'module.encoder.layers.9.conv3x1_2.weight', 'module.encoder.layers.9.conv3x1_2.bias', 'module.encoder.layers.9.conv1x3_2.weight', 'module.encoder.layers.9.conv1x3_2.bias', 'module.encoder.layers.9.bn2.weight', 'module.encoder.layers.9.bn2.bias', 'module.encoder.layers.9.bn2.running_mean', 'module.encoder.layers.9.bn2.running_var', 'module.encoder.layers.9.bn2.num_batches_tracked', 'module.encoder.layers.10.conv3x1_1.weight', 'module.encoder.layers.10.conv3x1_1.bias', 'module.encoder.layers.10.conv1x3_1.weight', 'module.encoder.layers.10.conv1x3_1.bias', 'module.encoder.layers.10.bn1.weight', 'module.encoder.layers.10.bn1.bias', 'module.encoder.layers.10.bn1.running_mean', 'module.encoder.layers.10.bn1.running_var', 'module.encoder.layers.10.bn1.num_batches_tracked', 'module.encoder.layers.10.conv3x1_2.weight', 'module.encoder.layers.10.conv3x1_2.bias', 'module.encoder.layers.10.conv1x3_2.weight', 'module.encoder.layers.10.conv1x3_2.bias', 'module.encoder.layers.10.bn2.weight', 'module.encoder.layers.10.bn2.bias', 'module.encoder.layers.10.bn2.running_mean', 'module.encoder.layers.10.bn2.running_var', 'module.encoder.layers.10.bn2.num_batches_tracked', 'module.encoder.layers.11.conv3x1_1.weight', 'module.encoder.layers.11.conv3x1_1.bias', 'module.encoder.layers.11.conv1x3_1.weight', 'module.encoder.layers.11.conv1x3_1.bias', 'module.encoder.layers.11.bn1.weight', 'module.encoder.layers.11.bn1.bias', 'module.encoder.layers.11.bn1.running_mean', 'module.encoder.layers.11.bn1.running_var', 'module.encoder.layers.11.bn1.num_batches_tracked', 'module.encoder.layers.11.conv3x1_2.weight', 'module.encoder.layers.11.conv3x1_2.bias', 'module.encoder.layers.11.conv1x3_2.weight', 'module.encoder.layers.11.conv1x3_2.bias', 'module.encoder.layers.11.bn2.weight', 'module.encoder.layers.11.bn2.bias', 'module.encoder.layers.11.bn2.running_mean', 'module.encoder.layers.11.bn2.running_var', 'module.encoder.layers.11.bn2.num_batches_tracked', 'module.encoder.layers.12.conv3x1_1.weight', 'module.encoder.layers.12.conv3x1_1.bias', 'module.encoder.layers.12.conv1x3_1.weight', 'module.encoder.layers.12.conv1x3_1.bias', 'module.encoder.layers.12.bn1.weight', 'module.encoder.layers.12.bn1.bias', 'module.encoder.layers.12.bn1.running_mean', 'module.encoder.layers.12.bn1.running_var', 'module.encoder.layers.12.bn1.num_batches_tracked', 'module.encoder.layers.12.conv3x1_2.weight', 'module.encoder.layers.12.conv3x1_2.bias', 'module.encoder.layers.12.conv1x3_2.weight', 'module.encoder.layers.12.conv1x3_2.bias', 'module.encoder.layers.12.bn2.weight', 'module.encoder.layers.12.bn2.bias', 'module.encoder.layers.12.bn2.running_mean', 'module.encoder.layers.12.bn2.running_var', 'module.encoder.layers.12.bn2.num_batches_tracked', 'module.encoder.layers.13.conv3x1_1.weight', 'module.encoder.layers.13.conv3x1_1.bias', 'module.encoder.layers.13.conv1x3_1.weight', 'module.encoder.layers.13.conv1x3_1.bias', 'module.encoder.layers.13.bn1.weight', 'module.encoder.layers.13.bn1.bias', 'module.encoder.layers.13.bn1.running_mean', 'module.encoder.layers.13.bn1.running_var', 'module.encoder.layers.13.bn1.num_batches_tracked', 'module.encoder.layers.13.conv3x1_2.weight', 'module.encoder.layers.13.conv3x1_2.bias', 'module.encoder.layers.13.conv1x3_2.weight', 'module.encoder.layers.13.conv1x3_2.bias', 'module.encoder.layers.13.bn2.weight', 'module.encoder.layers.13.bn2.bias', 'module.encoder.layers.13.bn2.running_mean', 'module.encoder.layers.13.bn2.running_var', 'module.encoder.layers.13.bn2.num_batches_tracked', 'module.encoder.layers.14.conv3x1_1.weight', 'module.encoder.layers.14.conv3x1_1.bias', 'module.encoder.layers.14.conv1x3_1.weight', 'module.encoder.layers.14.conv1x3_1.bias', 'module.encoder.layers.14.bn1.weight', 'module.encoder.layers.14.bn1.bias', 'module.encoder.layers.14.bn1.running_mean', 'module.encoder.layers.14.bn1.running_var', 'module.encoder.layers.14.bn1.num_batches_tracked', 'module.encoder.layers.14.conv3x1_2.weight', 'module.encoder.layers.14.conv3x1_2.bias', 'module.encoder.layers.14.conv1x3_2.weight', 'module.encoder.layers.14.conv1x3_2.bias', 'module.encoder.layers.14.bn2.weight', 'module.encoder.layers.14.bn2.bias', 'module.encoder.layers.14.bn2.running_mean', 'module.encoder.layers.14.bn2.running_var', 'module.encoder.layers.14.bn2.num_batches_tracked', 'module.encoder.output_conv.weight', 'module.encoder.output_conv.bias', 'module.decoder.layers.0.conv.weight', 'module.decoder.layers.0.conv.bias', 'module.decoder.layers.0.bn.weight', 'module.decoder.layers.0.bn.bias', 'module.decoder.layers.0.bn.running_mean', 'module.decoder.layers.0.bn.running_var', 'module.decoder.layers.0.bn.num_batches_tracked', 'module.decoder.layers.1.conv3x1_1.weight', 'module.decoder.layers.1.conv3x1_1.bias', 'module.decoder.layers.1.conv1x3_1.weight', 'module.decoder.layers.1.conv1x3_1.bias', 'module.decoder.layers.1.bn1.weight', 'module.decoder.layers.1.bn1.bias', 'module.decoder.layers.1.bn1.running_mean', 'module.decoder.layers.1.bn1.running_var', 'module.decoder.layers.1.bn1.num_batches_tracked', 'module.decoder.layers.1.conv3x1_2.weight', 'module.decoder.layers.1.conv3x1_2.bias', 'module.decoder.layers.1.conv1x3_2.weight', 'module.decoder.layers.1.conv1x3_2.bias', 'module.decoder.layers.1.bn2.weight', 'module.decoder.layers.1.bn2.bias', 'module.decoder.layers.1.bn2.running_mean', 'module.decoder.layers.1.bn2.running_var', 'module.decoder.layers.1.bn2.num_batches_tracked', 'module.decoder.layers.2.conv3x1_1.weight', 'module.decoder.layers.2.conv3x1_1.bias', 'module.decoder.layers.2.conv1x3_1.weight', 'module.decoder.layers.2.conv1x3_1.bias', 'module.decoder.layers.2.bn1.weight', 'module.decoder.layers.2.bn1.bias', 'module.decoder.layers.2.bn1.running_mean', 'module.decoder.layers.2.bn1.running_var', 'module.decoder.layers.2.bn1.num_batches_tracked', 'module.decoder.layers.2.conv3x1_2.weight', 'module.decoder.layers.2.conv3x1_2.bias', 'module.decoder.layers.2.conv1x3_2.weight', 'module.decoder.layers.2.conv1x3_2.bias', 'module.decoder.layers.2.bn2.weight', 'module.decoder.layers.2.bn2.bias', 'module.decoder.layers.2.bn2.running_mean', 'module.decoder.layers.2.bn2.running_var', 'module.decoder.layers.2.bn2.num_batches_tracked', 'module.decoder.layers.3.conv.weight', 'module.decoder.layers.3.conv.bias', 'module.decoder.layers.3.bn.weight', 'module.decoder.layers.3.bn.bias', 'module.decoder.layers.3.bn.running_mean', 'module.decoder.layers.3.bn.running_var', 'module.decoder.layers.3.bn.num_batches_tracked', 'module.decoder.layers.4.conv3x1_1.weight', 'module.decoder.layers.4.conv3x1_1.bias', 'module.decoder.layers.4.conv1x3_1.weight', 'module.decoder.layers.4.conv1x3_1.bias', 'module.decoder.layers.4.bn1.weight', 'module.decoder.layers.4.bn1.bias', 'module.decoder.layers.4.bn1.running_mean', 'module.decoder.layers.4.bn1.running_var', 'module.decoder.layers.4.bn1.num_batches_tracked', 'module.decoder.layers.4.conv3x1_2.weight', 'module.decoder.layers.4.conv3x1_2.bias', 'module.decoder.layers.4.conv1x3_2.weight', 'module.decoder.layers.4.conv1x3_2.bias', 'module.decoder.layers.4.bn2.weight', 'module.decoder.layers.4.bn2.bias', 'module.decoder.layers.4.bn2.running_mean', 'module.decoder.layers.4.bn2.running_var', 'module.decoder.layers.4.bn2.num_batches_tracked', 'module.decoder.layers.5.conv3x1_1.weight', 'module.decoder.layers.5.conv3x1_1.bias', 'module.decoder.layers.5.conv1x3_1.weight', 'module.decoder.layers.5.conv1x3_1.bias', 'module.decoder.layers.5.bn1.weight', 'module.decoder.layers.5.bn1.bias', 'module.decoder.layers.5.bn1.running_mean', 'module.decoder.layers.5.bn1.running_var', 'module.decoder.layers.5.bn1.num_batches_tracked', 'module.decoder.layers.5.conv3x1_2.weight', 'module.decoder.layers.5.conv3x1_2.bias', 'module.decoder.layers.5.conv1x3_2.weight', 'module.decoder.layers.5.conv1x3_2.bias', 'module.decoder.layers.5.bn2.weight', 'module.decoder.layers.5.bn2.bias', 'module.decoder.layers.5.bn2.running_mean', 'module.decoder.layers.5.bn2.running_var', 'module.decoder.layers.5.bn2.num_batches_tracked', 'module.decoder.output_conv.weight', 'module.decoder.output_conv.bias'])\n",
            "odict_keys(['module.encoder.initial_block.conv.weight', 'module.encoder.initial_block.conv.bias', 'module.encoder.initial_block.bn.weight', 'module.encoder.initial_block.bn.bias', 'module.encoder.initial_block.bn.running_mean', 'module.encoder.initial_block.bn.running_var', 'module.encoder.initial_block.bn.num_batches_tracked', 'module.encoder.layers.0.conv.weight', 'module.encoder.layers.0.conv.bias', 'module.encoder.layers.0.bn.weight', 'module.encoder.layers.0.bn.bias', 'module.encoder.layers.0.bn.running_mean', 'module.encoder.layers.0.bn.running_var', 'module.encoder.layers.0.bn.num_batches_tracked', 'module.encoder.layers.1.conv3x1_1.weight', 'module.encoder.layers.1.conv3x1_1.bias', 'module.encoder.layers.1.conv1x3_1.weight', 'module.encoder.layers.1.conv1x3_1.bias', 'module.encoder.layers.1.bn1.weight', 'module.encoder.layers.1.bn1.bias', 'module.encoder.layers.1.bn1.running_mean', 'module.encoder.layers.1.bn1.running_var', 'module.encoder.layers.1.bn1.num_batches_tracked', 'module.encoder.layers.1.conv3x1_2.weight', 'module.encoder.layers.1.conv3x1_2.bias', 'module.encoder.layers.1.conv1x3_2.weight', 'module.encoder.layers.1.conv1x3_2.bias', 'module.encoder.layers.1.bn2.weight', 'module.encoder.layers.1.bn2.bias', 'module.encoder.layers.1.bn2.running_mean', 'module.encoder.layers.1.bn2.running_var', 'module.encoder.layers.1.bn2.num_batches_tracked', 'module.encoder.layers.2.conv3x1_1.weight', 'module.encoder.layers.2.conv3x1_1.bias', 'module.encoder.layers.2.conv1x3_1.weight', 'module.encoder.layers.2.conv1x3_1.bias', 'module.encoder.layers.2.bn1.weight', 'module.encoder.layers.2.bn1.bias', 'module.encoder.layers.2.bn1.running_mean', 'module.encoder.layers.2.bn1.running_var', 'module.encoder.layers.2.bn1.num_batches_tracked', 'module.encoder.layers.2.conv3x1_2.weight', 'module.encoder.layers.2.conv3x1_2.bias', 'module.encoder.layers.2.conv1x3_2.weight', 'module.encoder.layers.2.conv1x3_2.bias', 'module.encoder.layers.2.bn2.weight', 'module.encoder.layers.2.bn2.bias', 'module.encoder.layers.2.bn2.running_mean', 'module.encoder.layers.2.bn2.running_var', 'module.encoder.layers.2.bn2.num_batches_tracked', 'module.encoder.layers.3.conv3x1_1.weight', 'module.encoder.layers.3.conv3x1_1.bias', 'module.encoder.layers.3.conv1x3_1.weight', 'module.encoder.layers.3.conv1x3_1.bias', 'module.encoder.layers.3.bn1.weight', 'module.encoder.layers.3.bn1.bias', 'module.encoder.layers.3.bn1.running_mean', 'module.encoder.layers.3.bn1.running_var', 'module.encoder.layers.3.bn1.num_batches_tracked', 'module.encoder.layers.3.conv3x1_2.weight', 'module.encoder.layers.3.conv3x1_2.bias', 'module.encoder.layers.3.conv1x3_2.weight', 'module.encoder.layers.3.conv1x3_2.bias', 'module.encoder.layers.3.bn2.weight', 'module.encoder.layers.3.bn2.bias', 'module.encoder.layers.3.bn2.running_mean', 'module.encoder.layers.3.bn2.running_var', 'module.encoder.layers.3.bn2.num_batches_tracked', 'module.encoder.layers.4.conv3x1_1.weight', 'module.encoder.layers.4.conv3x1_1.bias', 'module.encoder.layers.4.conv1x3_1.weight', 'module.encoder.layers.4.conv1x3_1.bias', 'module.encoder.layers.4.bn1.weight', 'module.encoder.layers.4.bn1.bias', 'module.encoder.layers.4.bn1.running_mean', 'module.encoder.layers.4.bn1.running_var', 'module.encoder.layers.4.bn1.num_batches_tracked', 'module.encoder.layers.4.conv3x1_2.weight', 'module.encoder.layers.4.conv3x1_2.bias', 'module.encoder.layers.4.conv1x3_2.weight', 'module.encoder.layers.4.conv1x3_2.bias', 'module.encoder.layers.4.bn2.weight', 'module.encoder.layers.4.bn2.bias', 'module.encoder.layers.4.bn2.running_mean', 'module.encoder.layers.4.bn2.running_var', 'module.encoder.layers.4.bn2.num_batches_tracked', 'module.encoder.layers.5.conv3x1_1.weight', 'module.encoder.layers.5.conv3x1_1.bias', 'module.encoder.layers.5.conv1x3_1.weight', 'module.encoder.layers.5.conv1x3_1.bias', 'module.encoder.layers.5.bn1.weight', 'module.encoder.layers.5.bn1.bias', 'module.encoder.layers.5.bn1.running_mean', 'module.encoder.layers.5.bn1.running_var', 'module.encoder.layers.5.bn1.num_batches_tracked', 'module.encoder.layers.5.conv3x1_2.weight', 'module.encoder.layers.5.conv3x1_2.bias', 'module.encoder.layers.5.conv1x3_2.weight', 'module.encoder.layers.5.conv1x3_2.bias', 'module.encoder.layers.5.bn2.weight', 'module.encoder.layers.5.bn2.bias', 'module.encoder.layers.5.bn2.running_mean', 'module.encoder.layers.5.bn2.running_var', 'module.encoder.layers.5.bn2.num_batches_tracked', 'module.encoder.layers.6.conv.weight', 'module.encoder.layers.6.conv.bias', 'module.encoder.layers.6.bn.weight', 'module.encoder.layers.6.bn.bias', 'module.encoder.layers.6.bn.running_mean', 'module.encoder.layers.6.bn.running_var', 'module.encoder.layers.6.bn.num_batches_tracked', 'module.encoder.layers.7.conv3x1_1.weight', 'module.encoder.layers.7.conv3x1_1.bias', 'module.encoder.layers.7.conv1x3_1.weight', 'module.encoder.layers.7.conv1x3_1.bias', 'module.encoder.layers.7.bn1.weight', 'module.encoder.layers.7.bn1.bias', 'module.encoder.layers.7.bn1.running_mean', 'module.encoder.layers.7.bn1.running_var', 'module.encoder.layers.7.bn1.num_batches_tracked', 'module.encoder.layers.7.conv3x1_2.weight', 'module.encoder.layers.7.conv3x1_2.bias', 'module.encoder.layers.7.conv1x3_2.weight', 'module.encoder.layers.7.conv1x3_2.bias', 'module.encoder.layers.7.bn2.weight', 'module.encoder.layers.7.bn2.bias', 'module.encoder.layers.7.bn2.running_mean', 'module.encoder.layers.7.bn2.running_var', 'module.encoder.layers.7.bn2.num_batches_tracked', 'module.encoder.layers.8.conv3x1_1.weight', 'module.encoder.layers.8.conv3x1_1.bias', 'module.encoder.layers.8.conv1x3_1.weight', 'module.encoder.layers.8.conv1x3_1.bias', 'module.encoder.layers.8.bn1.weight', 'module.encoder.layers.8.bn1.bias', 'module.encoder.layers.8.bn1.running_mean', 'module.encoder.layers.8.bn1.running_var', 'module.encoder.layers.8.bn1.num_batches_tracked', 'module.encoder.layers.8.conv3x1_2.weight', 'module.encoder.layers.8.conv3x1_2.bias', 'module.encoder.layers.8.conv1x3_2.weight', 'module.encoder.layers.8.conv1x3_2.bias', 'module.encoder.layers.8.bn2.weight', 'module.encoder.layers.8.bn2.bias', 'module.encoder.layers.8.bn2.running_mean', 'module.encoder.layers.8.bn2.running_var', 'module.encoder.layers.8.bn2.num_batches_tracked', 'module.encoder.layers.9.conv3x1_1.weight', 'module.encoder.layers.9.conv3x1_1.bias', 'module.encoder.layers.9.conv1x3_1.weight', 'module.encoder.layers.9.conv1x3_1.bias', 'module.encoder.layers.9.bn1.weight', 'module.encoder.layers.9.bn1.bias', 'module.encoder.layers.9.bn1.running_mean', 'module.encoder.layers.9.bn1.running_var', 'module.encoder.layers.9.bn1.num_batches_tracked', 'module.encoder.layers.9.conv3x1_2.weight', 'module.encoder.layers.9.conv3x1_2.bias', 'module.encoder.layers.9.conv1x3_2.weight', 'module.encoder.layers.9.conv1x3_2.bias', 'module.encoder.layers.9.bn2.weight', 'module.encoder.layers.9.bn2.bias', 'module.encoder.layers.9.bn2.running_mean', 'module.encoder.layers.9.bn2.running_var', 'module.encoder.layers.9.bn2.num_batches_tracked', 'module.encoder.layers.10.conv3x1_1.weight', 'module.encoder.layers.10.conv3x1_1.bias', 'module.encoder.layers.10.conv1x3_1.weight', 'module.encoder.layers.10.conv1x3_1.bias', 'module.encoder.layers.10.bn1.weight', 'module.encoder.layers.10.bn1.bias', 'module.encoder.layers.10.bn1.running_mean', 'module.encoder.layers.10.bn1.running_var', 'module.encoder.layers.10.bn1.num_batches_tracked', 'module.encoder.layers.10.conv3x1_2.weight', 'module.encoder.layers.10.conv3x1_2.bias', 'module.encoder.layers.10.conv1x3_2.weight', 'module.encoder.layers.10.conv1x3_2.bias', 'module.encoder.layers.10.bn2.weight', 'module.encoder.layers.10.bn2.bias', 'module.encoder.layers.10.bn2.running_mean', 'module.encoder.layers.10.bn2.running_var', 'module.encoder.layers.10.bn2.num_batches_tracked', 'module.encoder.layers.11.conv3x1_1.weight', 'module.encoder.layers.11.conv3x1_1.bias', 'module.encoder.layers.11.conv1x3_1.weight', 'module.encoder.layers.11.conv1x3_1.bias', 'module.encoder.layers.11.bn1.weight', 'module.encoder.layers.11.bn1.bias', 'module.encoder.layers.11.bn1.running_mean', 'module.encoder.layers.11.bn1.running_var', 'module.encoder.layers.11.bn1.num_batches_tracked', 'module.encoder.layers.11.conv3x1_2.weight', 'module.encoder.layers.11.conv3x1_2.bias', 'module.encoder.layers.11.conv1x3_2.weight', 'module.encoder.layers.11.conv1x3_2.bias', 'module.encoder.layers.11.bn2.weight', 'module.encoder.layers.11.bn2.bias', 'module.encoder.layers.11.bn2.running_mean', 'module.encoder.layers.11.bn2.running_var', 'module.encoder.layers.11.bn2.num_batches_tracked', 'module.encoder.layers.12.conv3x1_1.weight', 'module.encoder.layers.12.conv3x1_1.bias', 'module.encoder.layers.12.conv1x3_1.weight', 'module.encoder.layers.12.conv1x3_1.bias', 'module.encoder.layers.12.bn1.weight', 'module.encoder.layers.12.bn1.bias', 'module.encoder.layers.12.bn1.running_mean', 'module.encoder.layers.12.bn1.running_var', 'module.encoder.layers.12.bn1.num_batches_tracked', 'module.encoder.layers.12.conv3x1_2.weight', 'module.encoder.layers.12.conv3x1_2.bias', 'module.encoder.layers.12.conv1x3_2.weight', 'module.encoder.layers.12.conv1x3_2.bias', 'module.encoder.layers.12.bn2.weight', 'module.encoder.layers.12.bn2.bias', 'module.encoder.layers.12.bn2.running_mean', 'module.encoder.layers.12.bn2.running_var', 'module.encoder.layers.12.bn2.num_batches_tracked', 'module.encoder.layers.13.conv3x1_1.weight', 'module.encoder.layers.13.conv3x1_1.bias', 'module.encoder.layers.13.conv1x3_1.weight', 'module.encoder.layers.13.conv1x3_1.bias', 'module.encoder.layers.13.bn1.weight', 'module.encoder.layers.13.bn1.bias', 'module.encoder.layers.13.bn1.running_mean', 'module.encoder.layers.13.bn1.running_var', 'module.encoder.layers.13.bn1.num_batches_tracked', 'module.encoder.layers.13.conv3x1_2.weight', 'module.encoder.layers.13.conv3x1_2.bias', 'module.encoder.layers.13.conv1x3_2.weight', 'module.encoder.layers.13.conv1x3_2.bias', 'module.encoder.layers.13.bn2.weight', 'module.encoder.layers.13.bn2.bias', 'module.encoder.layers.13.bn2.running_mean', 'module.encoder.layers.13.bn2.running_var', 'module.encoder.layers.13.bn2.num_batches_tracked', 'module.encoder.layers.14.conv3x1_1.weight', 'module.encoder.layers.14.conv3x1_1.bias', 'module.encoder.layers.14.conv1x3_1.weight', 'module.encoder.layers.14.conv1x3_1.bias', 'module.encoder.layers.14.bn1.weight', 'module.encoder.layers.14.bn1.bias', 'module.encoder.layers.14.bn1.running_mean', 'module.encoder.layers.14.bn1.running_var', 'module.encoder.layers.14.bn1.num_batches_tracked', 'module.encoder.layers.14.conv3x1_2.weight', 'module.encoder.layers.14.conv3x1_2.bias', 'module.encoder.layers.14.conv1x3_2.weight', 'module.encoder.layers.14.conv1x3_2.bias', 'module.encoder.layers.14.bn2.weight', 'module.encoder.layers.14.bn2.bias', 'module.encoder.layers.14.bn2.running_mean', 'module.encoder.layers.14.bn2.running_var', 'module.encoder.layers.14.bn2.num_batches_tracked', 'module.encoder.output_conv.weight', 'module.encoder.output_conv.bias', 'module.decoder.layers.0.conv.weight', 'module.decoder.layers.0.conv.bias', 'module.decoder.layers.0.bn.weight', 'module.decoder.layers.0.bn.bias', 'module.decoder.layers.0.bn.running_mean', 'module.decoder.layers.0.bn.running_var', 'module.decoder.layers.0.bn.num_batches_tracked', 'module.decoder.layers.1.conv3x1_1.weight', 'module.decoder.layers.1.conv3x1_1.bias', 'module.decoder.layers.1.conv1x3_1.weight', 'module.decoder.layers.1.conv1x3_1.bias', 'module.decoder.layers.1.bn1.weight', 'module.decoder.layers.1.bn1.bias', 'module.decoder.layers.1.bn1.running_mean', 'module.decoder.layers.1.bn1.running_var', 'module.decoder.layers.1.bn1.num_batches_tracked', 'module.decoder.layers.1.conv3x1_2.weight', 'module.decoder.layers.1.conv3x1_2.bias', 'module.decoder.layers.1.conv1x3_2.weight', 'module.decoder.layers.1.conv1x3_2.bias', 'module.decoder.layers.1.bn2.weight', 'module.decoder.layers.1.bn2.bias', 'module.decoder.layers.1.bn2.running_mean', 'module.decoder.layers.1.bn2.running_var', 'module.decoder.layers.1.bn2.num_batches_tracked', 'module.decoder.layers.2.conv3x1_1.weight', 'module.decoder.layers.2.conv3x1_1.bias', 'module.decoder.layers.2.conv1x3_1.weight', 'module.decoder.layers.2.conv1x3_1.bias', 'module.decoder.layers.2.bn1.weight', 'module.decoder.layers.2.bn1.bias', 'module.decoder.layers.2.bn1.running_mean', 'module.decoder.layers.2.bn1.running_var', 'module.decoder.layers.2.bn1.num_batches_tracked', 'module.decoder.layers.2.conv3x1_2.weight', 'module.decoder.layers.2.conv3x1_2.bias', 'module.decoder.layers.2.conv1x3_2.weight', 'module.decoder.layers.2.conv1x3_2.bias', 'module.decoder.layers.2.bn2.weight', 'module.decoder.layers.2.bn2.bias', 'module.decoder.layers.2.bn2.running_mean', 'module.decoder.layers.2.bn2.running_var', 'module.decoder.layers.2.bn2.num_batches_tracked', 'module.decoder.layers.3.conv.weight', 'module.decoder.layers.3.conv.bias', 'module.decoder.layers.3.bn.weight', 'module.decoder.layers.3.bn.bias', 'module.decoder.layers.3.bn.running_mean', 'module.decoder.layers.3.bn.running_var', 'module.decoder.layers.3.bn.num_batches_tracked', 'module.decoder.layers.4.conv3x1_1.weight', 'module.decoder.layers.4.conv3x1_1.bias', 'module.decoder.layers.4.conv1x3_1.weight', 'module.decoder.layers.4.conv1x3_1.bias', 'module.decoder.layers.4.bn1.weight', 'module.decoder.layers.4.bn1.bias', 'module.decoder.layers.4.bn1.running_mean', 'module.decoder.layers.4.bn1.running_var', 'module.decoder.layers.4.bn1.num_batches_tracked', 'module.decoder.layers.4.conv3x1_2.weight', 'module.decoder.layers.4.conv3x1_2.bias', 'module.decoder.layers.4.conv1x3_2.weight', 'module.decoder.layers.4.conv1x3_2.bias', 'module.decoder.layers.4.bn2.weight', 'module.decoder.layers.4.bn2.bias', 'module.decoder.layers.4.bn2.running_mean', 'module.decoder.layers.4.bn2.running_var', 'module.decoder.layers.4.bn2.num_batches_tracked', 'module.decoder.layers.5.conv3x1_1.weight', 'module.decoder.layers.5.conv3x1_1.bias', 'module.decoder.layers.5.conv1x3_1.weight', 'module.decoder.layers.5.conv1x3_1.bias', 'module.decoder.layers.5.bn1.weight', 'module.decoder.layers.5.bn1.bias', 'module.decoder.layers.5.bn1.running_mean', 'module.decoder.layers.5.bn1.running_var', 'module.decoder.layers.5.bn1.num_batches_tracked', 'module.decoder.layers.5.conv3x1_2.weight', 'module.decoder.layers.5.conv3x1_2.bias', 'module.decoder.layers.5.conv1x3_2.weight', 'module.decoder.layers.5.conv1x3_2.bias', 'module.decoder.layers.5.bn2.weight', 'module.decoder.layers.5.bn2.bias', 'module.decoder.layers.5.bn2.running_mean', 'module.decoder.layers.5.bn2.running_var', 'module.decoder.layers.5.bn2.num_batches_tracked', 'module.decoder.output_conv.weight', 'module.decoder.output_conv.bias'])\n",
            "Model and weights LOADED successfully\n",
            "  0% 0/2975 [00:00<?, ?it/s]/content/AnomalySegmentation/eval/mahalanobis.py:156: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  result = F.softmax(model(images).squeeze(0))\n",
            "100% 2975/2975 [08:30<00:00,  5.82it/s]\n",
            "Mean per class: (20, 512, 1024)\n",
            "Mean output saved as '/content/AnomalySegmentation/save/mean_cityscapes_erfnet.npy'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datadir = '/content/cityscapes'\n",
        "loadWeights = '/save/erfnet_training1/model_best.pth'\n",
        "loadDir = '/content/AnomalySegmentation'\n",
        "mean = \"/save/mean_cityscapes_erfnet.npy\"\n",
        "\n",
        "!python /content/AnomalySegmentation/eval/mahalanobis.py --datadir {datadir} --model erfnet --loadDir {loadDir} --loadWeights {loadWeights} --mean {mean} --num-workers 2"
      ],
      "metadata": {
        "id": "h5yP-t0skBGb",
        "outputId": "d02bd835-5a10-4f90-d9da-eacb70409382",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model: erfnet\n",
            "Loading weights: /content/AnomalySegmentation/save/erfnet_training1/model_best.pth\n",
            "pre_computed_mean torch.Size([20, 512, 1024])\n",
            "/content/cityscapes/leftImg8bit/train\n",
            "/content/AnomalySegmentation/eval/mahalanobis.py:133: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model = load_my_state_dict(model, torch.load(weightspath, map_location=lambda storage, loc: storage))\n",
            "odict_keys(['module.encoder.initial_block.conv.weight', 'module.encoder.initial_block.conv.bias', 'module.encoder.initial_block.bn.weight', 'module.encoder.initial_block.bn.bias', 'module.encoder.initial_block.bn.running_mean', 'module.encoder.initial_block.bn.running_var', 'module.encoder.initial_block.bn.num_batches_tracked', 'module.encoder.layers.0.conv.weight', 'module.encoder.layers.0.conv.bias', 'module.encoder.layers.0.bn.weight', 'module.encoder.layers.0.bn.bias', 'module.encoder.layers.0.bn.running_mean', 'module.encoder.layers.0.bn.running_var', 'module.encoder.layers.0.bn.num_batches_tracked', 'module.encoder.layers.1.conv3x1_1.weight', 'module.encoder.layers.1.conv3x1_1.bias', 'module.encoder.layers.1.conv1x3_1.weight', 'module.encoder.layers.1.conv1x3_1.bias', 'module.encoder.layers.1.bn1.weight', 'module.encoder.layers.1.bn1.bias', 'module.encoder.layers.1.bn1.running_mean', 'module.encoder.layers.1.bn1.running_var', 'module.encoder.layers.1.bn1.num_batches_tracked', 'module.encoder.layers.1.conv3x1_2.weight', 'module.encoder.layers.1.conv3x1_2.bias', 'module.encoder.layers.1.conv1x3_2.weight', 'module.encoder.layers.1.conv1x3_2.bias', 'module.encoder.layers.1.bn2.weight', 'module.encoder.layers.1.bn2.bias', 'module.encoder.layers.1.bn2.running_mean', 'module.encoder.layers.1.bn2.running_var', 'module.encoder.layers.1.bn2.num_batches_tracked', 'module.encoder.layers.2.conv3x1_1.weight', 'module.encoder.layers.2.conv3x1_1.bias', 'module.encoder.layers.2.conv1x3_1.weight', 'module.encoder.layers.2.conv1x3_1.bias', 'module.encoder.layers.2.bn1.weight', 'module.encoder.layers.2.bn1.bias', 'module.encoder.layers.2.bn1.running_mean', 'module.encoder.layers.2.bn1.running_var', 'module.encoder.layers.2.bn1.num_batches_tracked', 'module.encoder.layers.2.conv3x1_2.weight', 'module.encoder.layers.2.conv3x1_2.bias', 'module.encoder.layers.2.conv1x3_2.weight', 'module.encoder.layers.2.conv1x3_2.bias', 'module.encoder.layers.2.bn2.weight', 'module.encoder.layers.2.bn2.bias', 'module.encoder.layers.2.bn2.running_mean', 'module.encoder.layers.2.bn2.running_var', 'module.encoder.layers.2.bn2.num_batches_tracked', 'module.encoder.layers.3.conv3x1_1.weight', 'module.encoder.layers.3.conv3x1_1.bias', 'module.encoder.layers.3.conv1x3_1.weight', 'module.encoder.layers.3.conv1x3_1.bias', 'module.encoder.layers.3.bn1.weight', 'module.encoder.layers.3.bn1.bias', 'module.encoder.layers.3.bn1.running_mean', 'module.encoder.layers.3.bn1.running_var', 'module.encoder.layers.3.bn1.num_batches_tracked', 'module.encoder.layers.3.conv3x1_2.weight', 'module.encoder.layers.3.conv3x1_2.bias', 'module.encoder.layers.3.conv1x3_2.weight', 'module.encoder.layers.3.conv1x3_2.bias', 'module.encoder.layers.3.bn2.weight', 'module.encoder.layers.3.bn2.bias', 'module.encoder.layers.3.bn2.running_mean', 'module.encoder.layers.3.bn2.running_var', 'module.encoder.layers.3.bn2.num_batches_tracked', 'module.encoder.layers.4.conv3x1_1.weight', 'module.encoder.layers.4.conv3x1_1.bias', 'module.encoder.layers.4.conv1x3_1.weight', 'module.encoder.layers.4.conv1x3_1.bias', 'module.encoder.layers.4.bn1.weight', 'module.encoder.layers.4.bn1.bias', 'module.encoder.layers.4.bn1.running_mean', 'module.encoder.layers.4.bn1.running_var', 'module.encoder.layers.4.bn1.num_batches_tracked', 'module.encoder.layers.4.conv3x1_2.weight', 'module.encoder.layers.4.conv3x1_2.bias', 'module.encoder.layers.4.conv1x3_2.weight', 'module.encoder.layers.4.conv1x3_2.bias', 'module.encoder.layers.4.bn2.weight', 'module.encoder.layers.4.bn2.bias', 'module.encoder.layers.4.bn2.running_mean', 'module.encoder.layers.4.bn2.running_var', 'module.encoder.layers.4.bn2.num_batches_tracked', 'module.encoder.layers.5.conv3x1_1.weight', 'module.encoder.layers.5.conv3x1_1.bias', 'module.encoder.layers.5.conv1x3_1.weight', 'module.encoder.layers.5.conv1x3_1.bias', 'module.encoder.layers.5.bn1.weight', 'module.encoder.layers.5.bn1.bias', 'module.encoder.layers.5.bn1.running_mean', 'module.encoder.layers.5.bn1.running_var', 'module.encoder.layers.5.bn1.num_batches_tracked', 'module.encoder.layers.5.conv3x1_2.weight', 'module.encoder.layers.5.conv3x1_2.bias', 'module.encoder.layers.5.conv1x3_2.weight', 'module.encoder.layers.5.conv1x3_2.bias', 'module.encoder.layers.5.bn2.weight', 'module.encoder.layers.5.bn2.bias', 'module.encoder.layers.5.bn2.running_mean', 'module.encoder.layers.5.bn2.running_var', 'module.encoder.layers.5.bn2.num_batches_tracked', 'module.encoder.layers.6.conv.weight', 'module.encoder.layers.6.conv.bias', 'module.encoder.layers.6.bn.weight', 'module.encoder.layers.6.bn.bias', 'module.encoder.layers.6.bn.running_mean', 'module.encoder.layers.6.bn.running_var', 'module.encoder.layers.6.bn.num_batches_tracked', 'module.encoder.layers.7.conv3x1_1.weight', 'module.encoder.layers.7.conv3x1_1.bias', 'module.encoder.layers.7.conv1x3_1.weight', 'module.encoder.layers.7.conv1x3_1.bias', 'module.encoder.layers.7.bn1.weight', 'module.encoder.layers.7.bn1.bias', 'module.encoder.layers.7.bn1.running_mean', 'module.encoder.layers.7.bn1.running_var', 'module.encoder.layers.7.bn1.num_batches_tracked', 'module.encoder.layers.7.conv3x1_2.weight', 'module.encoder.layers.7.conv3x1_2.bias', 'module.encoder.layers.7.conv1x3_2.weight', 'module.encoder.layers.7.conv1x3_2.bias', 'module.encoder.layers.7.bn2.weight', 'module.encoder.layers.7.bn2.bias', 'module.encoder.layers.7.bn2.running_mean', 'module.encoder.layers.7.bn2.running_var', 'module.encoder.layers.7.bn2.num_batches_tracked', 'module.encoder.layers.8.conv3x1_1.weight', 'module.encoder.layers.8.conv3x1_1.bias', 'module.encoder.layers.8.conv1x3_1.weight', 'module.encoder.layers.8.conv1x3_1.bias', 'module.encoder.layers.8.bn1.weight', 'module.encoder.layers.8.bn1.bias', 'module.encoder.layers.8.bn1.running_mean', 'module.encoder.layers.8.bn1.running_var', 'module.encoder.layers.8.bn1.num_batches_tracked', 'module.encoder.layers.8.conv3x1_2.weight', 'module.encoder.layers.8.conv3x1_2.bias', 'module.encoder.layers.8.conv1x3_2.weight', 'module.encoder.layers.8.conv1x3_2.bias', 'module.encoder.layers.8.bn2.weight', 'module.encoder.layers.8.bn2.bias', 'module.encoder.layers.8.bn2.running_mean', 'module.encoder.layers.8.bn2.running_var', 'module.encoder.layers.8.bn2.num_batches_tracked', 'module.encoder.layers.9.conv3x1_1.weight', 'module.encoder.layers.9.conv3x1_1.bias', 'module.encoder.layers.9.conv1x3_1.weight', 'module.encoder.layers.9.conv1x3_1.bias', 'module.encoder.layers.9.bn1.weight', 'module.encoder.layers.9.bn1.bias', 'module.encoder.layers.9.bn1.running_mean', 'module.encoder.layers.9.bn1.running_var', 'module.encoder.layers.9.bn1.num_batches_tracked', 'module.encoder.layers.9.conv3x1_2.weight', 'module.encoder.layers.9.conv3x1_2.bias', 'module.encoder.layers.9.conv1x3_2.weight', 'module.encoder.layers.9.conv1x3_2.bias', 'module.encoder.layers.9.bn2.weight', 'module.encoder.layers.9.bn2.bias', 'module.encoder.layers.9.bn2.running_mean', 'module.encoder.layers.9.bn2.running_var', 'module.encoder.layers.9.bn2.num_batches_tracked', 'module.encoder.layers.10.conv3x1_1.weight', 'module.encoder.layers.10.conv3x1_1.bias', 'module.encoder.layers.10.conv1x3_1.weight', 'module.encoder.layers.10.conv1x3_1.bias', 'module.encoder.layers.10.bn1.weight', 'module.encoder.layers.10.bn1.bias', 'module.encoder.layers.10.bn1.running_mean', 'module.encoder.layers.10.bn1.running_var', 'module.encoder.layers.10.bn1.num_batches_tracked', 'module.encoder.layers.10.conv3x1_2.weight', 'module.encoder.layers.10.conv3x1_2.bias', 'module.encoder.layers.10.conv1x3_2.weight', 'module.encoder.layers.10.conv1x3_2.bias', 'module.encoder.layers.10.bn2.weight', 'module.encoder.layers.10.bn2.bias', 'module.encoder.layers.10.bn2.running_mean', 'module.encoder.layers.10.bn2.running_var', 'module.encoder.layers.10.bn2.num_batches_tracked', 'module.encoder.layers.11.conv3x1_1.weight', 'module.encoder.layers.11.conv3x1_1.bias', 'module.encoder.layers.11.conv1x3_1.weight', 'module.encoder.layers.11.conv1x3_1.bias', 'module.encoder.layers.11.bn1.weight', 'module.encoder.layers.11.bn1.bias', 'module.encoder.layers.11.bn1.running_mean', 'module.encoder.layers.11.bn1.running_var', 'module.encoder.layers.11.bn1.num_batches_tracked', 'module.encoder.layers.11.conv3x1_2.weight', 'module.encoder.layers.11.conv3x1_2.bias', 'module.encoder.layers.11.conv1x3_2.weight', 'module.encoder.layers.11.conv1x3_2.bias', 'module.encoder.layers.11.bn2.weight', 'module.encoder.layers.11.bn2.bias', 'module.encoder.layers.11.bn2.running_mean', 'module.encoder.layers.11.bn2.running_var', 'module.encoder.layers.11.bn2.num_batches_tracked', 'module.encoder.layers.12.conv3x1_1.weight', 'module.encoder.layers.12.conv3x1_1.bias', 'module.encoder.layers.12.conv1x3_1.weight', 'module.encoder.layers.12.conv1x3_1.bias', 'module.encoder.layers.12.bn1.weight', 'module.encoder.layers.12.bn1.bias', 'module.encoder.layers.12.bn1.running_mean', 'module.encoder.layers.12.bn1.running_var', 'module.encoder.layers.12.bn1.num_batches_tracked', 'module.encoder.layers.12.conv3x1_2.weight', 'module.encoder.layers.12.conv3x1_2.bias', 'module.encoder.layers.12.conv1x3_2.weight', 'module.encoder.layers.12.conv1x3_2.bias', 'module.encoder.layers.12.bn2.weight', 'module.encoder.layers.12.bn2.bias', 'module.encoder.layers.12.bn2.running_mean', 'module.encoder.layers.12.bn2.running_var', 'module.encoder.layers.12.bn2.num_batches_tracked', 'module.encoder.layers.13.conv3x1_1.weight', 'module.encoder.layers.13.conv3x1_1.bias', 'module.encoder.layers.13.conv1x3_1.weight', 'module.encoder.layers.13.conv1x3_1.bias', 'module.encoder.layers.13.bn1.weight', 'module.encoder.layers.13.bn1.bias', 'module.encoder.layers.13.bn1.running_mean', 'module.encoder.layers.13.bn1.running_var', 'module.encoder.layers.13.bn1.num_batches_tracked', 'module.encoder.layers.13.conv3x1_2.weight', 'module.encoder.layers.13.conv3x1_2.bias', 'module.encoder.layers.13.conv1x3_2.weight', 'module.encoder.layers.13.conv1x3_2.bias', 'module.encoder.layers.13.bn2.weight', 'module.encoder.layers.13.bn2.bias', 'module.encoder.layers.13.bn2.running_mean', 'module.encoder.layers.13.bn2.running_var', 'module.encoder.layers.13.bn2.num_batches_tracked', 'module.encoder.layers.14.conv3x1_1.weight', 'module.encoder.layers.14.conv3x1_1.bias', 'module.encoder.layers.14.conv1x3_1.weight', 'module.encoder.layers.14.conv1x3_1.bias', 'module.encoder.layers.14.bn1.weight', 'module.encoder.layers.14.bn1.bias', 'module.encoder.layers.14.bn1.running_mean', 'module.encoder.layers.14.bn1.running_var', 'module.encoder.layers.14.bn1.num_batches_tracked', 'module.encoder.layers.14.conv3x1_2.weight', 'module.encoder.layers.14.conv3x1_2.bias', 'module.encoder.layers.14.conv1x3_2.weight', 'module.encoder.layers.14.conv1x3_2.bias', 'module.encoder.layers.14.bn2.weight', 'module.encoder.layers.14.bn2.bias', 'module.encoder.layers.14.bn2.running_mean', 'module.encoder.layers.14.bn2.running_var', 'module.encoder.layers.14.bn2.num_batches_tracked', 'module.encoder.output_conv.weight', 'module.encoder.output_conv.bias', 'module.decoder.layers.0.conv.weight', 'module.decoder.layers.0.conv.bias', 'module.decoder.layers.0.bn.weight', 'module.decoder.layers.0.bn.bias', 'module.decoder.layers.0.bn.running_mean', 'module.decoder.layers.0.bn.running_var', 'module.decoder.layers.0.bn.num_batches_tracked', 'module.decoder.layers.1.conv3x1_1.weight', 'module.decoder.layers.1.conv3x1_1.bias', 'module.decoder.layers.1.conv1x3_1.weight', 'module.decoder.layers.1.conv1x3_1.bias', 'module.decoder.layers.1.bn1.weight', 'module.decoder.layers.1.bn1.bias', 'module.decoder.layers.1.bn1.running_mean', 'module.decoder.layers.1.bn1.running_var', 'module.decoder.layers.1.bn1.num_batches_tracked', 'module.decoder.layers.1.conv3x1_2.weight', 'module.decoder.layers.1.conv3x1_2.bias', 'module.decoder.layers.1.conv1x3_2.weight', 'module.decoder.layers.1.conv1x3_2.bias', 'module.decoder.layers.1.bn2.weight', 'module.decoder.layers.1.bn2.bias', 'module.decoder.layers.1.bn2.running_mean', 'module.decoder.layers.1.bn2.running_var', 'module.decoder.layers.1.bn2.num_batches_tracked', 'module.decoder.layers.2.conv3x1_1.weight', 'module.decoder.layers.2.conv3x1_1.bias', 'module.decoder.layers.2.conv1x3_1.weight', 'module.decoder.layers.2.conv1x3_1.bias', 'module.decoder.layers.2.bn1.weight', 'module.decoder.layers.2.bn1.bias', 'module.decoder.layers.2.bn1.running_mean', 'module.decoder.layers.2.bn1.running_var', 'module.decoder.layers.2.bn1.num_batches_tracked', 'module.decoder.layers.2.conv3x1_2.weight', 'module.decoder.layers.2.conv3x1_2.bias', 'module.decoder.layers.2.conv1x3_2.weight', 'module.decoder.layers.2.conv1x3_2.bias', 'module.decoder.layers.2.bn2.weight', 'module.decoder.layers.2.bn2.bias', 'module.decoder.layers.2.bn2.running_mean', 'module.decoder.layers.2.bn2.running_var', 'module.decoder.layers.2.bn2.num_batches_tracked', 'module.decoder.layers.3.conv.weight', 'module.decoder.layers.3.conv.bias', 'module.decoder.layers.3.bn.weight', 'module.decoder.layers.3.bn.bias', 'module.decoder.layers.3.bn.running_mean', 'module.decoder.layers.3.bn.running_var', 'module.decoder.layers.3.bn.num_batches_tracked', 'module.decoder.layers.4.conv3x1_1.weight', 'module.decoder.layers.4.conv3x1_1.bias', 'module.decoder.layers.4.conv1x3_1.weight', 'module.decoder.layers.4.conv1x3_1.bias', 'module.decoder.layers.4.bn1.weight', 'module.decoder.layers.4.bn1.bias', 'module.decoder.layers.4.bn1.running_mean', 'module.decoder.layers.4.bn1.running_var', 'module.decoder.layers.4.bn1.num_batches_tracked', 'module.decoder.layers.4.conv3x1_2.weight', 'module.decoder.layers.4.conv3x1_2.bias', 'module.decoder.layers.4.conv1x3_2.weight', 'module.decoder.layers.4.conv1x3_2.bias', 'module.decoder.layers.4.bn2.weight', 'module.decoder.layers.4.bn2.bias', 'module.decoder.layers.4.bn2.running_mean', 'module.decoder.layers.4.bn2.running_var', 'module.decoder.layers.4.bn2.num_batches_tracked', 'module.decoder.layers.5.conv3x1_1.weight', 'module.decoder.layers.5.conv3x1_1.bias', 'module.decoder.layers.5.conv1x3_1.weight', 'module.decoder.layers.5.conv1x3_1.bias', 'module.decoder.layers.5.bn1.weight', 'module.decoder.layers.5.bn1.bias', 'module.decoder.layers.5.bn1.running_mean', 'module.decoder.layers.5.bn1.running_var', 'module.decoder.layers.5.bn1.num_batches_tracked', 'module.decoder.layers.5.conv3x1_2.weight', 'module.decoder.layers.5.conv3x1_2.bias', 'module.decoder.layers.5.conv1x3_2.weight', 'module.decoder.layers.5.conv1x3_2.bias', 'module.decoder.layers.5.bn2.weight', 'module.decoder.layers.5.bn2.bias', 'module.decoder.layers.5.bn2.running_mean', 'module.decoder.layers.5.bn2.running_var', 'module.decoder.layers.5.bn2.num_batches_tracked', 'module.decoder.output_conv.weight', 'module.decoder.output_conv.bias'])\n",
            "odict_keys(['module.encoder.initial_block.conv.weight', 'module.encoder.initial_block.conv.bias', 'module.encoder.initial_block.bn.weight', 'module.encoder.initial_block.bn.bias', 'module.encoder.initial_block.bn.running_mean', 'module.encoder.initial_block.bn.running_var', 'module.encoder.initial_block.bn.num_batches_tracked', 'module.encoder.layers.0.conv.weight', 'module.encoder.layers.0.conv.bias', 'module.encoder.layers.0.bn.weight', 'module.encoder.layers.0.bn.bias', 'module.encoder.layers.0.bn.running_mean', 'module.encoder.layers.0.bn.running_var', 'module.encoder.layers.0.bn.num_batches_tracked', 'module.encoder.layers.1.conv3x1_1.weight', 'module.encoder.layers.1.conv3x1_1.bias', 'module.encoder.layers.1.conv1x3_1.weight', 'module.encoder.layers.1.conv1x3_1.bias', 'module.encoder.layers.1.bn1.weight', 'module.encoder.layers.1.bn1.bias', 'module.encoder.layers.1.bn1.running_mean', 'module.encoder.layers.1.bn1.running_var', 'module.encoder.layers.1.bn1.num_batches_tracked', 'module.encoder.layers.1.conv3x1_2.weight', 'module.encoder.layers.1.conv3x1_2.bias', 'module.encoder.layers.1.conv1x3_2.weight', 'module.encoder.layers.1.conv1x3_2.bias', 'module.encoder.layers.1.bn2.weight', 'module.encoder.layers.1.bn2.bias', 'module.encoder.layers.1.bn2.running_mean', 'module.encoder.layers.1.bn2.running_var', 'module.encoder.layers.1.bn2.num_batches_tracked', 'module.encoder.layers.2.conv3x1_1.weight', 'module.encoder.layers.2.conv3x1_1.bias', 'module.encoder.layers.2.conv1x3_1.weight', 'module.encoder.layers.2.conv1x3_1.bias', 'module.encoder.layers.2.bn1.weight', 'module.encoder.layers.2.bn1.bias', 'module.encoder.layers.2.bn1.running_mean', 'module.encoder.layers.2.bn1.running_var', 'module.encoder.layers.2.bn1.num_batches_tracked', 'module.encoder.layers.2.conv3x1_2.weight', 'module.encoder.layers.2.conv3x1_2.bias', 'module.encoder.layers.2.conv1x3_2.weight', 'module.encoder.layers.2.conv1x3_2.bias', 'module.encoder.layers.2.bn2.weight', 'module.encoder.layers.2.bn2.bias', 'module.encoder.layers.2.bn2.running_mean', 'module.encoder.layers.2.bn2.running_var', 'module.encoder.layers.2.bn2.num_batches_tracked', 'module.encoder.layers.3.conv3x1_1.weight', 'module.encoder.layers.3.conv3x1_1.bias', 'module.encoder.layers.3.conv1x3_1.weight', 'module.encoder.layers.3.conv1x3_1.bias', 'module.encoder.layers.3.bn1.weight', 'module.encoder.layers.3.bn1.bias', 'module.encoder.layers.3.bn1.running_mean', 'module.encoder.layers.3.bn1.running_var', 'module.encoder.layers.3.bn1.num_batches_tracked', 'module.encoder.layers.3.conv3x1_2.weight', 'module.encoder.layers.3.conv3x1_2.bias', 'module.encoder.layers.3.conv1x3_2.weight', 'module.encoder.layers.3.conv1x3_2.bias', 'module.encoder.layers.3.bn2.weight', 'module.encoder.layers.3.bn2.bias', 'module.encoder.layers.3.bn2.running_mean', 'module.encoder.layers.3.bn2.running_var', 'module.encoder.layers.3.bn2.num_batches_tracked', 'module.encoder.layers.4.conv3x1_1.weight', 'module.encoder.layers.4.conv3x1_1.bias', 'module.encoder.layers.4.conv1x3_1.weight', 'module.encoder.layers.4.conv1x3_1.bias', 'module.encoder.layers.4.bn1.weight', 'module.encoder.layers.4.bn1.bias', 'module.encoder.layers.4.bn1.running_mean', 'module.encoder.layers.4.bn1.running_var', 'module.encoder.layers.4.bn1.num_batches_tracked', 'module.encoder.layers.4.conv3x1_2.weight', 'module.encoder.layers.4.conv3x1_2.bias', 'module.encoder.layers.4.conv1x3_2.weight', 'module.encoder.layers.4.conv1x3_2.bias', 'module.encoder.layers.4.bn2.weight', 'module.encoder.layers.4.bn2.bias', 'module.encoder.layers.4.bn2.running_mean', 'module.encoder.layers.4.bn2.running_var', 'module.encoder.layers.4.bn2.num_batches_tracked', 'module.encoder.layers.5.conv3x1_1.weight', 'module.encoder.layers.5.conv3x1_1.bias', 'module.encoder.layers.5.conv1x3_1.weight', 'module.encoder.layers.5.conv1x3_1.bias', 'module.encoder.layers.5.bn1.weight', 'module.encoder.layers.5.bn1.bias', 'module.encoder.layers.5.bn1.running_mean', 'module.encoder.layers.5.bn1.running_var', 'module.encoder.layers.5.bn1.num_batches_tracked', 'module.encoder.layers.5.conv3x1_2.weight', 'module.encoder.layers.5.conv3x1_2.bias', 'module.encoder.layers.5.conv1x3_2.weight', 'module.encoder.layers.5.conv1x3_2.bias', 'module.encoder.layers.5.bn2.weight', 'module.encoder.layers.5.bn2.bias', 'module.encoder.layers.5.bn2.running_mean', 'module.encoder.layers.5.bn2.running_var', 'module.encoder.layers.5.bn2.num_batches_tracked', 'module.encoder.layers.6.conv.weight', 'module.encoder.layers.6.conv.bias', 'module.encoder.layers.6.bn.weight', 'module.encoder.layers.6.bn.bias', 'module.encoder.layers.6.bn.running_mean', 'module.encoder.layers.6.bn.running_var', 'module.encoder.layers.6.bn.num_batches_tracked', 'module.encoder.layers.7.conv3x1_1.weight', 'module.encoder.layers.7.conv3x1_1.bias', 'module.encoder.layers.7.conv1x3_1.weight', 'module.encoder.layers.7.conv1x3_1.bias', 'module.encoder.layers.7.bn1.weight', 'module.encoder.layers.7.bn1.bias', 'module.encoder.layers.7.bn1.running_mean', 'module.encoder.layers.7.bn1.running_var', 'module.encoder.layers.7.bn1.num_batches_tracked', 'module.encoder.layers.7.conv3x1_2.weight', 'module.encoder.layers.7.conv3x1_2.bias', 'module.encoder.layers.7.conv1x3_2.weight', 'module.encoder.layers.7.conv1x3_2.bias', 'module.encoder.layers.7.bn2.weight', 'module.encoder.layers.7.bn2.bias', 'module.encoder.layers.7.bn2.running_mean', 'module.encoder.layers.7.bn2.running_var', 'module.encoder.layers.7.bn2.num_batches_tracked', 'module.encoder.layers.8.conv3x1_1.weight', 'module.encoder.layers.8.conv3x1_1.bias', 'module.encoder.layers.8.conv1x3_1.weight', 'module.encoder.layers.8.conv1x3_1.bias', 'module.encoder.layers.8.bn1.weight', 'module.encoder.layers.8.bn1.bias', 'module.encoder.layers.8.bn1.running_mean', 'module.encoder.layers.8.bn1.running_var', 'module.encoder.layers.8.bn1.num_batches_tracked', 'module.encoder.layers.8.conv3x1_2.weight', 'module.encoder.layers.8.conv3x1_2.bias', 'module.encoder.layers.8.conv1x3_2.weight', 'module.encoder.layers.8.conv1x3_2.bias', 'module.encoder.layers.8.bn2.weight', 'module.encoder.layers.8.bn2.bias', 'module.encoder.layers.8.bn2.running_mean', 'module.encoder.layers.8.bn2.running_var', 'module.encoder.layers.8.bn2.num_batches_tracked', 'module.encoder.layers.9.conv3x1_1.weight', 'module.encoder.layers.9.conv3x1_1.bias', 'module.encoder.layers.9.conv1x3_1.weight', 'module.encoder.layers.9.conv1x3_1.bias', 'module.encoder.layers.9.bn1.weight', 'module.encoder.layers.9.bn1.bias', 'module.encoder.layers.9.bn1.running_mean', 'module.encoder.layers.9.bn1.running_var', 'module.encoder.layers.9.bn1.num_batches_tracked', 'module.encoder.layers.9.conv3x1_2.weight', 'module.encoder.layers.9.conv3x1_2.bias', 'module.encoder.layers.9.conv1x3_2.weight', 'module.encoder.layers.9.conv1x3_2.bias', 'module.encoder.layers.9.bn2.weight', 'module.encoder.layers.9.bn2.bias', 'module.encoder.layers.9.bn2.running_mean', 'module.encoder.layers.9.bn2.running_var', 'module.encoder.layers.9.bn2.num_batches_tracked', 'module.encoder.layers.10.conv3x1_1.weight', 'module.encoder.layers.10.conv3x1_1.bias', 'module.encoder.layers.10.conv1x3_1.weight', 'module.encoder.layers.10.conv1x3_1.bias', 'module.encoder.layers.10.bn1.weight', 'module.encoder.layers.10.bn1.bias', 'module.encoder.layers.10.bn1.running_mean', 'module.encoder.layers.10.bn1.running_var', 'module.encoder.layers.10.bn1.num_batches_tracked', 'module.encoder.layers.10.conv3x1_2.weight', 'module.encoder.layers.10.conv3x1_2.bias', 'module.encoder.layers.10.conv1x3_2.weight', 'module.encoder.layers.10.conv1x3_2.bias', 'module.encoder.layers.10.bn2.weight', 'module.encoder.layers.10.bn2.bias', 'module.encoder.layers.10.bn2.running_mean', 'module.encoder.layers.10.bn2.running_var', 'module.encoder.layers.10.bn2.num_batches_tracked', 'module.encoder.layers.11.conv3x1_1.weight', 'module.encoder.layers.11.conv3x1_1.bias', 'module.encoder.layers.11.conv1x3_1.weight', 'module.encoder.layers.11.conv1x3_1.bias', 'module.encoder.layers.11.bn1.weight', 'module.encoder.layers.11.bn1.bias', 'module.encoder.layers.11.bn1.running_mean', 'module.encoder.layers.11.bn1.running_var', 'module.encoder.layers.11.bn1.num_batches_tracked', 'module.encoder.layers.11.conv3x1_2.weight', 'module.encoder.layers.11.conv3x1_2.bias', 'module.encoder.layers.11.conv1x3_2.weight', 'module.encoder.layers.11.conv1x3_2.bias', 'module.encoder.layers.11.bn2.weight', 'module.encoder.layers.11.bn2.bias', 'module.encoder.layers.11.bn2.running_mean', 'module.encoder.layers.11.bn2.running_var', 'module.encoder.layers.11.bn2.num_batches_tracked', 'module.encoder.layers.12.conv3x1_1.weight', 'module.encoder.layers.12.conv3x1_1.bias', 'module.encoder.layers.12.conv1x3_1.weight', 'module.encoder.layers.12.conv1x3_1.bias', 'module.encoder.layers.12.bn1.weight', 'module.encoder.layers.12.bn1.bias', 'module.encoder.layers.12.bn1.running_mean', 'module.encoder.layers.12.bn1.running_var', 'module.encoder.layers.12.bn1.num_batches_tracked', 'module.encoder.layers.12.conv3x1_2.weight', 'module.encoder.layers.12.conv3x1_2.bias', 'module.encoder.layers.12.conv1x3_2.weight', 'module.encoder.layers.12.conv1x3_2.bias', 'module.encoder.layers.12.bn2.weight', 'module.encoder.layers.12.bn2.bias', 'module.encoder.layers.12.bn2.running_mean', 'module.encoder.layers.12.bn2.running_var', 'module.encoder.layers.12.bn2.num_batches_tracked', 'module.encoder.layers.13.conv3x1_1.weight', 'module.encoder.layers.13.conv3x1_1.bias', 'module.encoder.layers.13.conv1x3_1.weight', 'module.encoder.layers.13.conv1x3_1.bias', 'module.encoder.layers.13.bn1.weight', 'module.encoder.layers.13.bn1.bias', 'module.encoder.layers.13.bn1.running_mean', 'module.encoder.layers.13.bn1.running_var', 'module.encoder.layers.13.bn1.num_batches_tracked', 'module.encoder.layers.13.conv3x1_2.weight', 'module.encoder.layers.13.conv3x1_2.bias', 'module.encoder.layers.13.conv1x3_2.weight', 'module.encoder.layers.13.conv1x3_2.bias', 'module.encoder.layers.13.bn2.weight', 'module.encoder.layers.13.bn2.bias', 'module.encoder.layers.13.bn2.running_mean', 'module.encoder.layers.13.bn2.running_var', 'module.encoder.layers.13.bn2.num_batches_tracked', 'module.encoder.layers.14.conv3x1_1.weight', 'module.encoder.layers.14.conv3x1_1.bias', 'module.encoder.layers.14.conv1x3_1.weight', 'module.encoder.layers.14.conv1x3_1.bias', 'module.encoder.layers.14.bn1.weight', 'module.encoder.layers.14.bn1.bias', 'module.encoder.layers.14.bn1.running_mean', 'module.encoder.layers.14.bn1.running_var', 'module.encoder.layers.14.bn1.num_batches_tracked', 'module.encoder.layers.14.conv3x1_2.weight', 'module.encoder.layers.14.conv3x1_2.bias', 'module.encoder.layers.14.conv1x3_2.weight', 'module.encoder.layers.14.conv1x3_2.bias', 'module.encoder.layers.14.bn2.weight', 'module.encoder.layers.14.bn2.bias', 'module.encoder.layers.14.bn2.running_mean', 'module.encoder.layers.14.bn2.running_var', 'module.encoder.layers.14.bn2.num_batches_tracked', 'module.encoder.output_conv.weight', 'module.encoder.output_conv.bias', 'module.decoder.layers.0.conv.weight', 'module.decoder.layers.0.conv.bias', 'module.decoder.layers.0.bn.weight', 'module.decoder.layers.0.bn.bias', 'module.decoder.layers.0.bn.running_mean', 'module.decoder.layers.0.bn.running_var', 'module.decoder.layers.0.bn.num_batches_tracked', 'module.decoder.layers.1.conv3x1_1.weight', 'module.decoder.layers.1.conv3x1_1.bias', 'module.decoder.layers.1.conv1x3_1.weight', 'module.decoder.layers.1.conv1x3_1.bias', 'module.decoder.layers.1.bn1.weight', 'module.decoder.layers.1.bn1.bias', 'module.decoder.layers.1.bn1.running_mean', 'module.decoder.layers.1.bn1.running_var', 'module.decoder.layers.1.bn1.num_batches_tracked', 'module.decoder.layers.1.conv3x1_2.weight', 'module.decoder.layers.1.conv3x1_2.bias', 'module.decoder.layers.1.conv1x3_2.weight', 'module.decoder.layers.1.conv1x3_2.bias', 'module.decoder.layers.1.bn2.weight', 'module.decoder.layers.1.bn2.bias', 'module.decoder.layers.1.bn2.running_mean', 'module.decoder.layers.1.bn2.running_var', 'module.decoder.layers.1.bn2.num_batches_tracked', 'module.decoder.layers.2.conv3x1_1.weight', 'module.decoder.layers.2.conv3x1_1.bias', 'module.decoder.layers.2.conv1x3_1.weight', 'module.decoder.layers.2.conv1x3_1.bias', 'module.decoder.layers.2.bn1.weight', 'module.decoder.layers.2.bn1.bias', 'module.decoder.layers.2.bn1.running_mean', 'module.decoder.layers.2.bn1.running_var', 'module.decoder.layers.2.bn1.num_batches_tracked', 'module.decoder.layers.2.conv3x1_2.weight', 'module.decoder.layers.2.conv3x1_2.bias', 'module.decoder.layers.2.conv1x3_2.weight', 'module.decoder.layers.2.conv1x3_2.bias', 'module.decoder.layers.2.bn2.weight', 'module.decoder.layers.2.bn2.bias', 'module.decoder.layers.2.bn2.running_mean', 'module.decoder.layers.2.bn2.running_var', 'module.decoder.layers.2.bn2.num_batches_tracked', 'module.decoder.layers.3.conv.weight', 'module.decoder.layers.3.conv.bias', 'module.decoder.layers.3.bn.weight', 'module.decoder.layers.3.bn.bias', 'module.decoder.layers.3.bn.running_mean', 'module.decoder.layers.3.bn.running_var', 'module.decoder.layers.3.bn.num_batches_tracked', 'module.decoder.layers.4.conv3x1_1.weight', 'module.decoder.layers.4.conv3x1_1.bias', 'module.decoder.layers.4.conv1x3_1.weight', 'module.decoder.layers.4.conv1x3_1.bias', 'module.decoder.layers.4.bn1.weight', 'module.decoder.layers.4.bn1.bias', 'module.decoder.layers.4.bn1.running_mean', 'module.decoder.layers.4.bn1.running_var', 'module.decoder.layers.4.bn1.num_batches_tracked', 'module.decoder.layers.4.conv3x1_2.weight', 'module.decoder.layers.4.conv3x1_2.bias', 'module.decoder.layers.4.conv1x3_2.weight', 'module.decoder.layers.4.conv1x3_2.bias', 'module.decoder.layers.4.bn2.weight', 'module.decoder.layers.4.bn2.bias', 'module.decoder.layers.4.bn2.running_mean', 'module.decoder.layers.4.bn2.running_var', 'module.decoder.layers.4.bn2.num_batches_tracked', 'module.decoder.layers.5.conv3x1_1.weight', 'module.decoder.layers.5.conv3x1_1.bias', 'module.decoder.layers.5.conv1x3_1.weight', 'module.decoder.layers.5.conv1x3_1.bias', 'module.decoder.layers.5.bn1.weight', 'module.decoder.layers.5.bn1.bias', 'module.decoder.layers.5.bn1.running_mean', 'module.decoder.layers.5.bn1.running_var', 'module.decoder.layers.5.bn1.num_batches_tracked', 'module.decoder.layers.5.conv3x1_2.weight', 'module.decoder.layers.5.conv3x1_2.bias', 'module.decoder.layers.5.conv1x3_2.weight', 'module.decoder.layers.5.conv1x3_2.bias', 'module.decoder.layers.5.bn2.weight', 'module.decoder.layers.5.bn2.bias', 'module.decoder.layers.5.bn2.running_mean', 'module.decoder.layers.5.bn2.running_var', 'module.decoder.layers.5.bn2.num_batches_tracked', 'module.decoder.output_conv.weight', 'module.decoder.output_conv.bias'])\n",
            "Model and weights LOADED successfully\n",
            "  0% 0/2975 [00:00<?, ?it/s]/content/AnomalySegmentation/eval/mahalanobis.py:156: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  result = F.softmax(model(images).squeeze(0))\n",
            "100% 2975/2975 [07:46<00:00,  6.38it/s]\n",
            "Covariance matrix: torch.Size([512, 512])\n",
            "Covariance matrice saved as '/content/AnomalySegmentation/save/cov_matrix_erfnet.npy'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: print cov matrice and mean\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Assuming the mahalanobis.py script creates and saves the covariance matrix and mean\n",
        "# in the specified files (e.g., cov_matrix.npy and mean_vector.npy).\n",
        "# Adapt file paths if necessary.\n",
        "\n",
        "cov_matrix_file = \"/content/AnomalySegmentation/save/cov_matrix_erfnet.npy\"  # Replace with the actual path\n",
        "mean_vector_file = \"/content/AnomalySegmentation/save/mean_cityscapes_erfnet.npy\"  # Replace with the actual path\n",
        "\n",
        "\n",
        "try:\n",
        "    cov_matrix = np.load(cov_matrix_file)\n",
        "    mean_vector = np.load(mean_vector_file)\n",
        "\n",
        "    print(\"Covariance Matrix:\\n\", cov_matrix)\n",
        "    print(\"\\nMean Vector:\\n\", mean_vector)\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Could not find covariance matrix file at {cov_matrix_file} or mean vector file at {mean_vector_file}.\")\n",
        "    print(\"Make sure the mahalanobis.py script has created and saved these files correctly.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7XI7B7daNWF",
        "outputId": "f5590e2a-954d-4287-8f20-d54a4f485e7c"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Covariance Matrix:\n",
            " [[ 1.0593090e-03  1.0656898e-03  1.0867355e-03 ...  6.5079917e-08\n",
            "  -1.1738411e-07 -3.6996507e-08]\n",
            " [ 1.0656898e-03  1.0726821e-03  1.0942017e-03 ...  2.1067433e-07\n",
            "  -8.0121588e-08 -1.5510322e-08]\n",
            " [ 1.0867355e-03  1.0942017e-03  1.1217053e-03 ...  2.8606806e-07\n",
            "  -6.3656891e-08 -2.8215528e-09]\n",
            " ...\n",
            " [ 6.5079917e-08  2.1067433e-07  2.8606806e-07 ...  3.6829972e-04\n",
            "   1.0461096e-04  6.6147644e-05]\n",
            " [-1.1738411e-07 -8.0121588e-08 -6.3656891e-08 ...  1.0461096e-04\n",
            "   5.0056522e-05  3.3491546e-05]\n",
            " [-3.6996507e-08 -1.5510322e-08 -2.8215528e-09 ...  6.6147644e-05\n",
            "   3.3491546e-05  2.3426586e-05]]\n",
            "\n",
            "Mean Vector:\n",
            " [[[2.51862220e-02 2.42292900e-02 3.92709579e-03 ... 1.34595679e-02\n",
            "   2.86676556e-01 3.86060059e-01]\n",
            "  [1.29062915e-02 1.20819462e-02 2.70243129e-03 ... 7.86827132e-03\n",
            "   2.30695739e-01 3.22888047e-01]\n",
            "  [5.67900762e-03 4.19270108e-03 3.70931433e-04 ... 6.08964544e-03\n",
            "   1.86094359e-01 2.81152129e-01]\n",
            "  ...\n",
            "  [8.76554787e-01 8.58798087e-01 8.48824203e-01 ... 7.84928501e-01\n",
            "   8.96399856e-01 9.12348092e-01]\n",
            "  [9.68317270e-01 9.60417807e-01 9.41449642e-01 ... 8.90286803e-01\n",
            "   9.54113543e-01 9.59281266e-01]\n",
            "  [9.76349235e-01 9.70315278e-01 9.62419927e-01 ... 9.27616775e-01\n",
            "   9.67054248e-01 9.71283913e-01]]\n",
            "\n",
            " [[7.83653092e-03 7.71112181e-03 1.06917927e-03 ... 9.00478859e-04\n",
            "   5.10933064e-03 6.06303476e-03]\n",
            "  [6.18669903e-03 6.11983752e-03 1.14204374e-03 ... 7.19518284e-04\n",
            "   5.00598457e-03 6.14783540e-03]\n",
            "  [3.40832211e-03 2.96769990e-03 4.21183970e-04 ... 3.61802406e-04\n",
            "   2.73053721e-03 3.75893130e-03]\n",
            "  ...\n",
            "  [7.06590712e-02 7.37608448e-02 8.44705254e-02 ... 1.55017108e-01\n",
            "   8.05370361e-02 7.46756643e-02]\n",
            "  [1.93182454e-02 2.23227702e-02 3.91282886e-02 ... 8.69035721e-02\n",
            "   3.18994969e-02 3.04121748e-02]\n",
            "  [1.03322268e-02 1.20314779e-02 2.17564199e-02 ... 5.34567684e-02\n",
            "   2.11925451e-02 2.00444814e-02]]\n",
            "\n",
            " [[5.15200675e-01 5.11866748e-01 5.26650667e-01 ... 6.24604821e-01\n",
            "   3.78048301e-01 3.84589463e-01]\n",
            "  [5.17958820e-01 5.14082968e-01 5.26144385e-01 ... 6.22323751e-01\n",
            "   3.94754887e-01 4.11265135e-01]\n",
            "  [5.39456546e-01 5.22842705e-01 5.35438657e-01 ... 6.18474603e-01\n",
            "   4.55967039e-01 4.72721875e-01]\n",
            "  ...\n",
            "  [1.39537442e-03 1.61635806e-03 5.13462524e-04 ... 3.13424878e-03\n",
            "   9.08245682e-04 6.69431989e-04]\n",
            "  [5.11297490e-04 6.01683802e-04 2.11660838e-04 ... 8.88664334e-04\n",
            "   8.86526133e-04 7.36442220e-04]\n",
            "  [5.42524678e-04 6.41164836e-04 1.71316191e-04 ... 7.68101483e-04\n",
            "   7.18783936e-04 6.05756708e-04]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[5.94582467e-04 7.68701080e-04 1.90940220e-04 ... 4.41732613e-04\n",
            "   1.15597469e-03 9.86410305e-04]\n",
            "  [6.27821020e-04 8.18162225e-04 2.21628259e-04 ... 5.24431583e-04\n",
            "   1.49477692e-03 1.31809840e-03]\n",
            "  [4.15062095e-04 5.24200848e-04 9.31844697e-05 ... 1.80705814e-04\n",
            "   6.75693154e-04 4.96448134e-04]\n",
            "  ...\n",
            "  [8.60510452e-04 1.34366995e-03 4.80259012e-04 ... 4.74690896e-04\n",
            "   2.96031882e-04 1.44873149e-04]\n",
            "  [2.37996996e-04 3.92657676e-04 1.36347750e-04 ... 1.75412148e-04\n",
            "   1.79270952e-04 1.42577672e-04]\n",
            "  [2.14724001e-04 3.56495759e-04 1.05994055e-04 ... 1.37982250e-04\n",
            "   1.51648928e-04 1.20485347e-04]]\n",
            "\n",
            " [[1.12674455e-03 1.47799752e-03 6.56011107e-04 ... 3.86727916e-04\n",
            "   1.01742044e-03 7.26182596e-04]\n",
            "  [1.11647113e-03 1.49232452e-03 7.37327617e-04 ... 4.23456775e-04\n",
            "   1.20785355e-03 8.96268175e-04]\n",
            "  [1.07898563e-03 1.45238906e-03 5.27470955e-04 ... 2.56389583e-04\n",
            "   9.62106104e-04 5.74732723e-04]\n",
            "  ...\n",
            "  [9.47635970e-04 1.62480457e-03 1.27782917e-03 ... 1.01385021e-03\n",
            "   3.90804082e-04 1.57244271e-04]\n",
            "  [1.70234285e-04 2.74242862e-04 2.28272431e-04 ... 2.50974990e-04\n",
            "   1.77492955e-04 1.22666912e-04]\n",
            "  [1.48609222e-04 2.44165334e-04 1.77661306e-04 ... 2.08205834e-04\n",
            "   1.50346052e-04 1.04157043e-04]]\n",
            "\n",
            " [[1.25267857e-03 1.22786046e-03 5.25525364e-04 ... 5.36498555e-04\n",
            "   1.87793758e-03 1.36123062e-03]\n",
            "  [1.08842994e-03 1.18835596e-03 4.92178835e-04 ... 5.73658675e-04\n",
            "   2.04786658e-03 1.68854499e-03]\n",
            "  [8.30547826e-04 8.13707127e-04 2.43678238e-04 ... 3.04862508e-04\n",
            "   1.28512061e-03 9.05665394e-04]\n",
            "  ...\n",
            "  [3.85744061e-04 5.32888109e-04 2.54208950e-04 ... 3.42183805e-04\n",
            "   2.24308547e-04 1.17268821e-04]\n",
            "  [2.09370206e-04 2.73160724e-04 1.68378014e-04 ... 2.59080058e-04\n",
            "   2.25663593e-04 1.62336582e-04]\n",
            "  [2.00660128e-04 2.68129370e-04 1.45213737e-04 ... 2.27893266e-04\n",
            "   2.03116564e-04 1.46805891e-04]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "no_execute = False\n",
        "just_once = False\n",
        "\n",
        "for dataset_dir in ['RoadAnomaly21', 'RoadObsticle21', 'FS_LostFound_full', 'fs_static', 'RoadAnomaly']:\n",
        "  if no_execute:\n",
        "    break\n",
        "  net = \"erfnet\"\n",
        "  method = \"Mahalanobis\"\n",
        "  load_dir = f'/content/AnomalySegmentation/save/erfnet_training1'\n",
        "  weights = f'/model_best.pth'\n",
        "  format_file = os.listdir(f'/content/Validation_Dataset/{dataset_dir}/images')[0].split(\".\")[1]\n",
        "  input =f'/content/Validation_Dataset/{dataset_dir}/images/\\*.{format_file}'\n",
        "  print(f\"\\nDataset: {dataset_dir} method: {method}\")\n",
        "\n",
        "  if torch.cuda.is_available():\n",
        "    !python -W ignore /content/AnomalySegmentation/eval/evalAnomaly.py --input {input} --method  {method} --model {net} --loadDir {load_dir} --loadWeights {weights}\n",
        "  else:\n",
        "    !python -W ignore /content/AnomalySegmentation/eval/evalAnomaly.py --input {input} --method  {method} --model {net} --loadDir {load_dir} --loadWeights {weights} -cpu | tail -n 2\n",
        "  if just_once:\n",
        "    no_execute = True\n",
        "    just_once = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8Ne14rpyCsD",
        "outputId": "c0802684-340a-44be-e249-29f37f42c6a6"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dataset: RoadAnomaly21 method: Mahalanobis\n",
            "Loading model: /content/AnomalySegmentation/save/erfnet_training1/erfnet.py\n",
            "Loading weights: /content/AnomalySegmentation/save/erfnet_training1/model_best.pth\n",
            "Model and weights LOADED successfully\n",
            "mean shape:  (20, 512, 1024)\n",
            "cov shape:  (512, 512)\n",
            "mean shape: torch.Size([512, 1024])\n",
            "covariance_inv shape: torch.Size([512, 512])\n",
            "f_x shape: torch.Size([20, 512, 1024])\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/AnomalySegmentation/eval/evalAnomaly.py\", line 260, in <module>\n",
            "    main()\n",
            "  File \"/content/AnomalySegmentation/eval/evalAnomaly.py\", line 191, in main\n",
            "    anomaly_result = mahalanobis_score(F.softmax(result, dim=0), means, cov_inv)\n",
            "  File \"/content/AnomalySegmentation/eval/evalAnomaly.py\", line 77, in mahalanobis_score\n",
            "    distance = mahalanobis_distance(f_x, means[c], covariance_inv)\n",
            "  File \"/content/AnomalySegmentation/eval/evalAnomaly.py\", line 63, in mahalanobis_distance\n",
            "    return diff.T @ (covariance_inv @ diff)\n",
            "RuntimeError: The size of tensor a (1024) must match the size of tensor b (20) at non-singleton dimension 0\n",
            "\n",
            "Dataset: RoadObsticle21 method: Mahalanobis\n",
            "Loading model: /content/AnomalySegmentation/save/erfnet_training1/erfnet.py\n",
            "Loading weights: /content/AnomalySegmentation/save/erfnet_training1/model_best.pth\n",
            "Model and weights LOADED successfully\n",
            "mean shape:  (20, 512, 1024)\n",
            "cov shape:  (512, 512)\n",
            "mean shape: torch.Size([512, 1024])\n",
            "covariance_inv shape: torch.Size([512, 512])\n",
            "f_x shape: torch.Size([20, 512, 1024])\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/AnomalySegmentation/eval/evalAnomaly.py\", line 260, in <module>\n",
            "    main()\n",
            "  File \"/content/AnomalySegmentation/eval/evalAnomaly.py\", line 191, in main\n",
            "    anomaly_result = mahalanobis_score(F.softmax(result, dim=0), means, cov_inv)\n",
            "  File \"/content/AnomalySegmentation/eval/evalAnomaly.py\", line 77, in mahalanobis_score\n",
            "    distance = mahalanobis_distance(f_x, means[c], covariance_inv)\n",
            "  File \"/content/AnomalySegmentation/eval/evalAnomaly.py\", line 63, in mahalanobis_distance\n",
            "    return diff.T @ (covariance_inv @ diff)\n",
            "RuntimeError: The size of tensor a (1024) must match the size of tensor b (20) at non-singleton dimension 0\n",
            "\n",
            "Dataset: FS_LostFound_full method: Mahalanobis\n",
            "Loading model: /content/AnomalySegmentation/save/erfnet_training1/erfnet.py\n",
            "Loading weights: /content/AnomalySegmentation/save/erfnet_training1/model_best.pth\n",
            "Model and weights LOADED successfully\n",
            "mean shape:  (20, 512, 1024)\n",
            "cov shape:  (512, 512)\n",
            "mean shape: torch.Size([512, 1024])\n",
            "covariance_inv shape: torch.Size([512, 512])\n",
            "f_x shape: torch.Size([20, 512, 1024])\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/AnomalySegmentation/eval/evalAnomaly.py\", line 260, in <module>\n",
            "    main()\n",
            "  File \"/content/AnomalySegmentation/eval/evalAnomaly.py\", line 191, in main\n",
            "    anomaly_result = mahalanobis_score(F.softmax(result, dim=0), means, cov_inv)\n",
            "  File \"/content/AnomalySegmentation/eval/evalAnomaly.py\", line 77, in mahalanobis_score\n",
            "    distance = mahalanobis_distance(f_x, means[c], covariance_inv)\n",
            "  File \"/content/AnomalySegmentation/eval/evalAnomaly.py\", line 63, in mahalanobis_distance\n",
            "    return diff.T @ (covariance_inv @ diff)\n",
            "RuntimeError: The size of tensor a (1024) must match the size of tensor b (20) at non-singleton dimension 0\n",
            "\n",
            "Dataset: fs_static method: Mahalanobis\n",
            "Loading model: /content/AnomalySegmentation/save/erfnet_training1/erfnet.py\n",
            "Loading weights: /content/AnomalySegmentation/save/erfnet_training1/model_best.pth\n",
            "Model and weights LOADED successfully\n",
            "mean shape:  (20, 512, 1024)\n",
            "cov shape:  (512, 512)\n",
            "mean shape: torch.Size([512, 1024])\n",
            "covariance_inv shape: torch.Size([512, 512])\n",
            "f_x shape: torch.Size([20, 512, 1024])\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/AnomalySegmentation/eval/evalAnomaly.py\", line 260, in <module>\n",
            "    main()\n",
            "  File \"/content/AnomalySegmentation/eval/evalAnomaly.py\", line 191, in main\n",
            "    anomaly_result = mahalanobis_score(F.softmax(result, dim=0), means, cov_inv)\n",
            "  File \"/content/AnomalySegmentation/eval/evalAnomaly.py\", line 77, in mahalanobis_score\n",
            "    distance = mahalanobis_distance(f_x, means[c], covariance_inv)\n",
            "  File \"/content/AnomalySegmentation/eval/evalAnomaly.py\", line 63, in mahalanobis_distance\n",
            "    return diff.T @ (covariance_inv @ diff)\n",
            "RuntimeError: The size of tensor a (1024) must match the size of tensor b (20) at non-singleton dimension 0\n",
            "\n",
            "Dataset: RoadAnomaly method: Mahalanobis\n",
            "Loading model: /content/AnomalySegmentation/save/erfnet_training1/erfnet.py\n",
            "Loading weights: /content/AnomalySegmentation/save/erfnet_training1/model_best.pth\n",
            "Model and weights LOADED successfully\n",
            "mean shape:  (20, 512, 1024)\n",
            "cov shape:  (512, 512)\n",
            "mean shape: torch.Size([512, 1024])\n",
            "covariance_inv shape: torch.Size([512, 512])\n",
            "f_x shape: torch.Size([20, 512, 1024])\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/AnomalySegmentation/eval/evalAnomaly.py\", line 260, in <module>\n",
            "    main()\n",
            "  File \"/content/AnomalySegmentation/eval/evalAnomaly.py\", line 191, in main\n",
            "    anomaly_result = mahalanobis_score(F.softmax(result, dim=0), means, cov_inv)\n",
            "  File \"/content/AnomalySegmentation/eval/evalAnomaly.py\", line 77, in mahalanobis_score\n",
            "    distance = mahalanobis_distance(f_x, means[c], covariance_inv)\n",
            "  File \"/content/AnomalySegmentation/eval/evalAnomaly.py\", line 63, in mahalanobis_distance\n",
            "    return diff.T @ (covariance_inv @ diff)\n",
            "RuntimeError: The size of tensor a (1024) must match the size of tensor b (20) at non-singleton dimension 0\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}